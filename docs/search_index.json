[
["index.html", "The Examples Book Introduction How to contribute", " The Examples Book Introduction Click here for video This book contains a collection of examples that students can use to reinforce topics learned in The Data Mine seminar. It is an excellent resource for students to learn what they need to know in order to solve The Data Mine projects. How to contribute There are a variety of ways to make a contribution to this book. For simplicity, we will illustrate the method that we find most user friendly. Setup GitHub Desktop and clone the repository The first step in making a contribution to The Examples Book is to setup and install GitHub Desktop and clone our repository. These steps only need to be performed once on a system to have things configured to make contributions. Setup GitHub Desktop following the directions here. When you are presented with the following screen, select &quot;Clone a Repository from the Internet...&quot;: 3. Click on the &quot;URL&quot; tab, and fill in the first field with &quot;TheDataMine/the-examples-book&quot;. In the second field, enter the location where you'd like the repository to be cloned to. Note that this directory must be an empty directory. In this example, the repository will be cloned into /Users/kamstut/projects/the-examples-book. The result will be that the folder, /Users/kamstut/projects/the-examples-book, will contain all of the files and folders from the repository. Click &quot;Clone&quot;. Upon completion, you will be presented with the following screen. Congratulations, you've successfully setup GitHub Desktop and cloned The Examples Book repository. Making a contribution At this point in time, your current branch will be the master branch. Create a new branch with whatever name you'd like. For example, fix-spelling-errors-01 or add-fake-section. Important note: Think of a branch as a copy of the repository. Typically, a new branch is created for each feature of a project. In the case of our book, this could be an article or a project. Changes are made to the files within the branch, committed, a pull request is made, the changes are reviewed, and then the branch is merged into the master branch (which holds our source of truth, final, or deployed code). Open up RStudio. Click on File &gt; Open Project..., navigate an click on the project folder, in this case, /Users/kamstut/projects/the-examples-book. Click &quot;Open&quot;. Important note: There are currently 13 .Rmd files that can be modified to make additions or changes to the book: index.Rmd 01-scholar.Rmd 02-data-formats.Rmd 03-unix.Rmd 04-sql.Rmd 05-r.Rmd 06-python.Rmd 07-tools.Rmd 08-faqs.Rmd 09-projects.Rmd 10-fall2020-projects.Rmd 11-corporate-partners.Rmd 12-contributors.Rmd The content in the book starts in index.Rmd and continues sequentially, file by file. The preceding numbers indicate the order in which Bookdown will render the content. Each individual .Rmd file creates a single HTML file, and a top-level section of the book. If you wanted to add a section for Julia under the python section, for example, you would need to create a new RMarkdown file called 07-julia.Rmd, and rename the following RMarkdown files accordingly (08-tools.Rmd, 09-faqs.Rmd, etc.). First level markdown headers starting with # are rendered into top-level menu items in the left-hand table of contents. Second level markdown headers starting with ## are rendered into secondary menu items nested beneath the preceding first level markdown headers. Similarly, third level markdown headers starting with ### are rendered into tertiary menu items nested beneath the preceding second level markdown headers. In the following example, &quot;First second level header&quot; and &quot;Second second level header&quot; would be nested under &quot;First level header&quot;, and &quot;First third level header&quot; would be nested under &quot;First second level header&quot;: # First level header ## First second level header ### First third level header ## Second second level header At this time only the first three levels of headers are rendered in the table of contents. Level 4-6 headers only have a visual effect and automatic anchor links. Make modifications or additions to any of the 13+ .Rmd files. As you make changes, these changes will be shown in GitHub Desktop. Here, we've made a single addition of ## Test change to 01-scholar.Rmd. The next step is to commit your changes. Think of a commit as a local save that has an associated title (required) and description (optional). Here is an example. Once you are pleased with your modifications, you need to publish your branch. To do so click on &quot;Publish branch&quot;. This uploads your branch, in our case add-fake-section, to our remote repository, or the repository that GitHub is hosting. What you are working on in your computer is referred to as your local repository. Typically, you will make changes to a local repository and push those changes to the remote repository. Important note: In fact, if you navigate to https://github.com/TheDataMine/the-examples-book, you will see that the branch, add-fake-section is now visible online. Excellent. The next step is to create a pull request. To do so, click the &quot;Create Pull Request&quot; button on GitHub Desktop. This should launch your browser and you should be presented with a page that looks similar to the following. On this page, you will be able to provide a title and description for the request. Note that the description is markdown friendly (clicking &quot;Preview&quot; allows you to see the rendered result). In addition, you can assign a reviewer, add labels, etc. At a minimum, you should add a title and description. Once satisfied, click &quot;Create pull request&quot;. Important note: Prior to this terms like commit, branch, local, remote, etc., have been generic. Those terms apply to git, in general. A pull request is a GitHub specific term and not all git cloud providers use the same terminology. For example, GitLab uses the term merge request. At this point in time, the repository owner will be notified, changes will be reviewed, and eventually merged. Once merged, changes are launched to the website within about 10 minutes. Please note you can continue to make changes and commit to your branch at any time. Changes will automatically be included in your pending pull request. "],
["scholar.html", "Scholar Connecting to Scholar Other ways to connect Resources", " Scholar Connecting to Scholar There are a variety of ways to connect to Scholar; however, the primary method (and maybe the only method) we will use this semester is RStudio Server Pro. To see how one may approach solving a project this semester, watch Dr. Ward connect to RStudio Server Pro and demonstrate how to compile a project here. RStudio Server Pro Getting started with Scholar and RStudio: part I Getting started with Scholar and RStudio: part II Open a browser and navigate to https://rstudio.scholar.rcac.purdue.edu/. Enter your Purdue Career Account credentials (using BoilerKey, namely, your 4 digit code, then a comma, and then a BoilerKey numerical sequence). Congratulations, you should now be able to create and run R scripts on Scholar! Other ways to connect These are some other ways to connect to Scholar. Please feel free to explore; however, note that, at this time, there is no real reason to connect using these methods. You are encouraged to use RStudio Server Pro, and go through the video provided here to get started. ThinLinc web client Open a browser and navigating to https://desktop.scholar.rcac.purdue.edu/. Login with your Purdue Career Account credentials (using BoilerKey, namely, your 4 digit code, then a comma, and then a Boilerkey numerical sequence). Congratulations, you should now be connected to Scholar using the ThinLinc web client. ThinLinc client Navigate to https://www.cendio.com/thinlinc/download, and download the ThinLinc client application for your operating system. Install and launch the ThinLinc client: Enter your Purdue Career Account information (using BoilerKey, namely, your 4 digit code, then a comma, and then a Boilerkey numerical sequence), as well as the server: desktop.scholar.rcac.purdue.edu. Click on &quot;Options...&quot; and fill out the &quot;Screen&quot; tab as shown below: Click &quot;OK&quot; and then &quot;Connect&quot;. Make sure you are connected to Purdue's VPN using AnyConnect before clicking &quot;Connect&quot;! If you are presented with a choice like below, click &quot;Continue&quot;. Congratulations, you are now successfully connected to Scholar using the ThinLinc client. NOTE: If you do accidentally get stuck in full screen mode, the F8 key will help you to escape. NOTE: The very first time that you log onto Scholar, you will have an option of “use default config” or “one empty panel”. PLEASE choose the “use default config”. SSH Windows MacOS Linux JupyterHub Open a browser and navigate to https://notebook.scholar.rcac.purdue.edu/. Enter your Purdue Career Account credentials (using BoilerKey, namely, your 4 digit code, then a comma, and then a Boilerkey numerical sequence). Congratulations, you should now be able to create and run Jupyter notebooks on Scholar! Resources "],
["data-formats.html", "Data Formats HTML XML", " Data Formats HTML HTML stands for Hypertext Markup Language. It is the standard language of the web intended to be displayed using web browsers like Firefox, Chrome, and Edge. The following is a good example of an HTML document with examples of many basic HTML elements: &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;HTML5 Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;top&quot; class=&quot;page&quot; role=&quot;document&quot;&gt; &lt;header role=&quot;banner&quot;&gt; &lt;h1&gt;HTML5 Example&lt;/h1&gt; &lt;p&gt;This is an example page full of HTML.&lt;/p&gt; &lt;/header&gt; &lt;nav role=&quot;nav&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;#tech&quot; class=&quot;big title&quot; custom-attr=&quot;ok&quot;&gt;Tech&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://google.com&quot;&gt;Google&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloudflare.com&quot;&gt;Cloudflare&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://microsoft.com&quot;&gt;Microsoft&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;#news&quot; data-news=&quot;true&quot;&gt;News&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://nytimes.com&quot;&gt;NYTimes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://washingtonpost.com&quot;&gt;WAPO&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/nav&gt; &lt;main&gt; &lt;section id=&quot;headings&quot;&gt; &lt;div&gt; &lt;h1&gt;Heading 1&lt;/h1&gt; &lt;h2&gt;Heading 2&lt;/h2&gt; &lt;h3&gt;Heading 3&lt;/h3&gt; &lt;h4&gt;Heading 4&lt;/h4&gt; &lt;h5&gt;Heading 5&lt;/h5&gt; &lt;h6&gt;Heading 6&lt;/h6&gt; &lt;/div&gt; &lt;/section&gt; &lt;section id=&quot;other&quot;&gt; &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;p&gt;This is another paragraph.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ordered list 1&lt;/li&gt; &lt;li&gt;Ordered list 2&lt;/li&gt; &lt;li&gt;Ordered list 3&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Unordered list 1&lt;/li&gt; &lt;li&gt;Unordered list 2&lt;/li&gt; &lt;li&gt;Unordered list 3&lt;/li&gt; &lt;/ul&gt; &lt;/section&gt; &lt;h3&gt;Horizontal rule&lt;/h3&gt; &lt;hr&gt; &lt;h3&gt;Line break&lt;/h3&gt; &lt;br&gt; &lt;section id=&quot;table&quot;&gt; &lt;table&gt; &lt;caption&gt;Caption&lt;/caption&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Heading 1&lt;/th&gt; &lt;th&gt;Heading 2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;th&gt;Footer 1&lt;/th&gt; &lt;th&gt;Footer 2&lt;/th&gt; &lt;/tr&gt; &lt;/tfoot&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Cell 1&lt;/td&gt; &lt;td&gt;Cell 2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cell 1&lt;/td&gt; &lt;td&gt;Cell 2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/section&gt; &lt;section&gt; &lt;div&gt; &lt;p&gt;This is a paragraph with text that has been modified to appear &lt;strong&gt;strong&lt;/strong&gt;, &lt;em&gt;emphasized&lt;/em&gt;, &lt;b&gt;bold&lt;/b&gt;, &lt;i&gt;italicized&lt;/i&gt;, &lt;u&gt;underlined&lt;/u&gt;, &lt;s&gt;struckthrough&lt;/s&gt;, etc. &lt;/p&gt; &lt;/div&gt; &lt;/section&gt; &lt;/main&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; An HTML document is comprised of elements. An element is the combination of a start tag, content, and end tag. For example, the following is an element: &lt;title&gt;HTML5 Example&lt;/title&gt; In that example, &quot;&lt;title&gt;&quot; is the start tag, &quot;HTML5 Example&quot; is the content, and &quot;&lt;/title&gt;&quot; is the end tag. HTML elements can be nested. For example: &lt;section&gt; &lt;div&gt; &lt;p&gt;This is a paragraph with text that has been modified to appear &lt;strong&gt;strong&lt;/strong&gt;, &lt;em&gt;emphasized&lt;/em&gt;, &lt;b&gt;bold&lt;/b&gt;, &lt;i&gt;italicized&lt;/i&gt;, &lt;u&gt;underlined&lt;/u&gt;, &lt;s&gt;struckthrough&lt;/s&gt;, etc. &lt;/p&gt; &lt;/div&gt; &lt;/section&gt; This is an element where &quot;&lt;section&gt;&quot; is the start tag, &quot;&lt;/section&gt;&quot; is the end tag, and the rest: &lt;div&gt; &lt;p&gt;This is a paragraph with text that has been modified to appear &lt;strong&gt;strong&lt;/strong&gt;, &lt;em&gt;emphasized&lt;/em&gt;, &lt;b&gt;bold&lt;/b&gt;, &lt;i&gt;italicized&lt;/i&gt;, &lt;u&gt;underlined&lt;/u&gt;, &lt;s&gt;struckthrough&lt;/s&gt;, etc. &lt;/p&gt; &lt;/div&gt; is the content. The content in this example is a nested element, where the start tag is &quot;&lt;div&gt;&quot;, the end tag is &quot;&lt;/div&gt;&quot; and the content is: &lt;p&gt;This is a paragraph with text that has been modified to appear &lt;strong&gt;strong&lt;/strong&gt;, &lt;em&gt;emphasized&lt;/em&gt;, &lt;b&gt;bold&lt;/b&gt;, &lt;i&gt;italicized&lt;/i&gt;, &lt;u&gt;underlined&lt;/u&gt;, &lt;s&gt;struckthrough&lt;/s&gt;, etc. &lt;/p&gt; ... yet another nested element. HTML has a variety of different tags, each with a distinct purpose. Whether or not a website uses a specific tag for it's intended purpose is another story. Certain HTML tags are oft &quot;misused&quot; in modern web development. HTML tags can have attributes. Attributes are always shown in the start tag, and can come in both name=&quot;value&quot; pairs, like class=&quot;h1-strong&quot;: &lt;div class=&quot;h1-strong&quot;&gt; , or alone, like async: &lt;script src=&quot;my_script.js&quot; async&gt; There are a variety of valid html attributes. It is important to note that if an attribute in an HTML tag is not one of the official attributes, the browser will ignore it. With that being said, the &quot;unofficial&quot; attributes will still be a part of the Document Object Model (DOM), and therefore accessible to javascript. For this reason, it is not uncommon to come across technically &quot;invalid&quot; HTML attributes in website's source code. In HTML5, there now exists an HTML compliant version of these custom attributes called data-*. The * in data-* is a wildcard that can represent anything as long as it is at least 1 character long and contains no uppercase letters. You can see an example of a data-* attribute in our example, where we use data-news to store whether or not the link is news data: &lt;a href=&quot;#news&quot; data-news=&quot;true&quot;&gt;News&lt;/a&gt; Unless you have a very good reason to use a custom attribute that is not a data-* attribute, you would be well-advised to just use data-* when possible. XML XML stands for Extensible Markup Language. XML and HTML appear and sound very similar but have different goals. Whereas HTML was designed to display data, XML was designed to store data. Like HTML, XML still has start tags, end tags, and content, however, rather than using a finite set of legal tags, XML tags are defined and created by the user, so XML like the following is perfectly legal. &lt;exam&gt; &lt;question&gt; &lt;text&gt;What is red?&lt;/text&gt; &lt;solution&gt;A color.&lt;/solution&gt; &lt;points&gt;2&lt;/points&gt; &lt;/question&gt; &lt;question&gt; &lt;text&gt;What is a square?&lt;/text&gt; &lt;solution&gt;A shape.&lt;/solution&gt; &lt;points&gt;1&lt;/points&gt; &lt;/question&gt; &lt;/exam&gt; Rather than opening that content in a web browser and expecting it to do something like we would for HTML, in order for that, valid, XML to do anything, someone must write software to send, receive, store, display, or do something with the data. XPath Expressions Xpath Expressions are expressions created to select nodes or node-sets from an XML document. The following table (from w3schools) shows some of the most useful expressions that can be used to select nodes programmatically. Expression Description nodename Selects all nodes with the name &quot;nodename&quot; / Selects from the root node // Selects nodes in the document from the current node that match the selection no matter where they are .// Selects nodes in the document from the current node that match the selection only if they are in the current element . Selects the current node .. Selects the parent of the current node @ Selects attributes Predicates A predicate is a way to filter a node-set by evaluating the expression contained within a set of square brackets []. For example, let's say you wanted to get all of the questions that have points &gt; 1. Doing this using predicates is easy: &lt;exam&gt; &lt;question&gt; &lt;text&gt;What is red?&lt;/text&gt; &lt;solution&gt;A color.&lt;/solution&gt; &lt;points&gt;2&lt;/points&gt; &lt;/question&gt; &lt;question&gt; &lt;text&gt;What is a square?&lt;/text&gt; &lt;solution&gt;A shape.&lt;/solution&gt; &lt;points&gt;1&lt;/points&gt; &lt;/question&gt; &lt;/exam&gt; //question[points &gt; 1] Operators In order to make comparisons, we need operators. The following is a list of xpath operators from here. Operator Description Example | Computes two node-sets. //book | //cd + Addition 6 + 4 - Subtraction 6 - 4 * Multiplication 6 * 4 div Division 8 div 4 = Equal to (relation, not assignment) price = 9.80 != Not equal price != 9.80 &lt; Less than price &lt; 9.80 &lt;= Less than or equal to price &lt;= 9.80 &gt; Greater than price &gt; 9.80 &gt;= Greater than or equal to price &gt;= 9.80 or Logical or price = 9.80 or price = 9.70 and Logical and price &gt; 9.00 and price &lt; 10.00 mod Modulus (division remainder) 5 mod 2 Function xpath functions are an easy way to filter data with more control. You can find a list of functions here. Of particular use are the contains and translate functions. contains allows you to see if the first argument string contains the second argument string. It is particularly useful to see if a class or attribute contains some substring. translate allows you to, among other things, change the case of some text prior to using the contains function, for example. Examples Given the following XML, answer the questions. &lt;html&gt; &lt;head&gt; &lt;title&gt;My Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;div class=&quot;abc123 sktoe-sldjkt dkjfg3-dlgsk&quot;&gt; &lt;div class=&quot;glkjr-slkd dkgj-0 dklfgj-00&quot;&gt; &lt;a class=&quot;slkdg43lk dlks&quot; href=&quot;https://example.com/123456&quot;&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;div class=&quot;ldskfg4&quot;&gt; &lt;span class=&quot;slktjoe&quot; aria-label=&quot;123 comments, 43 Retweets, 4000 likes&quot;&gt;Love it.&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-amount=&quot;12&quot;&gt;13&lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;div class=&quot;abc123 sktoe-sls dkjfg-dlgsk&quot;&gt; &lt;div class=&quot;glkj-slkd dkgj-0 dklfj-00&quot;&gt; &lt;a class=&quot;slkd3lk dls&quot; href=&quot;https://example.com/123456&quot;&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;div class=&quot;ldg4&quot;&gt; &lt;span class=&quot;sktjoe&quot; aria-label=&quot;1000 comments, 455 Retweets, 40000 likes&quot;&gt;Love it.&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-amount=&quot;122&quot;&gt;133&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; Write an xpath expression to get the &quot;title&quot; element. Click here for solution //title Write an xpath expression to get the content of the &quot;title&quot; element. Click here for solution //title/text Write an xpath expression to get every &quot;div&quot; element. Click here for solution //div Write an xpath expression to get every &quot;div&quot; element with class=&quot;ldskfg4&quot;. Click here for solution //div[@class='ldskfg4'] Write an xpath expression to get every &quot;div&quot; element where &quot;abc123&quot; is in the class attribute's value. Click here for solution //div[contains(@class, 'abc123')] Write an xpath expression to get every &quot;div&quot; element with an aria-label attribute. Click here for solution //div[@aria-label] Resources XPath Tutorial A w3schools tutorial on XPath expressions. A nice, quick 1 page summary. XPath Expression Cheatsheet A simple, but very thorough cheatsheet to recall xpath expressions. "],
["unix.html", "Unix Getting started Standard utilities awk ~ &amp; . &amp; .. Piping &amp; Redirection Cron Emacs Nano Vim Writing scripts", " Unix Getting started We made a video to remind people about how to get comfortable with UNIX commands: Click here for video This is the easiest book for learning this stuff; it is short and gets right to the point: https://go.oreilly.com/purdue-university/library/view/-/0596002610 you just log in and you can see it all; we suggest Chapters 1, 3, 4, 5, 7 (you can basically skip chapters 2 and 6 the first time through). It is a very short read (maybe, say, 2 or 3 hours altogether?), just a thin book that gets right to the details. Zoe Yang asked us about the difference in these 5 words: bash/Linux/terminal/shell/UNIX. Here you go: UNIX(Unix) and Linux are operating systems, just like Mac OS X and Windows 10 are operating systems. There are many variants. Within Linux, the main different is the kernel (the main piece of code that makes things work) and sometimes the default configurations, like the GUI (i.e., the way stuff looks when you log in and see your desktop and interact with the windows and folders and files). UNIX dates back to the 1970's, and was from AT&amp;T Bell Labs, and then people decided to make lots and lots of variants of this, and hence, the many flavors of Linux. OK? The terminal is an application that runs in UNIX or Linux. It is the thing that you open and type things into it, and you see the output. It is hard to tell the difference between the terminal and the shell. The shell is the way that you interact with UNIX/Linux directly (without pointing and clicking). You can tell the shell directly what you want to do with the files on the computer, for instance. You might think that the terminal and the shell are the same thing, but they are not quite. There are lots of different types of shells that can run in the terminal. To see which one you are using, you can type: echo $SHELL By default, it will say: /bin/bash There are other shells in your /bin directory. bash (Bourne Again SHell) is the default one. Many people consider this to be the &quot;best&quot; shell, or at least, the one that people know the most. Others are Bourne (sh), Korn (ksh), Z shell (zsh), C shell (csh), TENEX C shell (tcsh), and dozens more. Any of these shells would run in the terminal, just like bash does, and you might not even realize at the start which shell you are using, unless you type the command mentioned above: echo $SHELL They each have differences, but some of the differences are small. Again, bash is still the default on most Linux operating systems. A big recent change is that Mac OS Catalina just started using zsh instead of bash as the default shell but it is just because of a licensing issue, and Dr Ward thinks that Mac users who open the terminal and use the shell are very likely to switch from zsh back to bash. That's what Dr Ward did immediately when Apple made this change to zsh, i.e., he switched back to bash. Wow, sorry for the long-winded answer. Standard utilities man man stand for manual and is a command which presents all of the information you need in order to use a command. To use man simply execute man &lt;command&gt; where command is the command for which you want to read the manual. You can scroll up by typing &quot;k&quot; or the up arrow. You can scroll down by typing &quot;j&quot; or the down arrow. To exit the man pages, type &quot;q&quot; (for quit). How do I show the man pages for the wc utility? Click here for solution man wc cat cat stands for concatenate and print files. It is an extremely useful tool that prints the entire contents of a file by default. This is especially useful when we want to quickly check to see what is inside of a file. It can be used as a tool to output the contents of a file and immediately pipe the contents to another tool for some sort of analysis if the other tool doesn't natively support reading the contents from the file. A similar, but alternative UNIX command that incrementally shows the contents of the file is called less. less starts at the top of the file and scrolls through the rest of the file as the user pages down. head head is a simple utility that displays the first n lines of a file, or input. How do I show the first 5 lines of a file called input.txt? Click here for solution head -n5 input.txt Alternatively: cat input.txt | head -n5 tail tail is a similar utility to head, that displays the last n lines of a file, or input. How do I show the last 5 lines of a file called input.txt? Click here for solution tail -n5 input.txt Alternatively: cat input.txt | tail -n5 ls ls is a utility that lists files and folders. By default, ls will list the files and folders in your current working directory. To list files in a certain directory, simply provide the directory to ls as the first argument. How do I list the files in my $HOME directory? Click here for solution ls $HOME # or ls ~ How do I list the files in the directory /home/$USER/projects? Click here for solution ls /home/$USER/projects How do I list all files and folders, including hidden files and folders in /home/$USER/projects? Click here for solution ls -a /home/$USER/projects How do I list all files and folders in /home/$USER/projects in a list format, including information like permissions, filesize, etc? Click here for solution ls -l /home/$USER/projects How do I list all files and folders, including hidden files and folders in /home/$USER/projects in a list format, including information like permissions, filesize, etc? Click here for solution ls -la /home/$USER/projects # or ls -al /home/$USER/projects # or ls -l -a /home/$USER/projects du du is a tool used to get file space usage. Examples How do I get the size of a file called ./metadata.csv in bytes? Click here for solution du -b ./metadata.csv How do I get the size of a file called ./metadata.csv in kilobytes? Click here for solution du -k ./metadata.csv ## 1792 ./metadata.csv Why is the result of du -b ./metadata.csv divided by 1024 not the result of du -k ./metadata.csv? Click here for solution du reports disk usage by default not necessarily actual size. File systems typically divide a disk into blocks. When a program tells the file system it wants say 3 bytes of space, if the block size is 1024 bytes, the file system may allocate 1024 bytes of space to store the 3 bytes of data. To see the apparent size, do this: du -b ./metadata.csv du -k --apparent-size ./metadata.csv cp cp is a utility used for copying files an folders from one location to another. How do I copy /home/$USER/some_file.txt to /home/$USER/projects/same_file.txt? Click here for solution cp /home/$USER/some_file.txt /home/$USER/projects/same_file.txt # If currently in /home/$USER cd $HOME cp some_file.txt projects/same_file.txt # If currently in /home/$USER/projects cd $HOME/projects cp ../some_file.txt . mv mv very similar to cp, but rather than copy a file, mv moves the file. Moving a file removes it from its old location and places it in the new location. How do I move /home/$USER/some_file.txt to /home/$USER/projects/same_file.txt? Click here for solution mv /home/$USER/some_file.txt /home/$USER/projects/same_file.txt # If currently in /home/$USER cd $HOME mv some_file.txt projects/same_file.txt # If currently in /home/$USER/projects cd $HOME/projects mv ../some_file.txt . touch touch is a command used to update the access and modification times of a file to the current time. More commonly, it is used to create an empty file that you can add contents to later on. To use this command, type touch followed by the file name (with the intended file path added when necessary). mkdir mkdir is the command to create a directory. It is simple to use, just type mkdir followed by a path to the new directory. Examples How do I create a new directory called my_directory in the current directory? Click here for solution mkdir my_directory How do I create a new directory called my_directory in the parent directory? Click here for solution mkdir ../my_directory How do I create a set of two new nested directories in the current directory? Click here for solution # You can either make the directories one at a time like this: mkdir first_dir cd first_dir mkdir second_dir # Or, you can use the -p option: mkdir -p first_dir/second_dir rm rm is the command to remove files or directories. You can find the available options by checking its manual page. Examples How do I remove a folder called my_folder and all of its contents recursively. Assume my_folder is in /home/user/projects. Click here for solution rm -r /home/user/projects/my_folder How do I remove all files in a folder ending in .txt? Assume we are looking at files in /home/user/projects. Click here for solution rm /home/user/projects/*.txt rmdir rmdir is a tool to remove empty directories. Simply type rmdir followed by the path to the empty directory you'd like to remove. Note that this command only removes empty directories. For this reason, rm is better suited to remove directories with content. pwd pwd stands for print working directory and it does just that -- it prints the current working directory to standard output. type type is a useful command to find the location of some command, or whether the command is an alias, function, or something else. Where is the file that is executed when I type ls? Click here for solution type ls ## ls is /bin/ls uniq uniq reads the lines of a specified input file and compares each adjacent line and returns each unique line. Repeated lines in the input will not be detected if they are not adjacent. What this means is you must sort prior to using uniq if you want to ensure you have no duplicates. wc You can think of wc as standing for &quot;word count&quot;. wc displays the number of lines, words, and bytes from the input file. How do I count the number of lines of an input file called input.txt? Click here for solution wc -l input.txt How do I count the number of characters of an input file called input.txt? Click here for solution wc -m input.txt How do I count the number of words of an input file called input.txt? Click here for solution wc -w input.txt ssh mosh scp cut cut is a tool to cut out parts of a line based on position/character/delimiter/etc and directing the output to stdout. It is particularly useful to get a certain column of data. How do I get the first column of a csv file called 'office.csv`? Click here for solution cut -d, -f1 office.csv How do I get the first and third column of a csv file called 'office.csv`? Click here for solution cut -d, -f1,3 office.csv How do I get the first and third column of a file with columns separated by the &quot;|&quot; character? Click here for solution cut -d &#39;|&#39; -f1,3 office.csv sed grep It is very simple to get started searching for patterns in files using grep. How do I search for lines with the word &quot;Exact&quot; in the file located /home/john/report.txt? Click here for solution grep Exact /home/john/report.txt # or grep &#39;Exact&#39; &#39;/home/john/report.txt&#39; How do I search for lines with the word &quot;Exact&quot; or &quot;exact&quot; in the file located /home/john/report.txt? Click here for solution # The -i option means that the text we are searching for is # not case-sensitive. So the following lines will match # lines that contain &quot;Exact&quot; or &quot;exact&quot; or &quot;ExAcT&quot;. grep -i Exact /home/john/report.txt # or grep -i &#39;Exact&#39; &#39;/home/john/report.txt&#39; How do I search for lines with a string containing multiple words, like &quot;how do I&quot;? Click here for solution # The -i option means that the text we are searching for is # not case-sensitive. So the following lines will match # lines that contain &quot;Exact&quot; or &quot;exact&quot; or &quot;ExAcT&quot;. # By adding quotes, we are able to search for the entire # string &quot;how do i&quot;. Without the quotes this would only # search for &quot;how&quot;. grep -i &#39;how do i&#39; /home/john/report.txt How do I search for lines with the word &quot;Exact&quot; or &quot;exact&quot; in the files in the folder and all sub-folders located /home/john/? Click here for solution # The -R option means to search recursively in the folder # /home/john. A recursive search means that it will search # all folders and sub-folders starting with /home/john. grep -Ri Exact /home/john How do I search for the lines that don't contain the words &quot;Exact&quot; or &quot;exact&quot; in the folder and all sub-folders located /home/john/? Click here for solution # The -v option means to search for an inverted match. # In this case it means search for all lines of text # where the word &quot;exact&quot; is not found. grep -Rvi Exact /home/john How do I search for lines where one or more of the words &quot;first&quot; or &quot;second&quot; appears in the current folder and all sub-folders? Click here for solution # The &quot;|&quot; character in grep is the logical OR operator. # If we do not escape the &quot;|&quot; character with a preceding # &quot;\\&quot; grep searches for the literal string &quot;first|second&quot; # instead of &quot;first&quot; OR &quot;second&quot;. grep -Ri &#39;first\\|second&#39; . How do I search for lines that begin with the word &quot;Exact&quot; (case insensitive) in the folder and all sub-folders located in the current directory? Click here for solution The &quot;^&quot; is called an anchor and indicates the start of a line. grep -Ri &#39;^Exact&#39; . How do I search for lines that end with the word &quot;Exact&quot; (case insensitive) in the files in the current folder and all sub-folders? Click here for solution The &quot;$&quot; is called an anchor and indicates the end of a line. grep -Ri &#39;Exact$&#39; . How do I search for lines that contain only the word &quot;Exact&quot; (case insensitive) in the files in the current folder and all sub-folders? Click here for solution grep -Ri &#39;^Exact$&#39; . How do I search for strings or sub-strings where the first character could be anything, but the next two characters are &quot;at&quot;? For example: &quot;cat&quot;, &quot;bat&quot;, &quot;hat&quot;, &quot;rat&quot;, &quot;pat&quot;, &quot;mat&quot;, etc. Click here for solution The &quot;.&quot; is a wildcard, meaning it matches any character (including spaces). grep -Ri &#39;.at&#39; . How do I search for zero or one of, zero or more of, one or more of, exactly n of a certain character using grep and regular expressions? Click here for solution &quot;*&quot; stands for 0+ of the previous character. &quot;+&quot; stands for 1+ of the previous character. &quot;?&quot; stands for 0 or 1 of the previous character. &quot;{n}&quot; stands for exactly n of the previous character. # Matches any lines with text like &quot;cat&quot;, &quot;bat&quot;, &quot;hat&quot;, &quot;rat&quot;, &quot;pat&quot;, &quot;mat&quot;, etc. # Does NOT match &quot;at&quot;, but does match &quot; at&quot;. The &quot;.&quot; indicates a single character. grep -Ri &#39;.at&#39; . # Matches any lines with text like &quot;cat&quot;, &quot;bat&quot;, &quot;hat&quot;, &quot;rat&quot;, &quot;pat&quot;, &quot;mat&quot;, etc. # Matches &quot;at&quot; as well as &quot; at&quot;. The &quot;.&quot; followed by the &quot;?&quot; means # 0 or 1 of any character. grep -Ri &#39;.?at&#39; . # Matches any lines with any amount of text followed by &quot;at&quot;. grep -Ri &#39;.*at&#39; . # Only matches words that end in &quot;at&quot;: &quot;bat&quot;, &quot;cat&quot;, &quot;spat&quot;, &quot;at&quot;. Does not match &quot;spatula&quot;. grep -Ri &#39;.*at$&#39; . # Matches lines that contain consecutive &quot;e&quot;&#39;s. grep -Ri &#39;.*e{2}.*&#39; . # Matches any line. 0+ of the previous character, which in this case is the wildcard &quot;.&quot; # that represents any character. So 0+ of any character. grep -Ri &#39;.*&#39; Resources Regex Tester https://regex101.com/ is an excellent tool that helps you quickly test and better understand writing regular expressions. It allows you to test four different &quot;flavors&quot; or regular expressions: PCRE (PHP), ECMAScript (JavaScript), Python, and Golang. regex101 also provides a library of useful, pre-made regular expressions. Lookahead and Lookbehinds This is an excellent resource to better understand positive and negative lookahead and lookbehind operations using grep. ReExCheatsheet An excellent quick reference for regular expressions. Examples using grep in R. ripgrep ripgrep is a &quot;line-oriented search tool that recursively searches your current directory for a regex pattern.&quot; You can read about why you may want to use ripgrep here. Generally, ripgrep is frequently faster than grep. If you are working with code it has sane defaults (respects .gitignore). You can easily search for specific types of files. How do I exclude a filetype when searching for foo in my_directory? Click here for solution # exclude javascript (.js) files rg -Tjs foo my_directory # exclude r (.r) files rg -Tr foo my_directory # exclude Python (.py) files rg -Tpy foo my_directory How do I search for a particular filetype when searching for foo in my_directory? Click here for solution # search javascript (.js) files rg -tjs foo my_directory # search r (.r) files rg -tr foo my_directory # search Python (.py) files rg -tpy foo my_directory How do I search for a specific word, where the word isn't part of another word? Click here for solution # this is roughly equivalent to putting \\b before and after all search patterns in grep rg -w foo my_directory How do I replace every match foo in my_directory with the text given, bar, when printing results? Click here for solution rg foo my_directory -r bar How do I trim whitespace from the beginning and ending of each printed line? Click here for solution rg foo my_directory --trim How do I follow symbolic links when searching a directory, my_directory? Click here for solution rg -L foo my_directory find find is an aptly named tool that traverses directories and searches for files. Examples How do I find a file named foo.txt in the current working directory or subdirectories? Click here for solution find . -name foo.txt How do I find a file named foo.txt or Foo.txt or FoO.txt (i.e. ignoring case) in the current working directory or subdirectories? Click here for solution find . -iname foo.txt # or find . -i -name foo.txt How do I find a directory named foo in the current working directory or subdirectories? Click here for solution find . -type d -name foo How do I find all of the Python files in the current working directory or subdirectories? Click here for solution find . -name &quot;*.py&quot; How do I find files over 1gb in size in the current working directory or subdirectories? Click here for solution find . -size +1G How do I find files under 10mb in size in the current working directory or subdirectories? Click here for solution find . -size -10M less less is a utility that opens a page of text from a file and allows the user to scroll forward or backward in the file using &quot;j&quot; and &quot;k&quot; keys or down and up arrows. less does not read the entire file into memory at once, and is therefore faster when loading large files. How do I display the contents of a file, foo.txt? Click here for solution less foo.txt How do I scroll up and down in less? Click here for solution To scroll down use &quot;j&quot; or the down arrow. To scroll up use &quot;k&quot; or the up arrow. How do I exit less? Click here for solution Press the &quot;q&quot; key on your keyboard. sort sort is a utility that sorts lines of text. Examples How do I sort a csv, flights_sample.csv alphabetically by the 18th column? Click here for solution # the r option sorts ascending sort -t, -k18,18 flights_sample.csv ## 1990,10,18,7,729,730,847,849,PS,1451,NA,78,79,NA,-2,-1,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,19,1,749,730,922,849,PS,1451,NA,93,79,NA,33,19,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,21,3,728,730,848,849,PS,1451,NA,80,79,NA,-1,-2,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,22,4,728,730,852,849,PS,1451,NA,84,79,NA,3,-2,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,23,5,731,730,902,849,PS,1451,NA,91,79,NA,13,1,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,24,6,744,730,908,849,PS,1451,NA,84,79,NA,19,14,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay ## 1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1990,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1990,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA How do I sort a csv, flights_sample.csv alphabetically by the 18th column, and then in descending order by the 4th column? Click here for solution sort -t, -k18,18 -k4,4r flights_sample.csv ## 1990,10,18,7,729,730,847,849,PS,1451,NA,78,79,NA,-2,-1,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,24,6,744,730,908,849,PS,1451,NA,84,79,NA,19,14,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,23,5,731,730,902,849,PS,1451,NA,91,79,NA,13,1,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,22,4,728,730,852,849,PS,1451,NA,84,79,NA,3,-2,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,21,3,728,730,848,849,PS,1451,NA,80,79,NA,-1,-2,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1991,10,19,1,749,730,922,849,PS,1451,NA,93,79,NA,33,19,SAN,ABC,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay ## 1990,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1990,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA ## 1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA git See here. awk awk is a powerful programming language that specializes in processing and manipulating text data. In awk, a command looks something like this: awk -F, 'BEGIN{ } { } END{ }' The delimiter is specified with the -F option (in this case our delimiter is a comma). The BEGIN chunk is run only once at the start of execution. The middle chunk is run once per line of the file. The END chunk is run only once, at the end of execution. The BEGIN and END portions are always optional. The variables: $1, $2, $3, etc., refer to the 1st, 2nd, and 3rd fields in a line of data. For example, the following would print the 4th field of every row in a csv file: awk -F, &#39;{print $4}&#39; $0 represents the entire row. awk is very powerful. We can achieve the same effect as using cut: head 5000_products.csv | cut -d, -f3 # or head 5000_products.csv | awk -F, &#39;{print $3}&#39; Built in variables awk has some special built in variables that can be very useful. See here. Examples How do I print only rows where the DAYOFWEEK is 5? Click here for solution head metadata.csv | awk -F, &#39;{if ($3 == 5) {print $0}}&#39; ## 01/01/2015,,5,0,0,1,2015,CHRISTMAS PEAK,0,5,nyd,1,,,,0,0,CHRISTMAS PEAK,73.02,59.81,66.41,,0,,0,,0,,0,,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,17:42,1,1,0,0,18,19,17,0,0,0,0,0,0,0,1,13,17,15,0,0,0,0,1,0,14,16,14,0,1,0,0,0,0,11,15,12,8:00,25:00,17,7:00,25:00,8:00,26:00,18,8:00,25:00,17,8:00,21:00,13,8:00,21:00,8:00,25:00,17,8:00,21:00,13,8:00,22:00,14,8:00,22:00,8:00,24:00,16,8:00,22:00,14,8:00,19:00,11,8:00,19:00,8:00,22:00,14,8:00,20:00,12,1,1,0,0,NONE,53.375714286,70.3,50.2,0.12,616246,367265,296273,236654,53904354,34718635,26907827,20971646,1600,1000,2,12:00,15:30,Disney Festival of Fantasy Parade,1,22:15,,Main Street Electrical Parade,1,21:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,3,18:30,20:00,Fantasmic!,1,0,,,,,0,,, ## 01/08/2015,,5,7,1,1,2015,CHRISTMAS,8,0,,0,,marwk,,0,1,CHRISTMAS,59.44,38.7,49.07,,0,,0,,0,,0,,88%,94%,99%,78%,97%,83%,69%,94%,100%,100%,100%,76%,100%,100%,93%,100%,100%,100%,100%,100%,100%,63%,93%,17:47,1,0,0,0,13,12,12,0,0,0,0,0,0,0,1,12,12,14,0,0,0,0,0,0,10,10,10,0,1,0,0,0,0,8,9,9,9:00,21:00,12,8:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,19:00,10,9:00,19:00,9:00,19:00,10,9:00,19:00,10,9:00,17:00,8,9:00,17:00,9:00,17:00,8,9:00,18:00,9,1,1,0,0,NONE,48.372142857,70.3,49.4,0.08,615046,367265,296273,236654,53894754,34718635,26907827,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,2,19:00,21:00,Main Street Electrical Parade,1,20:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,1,19:00,,Fantasmic!,1,0,,,,,0,,, How do I print the first, fourth, and fifth columns of rows where the DAYOFWEEK is 5? Click here for solution head metadata.csv | awk -F, &#39;{if ($3 == 5) {print $1, $4, $5}}&#39; ## 01/01/2015 0 0 ## 01/08/2015 7 1 How do I print only rows where DAYOFWEEK is 5 OR YEAR is 2015? Click here for solution head metadata.csv | awk -F, &#39;{if ($3 == 5 || $7 == 2015) {print $0}}&#39; ## 01/01/2015,,5,0,0,1,2015,CHRISTMAS PEAK,0,5,nyd,1,,,,0,0,CHRISTMAS PEAK,73.02,59.81,66.41,,0,,0,,0,,0,,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,17:42,1,1,0,0,18,19,17,0,0,0,0,0,0,0,1,13,17,15,0,0,0,0,1,0,14,16,14,0,1,0,0,0,0,11,15,12,8:00,25:00,17,7:00,25:00,8:00,26:00,18,8:00,25:00,17,8:00,21:00,13,8:00,21:00,8:00,25:00,17,8:00,21:00,13,8:00,22:00,14,8:00,22:00,8:00,24:00,16,8:00,22:00,14,8:00,19:00,11,8:00,19:00,8:00,22:00,14,8:00,20:00,12,1,1,0,0,NONE,53.375714286,70.3,50.2,0.12,616246,367265,296273,236654,53904354,34718635,26907827,20971646,1600,1000,2,12:00,15:30,Disney Festival of Fantasy Parade,1,22:15,,Main Street Electrical Parade,1,21:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,3,18:30,20:00,Fantasmic!,1,0,,,,,0,,, ## 01/02/2015,,6,1,0,1,2015,CHRISTMAS,2,5,,0,,,,0,0,CHRISTMAS,78,60.72,69.36,,0,,0,,0,,0,,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,17:43,0,1,0,0,17,18,16,0,0,0,0,0,1,0,0,15,13,12,0,0,1,0,0,0,14,14,14,0,0,0,0,0,0,12,11,11,8:00,25:00,17,8:00,25:00,8:00,25:00,17,9:00,25:00,16,8:00,21:00,13,8:00,23:00,8:00,21:00,13,9:00,21:00,12,8:00,22:00,14,8:00,22:00,8:00,22:00,14,9:00,22:00,13,8:00,20:00,12,8:00,20:00,8:00,19:00,11,8:00,19:00,11,1,1,0,0,NONE,53.750714286,70.3,50,0.12,616246,367265,296273,236654,53904354,34718635,26907827,20971646,1600,1000,2,12:00,15:30,Disney Festival of Fantasy Parade,1,22:15,,Main Street Electrical Parade,1,21:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,3,18:30,20:00,Fantasmic!,1,0,,,,,0,,, ## 01/03/2015,,7,2,0,1,2015,CHRISTMAS,3,0,,0,,,,0,0,CHRISTMAS,83.12,67.31,75.22,,0,,0,,0,,0,,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,17:44,0,0,0,0,16,17,15,0,0,0,0,0,0,1,0,12,15,12,1,0,0,0,0,0,14,14,11,0,0,1,0,0,0,11,12,12,9:00,25:00,16,9:00,25:00,8:00,25:00,17,9:00,24:00,15,9:00,21:00,12,9:00,21:00,8:00,21:00,13,9:00,21:00,12,9:00,22:00,13,8:00,22:00,8:00,22:00,14,9:00,20:00,11,8:00,19:00,11,8:00,19:00,8:00,20:00,12,9:00,20:00,11,1,1,0,0,NONE,49.212857143,70.3,49.9,0.07,616246,367265,296273,236654,53904354,34718635,26907827,20971646,1600,1000,2,12:00,15:30,Disney Festival of Fantasy Parade,1,22:15,,Main Street Electrical Parade,1,21:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,2,18:30,20:00,Fantasmic!,1,0,,,,,0,,, ## 01/04/2015,,1,3,1,1,2015,CHRISTMAS,4,0,,0,,,,0,0,CHRISTMAS,83.93,67.97,75.95,,0,,0,,0,,0,,67%,74%,77%,74%,74%,70%,66%,94%,68%,57%,56%,70%,79%,43%,93%,100%,100%,100%,100%,100%,48%,63%,84%,17:44,0,0,0,0,15,16,14,0,0,0,0,0,0,0,0,12,12,12,0,1,0,0,0,1,11,14,13,1,0,0,0,0,0,12,11,8,9:00,24:00,15,9:00,24:00,9:00,25:00,16,9:00,23:00,14,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,20:00,11,9:00,20:00,9:00,22:00,13,9:00,20:00,11,9:00,20:00,11,8:00,20:00,8:00,19:00,11,9:00,17:00,8,1,1,0,0,NONE,48.270714286,70.3,49.8,0.12,616246,367265,296273,236654,53904354,34718635,26907827,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,2,20:00,22:00,Main Street Electrical Parade,1,21:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,2,19:00,20:30,Fantasmic!,1,0,,,,,0,,, ## 01/05/2015,,2,4,1,1,2015,CHRISTMAS,5,0,,0,,,,0,0,CHRISTMAS,72.3,56.89,64.6,,0,,0,,0,,0,,67%,74%,77%,74%,74%,70%,66%,94%,68%,57%,56%,70%,79%,43%,93%,100%,100%,100%,100%,100%,48%,63%,84%,17:45,0,0,0,0,14,15,12,0,0,0,0,1,0,0,0,12,12,13,0,0,0,1,0,0,13,11,10,0,1,0,0,0,0,8,12,8,9:00,23:00,14,9:00,23:00,9:00,24:00,15,9:00,21:00,12,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,20:00,11,9:00,22:00,9:00,20:00,11,9:00,19:00,10,9:00,17:00,8,9:00,17:00,9:00,20:00,11,9:00,17:00,8,1,1,0,0,NONE,48.971538462,70.3,49.6,0.12,616246,367265,306272,236654,53904354,34718635,27897728,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,2,20:00,22:00,Main Street Electrical Parade,1,21:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,2,19:00,20:30,Fantasmic!,1,0,,,,,0,,, ## 01/06/2015,,3,5,1,1,2015,CHRISTMAS,6,0,,0,,,,0,0,CHRISTMAS,77.67,54.88,66.28,,0,,0,,0,,0,,86%,92%,98%,77%,96%,82%,69%,94%,100%,98%,98%,76%,100%,96%,93%,100%,100%,83%,100%,100%,92%,63%,93%,17:46,0,0,0,0,12,14,12,0,0,1,0,0,0,0,0,13,12,12,0,0,0,0,1,0,10,13,10,0,0,1,0,0,0,8,8,9,9:00,21:00,12,9:00,21:00,9:00,23:00,14,9:00,21:00,12,9:00,21:00,12,8:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,19:00,10,9:00,19:00,9:00,20:00,11,9:00,19:00,10,9:00,17:00,8,9:00,17:00,9:00,17:00,8,9:00,17:00,8,1,1,0,0,NONE,50.093571429,70.2,49.5,0.12,615046,367265,296273,236654,53894754,34718635,26907827,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,0,,,,1,20:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,1,19:00,,Fantasmic!,1,0,,,,,0,,, ## 01/07/2015,,4,6,1,1,2015,CHRISTMAS,7,0,,0,,marwk,,0,1,CHRISTMAS,67.24,48.56,57.9,,0,,0,,0,,0,,88%,94%,99%,78%,97%,83%,69%,94%,100%,100%,100%,76%,100%,100%,93%,100%,100%,100%,100%,100%,100%,63%,93%,17:47,0,0,1,0,12,12,13,0,0,0,1,0,0,0,0,12,13,12,0,0,0,0,0,0,10,10,10,1,0,0,0,0,0,9,8,8,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,19:00,10,9:00,19:00,9:00,19:00,10,9:00,19:00,10,9:00,17:00,8,8:00,17:00,9:00,17:00,8,9:00,17:00,8,1,1,0,0,NONE,47.188571429,70.3,49.5,0.12,615046,367265,296273,236654,53894754,34718635,26907827,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,0,,,,1,20:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,1,19:00,,Fantasmic!,1,0,,,,,0,,, ## 01/08/2015,,5,7,1,1,2015,CHRISTMAS,8,0,,0,,marwk,,0,1,CHRISTMAS,59.44,38.7,49.07,,0,,0,,0,,0,,88%,94%,99%,78%,97%,83%,69%,94%,100%,100%,100%,76%,100%,100%,93%,100%,100%,100%,100%,100%,100%,63%,93%,17:47,1,0,0,0,13,12,12,0,0,0,0,0,0,0,1,12,12,14,0,0,0,0,0,0,10,10,10,0,1,0,0,0,0,8,9,9,9:00,21:00,12,8:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,19:00,10,9:00,19:00,9:00,19:00,10,9:00,19:00,10,9:00,17:00,8,9:00,17:00,9:00,17:00,8,9:00,18:00,9,1,1,0,0,NONE,48.372142857,70.3,49.4,0.08,615046,367265,296273,236654,53894754,34718635,26907827,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,2,19:00,21:00,Main Street Electrical Parade,1,20:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,1,19:00,,Fantasmic!,1,0,,,,,0,,, ## 01/09/2015,,6,8,1,1,2015,CHRISTMAS,9,0,,0,,marwk,,0,1,CHRISTMAS,54.89,45.37,50.13,,0,,0,,0,,0,,88%,94%,99%,78%,97%,83%,69%,94%,100%,100%,100%,76%,100%,100%,93%,100%,100%,100%,100%,100%,100%,63%,93%,17:48,0,1,0,0,12,13,14,0,1,0,0,0,1,0,0,14,12,12,0,0,1,0,0,0,10,10,12,0,0,0,0,0,0,9,8,11,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,23:00,14,9:00,21:00,12,9:00,23:00,9:00,21:00,12,9:00,21:00,12,9:00,19:00,10,9:00,19:00,9:00,19:00,10,9:00,20:00,11,9:00,18:00,9,9:00,18:00,9:00,17:00,8,9:00,20:00,11,1,1,0,0,NONE,51.094285714,70.3,49.3,0.11,615046,367265,296273,236654,53894754,34718635,26907827,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,1,19:00,,Main Street Electrical Parade,1,20:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,1,19:00,,Fantasmic!,1,0,,,,,0,,, How do I print only rows where DAYOFWEEK is 5 AND YEAR is 2015? Click here for solution head metadata.csv | awk -F, &#39;{if ($3 == 5 &amp;&amp; $7 == 2015) {print $0}}&#39; ## 01/01/2015,,5,0,0,1,2015,CHRISTMAS PEAK,0,5,nyd,1,,,,0,0,CHRISTMAS PEAK,73.02,59.81,66.41,,0,,0,,0,,0,,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,0%,17:42,1,1,0,0,18,19,17,0,0,0,0,0,0,0,1,13,17,15,0,0,0,0,1,0,14,16,14,0,1,0,0,0,0,11,15,12,8:00,25:00,17,7:00,25:00,8:00,26:00,18,8:00,25:00,17,8:00,21:00,13,8:00,21:00,8:00,25:00,17,8:00,21:00,13,8:00,22:00,14,8:00,22:00,8:00,24:00,16,8:00,22:00,14,8:00,19:00,11,8:00,19:00,8:00,22:00,14,8:00,20:00,12,1,1,0,0,NONE,53.375714286,70.3,50.2,0.12,616246,367265,296273,236654,53904354,34718635,26907827,20971646,1600,1000,2,12:00,15:30,Disney Festival of Fantasy Parade,1,22:15,,Main Street Electrical Parade,1,21:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,3,18:30,20:00,Fantasmic!,1,0,,,,,0,,, ## 01/08/2015,,5,7,1,1,2015,CHRISTMAS,8,0,,0,,marwk,,0,1,CHRISTMAS,59.44,38.7,49.07,,0,,0,,0,,0,,88%,94%,99%,78%,97%,83%,69%,94%,100%,100%,100%,76%,100%,100%,93%,100%,100%,100%,100%,100%,100%,63%,93%,17:47,1,0,0,0,13,12,12,0,0,0,0,0,0,0,1,12,12,14,0,0,0,0,0,0,10,10,10,0,1,0,0,0,0,8,9,9,9:00,21:00,12,8:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,21:00,12,9:00,21:00,9:00,21:00,12,9:00,21:00,12,9:00,19:00,10,9:00,19:00,9:00,19:00,10,9:00,19:00,10,9:00,17:00,8,9:00,17:00,9:00,17:00,8,9:00,18:00,9,1,1,0,0,NONE,48.372142857,70.3,49.4,0.08,615046,367265,296273,236654,53894754,34718635,26907827,20971646,1600,1000,1,15:00,,Disney Festival of Fantasy Parade,2,19:00,21:00,Main Street Electrical Parade,1,20:00,,Wishes Nighttime Spectacular,1,21:00,,IllumiNations: Reflections of Earth,0,,,0,,,,1,19:00,,Fantasmic!,1,0,,,,,0,,, How do I get the average of values in a column containing the max temperature, WDWMAXTEMP? Click here for solution # Here NR represents the number of rows head metadata.csv | awk -F, &#39;{sum = sum + $19}END{print &quot;Average max temp: &quot; sum/NR}&#39; # Or alternatively we could track the number of rows as we go head metadata.csv | awk -F, &#39;{sum = sum + $19; count++}END{print &quot;Average max temp: &quot; sum/count}&#39; ## Average max temp: 64.961 ## Average max temp: 64.961 How do I get counts of each unique value in a column, SEASON? Click here for solution When executing the middle chunk of code, awk will create a set of values called seasons, whose elements are named by unique values in the 8-th column SEASON. For the SEASON value in a line, awk will add 1 to the corresponding element (this is ++). Thus, we get the count for each unique value. In the END chunk of code, we print out season by going through its elements. The season in for (season in seasons) refers to the name of the elements. To access the actual value, we use seasons[season]. This is just one example of arrays in awk. You can find more details here: https://www.gnu.org/software/gawk/manual/html_node/Arrays.html cat metadata.csv | awk -F, &#39;{seasons[$8]++}END{for (season in seasons) {print season, seasons[season]}}&#39; ## SUMMER BREAK 236 ## CHRISTMAS 245 ## JERSEY WEEK 50 ## SEPTEMBER LOW 140 ## PRESIDENTS WEEK 55 ## FALL 212 ## HALLOWEEN 26 ## MEMORIAL DAY 20 ## CHRISTMAS PEAK 176 ## SEASON 1 ## COLUMBUS DAY 20 ## SPRING 490 ## THANKSGIVING 60 ## EASTER 95 ## MARTIN LUTHER KING JUNIOR DAY 45 ## MARDI GRAS 15 ## JULY 4TH 25 ## WINTER 222 How do I get counts of each unique value in a column, SEASON, but only print the values for FALL, WINTER, SUMMER, and SPRING? Click here for solution cat metadata.csv | awk -F, &#39;{seasons[$8]++}END{for (season in seasons) {if (season == &quot;FALL&quot; || season == &quot;SUMMER&quot; || season == &quot;WINTER&quot; || season == &quot;SPRING&quot;) print season, seasons[season]}}&#39; ## FALL 212 ## SPRING 490 ## WINTER 222 Or a better solution would be to use the ~ operator: cat metadata.csv | awk -F, &#39;{seasons[$8]++}END{for (season in seasons) {if (season ~ /WINTER|SPRING|SUMMER|FALL/) print season, seasons[season]}}&#39; ## SUMMER BREAK 236 ## FALL 212 ## SPRING 490 ## WINTER 222 If you want to exclude &quot;SUMMER BREAK&quot;, use the $ regular expression anchor. This forces it to only accept strings where the entire string ends in &quot;SUMMER&quot; so &quot;SUMMER BREAK&quot; is excluded as it ends in &quot; BREAK&quot; not &quot;SUMMER&quot;: cat metadata.csv | awk -F, &#39;{seasons[$8]++}END{for (season in seasons) {if (season ~ /WINTER|SPRING|SUMMER$|FALL/) print season, seasons[season]}}&#39; ## FALL 212 ## SPRING 490 ## WINTER 222 ~ &amp; . &amp; .. ~ represents the location which is in the environment variable $HOME. If you change $HOME, ~ also changes. As you are navigating directories, to jump to the most previously visited directory, you can run ~-. For example, if you navigate to /home/$USER/projects/project1/output, then to /home/$USER, and you'd like to jump directly back to /home/$USER/projects/project1/output, simply run ~-. ~- is simply a reference to the location stored in $OLDPWD. . represents the current working directory. For example, if you are in your home directory /home/$USER, . means &quot;in this directory&quot;, and ./some_file.txt would represent a file named some_file.txt which is in your home directory /home/$USER. .. represents the parent directory. For example, /home is the parent directory of /home/$USER. If you are currently in /home/$USER/projects and you want to access some file in the home directory, you could do ../some_file.txt. ../some_file.txt is called a relative path as it is relative to your current location. If we accessed ../some_file.txt from the home directory, this would be different than accessing ../some_file.txt from a different directory. /home/$USER/some_file.txt is an absolute or full path of a file some_file.txt. Examples If I am in the directory /home/kamstut/projects directory, what is the relative path to /home/mdw/? Click here for solution ../../mdw If I am in the directory /home/kamstut/projects/project1, what is the absolute path to the file ../../scripts/runthis.sh? Click here for solution /home/kamstut/scripts/runthis.sh How can I navigate to my $HOME directory? Click here for solution cd cd ~ cd $HOME cd /home/$USER Piping &amp; Redirection Redirection is the act of writing standard input (stdin) or standard output (stdout) or standard error (stderr) somewhere else. stdin, stdout, and stderr all have numeric representations of 0, 1, &amp; 2 respectively. Piping is a form of redirection, but rather than redirect output to stdin, stdout, or stderr, we redirect the output to further commands for more processing. Redirection Examples For the following examples we use the example file redirection.txt. The contents of which are: cat redirection.txt ## This is a simple file with some text. ## It has a couple of lines of text. ## Here is some more. How do I redirect text from a command like ls to a file like redirection.txt, completely overwriting any text already within redirection.txt? Click here for solution # Save the stdout from the ls command to redirection.txt ls &gt; redirection.txt # The new contents of redirection.txt head redirection.txt ## 01-scholar.Rmd ## 02-data-formats.Rmd ## 03-unix.Rmd ## 04-sql.Rmd ## 05-r.Rmd ## 06-python.Rmd ## 07-tools.Rmd ## 08-faqs.Rmd ## 09-projects.Rmd ## 10-fall-2020-projects.Rmd How do I redirect text from a command like ls to a file like redirection.txt, without overwriting any text, but rather appending the text to the end of the file? Click here for solution # Append the stdout from the ls command to the end of redirection.txt ls &gt;&gt; redirection.txt head redirection.txt ## This is a simple file with some text. ## It has a couple of lines of text. ## Here is some more. ## 01-scholar.Rmd ## 02-data-formats.Rmd ## 03-unix.Rmd ## 04-sql.Rmd ## 05-r.Rmd ## 06-python.Rmd ## 07-tools.Rmd How can I redirect text from a file to be used as stdin for another program or command? Click here for solution # Let&#39;s count the number of words in redirection.txt wc -w &lt; redirection.txt ## 20 How can I use multiple redirects in a single line? Click here for solution # Here we count the number of words in redirection.txt and then # save that value to value.txt. wc -w &lt; redirection.txt &gt; value.txt head value.txt ## 20 Piping Piping is the act of taking the output of one or more commands and making the output the input of another command. This is accomplished using the &quot;|&quot; character. Examples For the following examples we use the example file piping.txt. The contents of which are: cat piping.txt ## apples, oranges, grapes ## pears, apples, peaches, ## celery, carrots, peanuts ## fruits, vegetables, ok How can I use the output from a grep command to another command? Click here for solution grep -i &quot;p\\{2\\}&quot; piping.txt | wc -w ## 6 How can I chain multiple commands together? Click here for solution # Get the third column of piping.txt and # get all lines that end in &quot;s&quot; and sort # the words in reverse order, and append # to a file called food.txt. cut -d, -f3 piping.txt | grep -i &quot;.*s$&quot; | sort -r &gt; food.txt Resources Intro to I/O Redirection A quick introduction to stdin, stdout, stderr, redirection, and piping. Cron Cron is a unix application used to schedule commands or tasks to run at a specific time or at a specific time interval. For example, let's say you have a program called generate_report.py that reads some data from the system and generates a report to email to your superiors. Cron would be perfectly suited to do this at the end of each month, without you needing to do a single manual task. To do so, do the following: Open the crontab. The crontab is the text document containing your cron jobs. # -e stands for &quot;edit&quot; crontab -e This command will open a text editor for you to write your crontab. Then, on a single line, paste the following content: 0 0 1 * * /full/path/to/generate_report.py Once you save the file, the cron job will take effect. This cron job would run at midnight, on the first day of every month. The rough format of a cron job is: minute hour day (of month) month day (of week) So, the first 0 represented minute 0. The second 0, hour 0. The first 1, the first day of the month. The first *, every month. The second *, any day of the week. If you are uncomfortable using the text editor on Scholar (nano/vim/emacs), there is an alternative way to modify your crontab. Create a text file in RStudio by clicking File &gt; New File &gt; Text File. To the first line, paste the following content: 0 0 1 * * /full/path/to/generate_report.py Important note: You must include the newline following the line of text. Save the file to your $HOME directory as my_cron.txt. Once you've saved your file, you should be able to see it in the bottom right hand corner of RStudio (you may need to click the refresh button to make it appear). Once complete, open a terminal by clicking Code &gt; Terminal &gt; Open New Terminal at File Location. If this option isn't present, it is likely you already have a terminal tab open in RStudio. Navigate to the terminal. To update your crontab to the contents of your text file, my_cron.txt, type the following (into the terminal): crontab $HOME/my_cron.txt Important note: If you get an error that says &quot;premature EOF&quot;, you forgot to add a newline (empty line) to the end of your my_cron.txt. If the command runs without error, your crontab has been successfully installed! You can check by running the following command in the terminal: crontab -l Examples Write a cron job that runs generate_report.py every minute. Click here for solution * * * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py every 5 minutes. Click here for solution */5 * * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py every 10 minutes. Click here for solution */10 * * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py on the 5th minute of every hour. Click here for solution 5 * * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py every hour. Click here for solution 0 * * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py every other hour. Click here for solution 0 */2 * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py every other minute of every other hour. Click here for solution */2 */2 * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py every day at 5 AM. Click here for solution 0 5 * * * /full/path/to/generate_report.py Write a cron job that runs generate_report.py every day at 2:22 PM. Click here for solution 22 14 * * * /full/path/to/generate_report.py How do I remove a cron job when I no longer want it to run? Click here for solution First, open the crontabs: crontab -e Then, delete the line containing the cron job you no longer wish to run. Save the file. Upon saving the file, the cron job you deleted will no longer run. Resources Crontab Guru An incredibly helpful tool for writing cron jobs. Emacs Nano Vim Writing scripts bash stands for &quot;Bourne Again Shell&quot;. There are many types of shells, including but not limited to: ksh, zsh, csh, tcsh, fish. When you open a terminal emulator, it will typically run a shell. You can write a bash script, zsh script, csh script, etc. Typically, when you have an interpreter, you can write scripts for them. For example, even though R and Python are not shells, we can write scripts for those languages. As bash is the default shell for many linux operating systems today, we will keep referring to scripts as &quot;bash scripts&quot;, but take note that in general the same applies for other shells too. A bash script is more or less a series of bash commands used to perform a sequence of actions. It is similar to a .R script, but instead of R code, we have bash commands. A bash script starts with the &quot;shebang&quot; or &quot;bang&quot; line or &quot;hash-bang&quot; -- #!/bin/bash. The shebang is used to indicate which interpreter to use to execute the script. For example, if you were using zsh instead, your shebang might read #!/bin/zsh. Take the following bash script: #!/bin/bash echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot; If you were to place that text inside of a file called my_script: echo &#39;#!/bin/bash echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot;&#39; &gt; $HOME/my_script And then run it: cd $HOME chmod +x ./my_script ./my_script okay cool The second line of code is to set the permission so that your script is executable. You would get the following result: First argument: okay Second argument: cool The operating system would use the interpreter located /bin/bash to execute the script. This would produce the same results: cd $HOME /bin/bash my_script okay cool But instead we only have to run: cd $HOME ./my_script okay cool Note that if you were to change the shebang to say #!/usr/bin/python and try running the following: cd $HOME ./my_script okay cool You would get an error that reads: File &quot;./my_script&quot;, line 3 echo &quot;First argument: $1&quot; ^ SyntaxError: invalid syntax The reason is that the operating system is using the Python interpreter located /usr/bin/python to run the bash code in our script, my_script. Since our code is not Python code, we get this error. Arguments A bash script can accept arguments. This is just like many programs we've used to date (grep, cut, awk, etc.). For example: grep -i &#39;special&#39; Here, -i and 'special' are arguments to grep. -i is the first argument, and 'special' is the second. If you run the following script: #!/bin/bash echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot; You can see that this is indeed the truth: cd $HOME ./my_script -i &#39;special&#39; First argument: -i Second argument: special In a bash script the first argument is denoted by $1 the second by $2 the third by $3 etc. In fact, $0 denotes the command used to run the script: #!/bin/bash echo &quot;Command: $0&quot; echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot; cd $HOME ./my_script okay cool Command: ./my_script First argument: okay Second argument: cool Examples Write a script called indyflights.sh that takes a file from this directoy as its input: /class/datamine/data/flights/subset and returns the number of flights that have IND as the origin or destination. Click here for solution #!/bin/bash cat /class/datamine/data/flights/subset/$1 | cut -d, -f17,18 | grep IND | wc -l Modify your script from this problem to accept an argument containing an airport code (for example IND). Your script should determine how many flights have origin or destination IND (or your given airport code) altogether (across all years in all of the flights files). Click here for solution #!/bin/bash for i in {1987..2008}; do count=$(cat /class/datamine/data/flights/subset/$i.csv | cut -d, -f17,18 | grep $1 | wc -l) sum=$((sum + count)) done echo &quot;$sum&quot; or Note: This option would work better if you need to use variable substitution in your range (from 1987 to 2008). #!/bin/bash for ((i=1987; i&lt;=2008; i++)); do count=$(cat /class/datamine/data/flights/subset/$i.csv | cut -d, -f17,18 | grep $1 | wc -l) sum=$((sum + count)) done echo &quot;$sum&quot; "],
["sql.html", "SQL Joins Aliasing", " SQL library(RMariaDB) library(RSQLite) library(DBI) # Establish a connection to sqlite databases chinook &lt;- dbConnect(RSQLite::SQLite(), &quot;chinook.db&quot;) lahman &lt;- dbConnect(RSQLite::SQLite(), &quot;lahman.db&quot;) # Establish a connection to mysql databases connection &lt;- dbConnect(RMariaDB::MariaDB(), host=&quot;your-host.com&quot;, db=&quot;your-database-name&quot;, user=&quot;your-username&quot;, password=&quot;your-password&quot;) Joins Joins are SQL clauses that combine data from two tables. There are 4 primary types of SQL joins: INNER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN, and FULL OUTER JOIN. When talking about an SQL JOIN statement, sometimes the first table in the SQL statement is referred to as the &quot;left&quot; table, and the second table is referred to as the right table. For instance, in the following query, A is the left table and B is the right table. SELECT * FROM A INNER JOIN B ON A.id=B.a_id; While there can be cases where using RIGHT JOIN and FULL JOIN can make your SQL statement more concise, both RIGHT JOIN and FULL JOIN are redundant and can be fully emulated using LEFT JOIN and UNION ALL clauses. For the purposes of illustration, we will be using a common example of a database for an online store. This online store has two primary tables, orders and customers, shown below. orders id description customer_id value 1 Water bottle 1 15.00 2 Key chain 1 7.50 3 Computer 3 2000.00 4 Thumb drive 3 25.00 5 Notebook 4 9.00 6 Shampoo 5.00 7 Paper 4.00 customers id first_name last_name email 1 Natalie Wright wright@example.com 2 Ana Sousa sousa@example.com 3 Ben Schwartz schwartz@example.com 4 Chen Xi xi@example.com 5 Frank Zhang zhang@example.com 6 Tianchi Liu liu@example.com 7 Jake Jons jons@example.com INNER JOIN An INNER JOIN, often referred to as simply JOIN, returns rows/records where there is a match in the right table from the left table. Records from the left table that don't have a match in the right table are excluded. Records from the right table that don't have a match in the left table are also excluded. This is appropriate any time you need data from two separate tables, but only when the two tables have something in common. For example, what if our online company decided it wanted to query the database to send an email of appreciation for all customers who have placed at least 1 order. In this case, we want only the emails of those who don't appear in both the customers and orders table. SELECT customers.email FROM orders INNER JOIN customers ON orders.customer_id=customers.id; Which would result in the following table. email wright@example.com schwartz@example.com xi@example.com LEFT OUTER JOIN A LEFT OUTER JOIN, often referred to as simply a LEFT JOIN, returns rows/records where every value in the left table is present in addition to additional data from the right table, when there exists a match in the right table. This is appropriate any time you want all of the data from the left table, and any extra data from the right table if there happens to be a match. For example, what if our online company wanted a list of all orders placed, and if the order wasn't placed from a guest account, send an email to the customer thanking them for their purchase? In this case, it would make sense to append email information to the order when there is a match. SELECT orders.description, orders.value, customers.email FROM orders LEFT JOIN customers ON order.customer_id=customers.id; Which would result in the following table, enabling the employee to see orders as well as send out thank you emails. description value first_name last_name email Water bottle 15.00 Natalie Wright wright@example.com Key chain 7.50 Natalie Wright wright@example.com Computer 2000.00 Ben Schwartz schwartz@example.com Thumb drive 25.00 Ben Schwartz schwartz@example.com Notebook 9.00 Chen Xi xi@example.com Shampoo 5.00 Paper 4.00 Had we instead used an INNER JOIN, our list would be missing critical order information. SELECT orders.description, orders.value, customers.email FROM orders INNER JOIN customers ON order.customer_id=customers.id; description value first_name last_name email Water bottle 15.00 Natalie Wright wright@example.com Key chain 7.50 Natalie Wright wright@example.com Computer 2000.00 Ben Schwartz schwartz@example.com Thumb drive 25.00 Ben Schwartz schwartz@example.com Notebook 9.00 Chen Xi xi@example.com Aliasing Aliasing is the process of giving a table or a table column a temporary name. Aliases are commonly used to either make the query easier to write, or more readable. An example of using table aliases to make a query shorter would be the following. SELECT orders.description, orders.value, customers.email FROM orders INNER JOIN customers ON order.customer_id=customers.id; By using table aliases, this can be reduced greatly. SELECT o.description, o.value, c.email FROM orders AS o INNER JOIN customers AS c ON o.customer_id=c.id; Note that aliases only last for the duration of a single query. If we were to subsequently use the following query, it would fail. SELECT o.description, o.value, c.email FROM o INNER JOIN c ON o.customer_id=c.id; In addition to table aliases, we can give fields aliases as well. For example, we could reduce customer_id to just c_id. SELECT orders.customer_id AS c_id FROM orders INNER JOIN customers ON order.c_id=customers.id; Alternatively, we could change customer_id to Customer ID, however, whenever we want an alias to contain spaces, we need to use either double quotes or square brackets. SELECT orders.customer_id AS &quot;Customer ID&quot; FROM orders INNER JOIN customers ON order.&quot;Customer ID&quot;=customers.id; RDBMS SQL in R Click here for video Examples Please see here for a variety of examples demonstrating using SQL within R. SQL in Python Examples The following examples use the lahman.db sqlite database. Display the first 10 ballparks in the ballparks table. Click here for solution SELECT * FROM parks LIMIT 10; Table 1: Displaying records 1 - 10 ID parkalias parkkey parkname city state country 1 NA ALB01 Riverside Park Albany NY US 2 NA ALT01 Columbia Park Altoona PA US 3 Edison Field; Anaheim Stadium ANA01 Angel Stadium of Anaheim Anaheim CA US 4 NA ARL01 Arlington Stadium Arlington TX US 5 The Ballpark in Arlington; Ameriquest Fl ARL02 Rangers Ballpark in Arlington Arlington TX US 6 NA ATL01 Atlanta-Fulton County Stadium Atlanta GA US 7 NA ATL02 Turner Field Atlanta GA US 8 NA ATL03 Suntrust Park Atlanta GA US 9 NA BAL01 Madison Avenue Grounds Baltimore MD US 10 NA BAL02 Newington Park Baltimore MD US Make a list of the names of all of the inactive teams in baseball history. Click here for solution Remove the LIMIT 10 for full results. SELECT franchName FROM teamsfranchises WHERE active==&#39;N&#39; LIMIT 10; Table 2: Displaying records 1 - 10 franchName Altoona Mountain City Philadelphia Athletics Buffalo Bisons Buffalo Bisons Baltimore Orioles Baltimore Terrapins Baltimore Monumentals Boston Reds Brooklyn Gladiators Boston Reds Find the player with the most Runs Batted In (RBIs) in a season in queries. In the first query find the playerID of the player with the most RBIs. In the second query find the player's name in the people table. Click here for solution In addition to his RBI record, Hack Wilson also held the NL home run record for a long time as well with 56. In 1999, Manny Ramirez tried to pursue the RBI record, but only was able to accrue 165 RBIs. -- Find the playerID SELECT playerID FROM batting WHERE RBI==191; -- Display the name SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;wilsoha01&#39;; Table 3: 1 records playerID wilsoha01 Who was the manager of the 1976 &quot;Big Red Machine&quot; (CIN)? Complete this in 2 queries. Click here for solution The &quot;Big Red Machine&quot; was a famous nickname for the dominant Cincinnati Reds of the early 1970s. Many of its team members are Hall of Famers, including their manager, Sparky Anderson. SELECT playerID FROM managers WHERE yearID==1976 AND teamID==&#39;CIN&#39;; SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;andersp01&#39;; Table 4: 1 records playerID andersp01 Make a list of the teamIDs that were managed by Tony LaRussa. Complete this in 2 queries. Click here for solution Tony LaRussa is very well known for being a manager that was involved in baseball for a very long time. He won the World Series with the St. Louis Cardinals and the Oakland Athletics. SELECT playerID FROM people WHERE nameLast==&#39;LaRussa&#39; AND nameFirst==&#39;Tony&#39;; SELECT DISTINCT teamID FROM managers WHERE playerID==&#39;larusto01&#39;; Table 5: 1 records playerID larusto01 What was Cecil Fielder's salary in 1987? Display the teamID with the salary. Click here for solution Cecil Fielder was a power hitting DH in the 1980s and 1990s. His son, Prince Fielder, played in the major leagues as well. SELECT playerID FROM people WHERE nameFirst==&#39;Cecil&#39; AND nameLast==&#39;Fielder&#39;; SELECT teamID, salary FROM salaries WHERE playerID==&#39;fieldce01&#39; AND yearID==1987; Table 6: 1 records playerID fieldce01 Make a list of all the teams who have lost a World Series (WS) since 1990. Put the list in ascending order by yearID. Click here for solution SELECT teamIDloser, yearID FROM seriespost WHERE yearID &gt;= 1990 AND round==&#39;WS&#39; ORDER BY yearID ASC LIMIT 10; Table 7: Displaying records 1 - 10 teamIDloser yearID OAK 1990 ATL 1991 ATL 1992 PHI 1993 CLE 1995 ATL 1996 CLE 1997 SDN 1998 ATL 1999 NYN 2000 Let's find out about Cal Ripken, Jr. What was his height and weight? Did he bat right or left handed? When did he play his final game? Find all of this information in one query. Click here for solution Cal Ripken, Jr's nickname is the &quot;Iron Man&quot; of baseball due to the fact that he started in 2,632 straight games. That means in just over 16 seasons, Cal Ripken, Jr. never missed a game! SELECT height, weight, bats, finalgame FROM people WHERE nameFirst==&#39;Cal&#39; AND nameLast==&#39;Ripken&#39; AND deathState IS NULL; Table 8: 1 records height weight bats finalGame 76 200 R 2001-10-06 Select all the playerIDs and yearIDs of the players who were inducted in the hall of fame and voted in by the Veterans committee, between 1990 and 2000. Put the list in descending order. Click here for solution The veterans committee in the Hall of Fame voting process place players in the Hall of Fame that are forgotten by the writers, fans, etc. This is a way for players to recognize who they think were the greatest players of all time, or are skipped over for a variety of reasons. This is one reason why there is a lot of scrutiny in the process for how players are selected to the baseball hall of fame. SELECT playerID, yearID FROM halloffame WHERE votedBy==&#39;Veterans&#39; AND inducted==&#39;Y&#39; AND yearID BETWEEN 1990 AND 2000 ORDER BY yearID DESC LIMIT 10; Table 9: Displaying records 1 - 10 playerID yearid andersp01 2000 mcphebi01 2000 steartu99 2000 cepedor01 1999 chylane99 1999 seleefr99 1999 willijo99 1999 davisge01 1998 dobyla01 1998 macphle99 1998 Get a list of the attendance by season of the Toronto Blue Jays (TOR). What season was the highest attendance? Click here for solution The Toronto Blue Jays were the 1993 season's World Series champion. This means that, yes, a non-USA team has won the World Series for baseball! SELECT yearkey, attendance FROM homegames WHERE teamkey==&#39;TOR&#39; ORDER BY attendance DESC LIMIT 10; Table 10: Displaying records 1 - 10 yearkey attendance 1993 4057747 1992 4028318 1991 4001526 1990 3884384 2016 3392099 2017 3203886 1994 2907949 1995 2826445 2015 2794891 1987 2778459 How many different leagues have represented Major League Baseball over time? Click here for solution Major League Baseball has had several leagues that have been represented in its history. There are only two current leagues: National League and the American League. SELECT DISTINCT league FROM leagues; Table 11: 8 records league American Association American League Federal League Major League National Association National League Players' League Union Association Find the teams that have won the World Series. Click here for solution SELECT teamID, yearID FROM teams WHERE WSWin==&#39;Y&#39; LIMIT 10; Table 12: Displaying records 1 - 10 teamID yearID PRO 1884 SL4 1886 DTN 1887 NY1 1888 NY1 1889 BOS 1903 NY1 1905 CHA 1906 CHN 1907 CHN 1908 List the top 10 season win totals of teams. Include the yearID and teamID. Click here for solution SELECT teamID, yearID, W FROM teams ORDER BY W DESC LIMIT 10; Table 13: Displaying records 1 - 10 teamID yearID W CHN 1906 116 SEA 2001 116 NYA 1998 114 CLE 1954 111 PIT 1909 110 NYA 1927 110 NYA 1961 109 BAL 1969 109 BAL 1970 108 CIN 1975 108 List the pitchers with their teamID, wins (W), and losses (L) that threw complete games (CG) in the 1995 season. Include their number of complete games as well. Click here for solution SELECT playerID, teamID, W, L, CG FROM pitching WHERE CG &gt; 0 AND yearID==1995 ORDER BY W DESC LIMIT 10; Table 14: Displaying records 1 - 10 playerID teamID W L CG maddugr01 ATL 19 2 10 mussimi01 BAL 19 9 7 johnsra05 SEA 18 2 6 schoupe01 CIN 18 7 2 martira02 LAN 17 7 4 rogerke01 TEX 17 7 3 glavito02 ATL 16 7 3 hershor01 CLE 16 6 1 nagych01 CLE 16 6 2 wakefti01 BOS 16 8 6 Get a printout of the Hits (H), and home runs (HR) of Ichiro Suzuki's career. Do this is in two queries. In the first query, find Ichiro Suzuki's playerID. In the second one list the teamID, yearID, hits and home runs. Click here for solution Ichiro Suzuki is regarded as one of the greatest hitters of all time because of his prowess in both American and Japanese professional baseball. SELECT playerID FROM people WHERE nameFirst==&#39;Ichiro&#39; AND nameLast==&#39;Suzuki&#39;; SELECT teamID, yearID, H, HR FROM batting WHERE playerID==&#39;suzukic01&#39;; Table 15: 1 records playerID suzukic01 How many walks (BB) and strikeouts (SO) did Mariano Rivera achieve in the playoffs? Which year did Mariano Rivera give up the most post-season walks? Click here for solution More men have walked on the moon than have scored a run on Mariano Rivera in a playoff game. Mariano Rivera made the hall of fame in 2019. SELECT playerID FROM people WHERE nameFirst==&#39;Mariano&#39; AND nameLast==&#39;Rivera&#39;; SELECT yearID, teamID, BB, SO FROM pitchingpost WHERE playerID==&#39;riverma01&#39; ORDER BY BB DESC; Table 16: 1 records playerID riverma01 Find the pitcher with most strikeouts (SO), and the batter that struck out the most in the 2014 season. Get the first and last name of the pitcher and batter, respectively. Click here for solution Corey Kluber is a two-time AL Cy Young winner. He is well known for his two-seam fastball that is difficult to hit. SELECT playerID, SO FROM pitching WHERE yearID==2014 ORDER BY SO DESC LIMIT(10); SELECT playerID, SO FROM batting WHERE yearID==2014 ORDER BY SO DESC LIMIT(10); SELECT nameFirst,nameLast FROM people WHERE playerID==&quot;klubeco01&quot; OR playerID==&quot;howarry01&quot;; Table 17: Displaying records 1 - 10 playerID SO klubeco01 269 scherma01 252 hernafe02 248 cuetojo01 242 strasst01 242 kershcl01 239 bumgama01 219 salech01 208 greinza01 207 kenneia01 207 How many different teams did Bartolo Colon pitch for? Click here for solution Bartolo Colon is a well-known journeyman pitcher in baseball. He has pitched with a lot of teams, but it wasn't until he played for the New York Mets when he needed to come to the plate. He had a weird batting stance that is funny to watch. He even hit a home run one season! SELECT playerID FROM people WHERE nameFirst==&#39;Bartolo&#39; AND nameLast==&#39;Colon&#39;; SELECT DISTINCT teamID FROM pitching WHERE playerID==&#39;colonba01&#39;; Table 18: 1 records playerID colonba01 How many times did Trevor Bauer come to bat (AB) in 2016? How many hits (H) did he get? Click here for solution Trevor Bauer is much more known for his pitching than he is known for hitting. This is common for pitchers, as many are not very good at hitting. SELECT playerID FROM people WHERE nameFirst==&quot;Trevor&quot; AND nameLast==&quot;Bauer&quot;; Table 19: 1 records playerID bauertr01 SELECT AB, H FROM batting WHERE playerID==&quot;bauertr01&quot; AND yearID==&quot;2016&quot;; Table 20: 1 records AB H 5 0 Let's compare Mike Trout and Giancarlo Stanton by season. Who has hit more RBIs in a season? Who has been caught stealing (CS) more in a season? Click here for solution Mike Trout and Giancarlo Stanton are considered two of the of the best hitters in Major League Baseball for very different reasons. Trout is an all-around player known for being indispensible, where Stanton is known as a power hitter. SELECT playerID, nameFirst, nameLast FROM people WHERE (nameFirst==&#39;Giancarlo&#39; AND nameLast==&#39;Stanton&#39;) OR (nameFirst==&#39;Mike&#39; AND nameLast==&#39;Trout&#39;); Table 21: 2 records playerID nameFirst nameLast stantmi03 Giancarlo Stanton troutmi01 Mike Trout SELECT playerID, yearID, teamID, RBI, CS FROM batting WHERE playerID==&#39;stantmi03&#39; OR playerID==&#39;troutmi01&#39; ORDER BY RBI DESC LIMIT 1; Table 22: 1 records playerID yearID teamID RBI CS stantmi03 2017 MIA 132 2 SELECT playerID, yearID, teamID, RBI, CS FROM batting WHERE playerID==&#39;stantmi03&#39; OR playerID==&#39;troutmi01&#39; ORDER BY CS DESC LIMIT 1; Table 23: 1 records playerID yearID teamID RBI CS troutmi01 2013 LAA 97 7 Make a list of players who walked (BB) more than they struck out (SO) between 1980 and 1985. Of these players, who walked the most? Use the BETWEEN command in this queries. Use a second query to get the player's first and last name. Click here for solution SELECT playerID, yearID, teamID, BB, SO FROM batting WHERE BB &gt; SO LIMIT 10; Table 24: Displaying records 1 - 10 playerID yearID teamID BB SO addybo01 1871 RC1 4 0 ansonca01 1871 RC1 2 1 barkeal01 1871 RC1 1 0 barnero01 1871 BS1 13 1 battijo01 1871 CL1 1 0 bealsto01 1871 WS3 2 0 bellast01 1871 TRO 9 2 berthha01 1871 WS3 4 2 biermch01 1871 FW1 1 0 birdge01 1871 RC1 3 2 SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;randowi01&#39;; Table 25: 1 records nameFirst nameLast Willie Randolph How many different NL catchers (C) won gold glove winners between 1990 and 2000? Click here for solution There were 6 different catchers. SELECT DISTINCT playerID FROM awardsplayers WHERE awardID==&#39;Gold Glove&#39; AND notes==&#39;C&#39; AND lgID==&#39;NL&#39; AND yearID BETWEEN 1990 AND 2000; Table 26: 6 records playerID santibe01 pagnoto01 manwaki01 johnsch04 liebemi01 mathemi01 How many different 3rd Basemen played for the Seattle Mariners between 2000 and 2005? Who had the most Errors? Click here for solution SELECT DISTINCT playerID, yearID, E FROM fielding WHERE yearID BETWEEN 2000 AND 2005 AND teamID==&#39;SEA&#39; AND POS==&#39;3B&#39; ORDER BY E DESC LIMIT 10; Table 27: Displaying records 1 - 10 playerID yearID E guillca01 2000 17 bellda01 2001 14 beltrad01 2005 14 bellda01 2000 12 cirilje01 2002 9 leoneju01 2004 8 mclemma01 2001 7 spiezsc01 2004 7 bloomwi01 2004 5 mabryjo01 2000 4 SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;camermi01&#39;; Table 28: 1 records nameFirst nameLast Mike Cameron Craig Biggio was more known for his play at second base over his major league baseball career, but he didn't always play second base. What seasons did Craig Biggio play Catcher? Click here for solution SELECT playerID FROM people WHERE nameFirst==&#39;Craig&#39; AND nameLast==&#39;Biggio&#39;; Table 29: 1 records playerID biggicr01 SELECT teamID, yearID, POS FROM fielding WHERE playerID==&#39;biggicr01&#39; AND POS==&#39;C&#39;; Table 30: 5 records teamID yearID POS HOU 1988 C HOU 1989 C HOU 1990 C HOU 1991 C HOU 2007 C Find the teams that have won the World Series that represented the National League. Display the list with the yearID and teamID in ascending order. Click here for solution SELECT teamID, yearID FROM teams WHERE WSWin==&#39;Y&#39; AND lgID==&#39;NL&#39; ORDER BY yearID ASC LIMIT 10; Table 31: Displaying records 1 - 10 teamID yearID PRO 1884 DTN 1887 NY1 1888 NY1 1889 NY1 1905 CHN 1907 CHN 1908 PIT 1909 BSN 1914 CIN 1919 List the pitchers that threw at least one complete game (CG) in the 1995 season. Please include the wins and losses of the top 10 pitchers. Use the playerID of the pitcher who threw the most complete games to find out the name of the pitcher that had the most complete games. Click here for solution SELECT playerID, W, L, CG FROM pitching WHERE CG &gt; 0 AND yearID==1995 ORDER BY CG DESC LIMIT 10; Table 32: Displaying records 1 - 10 playerID W L CG maddugr01 19 2 10 mcdowja01 15 10 8 ericksc01 9 4 7 leitema01 10 12 7 mussimi01 19 9 7 johnsra05 18 2 6 valdeis01 13 11 6 wakefti01 16 8 6 coneda01 9 6 5 fernaal01 12 8 5 SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;maddugr01&#39;; Table 33: 1 records nameFirst nameLast Greg Maddux Who was the most recent player manager? Click here for solution SELECT playerID, yearID FROM managers WHERE plyrMgr==&#39;Y&#39; ORDER BY yearID DESC LIMIT 10; Table 34: Displaying records 1 - 10 playerID yearID rosepe01 1986 rosepe01 1985 rosepe01 1984 kessido01 1979 torrejo01 1977 robinfr02 1976 robinfr02 1975 tappeel01 1962 bauerha01 1961 hemusso01 1959 SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;rosepe01&#39;; Table 35: 1 records nameFirst nameLast Pete Rose Get the at-bats, homeruns, stolen bases for Roberto Clemente by year in ascending order. Click here for solution Roberto Clemente is known as being a leader for the Pittsburgh Pirates. He died in a 1972 plane crash on a humanitarian mission to Puerto Rico, where he grew up. SELECT playerID FROM people WHERE nameFirst==&#39;Roberto&#39; AND nameLast==&#39;Clemente&#39;; Table 36: 1 records playerID clemero01 SELECT yearID,AB,HR,SB FROM battingpost WHERE playerID==&#39;clemero01&#39; ORDER BY yearID ASC; Table 37: 5 records yearID AB HR SB 1960 29 0 0 1970 14 0 0 1971 18 0 0 1971 29 2 0 1972 17 1 0 Get a list of distinct World Series winners from the years Tom Lasorda managed the Los Angeles Dodgers (LAN). First find the years Tom Lasorda was the manager of the Los Angeles Dodgers, and then find the distinct teams that won a World Series in that time frame. Click here for solution SELECT playerID FROM people WHERE nameFirst==&#39;Tom&#39; AND nameLast==&#39;Lasorda&#39;; Table 38: 1 records playerID lasorto01 SELECT yearID FROM managers WHERE playerID==&#39;lasorto01&#39; LIMIT 10; Table 39: Displaying records 1 - 10 yearID 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 SELECT DISTINCT teamID FROM teams WHERE WSWin==&#39;Y&#39; AND yearID BETWEEN 1976 AND 1996; Table 40: Displaying records 1 - 10 teamID CIN NYA PIT PHI LAN SLN BAL DET KCA NYN Which teams did Kenny Lofton steal more than 20 bases in a season after the year 2000? Click here for solution SELECT playerID FROM people WHERE nameFirst==&#39;Kenny&#39; AND nameLast==&#39;Lofton&#39;; Table 41: 1 records playerID loftoke01 SELECT teamID, yearID, SB FROM batting WHERE playerID==&#39;loftoke01&#39; AND SB &gt; 20 AND yearID &gt;2000; Table 42: 4 records teamID yearID SB CHA 2002 22 PHI 2005 22 LAN 2006 32 TEX 2007 21 How much did the Tampa Bay Rays (TBL) pay Wade Boggs in 1998? Who paid Boggs the most in a season during his career? Click here for solution SELECT playerID FROM people WHERE nameFirst==&#39;Wade&#39; AND nameLast==&#39;Boggs&#39;; Table 43: 1 records playerID boggswa01 SELECT teamID, yearID, salary FROM salaries WHERE playerID==&#39;boggswa01&#39; AND yearID==1998; Table 44: 1 records teamID yearID salary TBA 1998 1150000 SELECT teamID, yearID, salary FROM salaries WHERE playerID==&#39;boggswa01&#39; ORDER BY salary DESC LIMIT 10; Table 45: Displaying records 1 - 10 teamID yearID salary NYA 1995 4724316 NYA 1994 3200000 NYA 1993 2950000 BOS 1991 2750000 BOS 1992 2700000 NYA 1996 2050000 NYA 1997 2000000 BOS 1990 1900000 BOS 1989 1850000 BOS 1987 1675000 Click here for solution SELECT teamID, yearID, W, L, HR, HRA, attendance FROM teams WHERE teamID==&#39;DET&#39; AND (WSWin==&#39;Y&#39; OR LgWin==&#39;Y&#39;); Table 46: Displaying records 1 - 10 teamID yearID W L HR HRA attendance DET 1907 92 58 11 8 297079 DET 1908 90 63 19 12 436199 DET 1909 98 54 19 16 490490 DET 1934 101 53 74 86 919161 DET 1935 93 58 106 78 1034929 DET 1940 90 64 134 102 1112693 DET 1945 88 65 77 48 1280341 DET 1968 103 59 185 129 2031847 DET 1984 104 58 187 130 2704794 DET 2006 95 67 203 160 2595937 The standings you would find in a newspaper often have Wins and Losses in order of most to least wins. There are often other numbers that are involved like winning percentage, and other team statistics, but we won't deal with that for now. Get the NL East Standings in 2015. Click here for solution SELECT teamID, W, L FROM teams WHERE divID==&#39;E&#39; AND lgID==&#39;NL&#39; AND yearID==2015 ORDER BY teamrank ASC; Table 47: 5 records teamID W L NYN 90 72 WAS 83 79 MIA 71 91 ATL 67 95 PHI 63 99 Make a list of the teams, wins, losses, years for NL East teams that have won the World Series. Which team had the most wins? Click here for solution SELECT teamID, yearID, W, L FROM teams WHERE lgID==&#39;NL&#39; AND divID==&#39;E&#39; AND WSWin==&#39;Y&#39; ORDER BY W DESC; Table 48: Displaying records 1 - 10 teamID yearID W L NYN 1986 108 54 NYN 1969 100 62 PIT 1979 98 64 PIT 1971 97 65 WAS 2019 93 69 SLN 1982 92 70 FLO 1997 92 70 PHI 2008 92 70 PHI 1980 91 71 FLO 2003 91 71 Get a list of the playerIDs of managers who won more games than they lost between 1930 and 1950. Get the manager's name, and the name of the team of the manager with the most wins on the list. Click here for solution SELECT playerID, teamID, yearID, W, L FROM managers WHERE yearID BETWEEN 1930 AND 1950 AND W &gt; L ORDER BY W DESC LIMIT 10; Table 49: Displaying records 1 - 10 playerID teamID yearID W L mackco01 PHA 1931 107 45 mccarjo99 NYA 1932 107 47 mccarjo99 NYA 1939 106 45 southbi01 SLN 1942 106 48 southbi01 SLN 1943 105 49 southbi01 SLN 1944 105 49 durocle01 BRO 1942 104 50 cronijo01 BOS 1946 104 50 mccarjo99 NYA 1942 103 51 mackco01 PHA 1930 102 52 SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;mackco01&#39;; Table 50: 1 records nameFirst nameLast Connie Mack SELECT franchName FROM teamsfranchises WHERE franchID==&#39;PHA&#39;; Table 51: 1 records franchName Philadelphia Athletics Get the top 5 seasons from Florida Teams (Florida Marlins, Tampa Bay Rays, and Miami Marlins) in attendance. How many have occured since 2000? Click here for solution Florida baseball teams are not known for their attendance for a variety of reasons. Both MLB franchises play in domed fields, but usually do not draw large crowds. SELECT franchID, franchName FROM teamsfranchises WHERE franchName==&#39;Tampa Bay Rays&#39; OR franchName==&#39;Florida Marlins&#39;; Table 52: 2 records franchID franchName FLA Florida Marlins TBD Tampa Bay Rays SELECT teamID, yearID, attendance FROM teams WHERE franchID==&#39;TBD&#39; OR franchID==&#39;FLA&#39; ORDER BY attendance DESC LIMIT 10; Table 53: Displaying records 1 - 10 teamID yearID attendance FLO 1993 3064847 TBA 1998 2506293 FLO 1997 2364387 MIA 2012 2219444 FLO 1994 1937467 TBA 2009 1874962 FLO 2005 1852608 TBA 2010 1843445 TBA 2008 1811986 MIA 2015 1752235 What pitcher has thrown the most Shutouts (SHO) in the AL since 2010? What about the NL? Please get their first and last names respectively. Click here for solution SELECT playerID,teamID, yearID, SHO FROM pitching WHERE yearID&gt;2010 AND lgID==&#39;NL&#39; ORDER BY SHO DESC LIMIT 10; Table 54: Displaying records 1 - 10 playerID teamID yearID SHO leecl02 PHI 2011 6 dickera01 NYN 2012 3 alvarhe01 MIA 2014 3 wainwad01 SLN 2014 3 arrieja01 CHN 2015 3 kershcl01 LAN 2015 3 scherma01 WAS 2015 3 kershcl01 LAN 2016 3 carpech01 SLN 2011 2 garcija02 SLN 2011 2 SELECT playerID,teamID, yearID, SHO FROM pitching WHERE yearID&gt;2010 AND lgID==&#39;AL&#39; ORDER BY SHO DESC LIMIT 10; Table 55: Displaying records 1 - 10 playerID teamID yearID SHO hernafe02 SEA 2012 5 hollade01 TEX 2011 4 shielja02 TBA 2011 4 harenda01 LAA 2011 3 vargaja01 SEA 2011 3 morrobr01 TOR 2012 3 colonba01 OAK 2013 3 masteju01 CLE 2013 3 porceri01 DET 2014 3 klubeco01 CLE 2017 3 SELECT nameFirst, nameLast FROM people WHERE playerID==&#39;leecl02&#39; OR playerID==&#39;hernafe02&#39;; Table 56: 2 records nameFirst nameLast Felix Hernandez Cliff Lee The following examples use the chinook.db sqlite database. dbListTables(chinook) ## [1] &quot;advisors&quot; &quot;albums&quot; &quot;artists&quot; &quot;customers&quot; ## [5] &quot;employees&quot; &quot;genres&quot; &quot;invoice_items&quot; &quot;invoices&quot; ## [9] &quot;media_types&quot; &quot;playlist_track&quot; &quot;playlists&quot; &quot;sqlite_sequence&quot; ## [13] &quot;sqlite_stat1&quot; &quot;students&quot; &quot;tracks&quot; How do I select all of the rows of a table called employees? Click here for solution SELECT * FROM employees; Table 57: 8 records EmployeeId LastName FirstName Title ReportsTo BirthDate HireDate Address City State Country PostalCode Phone Fax Email 1 Adams Andrew General Manager NA 1962-02-18 00:00:00 2002-08-14 00:00:00 11120 Jasper Ave NW Edmonton AB Canada T5K 2N1 +1 (780) 428-9482 +1 (780) 428-3457 andrew@chinookcorp.com 2 Edwards Nancy Sales Manager 1 1958-12-08 00:00:00 2002-05-01 00:00:00 825 8 Ave SW Calgary AB Canada T2P 2T3 +1 (403) 262-3443 +1 (403) 262-3322 nancy@chinookcorp.com 3 Peacock Jane Sales Support Agent 2 1973-08-29 00:00:00 2002-04-01 00:00:00 1111 6 Ave SW Calgary AB Canada T2P 5M5 +1 (403) 262-3443 +1 (403) 262-6712 jane@chinookcorp.com 4 Park Margaret Sales Support Agent 2 1947-09-19 00:00:00 2003-05-03 00:00:00 683 10 Street SW Calgary AB Canada T2P 5G3 +1 (403) 263-4423 +1 (403) 263-4289 margaret@chinookcorp.com 5 Johnson Steve Sales Support Agent 2 1965-03-03 00:00:00 2003-10-17 00:00:00 7727B 41 Ave Calgary AB Canada T3B 1Y7 1 (780) 836-9987 1 (780) 836-9543 steve@chinookcorp.com 6 Mitchell Michael IT Manager 1 1973-07-01 00:00:00 2003-10-17 00:00:00 5827 Bowness Road NW Calgary AB Canada T3B 0C5 +1 (403) 246-9887 +1 (403) 246-9899 michael@chinookcorp.com 7 King Robert IT Staff 6 1970-05-29 00:00:00 2004-01-02 00:00:00 590 Columbia Boulevard West Lethbridge AB Canada T1K 5N8 +1 (403) 456-9986 +1 (403) 456-8485 robert@chinookcorp.com 8 Callahan Laura IT Staff 6 1968-01-09 00:00:00 2004-03-04 00:00:00 923 7 ST NW Lethbridge AB Canada T1H 1Y8 +1 (403) 467-3351 +1 (403) 467-8772 laura@chinookcorp.com How do I select the first 5 rows of a table called employees? Click here for solution SELECT * FROM employees LIMIT 5; Table 58: 5 records EmployeeId LastName FirstName Title ReportsTo BirthDate HireDate Address City State Country PostalCode Phone Fax Email 1 Adams Andrew General Manager NA 1962-02-18 00:00:00 2002-08-14 00:00:00 11120 Jasper Ave NW Edmonton AB Canada T5K 2N1 +1 (780) 428-9482 +1 (780) 428-3457 andrew@chinookcorp.com 2 Edwards Nancy Sales Manager 1 1958-12-08 00:00:00 2002-05-01 00:00:00 825 8 Ave SW Calgary AB Canada T2P 2T3 +1 (403) 262-3443 +1 (403) 262-3322 nancy@chinookcorp.com 3 Peacock Jane Sales Support Agent 2 1973-08-29 00:00:00 2002-04-01 00:00:00 1111 6 Ave SW Calgary AB Canada T2P 5M5 +1 (403) 262-3443 +1 (403) 262-6712 jane@chinookcorp.com 4 Park Margaret Sales Support Agent 2 1947-09-19 00:00:00 2003-05-03 00:00:00 683 10 Street SW Calgary AB Canada T2P 5G3 +1 (403) 263-4423 +1 (403) 263-4289 margaret@chinookcorp.com 5 Johnson Steve Sales Support Agent 2 1965-03-03 00:00:00 2003-10-17 00:00:00 7727B 41 Ave Calgary AB Canada T3B 1Y7 1 (780) 836-9987 1 (780) 836-9543 steve@chinookcorp.com How do I select specific rows of a table called employees? Click here for solution SELECT LastName, FirstName FROM employees; Table 59: 8 records LastName FirstName Adams Andrew Edwards Nancy Peacock Jane Park Margaret Johnson Steve Mitchell Michael King Robert Callahan Laura You can switch the order in which the columns are displayed as well: SELECT FirstName, LastName FROM employees; Table 60: 8 records FirstName LastName Andrew Adams Nancy Edwards Jane Peacock Margaret Park Steve Johnson Michael Mitchell Robert King Laura Callahan How do I select only unique values from a column? Click here for solution SELECT DISTINCT Title FROM employees; Table 61: 5 records Title General Manager Sales Manager Sales Support Agent IT Manager IT Staff How can I filter that match a certain criteria? Click here for solution Select only employees with a FirstName &quot;Steve&quot;: SELECT * FROM employees WHERE FirstName=&#39;Steve&#39;; Table 62: 1 records EmployeeId LastName FirstName Title ReportsTo BirthDate HireDate Address City State Country PostalCode Phone Fax Email 5 Johnson Steve Sales Support Agent 2 1965-03-03 00:00:00 2003-10-17 00:00:00 7727B 41 Ave Calgary AB Canada T3B 1Y7 1 (780) 836-9987 1 (780) 836-9543 steve@chinookcorp.com Select only employees with FirstName &quot;Steve&quot; OR FirstName &quot;Laura&quot;: SELECT * FROM employees WHERE FirstName=&#39;Steve&#39; OR FirstName=&#39;Laura&#39;; Table 63: 2 records EmployeeId LastName FirstName Title ReportsTo BirthDate HireDate Address City State Country PostalCode Phone Fax Email 5 Johnson Steve Sales Support Agent 2 1965-03-03 00:00:00 2003-10-17 00:00:00 7727B 41 Ave Calgary AB Canada T3B 1Y7 1 (780) 836-9987 1 (780) 836-9543 steve@chinookcorp.com 8 Callahan Laura IT Staff 6 1968-01-09 00:00:00 2004-03-04 00:00:00 923 7 ST NW Lethbridge AB Canada T1H 1Y8 +1 (403) 467-3351 +1 (403) 467-8772 laura@chinookcorp.com Select only employees with FirstName &quot;Steve&quot; AND LastName &quot;Laura&quot;: SELECT * FROM employees WHERE FirstName=&#39;Steve&#39; AND LastName=&#39;Laura&#39;; Table 64: 0 records EmployeeId LastName FirstName Title ReportsTo BirthDate HireDate Address City State Country PostalCode Phone Fax Email As expected, there are no results! There is nobody with the full name &quot;Steve Laura&quot;. List the first 10 tracks from the tracks table. Click here for solution SELECT * FROM tracks LIMIT 10; Table 65: Displaying records 1 - 10 TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 1 For Those About To Rock (We Salute You) 1 1 1 Angus Young, Malcolm Young, Brian Johnson 343719 11170334 0.99 2 Balls to the Wall 2 2 1 NA 342562 5510424 0.99 3 Fast As a Shark 3 2 1 F. Baltes, S. Kaufman, U. Dirkscneider &amp; W. Hoffman 230619 3990994 0.99 4 Restless and Wild 3 2 1 F. Baltes, R.A. Smith-Diesel, S. Kaufman, U. Dirkscneider &amp; W. Hoffman 252051 4331779 0.99 5 Princess of the Dawn 3 2 1 Deaffy &amp; R.A. Smith-Diesel 375418 6290521 0.99 6 Put The Finger On You 1 1 1 Angus Young, Malcolm Young, Brian Johnson 205662 6713451 0.99 7 Let's Get It Up 1 1 1 Angus Young, Malcolm Young, Brian Johnson 233926 7636561 0.99 8 Inject The Venom 1 1 1 Angus Young, Malcolm Young, Brian Johnson 210834 6852860 0.99 9 Snowballed 1 1 1 Angus Young, Malcolm Young, Brian Johnson 203102 6599424 0.99 10 Evil Walks 1 1 1 Angus Young, Malcolm Young, Brian Johnson 263497 8611245 0.99 How many rows or records are in the table named tracks? Click here for solution SELECT COUNT(*) FROM tracks; Table 66: 1 records COUNT(*) 3503 Are there any artists with the names: &quot;Elis Regina&quot;, &quot;Seu Jorge&quot;, or &quot;The Beatles&quot;? Click here for solution SELECT * FROM artists WHERE Name=&#39;Elis Regina&#39; OR Name=&#39;Seu Jorge&#39; OR Name=&#39;The Beatles&#39;; Table 67: 2 records ArtistId Name 41 Elis Regina 193 Seu Jorge What albums did the artist with ArtistId of 41 make? Click here for solution SELECT * FROM albums WHERE ArtistId=41; Table 68: 1 records AlbumId Title ArtistId 71 Elis Regina-Minha História 41 What are the tracks of the album with AlbumId of 71? Order the results from most Milliseconds to least. Click here for solution SELECT * FROM tracks WHERE AlbumId=71 ORDER BY Milliseconds DESC; Table 69: Displaying records 1 - 10 TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 890 Aprendendo A Jogar 71 1 7 NA 290664 9391041 0.99 886 Saudosa Maloca 71 1 7 NA 278125 9059416 0.99 880 Dois Pra Lá, Dois Pra Cá 71 1 7 NA 263026 8684639 0.99 887 As Aparências Enganam 71 1 7 NA 247379 8014346 0.99 882 Romaria 71 1 7 NA 242834 7968525 0.99 883 Alô, Alô, Marciano 71 1 7 NA 241397 8137254 0.99 889 Maria Rosa 71 1 7 NA 232803 7592504 0.99 877 O Bêbado e a Equilibrista 71 1 7 NA 223059 7306143 0.99 884 Me Deixas Louca 71 1 7 NA 214831 6888030 0.99 878 O Mestre-Sala dos Mares 71 1 7 NA 186226 6180414 0.99 What are the tracks of the album with AlbumId of 71? Order the results from longest to shortest and convert Milliseconds to seconds. Use aliasing to name the calculated field Seconds. Click here for solution SELECT Milliseconds/1000.0 AS Seconds, * FROM tracks WHERE AlbumId=71 ORDER BY Seconds DESC; Table 70: Displaying records 1 - 10 Seconds TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 290.664 890 Aprendendo A Jogar 71 1 7 NA 290664 9391041 0.99 278.125 886 Saudosa Maloca 71 1 7 NA 278125 9059416 0.99 263.026 880 Dois Pra Lá, Dois Pra Cá 71 1 7 NA 263026 8684639 0.99 247.379 887 As Aparências Enganam 71 1 7 NA 247379 8014346 0.99 242.834 882 Romaria 71 1 7 NA 242834 7968525 0.99 241.397 883 Alô, Alô, Marciano 71 1 7 NA 241397 8137254 0.99 232.803 889 Maria Rosa 71 1 7 NA 232803 7592504 0.99 223.059 877 O Bêbado e a Equilibrista 71 1 7 NA 223059 7306143 0.99 214.831 884 Me Deixas Louca 71 1 7 NA 214831 6888030 0.99 186.226 878 O Mestre-Sala dos Mares 71 1 7 NA 186226 6180414 0.99 What are the tracks that are at least 250 seconds long? Click here for solution SELECT Milliseconds/1000.0 AS Seconds, * FROM tracks WHERE Seconds &gt;= 250; Table 71: Displaying records 1 - 10 Seconds TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 343.719 1 For Those About To Rock (We Salute You) 1 1 1 Angus Young, Malcolm Young, Brian Johnson 343719 11170334 0.99 342.562 2 Balls to the Wall 2 2 1 NA 342562 5510424 0.99 252.051 4 Restless and Wild 3 2 1 F. Baltes, R.A. Smith-Diesel, S. Kaufman, U. Dirkscneider &amp; W. Hoffman 252051 4331779 0.99 375.418 5 Princess of the Dawn 3 2 1 Deaffy &amp; R.A. Smith-Diesel 375418 6290521 0.99 263.497 10 Evil Walks 1 1 1 Angus Young, Malcolm Young, Brian Johnson 263497 8611245 0.99 263.288 12 Breaking The Rules 1 1 1 Angus Young, Malcolm Young, Brian Johnson 263288 8596840 0.99 270.863 14 Spellbound 1 1 1 Angus Young, Malcolm Young, Brian Johnson 270863 8817038 0.99 331.180 15 Go Down 4 1 1 AC/DC 331180 10847611 0.99 366.654 17 Let There Be Rock 4 1 1 AC/DC 366654 12021261 0.99 267.728 18 Bad Boy Boogie 4 1 1 AC/DC 267728 8776140 0.99 What are the tracks that are between 250 and 300 seconds long? Click here for solution SELECT Milliseconds/1000.0 AS Seconds, * FROM tracks WHERE Seconds BETWEEN 250 AND 300 ORDER BY Seconds; Table 72: Displaying records 1 - 10 Seconds TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 250.017 1992 Lithium 163 1 1 Kurt Cobain 250017 8148800 0.99 250.031 3421 Nimrod (Adagio) from Variations On an Original Theme, Op. 36 &quot;Enigma&quot; 290 2 24 Edward Elgar 250031 4124707 0.99 250.070 2090 Romance Ideal 169 1 7 NA 250070 8260477 0.99 250.122 2451 Ela Desapareceu 199 1 1 Chico Amaral/Samuel Rosa 250122 8289200 0.99 250.226 2184 Thumbing My Way 180 1 1 Eddie Vedder 250226 8201437 0.99 250.253 2728 Pulse 220 1 4 The Tea Party 250253 8183872 0.99 250.357 974 Edge Of The World 77 1 4 Faith No More 250357 8235607 0.99 250.462 1530 Sem Sentido 123 1 7 NA 250462 8292108 0.99 250.565 3371 Wooden Jesus 269 2 23 NA 250565 4302603 0.99 250.697 2504 Real Love 202 1 4 Billy Corgan 250697 8025896 0.99 What is the GenreId of the genre with name Pop? Click here for solution SELECT GenreId FROM genres WHERE Name=&#39;Pop&#39;; Table 73: 1 records GenreId 9 What is the average length (in seconds) of a track with genre &quot;Pop&quot;? Click here for solution SELECT AVG(Milliseconds/1000.0) AS avg FROM tracks WHERE genreId=9; Table 74: 1 records avg 229.0341 What is the longest Bossa Nova track (in seconds)? Click here for solution What is the GenreId of Bossa Nova? SELECT GenreId FROM genres WHERE Name=&#39;Bossa Nova&#39;; Table 75: 1 records GenreId 11 SELECT *, MAX(Milliseconds/1000.0) AS Seconds FROM tracks WHERE genreId=11; Table 76: 1 records TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice Seconds 646 Samba Da Bênção 52 1 11 NA 409965 13490008 0.99 409.965 Get the average price per hour for Bossa Nova music (genreId of 11). Click here for solution SELECT AVG(UnitPrice/Milliseconds/1000.0/3600) AS &#39;Price per Hour&#39; FROM tracks WHERE genreId=11; Table 77: 1 records Price per Hour 0 Get the average time (in seconds) for tracks by genre. Click here for solution SELECT genreId, AVG(Milliseconds/1000.0) AS &#39;Average seconds per track&#39; FROM tracks GROUP BY genreId; Table 78: Displaying records 1 - 10 GenreId Average seconds per track 1 283.9100 2 291.7554 3 309.7494 4 234.3538 5 134.6435 6 270.3598 7 232.8593 8 247.1778 9 229.0341 10 244.3709 We can use an INNER JOIN to get the name of each genre as well. {#sql-inner-join} SELECT g.Name, track_time.&#39;Average seconds per track&#39; FROM genres AS g INNER JOIN (SELECT genreId, AVG(Milliseconds/1000.0) AS &#39;Average seconds per track&#39; FROM tracks GROUP BY genreId) AS track_time ON g.GenreId=track_time.GenreId ORDER BY track_time.&#39;Average seconds per track&#39; DESC; Table 79: Displaying records 1 - 10 Name Average seconds per track Sci Fi &amp; Fantasy 2911.7830 Science Fiction 2625.5491 Drama 2575.2838 TV Shows 2145.0410 Comedy 1585.2637 Metal 309.7494 Electronica/Dance 302.9858 Heavy Metal 297.4529 Classical 293.8676 Jazz 291.7554 What is the average price per track for each genre? Click here for solution SELECT genreId, AVG(UnitPrice) AS &#39;Average seconds per track&#39; FROM tracks GROUP BY genreId; Table 80: Displaying records 1 - 10 GenreId Average seconds per track 1 0.99 2 0.99 3 0.99 4 0.99 5 0.99 6 0.99 7 0.99 8 0.99 9 0.99 10 0.99 What is the average number of tracks per album? Click here for solution SELECT AVG(trackCount) FROM (SELECT COUNT(*) AS trackCount FROM tracks GROUP BY albumId) AS track_count; Table 81: 1 records AVG(trackCount) 10.0951 What is the average number of tracks per album per genre? Click here for solution SELECT genreId, AVG(trackCount) FROM (SELECT genreId, COUNT(*) AS trackCount FROM tracks GROUP BY albumId) AS track_count GROUP BY genreId; Table 82: Displaying records 1 - 10 genreId AVG(trackCount) 1 11.41379 2 10.00000 3 10.90625 4 14.43478 5 12.00000 6 13.85714 7 14.81579 8 15.00000 9 16.00000 10 10.75000 SELECT Name, avg_track_count.&#39;Average Track Count&#39; FROM genres AS g INNER JOIN (SELECT genreId, AVG(trackCount) AS &#39;Average Track Count&#39; FROM (SELECT genreId, COUNT(*) AS trackCount FROM tracks GROUP BY albumId) AS track_count GROUP BY genreId) AS avg_track_count ON g.GenreId=avg_track_count.genreId; Table 83: Displaying records 1 - 10 Name Average Track Count Rock 11.41379 Jazz 10.00000 Metal 10.90625 Alternative &amp; Punk 14.43478 Rock And Roll 12.00000 Blues 13.85714 Latin 14.81579 Reggae 15.00000 Pop 16.00000 Soundtrack 10.75000 The following examples us the lahman.db sqlite database. dbListTables(lahman) ## [1] &quot;allstarfull&quot; &quot;appearances&quot; &quot;awardsmanagers&quot; ## [4] &quot;awardsplayers&quot; &quot;awardssharemanagers&quot; &quot;awardsshareplayers&quot; ## [7] &quot;batting&quot; &quot;battingpost&quot; &quot;collegeplaying&quot; ## [10] &quot;divisions&quot; &quot;fielding&quot; &quot;fieldingof&quot; ## [13] &quot;fieldingofsplit&quot; &quot;fieldingpost&quot; &quot;halloffame&quot; ## [16] &quot;homegames&quot; &quot;leagues&quot; &quot;managers&quot; ## [19] &quot;managershalf&quot; &quot;parks&quot; &quot;people&quot; ## [22] &quot;pitching&quot; &quot;pitchingpost&quot; &quot;salaries&quot; ## [25] &quot;schools&quot; &quot;seriespost&quot; &quot;teams&quot; ## [28] &quot;teamsfranchises&quot; &quot;teamshalf&quot; "],
["r.html", "R Getting started Variables Logical operators Lists &amp; Vectors Basic R functions Data.frames Reading &amp; Writing data Control flow Apply functions Writing functions Plotting RMarkdown Tidyverse data.table SQL in R Scraping shiny", " R Getting started Examples using the 84.51 data set. Please see https://piazza.com/class/kdrxb6dxa8c6by?cid=110 for example code, to go along with this video. Click here for video Please see https://piazza.com/class/kdrxb6dxa8c6by?cid=110 for example code, to go along with this video. We read in the data from the 8451 data set (This is not the same data set from Project 2! It is only intended to give you an idea about how to use basic functions in R!) The read.csv function is used to read in a data frame. The variable myDF will be a data frame that stores the data. myDF &lt;- read.csv(&quot;/class/datamine/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv&quot;) Please give the data frame a minute or two, to load. It is big! The data frame has 10625553 rows and 9 columns: dim(myDF) ## [1] 1000000 9 This is the data that describes the first 6 purchases: head(myDF) ## BASKET_NUM HSHD_NUM PURCHASE_ PRODUCT_NUM SPEND UNITS STORE_R WEEK_NUM YEAR ## 1 24 1809 03-JAN-16 5817389 -1.50 -1 SOUTH 1 2016 ## 2 24 1809 03-JAN-16 5829886 -1.50 -1 SOUTH 1 2016 ## 3 34 1253 03-JAN-16 539501 2.19 1 EAST 1 2016 ## 4 60 1595 03-JAN-16 5260099 0.99 1 WEST 1 2016 ## 5 60 1595 03-JAN-16 4535660 2.50 2 WEST 1 2016 ## 6 168 3393 03-JAN-16 5602916 4.50 1 SOUTH 1 2016 Similarly, these are the amounts spent on the first 6 purchases. We use the dollar sign to pull out a specific column of the data and focus (only) on that column. head(myDF$SPEND) ## [1] -1.50 -1.50 2.19 0.99 2.50 4.50 These first 6 values in the SPEND column add up to a total sum of 7.18 (you can check by hand if you like!) sum(head(myDF$SPEND)) ## [1] 7.18 The average of the first 6 values in the SPEND column is 1.196667 mean(head(myDF$SPEND)) ## [1] 1.196667 The first 100 values in the SPEND column are: head(myDF$SPEND, n=100) ## [1] -1.50 -1.50 2.19 0.99 2.50 4.50 3.49 2.79 1.00 9.98 1.29 1.79 ## [13] 3.99 1.00 2.00 10.80 3.49 1.00 3.99 1.88 0.49 2.49 1.99 2.50 ## [25] 1.67 1.99 5.50 7.89 6.49 1.00 2.78 3.69 1.19 0.69 3.00 5.99 ## [37] 8.19 3.49 4.29 5.66 0.99 5.99 0.99 8.11 12.82 7.99 4.19 1.49 ## [49] 4.96 3.49 4.49 2.79 2.99 5.49 3.99 12.00 3.79 0.89 4.99 2.29 ## [61] 1.69 5.78 6.99 2.00 3.89 6.77 2.69 4.99 3.20 14.40 6.93 2.50 ## [73] 1.00 5.98 1.75 1.19 4.25 3.00 1.11 0.98 8.17 13.10 17.98 4.38 ## [85] 5.79 3.59 4.99 11.56 3.42 2.99 17.99 1.50 -0.38 3.14 2.49 3.99 ## [97] 3.39 1.49 0.53 1.25 Note that, in the line above, we have an &quot;index&quot; at the far left-hand side of the Console. It shows the position of the first value on each line. The values will change, depending on how wide your screen is. Here is the 1st value in the SPEND column: myDF$SPEND[1] ## [1] -1.5 Here is the 22nd value in the SPEND column: myDF$SPEND[22] ## [1] 2.49 Here is the 25th value in the SPEND column: myDF$SPEND[25] ## [1] 1.67 Here are the last 20 values in the SPEND column. (Notice that we changed head to tail, since tail refers to the end rather than the start.) tail(myDF$SPEND, n=20) ## [1] 1.00 1.39 19.98 2.97 0.89 2.89 5.99 1.79 1.99 1.34 1.34 1.99 ## [13] 6.49 4.00 1.00 8.00 3.79 2.99 3.00 4.99 We can load the help menu for a function in R by using a question mark before the function name. It takes some time to get familiar with the style of the R help menus, but once you get comfortable reading the help pages, they are very helpful indeed! ?head We already took an average of the first 6 entries in the SPEND column. Now we can take an average of the entire SPEND column. mean(myDF$SPEND) ## [1] 3.584366 Again, here are the first six entries in the SPEND column. head(myDF$SPEND) ## [1] -1.50 -1.50 2.19 0.99 2.50 4.50 Suppose that we want to see which entires are bigger than 2 and which ones are smaller than 2. Here are the first six results: head(myDF$SPEND &gt; 2) ## [1] FALSE FALSE TRUE FALSE TRUE TRUE Now we can see what the actual values are. Here are the first 100 such values that are each bigger than 2. head(myDF$SPEND[myDF$SPEND &gt; 2], n=100) ## [1] 2.19 2.50 4.50 3.49 2.79 9.98 3.99 10.80 3.49 3.99 2.49 2.50 ## [13] 5.50 7.89 6.49 2.78 3.69 3.00 5.99 8.19 3.49 4.29 5.66 5.99 ## [25] 8.11 12.82 7.99 4.19 4.96 3.49 4.49 2.79 2.99 5.49 3.99 12.00 ## [37] 3.79 4.99 2.29 5.78 6.99 3.89 6.77 2.69 4.99 3.20 14.40 6.93 ## [49] 2.50 5.98 4.25 3.00 8.17 13.10 17.98 4.38 5.79 3.59 4.99 11.56 ## [61] 3.42 2.99 17.99 3.14 2.49 3.99 3.39 8.99 3.34 14.38 5.49 2.47 ## [73] 3.49 5.98 7.99 5.98 5.77 4.00 5.49 3.79 3.34 3.69 2.39 10.00 ## [85] 2.97 5.00 4.79 3.49 5.99 3.99 4.99 3.49 4.54 2.79 2.68 6.78 ## [97] 7.99 3.47 2.69 3.49 You might want to plot the first 50 values in the SPEND column: plot(head(myDF$SPEND, n=50)) If the result says Error in plot.new() : figure margins too large then you just need to make your plotting window a little bigger, so that R has room to make the plot, and then run the line again. There are 10625553 entries in the SPEND column: length(myDF$SPEND) ## [1] 1000000 This makes sense, because the data frame has 10625553 rows and 9 columns. dim(myDF) ## [1] 1000000 9 There are 6322739 entries larger than 2. length(myDF$SPEND[myDF$SPEND &gt; 2]) ## [1] 593322 There are 451155 entries larger than 10. length(myDF$SPEND[myDF$SPEND &gt; 10]) ## [1] 42202 There are 4197 entries less than -3. length(myDF$SPEND[myDF$SPEND &lt;= -3]) ## [1] 420 We encourage you to play with the data sets, and to learn how to work with the data, by trying things yourself, and by asking questions. We always welcome your questions, and we love for you to post questions on Piazza. This is a great way for the entire community to learn together! Examples using the New York City yellow taxi cab data set. Please see https://piazza.com/class/kdrxb6dxa8c6by?cid=110 for example code, to go along with this video. Click here for video This data set contains the information about the yellow taxi cab rides in New York City in June 2019. myDF &lt;- read.csv(&quot;/class/datamine/data/taxi/yellow/yellow_tripdata_2019-06.csv&quot;) Here is the information about the first 6 taxi cab rides. You need to imagine that your computer monitor is much, much wider than it actually is, so that your data has room to stretch out in 6 rows across your screen. Instead, right now, the data wraps around, a few columns at a time. This is probably obvious when you look at it. Each column has a column header. head(myDF) ## VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count ## 1 1 2019-06-01 00:55:13 2019-06-01 00:56:17 1 ## 2 1 2019-06-01 00:06:31 2019-06-01 00:06:52 1 ## 3 1 2019-06-01 00:17:05 2019-06-01 00:36:38 1 ## 4 1 2019-06-01 00:59:02 2019-06-01 00:59:12 0 ## 5 1 2019-06-01 00:03:25 2019-06-01 00:15:42 1 ## 6 1 2019-06-01 00:28:31 2019-06-01 00:39:23 2 ## trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID ## 1 0.0 1 N 145 145 ## 2 0.0 1 N 262 263 ## 3 4.4 1 N 74 7 ## 4 0.8 1 N 145 145 ## 5 1.7 1 N 113 148 ## 6 1.6 1 N 79 125 ## payment_type fare_amount extra mta_tax tip_amount tolls_amount ## 1 2 3.0 0.5 0.5 0.00 0 ## 2 2 2.5 3.0 0.5 0.00 0 ## 3 2 17.5 0.5 0.5 0.00 0 ## 4 2 2.5 1.0 0.5 0.00 0 ## 5 1 9.5 3.0 0.5 2.65 0 ## 6 1 9.5 3.0 0.5 1.00 0 ## improvement_surcharge total_amount congestion_surcharge ## 1 0.3 4.30 0.0 ## 2 0.3 6.30 2.5 ## 3 0.3 18.80 0.0 ## 4 0.3 4.30 0.0 ## 5 0.3 15.95 2.5 ## 6 0.3 14.30 2.5 The mean cost (i.e., the average cost) of a taxi cab ride in New York City in June 2019 is 19.74, i.e., almost 20 dollars. mean(myDF$total_amount) ## [1] 19.33511 The mean number of passengers in a taxi cab ride is 1.567322. mean(myDF$passenger_count) ## [1] 1.567329 We can use the table function to tabulate the results of the number of taxi cab rides, according to the passenger_count For instance, in this case, there are 128130 taxi cab rides with 0 passengers, there are 4854651 taxi cab rides with 1 passenger, there are 1061648 taxi cab rides with 2 passengers, etc. table(myDF$passenger_count) ## ## 0 1 2 3 4 5 6 7 8 9 ## 19336 697349 154878 43720 20051 39156 25497 8 3 2 We can look at each passenger_count for which the passenger_count equals 4. Of course, the results are all just the value 4! head(myDF$passenger_count[myDF$passenger_count == 4]) ## [1] 4 4 4 4 4 4 On a more interesting note, we can look at the total cost of a taxi cab ride with 4 passengers. The first 6 rides that (each) have 4 passengers have these 6 costs: head(myDF$total_amount[myDF$passenger_count == 4]) ## [1] 8.30 16.80 14.80 9.95 10.30 37.56 The average cost of a taxi cab ride with 4 passengers is 20.42111, i.e., just a little more than 20 dollars. mean(myDF$total_amount[myDF$passenger_count == 4]) ## [1] 19.73445 Altogether, our data set has 6941024 rows and 18 columns. dim(myDF) ## [1] 1000000 18 For this reason, the total_amount column has 6941024 entries. length(myDF$total_amount) ## [1] 1000000 The amounts of the first 6 taxi cab rides are: head(myDF$total_amount) ## [1] 4.30 6.30 18.80 4.30 15.95 14.30 These are the amounts of the first 6 taxi cab rides that each cost more than 100 dollars. head(myDF$total_amount[myDF$total_amount &gt; 100]) ## [1] 104.30 120.80 158.90 181.30 112.35 116.30 There are 16681 taxi cab rides that (each) cost more than 100 dollars. length(myDF$total_amount[myDF$total_amount &gt; 100]) ## [1] 2180 If we only include the taxi cab rides that (each) cost more than 100 dollars, the average number of passengers is 1.545051. mean(myDF$passenger_count[myDF$total_amount &gt; 100]) ## [1] 1.563303 There are 6941024 taxi cab rides altogether. length(myDF$passenger_count) ## [1] 1000000 If we ask for the length of the taxi cab rides with total_amount &gt; 100, we might expect to get a smaller number, but again we get 6941024. length(myDF$total_amount &gt; 100) ## [1] 1000000 This might be confusing at first, but we can look at the head of those results. This is a vector of 6941024 occurrences of TRUE and FALSE, one per taxi cab ride. head(myDF$total_amount &gt; 100) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE The way to find out that there are only 16681 taxi cab rides that cost more than 100 dollars is (as we did before) to use the TRUE values as an index into another vector, like this: length(myDF$total_amount[myDF$total_amount &gt; 100]) ## [1] 2180 or like this sum(myDF$total_amount &gt; 100) ## [1] 2180 In this latter method, we turn the TRUE values into 1's and the FALSE values into 0's (this happens automatically when we sum them up) and so we have 16681 values of 1's and the rest are 0's so the sum is 16681, just like we saw above. Variables NA NA stands for not available and, in general, represents a missing value or a lack of data. How do I tell if a value is NA? Click here for solution # Test if value is NA. value &lt;- NA is.na(value) ## [1] TRUE # Does is.nan return TRUE for NA? is.nan(value) ## [1] FALSE NaN NaN stands for not a number and, in general, is used for arithmetic purposes, for example, the result of 0/0. How do I tell if a value is NaN? Click here for solution # Test if a value is NaN. value &lt;- NaN is.nan(value) ## [1] TRUE value &lt;- 0/0 is.nan(value) ## [1] TRUE # Does is.na return TRUE for NaN? is.na(value) ## [1] TRUE NULL NULL represents the null object, and is often returned when we have undefined values. How do I tell if a value is NULL? Click here for solution # Test if a value is NaN. value &lt;- NULL is.null(value) ## [1] TRUE class(value) ## [1] &quot;NULL&quot; # Does is.na return TRUE for NULL? is.na(value) ## logical(0) Dates Date is a class which allows you to perform special operations like subtraction, where the number of days between dates are returned. Or addition, where you can add 30 to a Date and a Date is returned where the value is 30 days in the future. You will usually need to specify the format argument based on the format of your date strings. For example, if you had a string 07/05/1990, the format would be: %m/%d/%Y. If your string was 31-12-90, the format would be %d-%m-%y. Replace %d, %m, %Y, and %y according to your date strings. A full list of formats can be found here. How do I convert a string &quot;07/05/1990&quot; to a Date? Click here for solution my_string &lt;- &quot;07/05/1990&quot; my_date &lt;- as.Date(my_string, format=&quot;%m/%d/%Y&quot;) my_date ## [1] &quot;1990-07-05&quot; How do I convert a string &quot;31-12-1990&quot; to a Date? Click here for solution my_string &lt;- &quot;31-12-1990&quot; my_date &lt;- as.Date(my_string, format=&quot;%d-%m-%Y&quot;) my_date ## [1] &quot;1990-12-31&quot; How do I convert a string &quot;12-31-1990&quot; to a Date? Click here for solution my_string &lt;- &quot;12-31-1990&quot; my_date &lt;- as.Date(my_string, format=&quot;%m-%d-%Y&quot;) my_date ## [1] &quot;1990-12-31&quot; How do I convert a string &quot;31121990&quot; to a Date? Click here for solution my_string &lt;- &quot;31121990&quot; my_date &lt;- as.Date(my_string, format=&quot;%d%m%Y&quot;) my_date ## [1] &quot;1990-12-31&quot; Factors A factor is R's way of representing a categorical variable. There are entries in a factor (just like there are entries in a vector), but they are constrained to only be chosen from a specific set of values, called the levels of the factor. They are useful when a vector has only a few different values it could be, like &quot;Male&quot; and &quot;Female&quot; or &quot;A&quot;, &quot;B&quot;, or &quot;C&quot;. How do I test whether or not a vector is a factor? Click here for solution test_factor &lt;- factor(&quot;Male&quot;) is.factor(test_factor) ## [1] TRUE test_factor_vec &lt;- factor(c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;)) is.factor(test_factor_vec) ## [1] TRUE How do I convert a vector of strings to a factor? Click here for solution vec &lt;- c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;) vec &lt;- factor(c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;)) How do I get the unique values a factor could hold, also known as levels? Click here for solution vec &lt;- factor(c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;)) levels(vec) ## [1] &quot;Female&quot; &quot;Male&quot; How can I rename the levels of a factor? Click here for solution vec &lt;- factor(c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;)) levels(vec) ## [1] &quot;Female&quot; &quot;Male&quot; levels(vec) &lt;- c(&quot;F&quot;, &quot;M&quot;) vec ## [1] M F F ## Levels: F M # Be careful! Order matters, this is wrong: vec &lt;- factor(c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;)) levels(vec) ## [1] &quot;Female&quot; &quot;Male&quot; levels(vec) &lt;- c(&quot;M&quot;, &quot;F&quot;) vec ## [1] F M M ## Levels: M F How can I find the number of levels of a factor? Click here for solution vec &lt;- factor(c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;)) nlevels(vec) ## [1] 2 Logical operators Logical operators are symbols that can be used within R to compare values or vectors of values. Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == equal to != not equal to !x negation, not x x|y x OR y x&amp;y x AND y Examples What are the values in a vector, vec that are greater than 5? Click here for solution vec &lt;- 1:10 vec &gt; 5 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE What are the values in a vector, vec that are greater than or equal to 5? Click here for solution vec &lt;- 1:10 vec &gt;= 5 ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE What are the values in a vector, vec that are less than 5? Click here for solution vec &lt;- 1:10 vec &lt; 5 ## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE What are the values in a vector, vec that are less than or equal to 5? Click here for solution vec &lt;- 1:10 vec &lt;= 5 ## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE What are the values in a vector that are greater than 7 OR less than or equal to 2? Click here for solution vec &lt;- 1:10 vec &gt; 7 | vec &lt;=2 ## [1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE What are the values in a vector that are greater than 3 AND less than 6? Click here for solution vec &lt;- 1:10 vec &gt; 3 &amp; vec &lt; 6 ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE How do I get the values in list1 that are in list2? Click here for solution list1 &lt;- c(&quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;test&quot;) list2 &lt;- c(&quot;this&quot;, &quot;a&quot;, &quot;exam&quot;) list1[list1 %in% list2] ## [1] &quot;this&quot; &quot;a&quot; How do I get the values in list1 that are not in list2? Click here for solution list1 &lt;- c(&quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;test&quot;) list2 &lt;- c(&quot;this&quot;, &quot;a&quot;, &quot;exam&quot;) list1[!(list1 %in% list2)] ## [1] &quot;is&quot; &quot;test&quot; How can I get the number of values in a vector that are greater than 5? Click here for solution vec &lt;- 1:10 sum(vec&gt;5) ## [1] 5 # Note, you do not need to do: length(vec[vec&gt;5]) ## [1] 5 # because TRUE==1 and FALSE==0 in R TRUE==1 ## [1] TRUE FALSE==0 ## [1] TRUE Resources Operators Summary A quick list of the various operators with a few simple examples. Lists &amp; Vectors A vector contains values that are all the same type. The following are some examples of vectors: # A logical vector lvec &lt;- c(F, T, TRUE, FALSE) class(lvec) ## [1] &quot;logical&quot; # A numeric vector nvec &lt;- c(1,2,3,4) class(nvec) ## [1] &quot;numeric&quot; # A character vector cvec &lt;- c(&quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;test&quot;) class(cvec) ## [1] &quot;character&quot; As soon as you try to mix and match types, elements are coerced to the simplest type required to represent all the data. The order of representation is: logical, numeric, character, list For example: class(c(F, 1, 2)) ## [1] &quot;numeric&quot; class(c(F, 1, 2, &quot;ok&quot;)) ## [1] &quot;character&quot; class(c(F, 1, 2, &quot;ok&quot;, list(1, 2, &quot;ok&quot;))) ## [1] &quot;list&quot; Lists are vectors that can contain any class of data. For example: list(TRUE, 1, 2, &quot;OK&quot;, c(1,2,3)) ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] 1 ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] &quot;OK&quot; ## ## [[5]] ## [1] 1 2 3 With lists, there are 3 ways you can index. my_list &lt;- list(TRUE, 1, 2, &quot;OK&quot;, c(1,2,3), list(&quot;OK&quot;, 1,2, F)) # The first way is with single square brackets []. # This will always return a list, even if the content # only has 1 component. class(my_list[1:2]) ## [1] &quot;list&quot; class(my_list[3]) ## [1] &quot;list&quot; # The second way is with double brackets [[]]. # This will return the content itself. If the # content is something other than a list it will # return the value itself. class(my_list[[1]]) ## [1] &quot;logical&quot; class(my_list[[3]]) ## [1] &quot;numeric&quot; # Of course, if the value is a list itself, it will # remain a list. class(my_list[[6]]) ## [1] &quot;list&quot; # The third way is using $ to extract a single, named variable. # We need to add names first! $ is like the double bracket, # in that it will return the simplest form. my_list &lt;- list(first=TRUE, second=1, third=2, fourth=&quot;OK&quot;, embedded_vector=c(1,2,3), embedded_list=list(&quot;OK&quot;, 1,2, F)) my_list$first ## [1] TRUE my_list$embedded_list ## [[1]] ## [1] &quot;OK&quot; ## ## [[2]] ## [1] 1 ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] FALSE How do get the type of a vector? Click here for solution my_vector &lt;- c(0, 1, 2) typeof(my_vector) ## [1] &quot;double&quot; How do I convert a character vector to a numeric? Click here for solution my_character_vector &lt;- c(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;) as.numeric(my_character_vector) ## [1] 1 2 3 4 How do I convert a numeric vector to a character? Click here for solution my_numeric_vector &lt;- c(1,2,3,4) as.character(my_numeric_vector) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; Indexing Indexing enables us to access a subset of the elements in vectors and lists. There are three types of indexing: positional/numeric, logical, and reference/named. You can create a named vector and a named list easily: my_vec &lt;- 1:5 names(my_vec) &lt;- c(&quot;alpha&quot;,&quot;bravo&quot;,&quot;charlie&quot;,&quot;delta&quot;,&quot;echo&quot;) my_list &lt;- list(1,2,3,4,5) names(my_list) &lt;- c(&quot;alpha&quot;,&quot;bravo&quot;,&quot;charlie&quot;,&quot;delta&quot;,&quot;echo&quot;) my_list2 &lt;- list(&quot;alpha&quot; = 1, &quot;beta&quot; = 2, &quot;charlie&quot; = 3, &quot;delta&quot; = 4, &quot;echo&quot; = 5) # Numeric (positional) indexing: my_vec[1:2] ## alpha bravo ## 1 2 my_vec[c(1,3)] ## alpha charlie ## 1 3 my_list[1:2] ## $alpha ## [1] 1 ## ## $bravo ## [1] 2 my_list[c(1,3)] ## $alpha ## [1] 1 ## ## $charlie ## [1] 3 # Logical indexing: my_vec[c(T, F, T, F, F)] ## alpha charlie ## 1 3 my_list[c(T, F, T, F, F)] ## $alpha ## [1] 1 ## ## $charlie ## [1] 3 # Named (reference) indexing: # if there are named values: my_vec[c(&quot;alpha&quot;, &quot;charlie&quot;)] ## alpha charlie ## 1 3 my_list[c(&quot;alpha&quot;, &quot;charlie&quot;)] ## $alpha ## [1] 1 ## ## $charlie ## [1] 3 Examples How can I get the first 2 values of a vector named my_vec? Click here for solution my_vec &lt;- c(1, 13, 2, 9) names(my_vec) &lt;- c(&#39;cat&#39;, &#39;dog&#39;,&#39;snake&#39;, &#39;otter&#39;) my_vec[1:2] ## cat dog ## 1 13 How can I get the values that are greater than 2? Click here for solution my_vec[my_vec&gt;2] ## dog otter ## 13 9 How can I get the values greater than 5 and smaller than 10? Click here for solution my_vec[my_vec &gt; 5 &amp; my_vec &lt; 10] ## otter ## 9 How can I get the values greater than 10 or smaller than 3? Click here for solution my_vec[my_vec &gt; 10 | my_vec &lt; 3] ## cat dog snake ## 1 13 2 How can I get the values for &quot;otter&quot; and &quot;dog&quot;? Click here for solution my_vec[c(&#39;otter&#39;,&#39;dog&#39;)] ## otter dog ## 9 13 Recycling Often operations in R on two or more vectors require them to be the same length. When R encounters vectors with different lengths, it automatically repeats (recycles) the shorter vector until the length of the vectors is the same. Examples Given two numeric vectors with different lengths, add them element-wise. Click here for solution x &lt;- c(1,2,3) y &lt;- c(0,1) x+y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 1 3 3 Basic R functions all all returns a logical value (TRUE or FALSE) if all values in a vector are TRUE. Examples Are all values in x positive? Click here for solution x &lt;- c(1, 2, 3, 4, 8, -1, 7, 3, 4, -2, 1, 3) all(x&gt;0) # FALSE ## [1] FALSE any any returns a logical value (TRUE or FALSE) if any values in a vector are TRUE. Examples Are any values in x positive? Click here for solution x &lt;- c(1, 2, 3, 4, 8, -1, 7, 3, 4, -2, 1, 3) any(x&gt;0) # TRUE ## [1] TRUE all.equal all.equal compares two objects and tests if they are &quot;nearly equal&quot; (up to some provided tolerance). Examples Is \\(\\pi\\) equal to 3.14? Click here for solution all.equal(pi, 3.14) # FALSE ## [1] &quot;Mean relative difference: 0.0005069574&quot; Is \\(\\pi\\) equal to 3.14 if our tolerance is 2 decimal cases? Click here for solution all.equal(pi, 3.14, tol=0.01) # TRUE ## [1] TRUE Are the vectors x and y equal? Click here for solution x &lt;- 1:5 y &lt;- c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;) all.equal(x, y) # difference in type (numeric vs. character) ## [1] &quot;Modes: numeric, character&quot; ## [2] &quot;target is numeric, current is character&quot; all.equal(x, as.numeric(y)) # TRUE ## [1] TRUE %in% Although %in% doesn't look like it, it is a function. Given two vectors, %in% returns a logical vector indicating if the respective values in the left operand have a match in the right operand. You can learn more about %in% by running ?&quot;%in%&quot;. Examples How do I find whether or not a value, 5 is in a given vector? Click here for solution 5 %in% c(1,2,3) ## [1] FALSE 5 %in% c(3,4,5) ## [1] TRUE How can I find which values in one vector are present in another? Click here for solution c(1,2,3) %in% c(1,2) c(1,2,3) %in% c(3,4,5) # order doesn&#39;t matter for the right operand c(1,2,3) %in% c(5,3,4) setdiff Given two vectors, the function setdiff returns the element of the first vector which do not exist in the second vector. Note: The order in which the vectors are listed in relation to the function setdiff matters, as illustrated in the first two examples. Examples Let x = (a, b, b, c) and y = (c, b, d, e, f). How to I find the elements in vector x that are not in vector y? Click here for solution x &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;) y &lt;- c(&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;) setdiff(x,y) ## [1] &quot;a&quot; setdiff(y,x) ## [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; How to I find the elements in vector y that are not in vector x? Click here for solution x &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;) y &lt;- c(&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;) setdiff(y,x) ## [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; intersect The intersect function returns the elements that two vectors or data.frames have in common. Note: The order in which the vectors are listed in relation to the function intersect only affects the order of the common elements returned. Examples Let x = (a, b, b, c) and y = (c, b, d, e, f). How to I find the elements shared both by vector x and by vector y? Click here for solution x &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;) y &lt;- c(&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;) intersect(x,y) ## [1] &quot;b&quot; &quot;c&quot; # as you can see, reversing the order # of the arguments only changes the order # in which the results are in the returned vector intersect(y,x) ## [1] &quot;c&quot; &quot;b&quot; dim dim returns the dimensions of a matrix or data.frame. The first value is the rows, the second is columns. Examples How many dimensions does the data.frame dat have? Click here for solution dat &lt;- data.frame(&quot;col1&quot;=c(1,2,3), &quot;col2&quot;=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) dim(dat) # 3 rows and 2 columns ## [1] 3 2 length length allows you to get or set the length of an object in R (for which a method has been defined). How do I get how many values are in a vector? Click here for solution # Create a vector of length 5 my_vector &lt;- c(1,2,3,4,5) # Calculate the length of my_vector length(my_vector) ## [1] 5 rep rep is short for replicate. rep accepts some object, x, and up to three additional arguments: times, length.out, and each. times is the number of non-negative times to repeat the whole object x. length.out specifies the end length you want the result to be. rep will repeat the values in x as many times as it takes to reach the provided length.out. each repeats each element in x the number of times specified by each. Examples How do I repeat values in a vector 3 times? Click here for solution vec &lt;- c(1,2,3) rep(vec, 3) ## [1] 1 2 3 1 2 3 1 2 3 # or rep(vec, times=3) ## [1] 1 2 3 1 2 3 1 2 3 How do I repeat the values in a vector enough times to be the same length as another vector? Click here for solution vec &lt;- c(1,2,3) other_vec &lt;- c(1,2,2,2,2,2,2,8) rep(vec, length.out=length(other_vec)) ## [1] 1 2 3 1 2 3 1 2 # Note that if the end goal is to do something # like add the two vectors, this can be done # using recycling. rep(vec, length.out=length(other_vec)) + other_vec ## [1] 2 4 5 3 4 5 3 10 vec + other_vec ## Warning in vec + other_vec: longer object length is not a multiple of shorter ## object length ## [1] 2 4 5 3 4 5 3 10 How can I repeat each value inside a vector a certain amount of times? Click here for solution vec &lt;- c(1,2,3) rep(vec, each=3) ## [1] 1 1 1 2 2 2 3 3 3 How can I repeat the values in one vector based on the values in another vector? Click here for solution vec &lt;- c(1,2,3) rep_by &lt;- c(3,2,1) rep(vec, times=rep_by) ## [1] 1 1 1 2 2 3 rbind and cbind rbind and cbind append objects (vectors, matrices or data.frames) as rows (rbind) or as columns (cbind). Examples How do I combine 3 vectors into a matrix? Click here for solution x &lt;- 1:10 y &lt;- 11:20 z &lt;- 10:1 # combining them as rows rbind(x,y,z) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## x 1 2 3 4 5 6 7 8 9 10 ## y 11 12 13 14 15 16 17 18 19 20 ## z 10 9 8 7 6 5 4 3 2 1 dim(rbind(x,y,z)) ## [1] 3 10 # combining them as columns cbind(x,y,z) ## x y z ## [1,] 1 11 10 ## [2,] 2 12 9 ## [3,] 3 13 8 ## [4,] 4 14 7 ## [5,] 5 15 6 ## [6,] 6 16 5 ## [7,] 7 17 4 ## [8,] 8 18 3 ## [9,] 9 19 2 ## [10,] 10 20 1 dim(cbind(x,y,z)) ## [1] 10 3 How do I add a vector as a column to a matrix? Click here for solution x &lt;- 1:10 my_mat &lt;- matrix(1:20, ncol=2) my_mat &lt;- cbind(my_mat, x) dim(my_mat) ## [1] 10 3 How do I append new rows to a matrix? Click here for solution my_mat1 &lt;- matrix(20:1, ncol=2) my_mat2 &lt;- matrix(1:20, ncol=2) my_mat &lt;- rbind(my_mat1, my_mat2) dim(my_mat) ## [1] 20 2 which, which.max, which.min which enables you to find the position of the elements that are TRUE in a logical vector. which.max and which.min finds the location of the maximum and minimum, respectively, of a numeric (or logical) vector. Examples Given a numeric vector, return the index of the maximum value. Click here for solution x &lt;- c(1,-10, 2,4,-3,9,2,-2,4,8) which.max(x) ## [1] 6 # which.max is just shorthand for: which(x==max(x)) ## [1] 6 Given a vector, return the index of the positive values. Click here for solution x &lt;- c(1,-10, 2,4,-3,9,2,-2,4,8) which(x&gt;0) ## [1] 1 3 4 6 7 9 10 Given a matrix, return the indexes (row and column) of the positive values. Click here for solution x &lt;- matrix(c(1,-10, 2,4,-3,9,2,-2,4,8), ncol=2) which(x&gt;0, arr.ind = TRUE) ## row col ## [1,] 1 1 ## [2,] 3 1 ## [3,] 4 1 ## [4,] 1 2 ## [5,] 2 2 ## [6,] 4 2 ## [7,] 5 2 grep, grepl, etc. grep allows you to use regular expressions to search for a pattern in a string or character vector, and returns the index where there is a match. grepl performs the same operation but rather than returning indices, returns a vector of logical TRUE or FALSE values. Examples Given a character vector, return the index of any words ending in &quot;s&quot;. Click here for solution grep(&quot;.*s$&quot;, c(&quot;waffle&quot;, &quot;waffles&quot;, &quot;pancake&quot;, &quot;pancakes&quot;)) ## [1] 2 4 Given a character vector, return a vector of the same length where each element is TRUE if there was a match for any word ending in &quot;s&quot;, and `FALSE otherwise. Click here for solution grepl(&quot;.*s$&quot;, c(&quot;waffle&quot;, &quot;waffles&quot;, &quot;pancake&quot;, &quot;pancakes&quot;)) ## [1] FALSE TRUE FALSE TRUE Resources ReExCheatsheet An excellent quick reference for regular expressions. Examples using grep in R. sum sum is a function that calculates the sum of a vector of values. Examples How do I get the sum of the values in a vector? Click here for solution sum(c(1,3,2,10,4)) ## [1] 20 How do I get the sum of the values in a vector when some of the values are: NA, NaN? Click here for solution sum(c(1,2,3,NaN), na.rm=T) ## [1] 6 sum(c(1,2,3,NA), na.rm=T) ## [1] 6 sum(c(1,2,NA,NaN,4), na.rm=T) ## [1] 7 mean mean is a function that calculates the average of a vector of values. How do I get the average of a vector of values? Click here for solution mean(c(1,2,3,4)) ## [1] 2.5 How do I get the average of a vector of values when some of the values are: NA, NaN? Click here for solution Many R functions have the na.rm argument available. This argument is &quot;a logical value indicating whether NA values should be stripped before the computation proceeds.&quot; mean(c(1,2,3,NaN), na.rm=T) ## [1] 2 mean(c(1,2,3,NA), na.rm=T) ## [1] 2 mean(c(1,2,NA,NaN,4), na.rm=T) ## [1] 2.333333 var var is a function that calculate the variance of a vector of values. How do I get the variance of a vector of values? Click here for solution var(c(1,2,3,4)) ## [1] 1.666667 How do I get the variance of a vector of values when some of the values are: NA, NaN? Click here for solution var(c(1,2,3,NaN), na.rm=T) ## [1] 1 var(c(1,2,3,NA), na.rm=T) ## [1] 1 var(c(1,2,NA,NaN,4), na.rm=T) ## [1] 2.333333 How do I get the standard deviation of a vector of values? Click here for solution The standard deviation is equal to the square root of the variance. sqrt(var(c(1,2,3,NaN), na.rm=T)) ## [1] 1 sqrt(var(c(1,2,3,NA), na.rm=T)) ## [1] 1 sqrt(var(c(1,2,NA,NaN,4), na.rm=T)) ## [1] 1.527525 colSums and rowSums colSums and rowSums calculates row and column sums for numeric matrices or data.frames. Examples How do I get the sum of the values for every column in a data frame? Click here for solution # First 6 values in mtcars head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # For every column, sum of all rows: colSums(mtcars) ## mpg cyl disp hp drat wt qsec vs ## 642.900 198.000 7383.100 4694.000 115.090 102.952 571.160 14.000 ## am gear carb ## 13.000 118.000 90.000 How do I get the sum of the values for every row in a data frame? Click here for solution # First 6 values in mtcars head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # For every row, sum of all columns: rowSums(mtcars) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 328.980 329.795 259.580 426.135 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## 590.310 385.540 656.920 270.980 ## Merc 230 Merc 280 Merc 280C Merc 450SE ## 299.570 350.460 349.660 510.740 ## Merc 450SL Merc 450SLC Cadillac Fleetwood Lincoln Continental ## 511.500 509.850 728.560 726.644 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## 725.695 213.850 195.165 206.955 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## 273.775 519.650 506.085 646.280 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## 631.175 208.215 272.570 273.683 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## 670.690 379.590 694.710 288.890 colMeans and rowMeans colMeans and rowMeans calculates row and column means for numeric matrices or data.frames. Examples Examples How do I get the mean for every column in a data frame? Click here for solution # First 6 values in mtcars head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Mean of each column colMeans(mtcars) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 How do I get the mean for every row in a data frame? Click here for solution # First 6 values in mtcars head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Mean of each row rowMeans(mtcars) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 29.90727 29.98136 23.59818 38.73955 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## 53.66455 35.04909 59.72000 24.63455 ## Merc 230 Merc 280 Merc 280C Merc 450SE ## 27.23364 31.86000 31.78727 46.43091 ## Merc 450SL Merc 450SLC Cadillac Fleetwood Lincoln Continental ## 46.50000 46.35000 66.23273 66.05855 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## 65.97227 19.44091 17.74227 18.81409 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## 24.88864 47.24091 46.00773 58.75273 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## 57.37955 18.92864 24.77909 24.88027 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## 60.97182 34.50818 63.15545 26.26273 unique unique &quot;returns a vector, data frame, or array like x but with duplicate elements/rows removed. Given a vector of values, how do I return a vector of values with all duplicates removed? Click here for solution vec &lt;- c(1, 2, 3, 3, 3, 4, 5, 5, 6) unique(vec) ## [1] 1 2 3 4 5 6 summary summary shows summary statistics for a vector, or for every column in a data.frame and/or matrix. The summary statistics shown are: mininum value, maximum value, first and third quartiles, mean and median. Examples How do I get summary statistics for a vector? Click here for solution summary(1:30) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 8.25 15.50 15.50 22.75 30.00 How do I get summary statistics for every column in a data frame? Click here for solution # First 6 values in mtcars head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Mean of each column summary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 order and sort sort allows you to arrange (or partially arrange) a vector into ascending or descending order. order returns the position of each element of a vector in ascending (or descending order). Examples Given a vector, arrange it in a ascending order. Click here for solution x &lt;- c(1,3,2,10,4) sort(x) ## [1] 1 2 3 4 10 Given a vector, arrange it in a descending order. Click here for solution x &lt;- c(1,3,2,10,4) sort(x, decreasing = TRUE) ## [1] 10 4 3 2 1 Given a character vector, arrange it in ascending order. Click here for solution sort(c(&quot;waffle&quot;, &quot;pancake&quot;, &quot;eggs&quot;, &quot;bacon&quot;)) ## [1] &quot;bacon&quot; &quot;eggs&quot; &quot;pancake&quot; &quot;waffle&quot; Given a matrix, arrange it in ascending order using the first column. Click here for solution my_mat &lt;- matrix(c(1,5,0, 2, 10, 1, 2, 8, 9, 1,0,2), ncol=3) my_mat[order(my_mat[,1]),] ## [,1] [,2] [,3] ## [1,] 0 2 0 ## [2,] 1 10 9 ## [3,] 2 8 2 ## [4,] 5 1 1 paste and paste0 paste is a useful function to &quot;concatenate vectors after converting to character.&quot; paste0 is a shorthand function where the sep argument is &quot;&quot;. How do I concatenate two vectors, element-wise, with a comma in between values from each vector? Click here for solution vector1 &lt;- c(&quot;one&quot;, &quot;three&quot;, &quot;five&quot;) vector2 &lt;- c(&quot;two&quot;, &quot;four&quot;, &quot;six&quot;) paste(vector1, vector2, sep=&quot;,&quot;) ## [1] &quot;one,two&quot; &quot;three,four&quot; &quot;five,six&quot; How do I paste together two strings? Click here for solution paste0(&quot;abra&quot;, &quot;kadabra&quot;) ## [1] &quot;abrakadabra&quot; How do I paste together three strings? Click here for solution paste0(&quot;abra&quot;, &quot;kadabra&quot;, &quot;alakazam&quot;) ## [1] &quot;abrakadabraalakazam&quot; head and tail head returns the first n (default is 6) parts of a vector, matrix, table, data.frame or function. For vectors, head shows the first 6 values, for matrices, tables and data.frame, head shows the first 6 rows, and for functions the first 6 rows of code. tail returns the last n (default is 6) parts of a vector, matrix, table, data.frame or function. Examples How do I get the first 6 rows of a data.frame? Click here for solution head(df) ## ## 1 function (x, df1, df2, ncp, log = FALSE) ## 2 { ## 3 if (missing(ncp)) ## 4 .Call(C_df, x, df1, df2, log) ## 5 else .Call(C_dnf, x, df1, df2, ncp, log) ## 6 } How do I get the first 10 rows of a data.frame? Click here for solution head(df, 10) ## ## 1 function (x, df1, df2, ncp, log = FALSE) ## 2 { ## 3 if (missing(ncp)) ## 4 .Call(C_df, x, df1, df2, log) ## 5 else .Call(C_dnf, x, df1, df2, ncp, log) ## 6 } How do I get the last 6 rows of a data.frame? Click here for solution tail(df) ## ## 1 function (x, df1, df2, ncp, log = FALSE) ## 2 { ## 3 if (missing(ncp)) ## 4 .Call(C_df, x, df1, df2, log) ## 5 else .Call(C_dnf, x, df1, df2, ncp, log) ## 6 } How do I get the last 8 rows of a data.frame? Click here for solution tail(df, 8) ## ## 1 function (x, df1, df2, ncp, log = FALSE) ## 2 { ## 3 if (missing(ncp)) ## 4 .Call(C_df, x, df1, df2, log) ## 5 else .Call(C_dnf, x, df1, df2, ncp, log) ## 6 } str str stands for structure. str gives you a glimpse at the variable of interest. Examples How do I get the number of columns or features in a data.frame? Click here for solution As you can see, there are 9 rows or obs. (short for observations), and 29 variables (which can be referred to as columns or features). str(df) strsplit strsplit accepts a vector of strings, and a vector of strings representing regular expressions. Each string in the first vector is split according to the respective string in the second vector. Examples How do I split a string containing multiple sentences into individual sentences? Click here for solution Note that you need to escape the &quot;.&quot; as &quot;.&quot; means &quot;any character&quot; in regular expressions. In R, you put two &quot;&quot; before it. multiple_sentences &lt;- &quot;This is the first sentence. This is the second sentence. This is the third sentence.&quot; unlist(strsplit(multiple_sentences, &quot;\\\\.&quot;)) ## [1] &quot;This is the first sentence&quot; &quot; This is the second sentence&quot; ## [3] &quot; This is the third sentence&quot; # remove extra whitespace trimws(unlist(strsplit(multiple_sentences, &quot;\\\\.&quot;))) ## [1] &quot;This is the first sentence&quot; &quot;This is the second sentence&quot; ## [3] &quot;This is the third sentence&quot; How do I split one string by a space, and the next string by a &quot;.&quot;? Click here for solution string_vec &lt;- c(&quot;Okay okay you win.&quot;, &quot;This. Is. Not. Okay.&quot;) strsplit(string_vec, c(&quot; &quot;, &quot;\\\\.&quot;)) ## [[1]] ## [1] &quot;Okay&quot; &quot;okay&quot; &quot;you&quot; &quot;win.&quot; ## ## [[2]] ## [1] &quot;This&quot; &quot; Is&quot; &quot; Not&quot; &quot; Okay&quot; names names is a function that returns the names of a an object. This includes the typical data structures: vectors, lists, and data.frames. By default, names will return the column names of a data.frame, not the row names. Examples How do I get the column names of a data.frame? Click here for solution # Get the column names of a data.frame names(df) ## [1] &quot;cat_1&quot; &quot;cat_2&quot; &quot;ok&quot; &quot;other&quot; How do I get the names of a list? Click here for solution # Get the names of a list names(list(col1=c(1,2,3), col2=c(987))) ## [1] &quot;col1&quot; &quot;col2&quot; How do I get the names of a vector? Click here for solution # Get the names of a vector names(c(val1=1, val2=2, val3=3)) ## [1] &quot;val1&quot; &quot;val2&quot; &quot;val3&quot; How do I change the column names of a data.frame? Click here for solution names(df) &lt;- c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;, &quot;col4&quot;) df ## col1 col2 col3 col4 ## 1 1 9 TRUE first ## 2 2 8 TRUE second ## 3 3 7 FALSE third colnames &amp; rownames colnames is the same as names but specifies the column names. rownames is the same as names but specifies the row names. table &amp; prop.table table is a function used to build a contingency table of counts of various factors. prop.table is a function that accepts the output of table and rather than returning counts, returns conditional proportions. Examples How do I get a count of the number of students in each year in our grades data.frame? Click here for solution table(grades$year) ## ## freshman junior senior sophomore ## 1 4 2 3 How do I get the precentages of students in each year in our grades data.frame? Click here for solution prop.table(table(grades$year)) ## ## freshman junior senior sophomore ## 0.1 0.4 0.2 0.3 How do I get a count of the number of students in each year by sex in our grades data.frame? Click here for solution table(grades$year, grades$sex) ## ## F M ## freshman 0 1 ## junior 2 2 ## senior 1 1 ## sophomore 1 2 How do I get the precentages of students in each year by sex in our grades data.frame? Click here for solution prop.table(table(grades$year, grades$sex)) ## ## F M ## freshman 0.0 0.1 ## junior 0.2 0.2 ## senior 0.1 0.1 ## sophomore 0.1 0.2 cut cut breaks a vector x into factors specified by the argument breaks. cut is particularly useful to break Date data into categories like &quot;Q1&quot;, &quot;Q2&quot;, or 1998, 1999, 2000, etc. You can find more useful information by running ?cut.POSIXt. Examples How can I create a new column in a data.frame df that is a factor based on the year? Click here for solution df$year &lt;- cut(df$times, breaks=&quot;year&quot;) str(df) ## &#39;data.frame&#39;: 24 obs. of 3 variables: ## $ times: POSIXct, format: &quot;2020-06-01 06:00:00&quot; &quot;2020-07-01 06:00:00&quot; ... ## $ value: int 96 35 77 61 13 16 48 55 19 70 ... ## $ year : Factor w/ 3 levels &quot;2020-01-01&quot;,&quot;2021-01-01&quot;,..: 1 1 1 1 1 1 1 2 2 2 ... How can I create a new column in a data.frame df that is a factor based on the quarter? Click here for solution df$quarter &lt;- cut(df$times, breaks=&quot;quarter&quot;) str(df) ## &#39;data.frame&#39;: 24 obs. of 4 variables: ## $ times : POSIXct, format: &quot;2020-06-01 06:00:00&quot; &quot;2020-07-01 06:00:00&quot; ... ## $ value : int 96 35 77 61 13 16 48 55 19 70 ... ## $ year : Factor w/ 3 levels &quot;2020-01-01&quot;,&quot;2021-01-01&quot;,..: 1 1 1 1 1 1 1 2 2 2 ... ## $ quarter: Factor w/ 9 levels &quot;2020-04-01&quot;,&quot;2020-07-01&quot;,..: 1 2 2 2 3 3 3 4 4 4 ... How can I create a new column in a data.frame df that is a factor based on every 2 weeks? Click here for solution df$biweekly &lt;- cut(df$times, breaks=&quot;2 weeks&quot;) For an example with the 7581 data set: myDF &lt;- read.csv(&quot;/class/datamine/data/fars/7581.csv&quot;) These are the values of the HOUR column: table(myDF$HOUR) We can break these values into 6-hour intervals: table( cut(myDF$HOUR, breaks=c(0,6,12,18,24,99), include.lowest=T) ) and then find the total number of PERSONS who are involved in accidents during each 6-hour interval tapply( myDF$PERSONS, cut(myDF$HOUR, breaks=c(0,6,12,18,24,99), include.lowest=T), sum ) Click here for video subset subset is a function that helps you take subsets of data. By default, subset removes NA rows, so use with care. subset does not perform any operation that can't be accomplished by indexing, but can sometimes be easier to read. Where we would normally write something like: grades[grades$year==&quot;junior&quot; | grades$sex==&quot;M&quot;,]$grade ## [1] 100 75 74 69 88 99 90 92 We can instead do: subset(grades, year==&quot;junior&quot; | sex==&quot;M&quot;, select=grade) ## grade ## 1 100 ## 3 75 ## 4 74 ## 6 69 ## 7 88 ## 8 99 ## 9 90 ## 10 92 But be careful, if we replace a grade with an NA, it will be removed by subset: grades$sex[8] &lt;- NA subset(grades, year==&quot;junior&quot; | sex==&quot;M&quot;, select=grade) ## grade ## 1 100 ## 3 75 ## 4 74 ## 6 69 ## 7 88 ## 9 90 ## 10 92 Whereas indexing will not unless you specify to: grades[grades$year==&quot;junior&quot; | grades$sex==&quot;M&quot;,]$grade ## [1] 100 75 74 69 88 NA 90 92 How can I easily make a subset of the 8451 data, using only 1 line of R, with the subset function? Click here for video In the 84.51 data set: myDF &lt;- read.csv(&quot;/class/datamine/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv&quot;) We recall that these are the variables: head(myDF) and there are 10625553 rows and 9 columns dim(myDF) We can use the subset command to focus on only the purchases from the CENTRAL store region, in the YEAR 2016. We can also pick which variables that we want to have in this new data frame. Please note: We do not need to specify myDF on each variable, because the subset function will keep track of this for us. The subset function knows which data set that we are working with, because we specify it as the first parameter in the subset function. The subset parameter of the subset function describes the rows that we are interested in. (In particular, we specify the conditions that we want the rows to satisfy.) The select parameter of the subset function describes the columns that we are interested in. (We list the columns by their names, and we need to put each such column name in double quotes.) myfocusedDF &lt;- subset(myDF, subset=(STORE_R==&quot;CENTRAL&quot;) &amp; (YEAR==2016), select=c(&quot;PURCHASE_&quot;,&quot;PRODUCT_NUM&quot;,&quot;SPEND&quot;,&quot;UNITS&quot;) ) This new data set has only 1246144 rows, i.e., about 12 percent of the purchases, as expected. It also has only the 4 columns that we specified in the subset function. dim(myfocusedDF) How can I easily make a subset of the election data, using only 1 line of R, with the subset function? Click here for video Here is an example of how to use the subset function with the data from the federal election campaign contributions from 2016: library(data.table) myDF &lt;- fread(&quot;/class/datamine/data/election/itcont2016.txt&quot;, sep=&quot;|&quot;) There were 20557796 donations made in 2016: dim(myDF) We can use the subset command to focus on the donations made from Midwest states, and limit our results to those donations that had positive TRANSACTION_AMT values. We can extract interesting variables, e.g., the NAME, CITY, STATE, and TRANSACTION_AMT. mymidwestDF &lt;- subset(myDF, subset=(STATE %in% c(&quot;IN&quot;,&quot;IL&quot;,&quot;OH&quot;,&quot;MI&quot;,&quot;WI&quot;)) &amp; (TRANSACTION_AMT &gt; 0), select=c(&quot;NAME&quot;,&quot;CITY&quot;,&quot;STATE&quot;,&quot;TRANSACTION_AMT&quot;) ) The resulting data frame has 2435825 rows. dim(mymidwestDF) From the data set, we can sum the TRANSACTION_AMT values, grouped according to the NAME of the donor, and we find that EYCHANER, FRED was the top donor living in the midwest, during the 2016 federal election campaigns. tail(sort(tapply(mymidwestDF$TRANSACTION_AMT, mymidwestDF$NAME, sum))) difftime {r#-difftime} The function difftime computes/creates a time interval between two dates/times and converts the interval to a chosen time unit. Examples How many days,hours and minutes are there between the dates 2015-04-06 and 2015-01-01? Click here for solution # number of days difftime(ymd(&quot;2015-04-06&quot;),ymd(&quot;2015-01-01&quot;), units=&quot;days&quot;) # number of hours difftime(ymd(&quot;2015-04-06&quot;),ymd(&quot;2015-01-01&quot;), units=&quot;hours&quot;) # number of minutes difftime(ymd(&quot;2015-04-06&quot;),ymd(&quot;2015-01-01&quot;), units=&quot;mins&quot;) merge merge is a function that can be used to combine data.frames by row names, or more commonly, by column names. merge can replicate the join operations in SQL. The documentation is quite clear, and a useful resource: ?merge. How can I easily merge the fars data with the state_names data, using only 1 line of R, with the merge function? Click here for video In STAT 19000, Project 6, we used the state_names data frame, to change the codes for the State's names into the State's actual names. We gave you the code to do so (in Question 1 of Project 6). It is easier, however, to use the merge function. dat &lt;- read.csv(&quot;/class/datamine/data/fars/7581.csv&quot;) state_names &lt;- read.csv(&quot;/class/datamine/data/fars/states.csv&quot;) We look at the heads of both data frames. head(dat) head(state_names) The STATE column of the dat data frame corresponds to the code column of the state_names data frame. Now we merge these two data frames, by corresponding values from this column. We call resulting data frame mynewDF mynewDF &lt;- merge(dat,state_names,by.x=&quot;STATE&quot;,by.y=&quot;code&quot;) The new column, called state (not to be confused with STATE) is the rightmost column in this new data frame. head(mynewDF) Now we can solve Project 6, Question 2, using this new data frame. sort(tapply(mynewDF$DRUNK_DR, mynewDF$state, mean)) How can I easily merge the data about flights with the data about the airports themselves, using only 1 line of R, with the merge function? Click here for video Here is the flight data from 1995. Notice that, for instance, the locations of the airports are not given. We only know the airport Origin and Dest codes. myDF &lt;- read.csv(&quot;/class/datamine/data/flights/subset/1995.csv&quot;) Here is a listing of the information about the airports themselves: airportsDF &lt;- read.csv(&quot;/class/datamine/data/flights/subset/airports.csv&quot;) We see that the 3-letter codes about the airports are given in the Origin and Dest columns of myDF. head(myDF) It is harder to tell which column in the airportsDF gives the 3-letter codes, but these are the iata codes head(airportsDF) It is perhaps easier to see this from the tale of airportsDF: tail(airportsDF) Now we merge the two data frames, and we display the information about the Origin airport, by linking the Origin column of myDF with the iata column of airportsDF: mynewDF &lt;- merge(myDF, airportsDF, by.x=&quot;Origin&quot;, by.y=&quot;iata&quot;) The resulting data frame has the same size as myDF: dim(myDF) dim(mynewDF) but now has extra columns, namely, with information about the Origin airport: head(mynewDF) tail(mynewDF) So now we can do things like calculating a sum of all Distances of flights with Origin in each state: sort(tapply( mynewDF$Distance, mynewDF$state, sum )) Here is another merge example: Examples Consider the data.frame's books and authors: books ## id title author_id rating ## 1 1 Harry Potter and the Sorcerer&#39;s Stone 1 4.47 ## 2 2 Harry Potter and the Chamber of Secrets 1 4.43 ## 3 3 Harry Potter and the Prisoner of Azkaban 1 4.57 ## 4 4 Harry Potter and the Goblet of Fire 1 4.56 ## 5 5 Harry Potter and the Order of the Phoenix 1 4.50 ## 6 6 Harry Potter and the Half Blood Prince 1 4.57 ## 7 7 Harry Potter and the Deathly Hallows 1 4.62 ## 8 8 The Way of Kings 2 4.64 ## 9 9 The Book Thief 3 4.37 ## 10 10 The Eye of the World 4 4.18 authors ## id name avg_rating ## 1 1 J.K. Rowling 4.46 ## 2 2 Brandon Sanderson 4.39 ## 3 3 Markus Zusak 4.34 ## 4 4 Robert Jordan 4.18 ## 5 5 Agatha Christie 4.00 ## 6 6 Alex Kava 4.02 ## 7 7 Nassim Nicholas Taleb 3.99 ## 8 8 Neil Gaiman 4.13 ## 9 9 Stieg Larsson 4.16 ## 10 10 Antoine de Saint-Exupéry 4.30 How do I merge the author information from authors based on author_id in books and id in authors, keeping only information from authors and books where there is a match? Click here for solution # In SQL this is referred to as an INNER JOIN. merge(books, authors, by.x=&quot;author_id&quot;, by.y=&quot;id&quot;, all=F) ## author_id id title rating ## 1 1 1 Harry Potter and the Sorcerer&#39;s Stone 4.47 ## 2 1 2 Harry Potter and the Chamber of Secrets 4.43 ## 3 1 3 Harry Potter and the Prisoner of Azkaban 4.57 ## 4 1 4 Harry Potter and the Goblet of Fire 4.56 ## 5 1 5 Harry Potter and the Order of the Phoenix 4.50 ## 6 1 6 Harry Potter and the Half Blood Prince 4.57 ## 7 1 7 Harry Potter and the Deathly Hallows 4.62 ## 8 2 8 The Way of Kings 4.64 ## 9 3 9 The Book Thief 4.37 ## 10 4 10 The Eye of the World 4.18 ## name avg_rating ## 1 J.K. Rowling 4.46 ## 2 J.K. Rowling 4.46 ## 3 J.K. Rowling 4.46 ## 4 J.K. Rowling 4.46 ## 5 J.K. Rowling 4.46 ## 6 J.K. Rowling 4.46 ## 7 J.K. Rowling 4.46 ## 8 Brandon Sanderson 4.39 ## 9 Markus Zusak 4.34 ## 10 Robert Jordan 4.18 How do I merge the author information from authors based on author_id in books and id in authors, keeping all information from authors regardless of whether or not there is match? Click here for solution merge(books, authors, by.x=&quot;author_id&quot;, by.y=&quot;id&quot;, all.y=T) ## author_id id title rating ## 1 1 1 Harry Potter and the Sorcerer&#39;s Stone 4.47 ## 2 1 2 Harry Potter and the Chamber of Secrets 4.43 ## 3 1 3 Harry Potter and the Prisoner of Azkaban 4.57 ## 4 1 4 Harry Potter and the Goblet of Fire 4.56 ## 5 1 5 Harry Potter and the Order of the Phoenix 4.50 ## 6 1 6 Harry Potter and the Half Blood Prince 4.57 ## 7 1 7 Harry Potter and the Deathly Hallows 4.62 ## 8 2 8 The Way of Kings 4.64 ## 9 3 9 The Book Thief 4.37 ## 10 4 10 The Eye of the World 4.18 ## 11 5 NA &lt;NA&gt; NA ## 12 6 NA &lt;NA&gt; NA ## 13 7 NA &lt;NA&gt; NA ## 14 8 NA &lt;NA&gt; NA ## 15 9 NA &lt;NA&gt; NA ## 16 10 NA &lt;NA&gt; NA ## name avg_rating ## 1 J.K. Rowling 4.46 ## 2 J.K. Rowling 4.46 ## 3 J.K. Rowling 4.46 ## 4 J.K. Rowling 4.46 ## 5 J.K. Rowling 4.46 ## 6 J.K. Rowling 4.46 ## 7 J.K. Rowling 4.46 ## 8 Brandon Sanderson 4.39 ## 9 Markus Zusak 4.34 ## 10 Robert Jordan 4.18 ## 11 Agatha Christie 4.00 ## 12 Alex Kava 4.02 ## 13 Nassim Nicholas Taleb 3.99 ## 14 Neil Gaiman 4.13 ## 15 Stieg Larsson 4.16 ## 16 Antoine de Saint-Exupéry 4.30 # or merge(authors, books, by.x=&quot;id&quot;, by.y=&quot;author_id&quot;, all.x=T) ## id name avg_rating id.y ## 1 1 J.K. Rowling 4.46 1 ## 2 1 J.K. Rowling 4.46 2 ## 3 1 J.K. Rowling 4.46 3 ## 4 1 J.K. Rowling 4.46 4 ## 5 1 J.K. Rowling 4.46 5 ## 6 1 J.K. Rowling 4.46 6 ## 7 1 J.K. Rowling 4.46 7 ## 8 2 Brandon Sanderson 4.39 8 ## 9 3 Markus Zusak 4.34 9 ## 10 4 Robert Jordan 4.18 10 ## 11 5 Agatha Christie 4.00 NA ## 12 6 Alex Kava 4.02 NA ## 13 7 Nassim Nicholas Taleb 3.99 NA ## 14 8 Neil Gaiman 4.13 NA ## 15 9 Stieg Larsson 4.16 NA ## 16 10 Antoine de Saint-Exupéry 4.30 NA ## title rating ## 1 Harry Potter and the Sorcerer&#39;s Stone 4.47 ## 2 Harry Potter and the Chamber of Secrets 4.43 ## 3 Harry Potter and the Prisoner of Azkaban 4.57 ## 4 Harry Potter and the Goblet of Fire 4.56 ## 5 Harry Potter and the Order of the Phoenix 4.50 ## 6 Harry Potter and the Half Blood Prince 4.57 ## 7 Harry Potter and the Deathly Hallows 4.62 ## 8 The Way of Kings 4.64 ## 9 The Book Thief 4.37 ## 10 The Eye of the World 4.18 ## 11 &lt;NA&gt; NA ## 12 &lt;NA&gt; NA ## 13 &lt;NA&gt; NA ## 14 &lt;NA&gt; NA ## 15 &lt;NA&gt; NA ## 16 &lt;NA&gt; NA Data.frames Data.frames are one of the primary data structure used very frequently when working in R. Data.frames are tables of same-sized, named columns, where each column has a single type. You can create a data.frame easily: df &lt;- data.frame(cat_1=c(1,2,3), cat_2=c(9,8,7), ok=c(T, T, F), other=c(&quot;first&quot;, &quot;second&quot;, &quot;third&quot;)) head(df) ## cat_1 cat_2 ok other ## 1 1 9 TRUE first ## 2 2 8 TRUE second ## 3 3 7 FALSE third Regular indexing rules apply as well. This is how you index rows. Pay close attention to the trailing comma: # Numeric indexing on rows: df[1:2,] ## cat_1 cat_2 ok other ## 1 1 9 TRUE first ## 2 2 8 TRUE second df[c(1,3),] ## cat_1 cat_2 ok other ## 1 1 9 TRUE first ## 3 3 7 FALSE third # Logical indexing on rows: df[c(T,F,T),] ## cat_1 cat_2 ok other ## 1 1 9 TRUE first ## 3 3 7 FALSE third # Named indexing on rows only works # if there are named rows: row.names(df) &lt;- c(&quot;row1&quot;, &quot;row2&quot;, &quot;row3&quot;) df[c(&quot;row1&quot;, &quot;row3&quot;),] ## cat_1 cat_2 ok other ## row1 1 9 TRUE first ## row3 3 7 FALSE third By default, if you don't include the comma in the square brackets, you are indexing the column: df[c(&quot;cat_1&quot;, &quot;ok&quot;)] ## cat_1 ok ## row1 1 TRUE ## row2 2 TRUE ## row3 3 FALSE To index columns, place expressions after the first comma: # Numeric indexing on columns: df[, 1] ## [1] 1 2 3 df[, c(1,3)] ## cat_1 ok ## row1 1 TRUE ## row2 2 TRUE ## row3 3 FALSE # Logical indexing on columns: df[, c(T, F, F, F)] ## [1] 1 2 3 # Named indexing on columns. # This is the more typical method of # column indexing: df$cat_1 ## [1] 1 2 3 # Another way to do named indexing on columns: df[,c(&quot;cat_1&quot;, &quot;ok&quot;)] ## cat_1 ok ## row1 1 TRUE ## row2 2 TRUE ## row3 3 FALSE Of course, you can index on columns and rows: # Numeric indexing on columns and rows: df[1:2, 1] ## [1] 1 2 df[1:2, c(1,3)] ## cat_1 ok ## row1 1 TRUE ## row2 2 TRUE # Logical indexing on columns and rows: df[c(T,F,T), c(T, F, F, F)] ## [1] 1 3 # Named indexing on columns and rows. # This is the more typical method of # column indexing: df$cat_1[c(T,F,T)] ## [1] 1 3 # Another way to do named indexing on columns and rows: row.names(df) &lt;- c(&quot;row1&quot;, &quot;row2&quot;, &quot;row3&quot;) df[c(&quot;row1&quot;, &quot;row3&quot;),c(&quot;cat_1&quot;, &quot;ok&quot;)] ## cat_1 ok ## row1 1 TRUE ## row3 3 FALSE Examples How can I get the first 2 rows of a data.frame named df? Click here for solution df &lt;- data.frame(cat_1=c(1,2,3), cat_2=c(9,8,7), ok=c(T, T, F), other=c(&quot;first&quot;, &quot;second&quot;, &quot;third&quot;)) df[1:2,] ## cat_1 cat_2 ok other ## 1 1 9 TRUE first ## 2 2 8 TRUE second How can I get the first 2 columns of a data.frame named df? Click here for solution df[,1:2] ## cat_1 cat_2 ## 1 1 9 ## 2 2 8 ## 3 3 7 How can I get the rows where values in the column named cat_1 are greater than 2? Click here for solution df[df$cat_1 &gt; 2,] ## cat_1 cat_2 ok other ## 3 3 7 FALSE third df[df[, c(&quot;cat_1&quot;)] &gt; 2,] ## cat_1 cat_2 ok other ## 3 3 7 FALSE third How can I get the rows where values in the column named cat_1 are greater than 2 and the values in the column named cat_2 are less than 9? Click here for solution df[df$cat_1 &gt; 2 &amp; df$cat_2 &lt; 9,] ## cat_1 cat_2 ok other ## 3 3 7 FALSE third How can I get the rows where values in the column named cat_1 are greater than 2 or the values in the column named cat_2 are less than 9? Click here for solution df[df$cat_1 &gt; 2 | df$cat_2 &lt; 9,] ## cat_1 cat_2 ok other ## 2 2 8 TRUE second ## 3 3 7 FALSE third How do I sample n rows randomly from a data.frame called df? Click here for solution df[sample(nrow(df), n),] Alternatively you could use the sample_n function from the package dplyr: sample_n(df, n) How can I get only columns whose names start with &quot;cat_&quot;? Click here for solution df &lt;- data.frame(cat_1=c(1,2,3), cat_2=c(9,8,7), ok=c(T, T, F), other=c(&quot;first&quot;, &quot;second&quot;, &quot;third&quot;)) df[, grep(&quot;^cat_&quot;, names(df))] ## cat_1 cat_2 ## 1 1 9 ## 2 2 8 ## 3 3 7 Reading &amp; Writing data Examples How do I read a csv file called grades.csv into a data.frame? Click here for solution Note that the &quot;.&quot; means the current working directory. So, if we were in &quot;/home/john/projects&quot;, &quot;./grades.csv&quot; would be the same as &quot;/home/john/projects/grades.csv&quot;. This is called a relative path. Read this for a better understanding. dat &lt;- read.csv(&quot;./grades.csv&quot;) head(dat) ## grade year ## 1 100 junior ## 2 99 sophomore ## 3 75 sophomore ## 4 74 sophomore ## 5 44 senior ## 6 69 junior How do I read a csv file called grades.csv into a data.frame using the function fread? Click here for solution Note: The function fread is part of the data.table package and which reads in dataset faster than read.csv. It is therefore recommended for reading in large datasets in R. ```r library(data.table) dat &lt;- data.frame(fread(&quot;./grades.csv&quot;)) head(dat) ``` ``` ## grade year ## 1 100 junior ## 2 99 sophomore ## 3 75 sophomore ## 4 74 sophomore ## 5 44 senior ## 6 69 junior ``` How do I read a csv file called grades2.csv where instead of being comma-separated, it is semi-colon-separated, into a data.frame? Click here for solution dat &lt;- read.csv(&quot;./grades_semi.csv&quot;, sep=&quot;;&quot;) head(dat) ## grade year ## 1 100 junior ## 2 99 sophomore ## 3 75 sophomore ## 4 74 sophomore ## 5 44 senior ## 6 69 junior How do I prevent R from reading in strings as factors when using a function like read.csv? Click here for solution In R 4.0+, strings are not read in as factors, so you do not need to do anything special. For R &lt; 4.0, use stringsAsFactors. dat &lt;- read.csv(&quot;./grades.csv&quot;, stringsAsFactors=F) head(dat) ## grade year ## 1 100 junior ## 2 99 sophomore ## 3 75 sophomore ## 4 74 sophomore ## 5 44 senior ## 6 69 junior How do I specify the type of 1 or more columns when reading in a csv file? Click here for solution dat &lt;- read.csv(&quot;./grades.csv&quot;, colClasses=c(&quot;grade&quot;=&quot;character&quot;, &quot;year&quot;=&quot;factor&quot;)) str(dat) ## &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ grade: chr &quot;100&quot; &quot;99&quot; &quot;75&quot; &quot;74&quot; ... ## $ year : Factor w/ 4 levels &quot;freshman&quot;,&quot;junior&quot;,..: 2 4 4 4 3 2 2 3 1 2 Given a list of csv files with the same columns, how can I read them in and combine them into a single dataframe? Click here for solution # We want to read in grades.csv, grades2.csv, and grades3.csv # into a single dataframe. list_of_files &lt;- c(&quot;grades.csv&quot;, &quot;grades2.csv&quot;, &quot;grades3.csv&quot;) results &lt;- data.frame() for (file in list_of_files) { dat &lt;- read.csv(file) results &lt;- rbind(results, dat) } dim(results) ## [1] 32 2 How do I create a data.frame with comma-separated data that I've copied onto my clipboard? Click here for solution # For mac dat &lt;- read.delim(pipe(&quot;pbpaste&quot;),header=F,sep=&quot;,&quot;) # For windows dat &lt;- read.table(&quot;clipboard&quot;,header=F,sep=&quot;,&quot;) Control flow If/else statements If, else if, and else statements are methods for controlling whether or not an operation is performed based on the result of some expression. How do I print &quot;Success!&quot; if my expression evaluates to TRUE, and &quot;Failure!&quot; otherwise? Click here for solution # Randomly assign either TRUE or FALSE to t_or_f. t_or_f &lt;- sample(c(TRUE,FALSE),1) if (t_or_f == TRUE) { # If t_or_f is TRUE, print success print(&quot;Success!&quot;) } else { # Otherwise, print failure print(&quot;Failure!&quot;) } ## [1] &quot;Success!&quot; # You don&#39;t need to put the full expression. # This is the same thing because t_or_f # is already TRUE or FALSE. # TRUE == TRUE evaluates to TRUE and # FALSE == TRUE evaluates to FALSE. if (t_or_f) { # If t_or_f is TRUE, print success print(&quot;Success!&quot;) } else { # Otherwise, print failure print(&quot;Failure!&quot;) } ## [1] &quot;Success!&quot; How do I print &quot;Success!&quot; if my expression evaluates to TRUE, &quot;Failure!&quot; if my expression evaluates to FALSE, and &quot;Huh?&quot; otherwise? Click here for solution # Randomly assign either TRUE or FALSE to t_or_f. t_or_f &lt;- sample(c(TRUE,FALSE, &quot;Something else&quot;),1) if (t_or_f == TRUE) { # If t_or_f is TRUE, print success print(&quot;Success!&quot;) } else if (t_or_f == FALSE) { # If t_or_f is FALSE, print failure print(&quot;Failure!&quot;) } else { # Otherwise print huh print(&quot;Huh?&quot;) } ## [1] &quot;Success!&quot; # In this case you need the full expression because # &quot;Something else&quot; does not evaluate to TRUE or FALSE # which will cause an error as the if and else if # statements expect a result of TRUE or FALSE. if (t_or_f == TRUE) { # If t_or_f is TRUE, print success print(&quot;Success!&quot;) } else if (t_or_f == FALSE) { # If t_or_f is FALSE, print failure print(&quot;Failure!&quot;) } else { # Otherwise print huh print(&quot;Huh?&quot;) } ## [1] &quot;Success!&quot; For loops For loops allow us to execute similar code over and over again until we've looped through all of the elements. They are useful for performing the same operation to an entire vector of input, for example. Using the suite of apply functions is more common in R. It is often said that the apply suite of function are much faster than for loops in R. While this used to be the case, this is no longer true. Examples How do I loop through every value in a vector and print the value? Click here for solution for (i in 1:10) { # In the first iteration of the loop, # i will be 1. The next, i will be 2. # Etc. print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 How do I break out of a loop before it finishes? Click here for solution for (i in 1:10) { if (i==7) { # When i==7, we will exit the loop. break } print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 How do I loop through a vector of names? Click here for solution friends &lt;- c(&quot;Phoebe&quot;, &quot;Ross&quot;, &quot;Rachel&quot;, &quot;Chandler&quot;, &quot;Joey&quot;, &quot;Monica&quot;) my_string &lt;- &quot;So no one told you life was gonna be this way, &quot; for (friend in friends) { print(paste0(my_string, friend, &quot;!&quot;)) } ## [1] &quot;So no one told you life was gonna be this way, Phoebe!&quot; ## [1] &quot;So no one told you life was gonna be this way, Ross!&quot; ## [1] &quot;So no one told you life was gonna be this way, Rachel!&quot; ## [1] &quot;So no one told you life was gonna be this way, Chandler!&quot; ## [1] &quot;So no one told you life was gonna be this way, Joey!&quot; ## [1] &quot;So no one told you life was gonna be this way, Monica!&quot; How do I skip a loop if some expression evaluates to TRUE? Click here for solution friends &lt;- c(&quot;Phoebe&quot;, &quot;Ross&quot;, &quot;Mike&quot;, &quot;Rachel&quot;, &quot;Chandler&quot;, &quot;Joey&quot;, &quot;Monica&quot;) my_string &lt;- &quot;So no one told you life was gonna be this way, &quot; for (friend in friends) { if (friend == &quot;Mike&quot;) { # next, skips over the rest of the code for this loop # and continues to the next element next } print(paste0(my_string, friend, &quot;!&quot;)) } ## [1] &quot;So no one told you life was gonna be this way, Phoebe!&quot; ## [1] &quot;So no one told you life was gonna be this way, Ross!&quot; ## [1] &quot;So no one told you life was gonna be this way, Rachel!&quot; ## [1] &quot;So no one told you life was gonna be this way, Chandler!&quot; ## [1] &quot;So no one told you life was gonna be this way, Joey!&quot; ## [1] &quot;So no one told you life was gonna be this way, Monica!&quot; Are there examples in which for loops are not appropriate to use? Click here for solution This is usually how we write loops in other languages, e.g., C, C++, Java, Python, etc., if we want to add the first 10 billion integers. mytotal &lt;- 0 for (i in 1:10000000000) { mytotal &lt;- mytotal + i } mytotal ## [1] 5e+19 but this takes a long time to evaluate. It is easier to write, and much faster to evaluate, if we use the sum function, which is vectorized, i.e., which works on an entire vector of data all at once. Here, for instance, we add the first 10 billion integers, and the computation occurs almost immediately. sum(1:10000000000) ## [1] 5e+19 Click here for video Can you show an example of how to do the same thing, with a for loop and without a for loop? Click here for solution Yes, here is an example about how to compute the average cost of a line of the grocery store data. myDF &lt;- read.csv(&quot;/class/datamine/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv&quot;) head(myDF) ## BASKET_NUM HSHD_NUM PURCHASE_ PRODUCT_NUM SPEND UNITS STORE_R WEEK_NUM YEAR ## 1 24 1809 03-JAN-16 5817389 -1.50 -1 SOUTH 1 2016 ## 2 24 1809 03-JAN-16 5829886 -1.50 -1 SOUTH 1 2016 ## 3 34 1253 03-JAN-16 539501 2.19 1 EAST 1 2016 ## 4 60 1595 03-JAN-16 5260099 0.99 1 WEST 1 2016 ## 5 60 1595 03-JAN-16 4535660 2.50 2 WEST 1 2016 ## 6 168 3393 03-JAN-16 5602916 4.50 1 SOUTH 1 2016 This is how we find the average cost per line in other languages, for instance, C/C++, Python, Java, etc. amountspent &lt;- 0 # we initialize a variable to keep track of the entire price of the purchases numberofitems &lt;- 0 # and we initialize a variable to keep track of the number of purchases for (myprice in myDF$SPEND) { amountspent &lt;- amountspent + myprice # we add the price of the current purchase numberofitems &lt;- numberofitems + 1 # and we increment (by 1) the number o purchases processed so far } amountspent # this is the total amount spent on all purchases ## [1] 3584366 numberofitems # this is the total number of purchases ## [1] 1e+06 amountspent/numberofitems # so this is the average ## [1] 3.584366 amountspent/length(myDF$SPEND) # this is an equivalent way to compute the average ## [1] 3.584366 For comparison, this is the much easier way that we can use a vectorized function in R, to accomplish the same purpose. The vector is the column myDF$SPEND. We can just focus our attention on that column from the data frame, and take a mean. mean(myDF$SPEND) ## [1] 3.584366 Click here for video Can you show an example of how to make a new column in a data frame, which classifies things, based on another column? Click here for solution Yes, we can make a new column in the grocery store data set. myDF &lt;- read.csv(&quot;/class/datamine/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv&quot;) head(myDF) ## BASKET_NUM HSHD_NUM PURCHASE_ PRODUCT_NUM SPEND UNITS STORE_R WEEK_NUM YEAR ## 1 24 1809 03-JAN-16 5817389 -1.50 -1 SOUTH 1 2016 ## 2 24 1809 03-JAN-16 5829886 -1.50 -1 SOUTH 1 2016 ## 3 34 1253 03-JAN-16 539501 2.19 1 EAST 1 2016 ## 4 60 1595 03-JAN-16 5260099 0.99 1 WEST 1 2016 ## 5 60 1595 03-JAN-16 4535660 2.50 2 WEST 1 2016 ## 6 168 3393 03-JAN-16 5602916 4.50 1 SOUTH 1 2016 Let's first make a new vector (the same length as a column of the data frame) in which all of the entries are safe. mystatus &lt;- rep(&quot;safe&quot;, times=nrow(myDF)) and then we can change the entries for the elements of mystatus that occurred on 05-JUL-16 or on 06-JUL-16 to be contaminated. mystatus[(myDF$PURCHASE_ == &quot;05-JUL-16&quot;)|(myDF$PURCHASE_ == &quot;06-JUL-16&quot;)] &lt;- &quot;contaminated&quot; and finally change this into a factor, and add it as a new column in the data frame. myDF$safetystatus &lt;- factor(mystatus) Now the head of the data frame looks like this: head(myDF) ## BASKET_NUM HSHD_NUM PURCHASE_ PRODUCT_NUM SPEND UNITS STORE_R WEEK_NUM YEAR ## 1 24 1809 03-JAN-16 5817389 -1.50 -1 SOUTH 1 2016 ## 2 24 1809 03-JAN-16 5829886 -1.50 -1 SOUTH 1 2016 ## 3 34 1253 03-JAN-16 539501 2.19 1 EAST 1 2016 ## 4 60 1595 03-JAN-16 5260099 0.99 1 WEST 1 2016 ## 5 60 1595 03-JAN-16 4535660 2.50 2 WEST 1 2016 ## 6 168 3393 03-JAN-16 5602916 4.50 1 SOUTH 1 2016 ## safetystatus ## 1 safe ## 2 safe ## 3 safe ## 4 safe ## 5 safe ## 6 safe and the number of contaminated rows versus safe rows is this: table(myDF$safetystatus) ## ## contaminated safe ## 2459 997541 Click here for video Apply functions apply lapply The lapply is a function that applies a function FUN to each element in a vector or list, and returns a list. Examples How do I get the mean value of each vector in our list, my_list, in another list? Click here for solution lapply(my_list, mean) ## $pages ## [1] 3 ## ## $words ## [1] 30 ## ## $letters ## [1] 300 How can I find the average of several variables in the flight data, using only 1 line of R, with the lapply function? Click here for video These are the flights from 2003: myDF &lt;- read.csv(&quot;/class/datamine/data/flights/subset/2003.csv&quot;) We can break the flights into categories, depending on the Distance of the flight: less than 100 miles; from 100 to 200 miles; from 200 to 500 miles; from 500 to 1000 miles; from 1000 to 2000 miles; more than 2000 miles my_distance_categories &lt;- cut(myDF$Distance, breaks = c(0,100,200,500,1000,2000,Inf), include.lowest=T) The numbers of flights in each category are: table(my_distance_categories) Here are the average values of 4 variables, in each of these 6 categories: tapply( myDF$DepDelay, my_distance_categories, mean, na.rm=T) # the DepDelay in each category tapply( myDF$ArrDelay, my_distance_categories, mean, na.rm=T) # the ArrDelay in each category tapply( myDF$TaxiOut, my_distance_categories, mean, na.rm=T) # the time to TaxiOut in each category tapply( myDF$TaxiIn, my_distance_categories, mean, na.rm=T) # the time to TaxiIn in each category OR, MUCH EASIER: We can do all of this with just 1 line of R. To make it easier to read, we can make a temporary data frame flights_by_distance with these 4 variables. Then we split the data into 6 data frames, according to the Distance of the flights, and we get the average DepDelay, ArrDelay, TaxiOut, and TaxiIn, in each of these 6 categories, with only 1 line of R. Notice that this agrees exactly with the results of the 4 separate tapply functions, but it only takes us 1 call to the lapply function!! flights_by_distance &lt;- split( data.frame(myDF$DepDelay, myDF$ArrDelay, myDF$TaxiOut, myDF$TaxiIn), my_distance_categories ) lapply( flights_by_distance, colMeans, na.rm=T ) Some closing remarks about this example: We use lapply on a list. It only takes two arguments, namely, a list and a function to run on each piece of our list. In this case, we are taking an average (colMeans) of each column in each piece of our list. The flights_by_distance is a list of 6 data frames You might want to check these out. class( flights_by_distance ) length( flights_by_distance ) class(flights_by_distance[[1]]) class(flights_by_distance[[2]]) class(flights_by_distance[[3]]) class(flights_by_distance[[4]]) class(flights_by_distance[[5]]) class(flights_by_distance[[6]]) head(flights_by_distance[[1]]) head(flights_by_distance[[2]]) head(flights_by_distance[[3]]) head(flights_by_distance[[4]]) head(flights_by_distance[[5]]) head(flights_by_distance[[6]]) You can take the colMeans within each of these data frames, like this: colMeans(flights_by_distance[[1]], na.rm=T) colMeans(flights_by_distance[[2]], na.rm=T) colMeans(flights_by_distance[[3]], na.rm=T) colMeans(flights_by_distance[[4]], na.rm=T) colMeans(flights_by_distance[[5]], na.rm=T) colMeans(flights_by_distance[[6]], na.rm=T) but this is all accomplished by the 1-line lapply that we did earlier, in a much easier way. How can I find the average of several variables in the fars data, using only 1 line of R, with the lapply function? Click here for video This is the fars data set, studied in STAT 19000 Project 6 (only the years 1975 to 1981) dat &lt;- read.csv(&quot;/class/datamine/data/fars/7581.csv&quot;) We will learn a more efficient way to add the state names but for now, we do this in the same way as Project 6. state_names &lt;- read.csv(&quot;/class/datamine/data/fars/states.csv&quot;) v &lt;- state_names$state names(v) &lt;- state_names$code dat$mystates &lt;- v[as.character(dat$STATE)] In Project 6, Question 2, we found the average number of DRUNK_DR, according to the state: tapply( dat$DRUNK_DR, dat$mystates, mean) We might also want to find the average number fatalities (FATALS) per accident, according to the state: tapply( dat$FATALS, dat$mystates, mean) and the average number of people (PERSONS) involved per accident, according to the state: tapply( dat$PERSONS, dat$mystates, mean) OR, MUCH EASIER: We can do all 3 of these calculations with just 1 line of R. To make it easier to read, we can make a temporary data frame accidents_by_state with these 3 variables. Then we split the data into 51 data frames, according to the state where the accident occurred, and we get the average DRUNK_DR, FATALS, and PERSONS in each of these 51 categories, with only 1 line of R. Notice that this agrees exactly with the results of the 3 separate tapply functions, but it only takes us 1 call to the lapply function!! accidents_by_state &lt;- split( data.frame(dat$DRUNK_DR, dat$FATALS, dat$PERSONS), dat$mystates ) lapply( accidents_by_state, colMeans ) Again, some closing remarks: We use lapply on a list. It only takes two arguments, namely, a list and a function to run on each piece of our list. In this case, we are taking an average (colMeans) of each column in each piece of our list. The accidents_by_state is a list of 51 data frames. You might want to check these out. class( accidents_by_state ) length( accidents_by_state ) class(accidents_by_state[[1]]) class(accidents_by_state[[2]]) # etc., etc. class(accidents_by_state[[50]]) class(accidents_by_state[[51]]) head(accidents_by_state[[1]]) head(accidents_by_state[[2]]) # etc., etc. head(accidents_by_state[[50]]) head(accidents_by_state[[51]]) You can also extract the elements of the list according to their names, e.g., head(accidents_by_state$Indiana) colMeans(accidents_by_state$Indiana) head(accidents_by_state$Illinois) colMeans(accidents_by_state$Illinois) head(accidents_by_state$Ohio) colMeans(accidents_by_state$Ohio) head(accidents_by_state$Michigan) colMeans(accidents_by_state$Michigan) but this is all accomplished by the 1-line lapply that we did earlier, in a much easier way. sapply sapply is very similar to lapply, however, where lapply always returns a list, sapply will simplify the output of applying the function FUN to each element. If you recall, when accessing an element in a list using single brackets my_list[1], the result will always return a list. If you access an element with double brackets my_list[[1]], R will attempt to simplify the result. This is analogous to lapply and sapply. Examples How do I get the mean value of each vector in our list, my_list, but rather than the result being a list, put the results in the simplest form? Click here for solution sapply(my_list, mean) ## pages words letters ## 3 30 300 Use the provided function to create a new column in the data.frame example_df named transformed. transformed should contain TRUE if the value in pre_transformed is &quot;t&quot;, FALSE if it is &quot;f&quot;, and NA otherwise. string_to_bool &lt;- function(value) { if (value == &quot;t&quot;) { return(TRUE) } else if (value == &quot;f&quot;) { return(FALSE) } else { return(NA) } } example_df &lt;- data.frame(pre_transformed=c(&quot;f&quot;, &quot;f&quot;, &quot;t&quot;, &quot;f&quot;, &quot;something&quot;, &quot;t&quot;, &quot;else&quot;, &quot;&quot;), other=c(1,2,3,4,5,6,7,8)) example_df ## pre_transformed other ## 1 f 1 ## 2 f 2 ## 3 t 3 ## 4 f 4 ## 5 something 5 ## 6 t 6 ## 7 else 7 ## 8 8 Click here for solution example_df$transformed &lt;- sapply(example_df$pre_transformed, string_to_bool) example_df ## pre_transformed other transformed ## 1 f 1 FALSE ## 2 f 2 FALSE ## 3 t 3 TRUE ## 4 f 4 FALSE ## 5 something 5 NA ## 6 t 6 TRUE ## 7 else 7 NA ## 8 8 NA tapply tapply is described in the documentation as a way to &quot;apply a function to each cell of a ragged array, that is to each (non-empty) group of values given by a unique combination of the levels of certain factors.&quot; This is not a very useful description. An alternative way to think about tapply, is as a function that allows you to calculate or apply function to data1 when data1 is grouped by data2. tapply(data1, data2, function) A concrete example would be getting the mean (function) grade (data1) when grade (data1) is grouped by year (data2): grades ## grade year sex ## 1 100 junior M ## 2 99 sophomore F ## 3 75 sophomore M ## 4 74 sophomore M ## 5 44 senior F ## 6 69 junior M ## 7 88 junior F ## 8 99 senior &lt;NA&gt; ## 9 90 freshman M ## 10 92 junior F tapply(grades$grade, grades$year, mean) ## freshman junior senior sophomore ## 90.00000 87.25000 71.50000 82.66667 If your function (in this case mean), requires extra arguments, you can pass those by name to tapply. This is what the ... argument in tapply is for. For example, if we want our mean function to remove na's prior to calculating a mean we could do the following: tapply(grades$grade, grades$year, mean, na.rm=T) ## freshman junior senior sophomore ## 90.00000 87.25000 71.50000 82.66667 Examples Amazon fine food tapply example Here is an example using the Amazon fine food reviews myDF &lt;- read.csv(&quot;/class/datamine/data/amazon/amazon_fine_food_reviews.csv&quot;) This is the data source: https://www.kaggle.com/snap/amazon-fine-food-reviews/ The people who wrote the most reviews are tail(sort(table(myDF$UserId))) In particular, user A3OXHLG6DIBRW8 wrote the most reviews. The total number of people who read reviews that were written by A3OXHLG6DIBRW8 is: sum(myDF$HelpfulnessDenominator[myDF$UserId == &quot;A3OXHLG6DIBRW8&quot;]) The number of people who found those reviews (written by A3OXHLG6DIBRW8) to be helpful is: sum(myDF$HelpfulnessNumerator[myDF$UserId == &quot;A3OXHLG6DIBRW8&quot;]) So, altogether, when people read the reviews written by user A3OXHLG6DIBRW8, these reviews were rated as helpful 0.9795918 of the time sum(myDF$HelpfulnessNumerator[myDF$UserId == &quot;A3OXHLG6DIBRW8&quot;])/sum(myDF$HelpfulnessDenominator[myDF$UserId == &quot;A3OXHLG6DIBRW8&quot;]) Now we can do this again, for all users. The total number of people who read reviews altogether, grouped by the user who wrote the review, is head( tapply(myDF$HelpfulnessDenominator, myDF$UserId, sum) ) The total number of people who rated reviews as helpful, grouped by the user who wrote the review, is head( tapply(myDF$HelpfulnessNumerator, myDF$UserId, sum) ) The percentages of people who found reviews to be helpful, grouped according to who wrote the review, are head( tapply(myDF$HelpfulnessNumerator, myDF$UserId, sum)/tapply(myDF$HelpfulnessDenominator, myDF$UserId, sum) ) We can double-check our result for user &quot;A3OXHLG6DIBRW8&quot; as follows ( tapply(myDF$HelpfulnessNumerator, myDF$UserId, sum)/tapply(myDF$HelpfulnessDenominator, myDF$UserId, sum) )[&quot;A3OXHLG6DIBRW8&quot;] Click here for video Writing functions In a nutshell, a function is a set of instructions or actions packaged together in a single definition or unit. Typically, function accept 0 or more arguments as input, and returns 0 or more results as output. The following is an example of a function in R: # word_count is a function that accepts a sentence as an argument, # strips punctuation and extra space, and returns the number of # words in the sentence. word_count &lt;- function(sentence) { # strip punctuation and save into an auxiliary variable aux &lt;- gsub(&#39;[[:punct:]]+&#39;,&#39;&#39;, sentence) # split the sentence by space and remove extra spaces result &lt;- sum(unlist(strsplit(aux, &quot; &quot;)) != &quot;&quot;) return(result) } test_sentence &lt;- &quot;this is a sentence, with 7 words.&quot; word_count(test_sentence) ## [1] 7 The function is named word_count. The function has a single parameter named sentence. The function returns a single value, result, which is the number of words in the provided sentence. test_sentence is the argument to word_count. An argument is the actual value passed to the function. We pass values to functions -- this just means we use the values as arguments to the function. The parameter, sentence, is the name shown in the function definition. Functions can have helper functions. A helper function is a function defined and used within another function in order to reduce complexity or make the task at hand more clear. For example, we could have written the previous function differently: # word_count is a function that accepts a sentence as an argument, # strips punctuation and extra space, and returns the number of # words in the sentence. word_count &lt;- function(sentence) { # a helper function that takes care of removing # punctuation and extra spaces. split_and_clean &lt;- function(sentence) { # strip punctuation and save into an auxiliary variable aux &lt;- gsub(&#39;[[:punct:]]+&#39;,&#39;&#39;, sentence) # remove extra spaces aux &lt;- unlist(strsplit(aux, &quot; &quot;)) return(aux[aux!=&quot;&quot;]) } # return the length of the sentence result &lt;- length(split_and_clean(sentence)) return(result) } test_sentence &lt;- &quot;this is a sentence, with 7 words.&quot; word_count(test_sentence) ## [1] 7 Here, our helper function is named split_and_clean. If you try to call split_and_clean outside of word_count, you will get an error. split_and_clean is defined within the scope of word_count and is not available outside that scope. In this example, word_count is the caller, the function that calls the other function, split_and_clean. The other function, split_and_clean, can be referred to as the callee. In R functions can be passed to other functions as arguments. In general, functions that accept another function as an argument or return functions, are called higher order functions. Some examples of higher order functions in R are sapply, lapply, tapply, Map, and Reduce. The function passed as an argument, is often referred to as a callback function, as the caller is expected to call back (execute) the argument at a later point in time. ... The ellipsis ... in R can be used to pass an unknown number of arguments to a function. For example, if you look at the documentation for sapply (?sapply), you will see the following in the usage section: sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE) In the arguments section, it says the ellipsis are &quot;optional arguments to FUN&quot;. sapply uses the ellipsis as a vehicle to pass an unknown number of arguments to the callback function. In practice, this could look something like: dims &lt;- function(..., sort=F) { args &lt;- list(...) arg_names &lt;- names(args) results &lt;- lapply(args, dim) if (is.null(arg_names) | sort==FALSE) { # arguments not passed with a name return(results) } return(results[order(names(results))]) } dims(grades) ## [[1]] ## [1] 10 3 dims(grades, my_mat) ## [[1]] ## [1] 10 3 ## ## [[2]] ## [1] 4 3 dims(xyz=grades, abc=my_mat) ## $xyz ## [1] 10 3 ## ## $abc ## [1] 4 3 dims(xyz=grades, abc=my_mat, sort=T) ## $abc ## [1] 4 3 ## ## $xyz ## [1] 10 3 Here, dims accepts any number of data.frame-like objects, ..., and a logical value indicating whether or not to sort the list by names. As you can see, if arguments are passed to dims with names, those names can be accessed within dims via names(list(...)). Examples Create a function named should_be_transformed that, given a value, returns TRUE if the value is &quot;t&quot;, and FALSE if the value is &quot;f&quot;, and NA otherwise. example_df &lt;- data.frame(column_to_test=c(&quot;f&quot;, &quot;f&quot;, &quot;t&quot;, &quot;f&quot;, &quot;something&quot;, &quot;t&quot;, &quot;else&quot;, &quot;&quot;), other=c(1,2,3,4,5,6,7,8)) example_df ## column_to_test other ## 1 f 1 ## 2 f 2 ## 3 t 3 ## 4 f 4 ## 5 something 5 ## 6 t 6 ## 7 else 7 ## 8 8 Click here for solution should_be_transformed &lt;- function(value) { if (value == &quot;t&quot;) { return(TRUE) } else if (value == &quot;f&quot;) { return(FALSE) } else { return(NA) } } should_be_transformed(example_df$column_to_test[1]) ## [1] FALSE should_be_transformed(example_df$column_to_test[3]) ## [1] TRUE should_be_transformed(example_df$column_to_test[5]) ## [1] NA Plotting barplot barplot is a function that creates a barplot. Barplots are used to display categorical data. The following is an example of plotting some data from the precip dataset. barplot(precip[1:10]) As you can see, the x-axis labels are bad. What if we turn the labels to be vertical? barplot(precip[1:10], las=2) Much better, however, some of the longer names go off of the plot. Let's fix this: par(oma=c(3,0,0,0)) # oma stands for outer margins. We increase the bottom margin to 3. barplot(precip[1:10], las=2) This is even better, however, it would be nice to have a title and axis label(s). par(oma=c(3,0,0,0)) # oma stands for outer margins. We increase the bottom margin to 3. barplot(precip[1:10], las=2, main=&quot;Average Precipitation&quot;, ylab=&quot;Inches of rain&quot;) We are getting there. Let's add some color. par(oma=c(3,0,0,0)) # oma stands for outer margins. We increase the bottom margin to 3. barplot(precip[1:10], las=2, main=&quot;Average Precipitation&quot;, ylab=&quot;Inches of rain&quot;, col=&quot;blue&quot;) What if we want different colors for the different cities? library(RColorBrewer) par(oma=c(3,0,0,0)) # oma stands for outer margins. We increase the bottom margin to 3. colors &lt;- brewer.pal(10, &quot;Set3&quot;) barplot(precip[1:10], las=2, main=&quot;Average Precipitation&quot;, ylab=&quot;Inches of rain&quot;, col=colors) What if instead of x-axis labels, we want to use a legend? library(RColorBrewer) par(oma=c(0,0,0,0)) # oma stands for outer margins. We increase the bottom margin to 3. colors &lt;- brewer.pal(10, &quot;Set3&quot;) barplot(precip[1:10], las=2, main=&quot;Average Precipitation&quot;, ylab=&quot;Inches of rain&quot;, col=colors, legend=T, names.arg=F) Pretty good, but now we don't need so much space at the bottom, and we need to make space for that legend. We use xlim to increase the x-axis, and args.legend to move the position of the legend along the x and y axes. library(RColorBrewer) colors &lt;- brewer.pal(10, &quot;Set3&quot;) barplot(precip[1:10], las=2, main=&quot;Average Precipitation&quot;, ylab=&quot;Inches of rain&quot;, col=colors, legend=T, names.arg=F, xlim=c(0, 15), args.legend=list(x=16.5, y=46)) It's looking good, let's remove the box around the legend: library(RColorBrewer) colors &lt;- brewer.pal(10, &quot;Set3&quot;) barplot(precip[1:10], las=2, main=&quot;Average Precipitation&quot;, ylab=&quot;Inches of rain&quot;, col=colors, legend=T, names.arg=F, xlim=c(0, 15), args.legend=list(x=16.5, y=46, bty=&quot;n&quot;)) boxplot boxplot is a function that creates a box and whisker plot, given some grouped data. The following is an example using the trees dataset. First, we break our data into groups based on height. dat &lt;- trees dat$size &lt;- cut(trees$Height, breaks=c(0,76,100)) levels(dat$size) &lt;- c(&quot;short&quot;, &quot;tall&quot;) Next, we start with a box plot: boxplot(dat$Girth ~ dat$size) Let's spruce things up with proper labels: boxplot(dat$Girth ~ dat$size, main=&quot;Tree girth&quot;, ylab=&quot;Girth in Inches&quot;, names=c(&quot;Short&quot;, &quot;Tall&quot;), xlab=&quot;&quot;) Let's add color: boxplot(dat$Girth ~ dat$size, main=&quot;Tree girth&quot;, ylab=&quot;Girth in Inches&quot;, names=c(&quot;Short&quot;, &quot;Tall&quot;), xlab=&quot;&quot;, border=&quot;darkgreen&quot;, col=&quot;lightgreen&quot;) pie pie is a function that creates a piechart.pie charts are used to display categorical data. The following is an example using the USPersonalExpenditure dataset. First, let's get the mean expenditure: # Quick look at data: USPersonalExpenditure ## 1940 1945 1950 1955 1960 ## Food and Tobacco 22.200 44.500 59.60 73.2 86.80 ## Household Operation 10.500 15.500 29.00 36.5 46.20 ## Medical and Health 3.530 5.760 9.71 14.0 21.10 ## Personal Care 1.040 1.980 2.45 3.4 5.40 ## Private Education 0.341 0.974 1.80 2.6 3.64 # Mean expenditure expenditure &lt;- rowMeans(USPersonalExpenditure) Now, we can create our pie chart. pie(expenditure) Let's use some different colors! pie(expenditure, col = c(&quot;#8E6F3E&quot;, &quot;#1c5253&quot;,&quot;#23395b&quot;,&quot;#6F727B&quot;, &quot;#F97B64&quot;)) Let's add the percentages next to the names. To do so, we must first get those values: # calculating percentages expenditure_percentage &lt;- 100*expenditure/sum(expenditure) # rounding percentages to 2 decimal places expenditure_percentage &lt;- round(expenditure_percentage, 2) # combining names with percentages expenditure_names &lt;- paste0(names(expenditure), &quot; (&quot;, expenditure_percentage, &quot;%)&quot;) # creating new labels pie(expenditure, labels = expenditure_names, col = c(&quot;#8E6F3E&quot;, &quot;#1c5253&quot;,&quot;#23395b&quot;,&quot;#6F727B&quot;, &quot;#F97B64&quot;)) Let's add a title: pie(expenditure, labels = expenditure_names, col = c(&quot;#8E6F3E&quot;, &quot;#1c5253&quot;,&quot;#23395b&quot;,&quot;#6F727B&quot;, &quot;#F97B64&quot;), main = &quot;Mean US expenditure from 1940 to 1960&quot;) dotchart dotchart draws a Cleveland dot plot. Fun Fact: Dr. Cleveland is a Distinguished Professor in the Statistics department at Purdue University! The following is an example using the built-in HairEyeColor dataset. First, let's consider only individuals with black hair. # Selecting only individuals with black hair black_hair = HairEyeColor[1,,] # Summing both Male and Female. black_hair = rowSums(black_hair) Now we can create our dotchart. dotchart(black_hair) Let's add a title, and labels to the x-axis and the y-axis. dotchart(black_hair, main=&#39;Eye color for individuals with black hair&#39;, xlab=&#39;Count&#39;, ylab=&#39;Eye color&#39;) That's better. Let's arrange the data in an ascending manner. # re-ordering the data black_hair &lt;- sort(black_hair) dotchart(black_hair, main=&#39;Eye color for individuals with black hair&#39;, xlab=&#39;Count&#39;, ylab=&#39;Eye color&#39;) How about some color? dotchart(black_hair, main=&#39;Eye color for individuals with black hair&#39;, xlab=&#39;Count&#39;, ylab=&#39;Eye color&#39;, bg=&#39;orange&#39;) plot plot is a generic plotting function. It creates scatter plots as well as line plots. The argument type allows you to define the type of plot that should be drawn. Most common types are &quot;p&quot; for points (default), &quot;l&quot; for lines, and &quot;b&quot; for both. Scatter plots Below is an example using the built-in Orange dataset. plot(Orange$age, Orange$circumference) The labels for x-axis and y-axis can be improved! plot(Orange$age, Orange$circumference, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;) We can also add a title. plot(Orange$age, Orange$circumference, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;) The argument pch specifies what symbol to use when plotting. pch set at &quot;21&quot; enables us to have colored circles. We can specify both the border and fill colors. Let's give it a try. plot(Orange$age, Orange$circumference, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, pch=21, bg=&#39;lightblue&#39;, col=&#39;tomato&#39;) How about coloring the points based on the tree? plot(Orange$age, Orange$circumference, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, pch=21, bg=Orange$Tree) Line plots Below is an example using the built-in Orange dataset. plot(Orange$age, Orange$circumference, type=&#39;l&#39;) Let's fix the title and axes labels. plot(Orange$age, Orange$circumference, type=&#39;l&#39;, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;) lty is an argument that allows us to change the linetype. This is the equivalent version of pch for lines. There 7 options: &quot;blank&quot;, &quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;, &quot;dotdash&quot;, &quot;longdash&quot;, and &quot;twodash&quot;. plot(Orange$age, Orange$circumference, type=&#39;l&#39;, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, lty=&#39;longdash&#39;) We can also modify the thickness of the lines using the argument lwd. Below is an example. plot(Orange$age, Orange$circumference, type=&#39;l&#39;, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, lty=&#39;longdash&#39;, lwd=1.5) lines lines draws additional lines to an existing graphic. For example, let's add lines to our orange scatter plot. # Original chart plot(Orange$age, Orange$circumference, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, pch=21, bg=Orange$Tree) # Adding lines lines(Orange$age, Orange$circumference) The lines are too strong. It will probably be nicer to have them in a different type, such as &quot;dotted&quot;. # Original chart plot(Orange$age, Orange$circumference, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, pch=21, bg=Orange$Tree) # Adding lines lines(Orange$age, Orange$circumference, lty=&#39;dotted&#39;) Note that we could continue to add lines. For example, suppose we now want to add the average orange growth line. # Original chart plot(Orange$age, Orange$circumference, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, pch=21, bg=Orange$Tree) # Adding lines lines(Orange$age, Orange$circumference, lty=&#39;dotted&#39;) # Getting average growth avg_growth &lt;- tapply(Orange$circumference, Orange$age, mean) # Adding the average growth line lines(unique(Orange$age), avg_growth, col=&#39;tomato&#39;, lwd=2.5) We can add lines to any plot. Here is an example adding lines to a barplot. # Original chart par(oma=c(3,0,0,0)) barplot(precip[1:10], las=2) # Adding a dot-dash vertical line lines(0:12, rep(20,13), lty=&#39;longdash&#39;) points points draws points on an existing graphic. For example, let's add the points to the line plot we did earlier. # Original chart plot(Orange$age, Orange$circumference, type=&#39;l&#39;, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;) # Adding points points(Orange$age, Orange$circumference) It's hard to see the points. It would help to have the lines be dark grey, and have the points be colored. # Original chart with grey lines plot(Orange$age, Orange$circumference, type=&#39;l&#39;, xlab=&#39;Tree age&#39;, ylab=&#39;Tree circumference&#39;, main=&#39;Growth of orange trees&#39;, col=&#39;grey&#39;) # Adding points points(Orange$age, Orange$circumference, pch=20, col=&#39;tomato&#39;) Much better! Similar to lines, we can add points to any plot. Here is an example adding lines to a barplot. # Original chart par(oma=c(3,0,0,0)) barplot(precip[1:10], las=2) # Adding a dot-dash vertical line x_values &lt;- seq(1,10, length=10) + seq(-.3,1.5,length=10) # adjusting x positions points(x_values, precip[1:10], pch=21, bg=&#39;steelblue&#39;) abline abline is similar to the lines function. Below are some examples. Let's add a Y=X line (with intercept=0 and slope=1). # Original chart plot(cars$speed, cars$dist, xlab=&quot;Speed (mph)&quot;, ylab=&quot;Stopping distance (ft)&quot;) # Adding Y=X line abline(a=0, b=1) # a = intercept, b=slope Let's add a horizontal line at 60. # Original chart plot(cars$speed, cars$dist, xlab=&quot;Speed (mph)&quot;, ylab=&quot;Stopping distance (ft)&quot;) # Adding a dotted horizontal line abline(h=60, lty=&#39;dotted&#39;) Let's add a vertical line at 15. # Original chart plot(cars$speed, cars$dist, xlab=&quot;Speed (mph)&quot;, ylab=&quot;Stopping distance (ft)&quot;) # Adding a dot-dash vertical line abline(v=15, lty=&#39;dotdash&#39;) As with lines and points, we can continue to add ablines. # Original chart plot(cars$speed, cars$dist, xlab=&quot;Speed (mph)&quot;, ylab=&quot;Stopping distance (ft)&quot;) # Adding Y=X line abline(a=0, b=1) # a = intercept, b=slope # Adding a dotted horizontal line abline(h=60, lty=&#39;dotted&#39;) # Adding a dot-dash vertical line abline(v=15, lty=&#39;dotdash&#39;) As lines and points we can add ablines to any plot. Here is an example adding lines to a dotchart. # Original chart dotchart(black_hair, main=&#39;Eye color for individuals with black hair&#39;, xlab=&#39;Count&#39;, ylab=&#39;Eye color&#39;, bg=&#39;orange&#39;) # Adding a dot-dash vertical line abline(v=15, lty=&#39;dotdash&#39;) text text enables us to add texts to our plots. Similarly to points,lines, and abline we can add text to any plot. For the example below, we will focus on scatter plots and the built-in dataset mtcars. # Original chart plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;) # Text with some additional comments # x and y enables us to select a location text(x=29,y=460,&#39;Note a downward trend&#39;) How about making it italicized? We can change the font using the font argument. It takes 4 values: 1 or plain, 2 or bold, 3 or italic, 4 and bold-italic. # Original chart plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;) # Text with some additional comments text(x=29,y=460,&#39;Note a downward trend&#39;, font=3) How about we add labels that show what cars are some (or all) of these points? We can do this using the argument labels. # Original chart plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;) # Text with some additional comments text(x=29,y=460,&#39;Note a downward trend&#39;, font=3) # Selecting some cars subset_mtcars &lt;- subset(mtcars, ((mpg&gt;18&amp;mpg&lt;20)&amp;disp&gt;300)) # Label to some cars text(x=subset_mtcars$mpg,y=subset_mtcars$disp,labels=row.names(subset_mtcars)) We can definitely improve the location of these labels. Let's add some offset to the x-axis. We can do this two ways: Literally add an offset to x, or Use the adj argument. Below is the example for option (1). # Original chart plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;) # Text with some additional comments text(x=29,y=460,&#39;Note a downward trend&#39;, font=3) # Label to some cars with an offset to x-axis text(x=subset_mtcars$mpg+4.5,y=subset_mtcars$disp,labels=row.names(subset_mtcars)) Below is the example for option (2). # Original chart plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;) # Text with some additional comments text(x=29,y=460,&#39;Note a downward trend&#39;, font=3) # Label to some cars text(x=subset_mtcars$mpg,y=subset_mtcars$disp,labels=row.names(subset_mtcars), adj=-0.1) Could we decrease the size of the labels? # Original chart plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;) # Text with some additional comments text(x=29,y=460,&#39;Note a downward trend&#39;, font=3) # Label to some cars text(x=subset_mtcars$mpg,y=subset_mtcars$disp,labels=row.names(subset_mtcars), adj=-0.1, cex=.8) mtext mtext is similar to the text function. However, it enables you to write in one of the four margins of the plot. Below is an example using the built-in mtcars dataset. # Original chart plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;, main=&#39;Motor trend car results&#39;) # Adding text to the top margin: mtext(&quot;Data from 1974 Motor Trend US magazine&quot;, font=3, cex=.7) # Recall that `cex` controls the font size. legend The legend function enables us to add legends to plots. The example below uses the built-in dataset iris. The scatter plot below colors the data based on the flower's species. # Original chart, colors are based on species plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) Let's create a legend for this plot to make it clear what the colors represent. # Original chart, colors are based on species plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20) We can improve the look of the legend by making the points bigger, and removing the box. # Original chart, colors are based on species plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20, pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box What if we made the legend's text smaller and italicized? # Original chart, colors are based on species plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box par par allows us to set several graphical parameters. Among the many parameters that can be set, some of the most commonly used ones are mfrow, mfcol, mar, and oma. mfrow and mfcol enables us to create a layout for plots, so that we can include several graphs side by side. mar and oma set margins using the following form c(bottom, left, top, right). oma looks at outer margins. Note that you can set several parameters all at once. mfrow, mfcol The example below uses the built-in data mtcars. mfrow and mfcol takes vector of the form c(nr, nc), where nr represents the number of rows and nc the number of columns. par(mfrow=c(2,3)) # two rows, three columns # Plot #1 plot(mtcars$mpg, mtcars$disp, xlab=&#39;Miles/(US) gallon&#39;, ylab=&#39;Displacement (cu.in.)&#39;, pch=21, bg=&#39;orange&#39;, main=&#39;Plot 1&#39;) # Plot #2 boxplot(mtcars$wt, xlab=&#39;Weight (1000 lbs)&#39;, col=&#39;steelblue&#39;,main=&#39;Plot 2&#39;) # Plot #3 barplot(table(mtcars$vs), col=c(&#39;tomato&#39;,&quot;#23395b&quot;), xlab=&#39;Engine&#39;, names.arg = c(&#39;V-shaped&#39;, &#39;Straight&#39;), main=&#39;Plot 3&#39;) # Plot #4 dotchart(mtcars$mpg, pch=21, bg=&quot;#43418A&quot;, xlim=c(10, 42), xlab=&#39;Miles/(US) gallon&#39;, main=&#39;Plot 4&#39;) text(mtcars$mpg[c(1:2, 31:32)], c(1:2, 31:32), labels=row.names(mtcars)[c(1:2, 31:32)], adj = -.2, cex = .75, font=4) # Plot #5 pie(table(mtcars$am), labels=c(&#39;Automatic&#39;, &#39;Manual&#39;), main=&#39;Plot 5&#39;) # Plot #6 boxplot(mtcars$hp ~mtcars$am, names=c(&quot;Automatic&quot;, &quot;Manual&quot;), xlab=&#39;Transmission&#39;, ylab=&#39;Horsepower&#39;, col=c(&quot;#ceb888&quot;,&quot;#03A696&quot;), main=&#39;Plot 6&#39;) mar, oma The example below uses the built-in data iris. # Original plot plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box Remove all margins. par(mar=c(0,0,0,0)) # Original plot plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box Add larger margins on the bottom and left side. par(mar=c(4,6,2,2)) # Original plot plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box How do these margins look set on two plots side by side? par(mar=c(4,6,2,2), mfrow=c(1,2)) # First plot plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box # Second plot plot(iris$Petal.Length, iris$Petal.Width, xlab=&#39;Petal length&#39;, ylab=&#39;Peta width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;bottomright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box Doesn't look very good. Let's try setting smaller margins. Note that the default values for mar are mar=c(5.1, 4.1, 4.1, 2.1). par(mar=c(4, 4, 2, 1), mfrow=c(1,2)) # First plot plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;topright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box # Second plot plot(iris$Petal.Length, iris$Petal.Width, xlab=&#39;Petal length&#39;, ylab=&#39;Peta width&#39;, pch=21, bg=iris$Species) # Adding a legend: legend(&quot;bottomright&quot;, legend=unique(iris$Species), col=1:3, pc=20, cex = .9, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;) # removing box Perhaps we don't need two legends. How about we increase the margins (outer and usual) for top and bottom to include legend at the bottom, and a join title at the top? par(mar=c(6, 4, 1, 1), mfrow=c(1,2), oma=c(2,0,3,0)) # First plot plot(iris$Sepal.Length, iris$Sepal.Width, xlab=&#39;Sepal length&#39;, ylab=&#39;Sepal width&#39;, pch=21, bg=iris$Species) # Adding a legend legend(&quot;bottom&quot;,legend=unique(iris$Species), col=1:3, pc=20, cex = .8, # text size text.font=3, # italic text pt.cex = 1.5, # changing just the point size bty=&#39;n&#39;,# removing box xpd = TRUE, horiz = TRUE, # make legend horizontal inset=c(2,-0.50)) # changes to x and y positions # Second plot plot(iris$Petal.Length, iris$Petal.Width, xlab=&#39;Petal length&#39;, ylab=&#39;Peta width&#39;, pch=21, bg=iris$Species) # Joint title mtext(&quot;Results for 3 species of iris flowers&quot;, outer=TRUE, font=2) plot_usmap usmap is a package dedicated to get maps of the US by varying region types. Includes the plot_usmap function which allows you do easily plot state or region level data on top of a map. First, load up the package: library(usmap) You can generate the default map pretty easily. plot_usmap(&quot;states&quot;, labels=T) The first argument, regions can be &quot;states&quot;, &quot;state&quot;, &quot;counties&quot;, or &quot;county&quot;. You can switch the borders by changing this argument. plot_usmap(&quot;counties&quot;, labels=T) As you can see, adding the labels in this case, obfuscates our map. plot_usmap(&quot;counties&quot;, labels=F) If we wanted to zoom in on a state, this is easy to do. plot_usmap(&quot;counties&quot;, include=c(&quot;IN&quot;)) Of course, you can still just zoom in on a group of states, you don't have to show the county lines. plot_usmap(&quot;states&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;)) Pretty incredible. You can change the label colors using the label_color argument. plot_usmap(&quot;states&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;), label_color=&quot;gold&quot;) You can even have different colors for each of the states. plot_usmap(&quot;states&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;), label_color=c(&quot;blue&quot;, &quot;green&quot;, &quot;gold&quot;, &quot;tomato&quot;)) Similarly, you can control the fill color using the fill argument. plot_usmap(&quot;states&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;), label_color=c(&quot;blue&quot;, &quot;green&quot;, &quot;gold&quot;, &quot;tomato&quot;), fill=&quot;grey&quot;) You can control the border color using the color argument. plot_usmap(&quot;states&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;), label_color=c(&quot;blue&quot;, &quot;green&quot;, &quot;gold&quot;, &quot;tomato&quot;), fill=&quot;grey&quot;, color=&quot;white&quot;) We can control the border width with the size argument as well. plot_usmap(&quot;states&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;), label_color=c(&quot;blue&quot;, &quot;green&quot;, &quot;gold&quot;, &quot;tomato&quot;), fill=&quot;grey&quot;, color=&quot;white&quot;, size=2) Of course, it is important to be able to utilize a dataset with plot_usmap. To do so you must use the data and values arguments. The data argument expects a data.frame with at least two columns. One column to indicate which state or county, and another to indicate the associated values (whatever they may be). The column indicating the state or value must be named either fips or state. The other column can be anything as long as you use the values argument to specify the name. myDF &lt;- data.frame(state=state.abb, val=datasets::state.area) head(myDF) ## state val ## 1 AL 51609 ## 2 AK 589757 ## 3 AZ 113909 ## 4 AR 53104 ## 5 CA 158693 ## 6 CO 104247 plot_usmap(data=myDF, values=&quot;val&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;)) To move the legend out of the way, you can use theme from ggplot2. library(ggplot2) plot_usmap(data=myDF, values=&quot;val&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;)) + theme(legend.position = &quot;right&quot;) If we wanted to change the colors and way the shading works, we can use scale_fill_continous from ggplot2. library(ggplot2) plot_usmap(data=myDF, values=&quot;val&quot;, labels=T, include=c(&quot;IL&quot;, &quot;MI&quot;, &quot;IN&quot;, &quot;OH&quot;)) + theme(legend.position = &quot;right&quot;) + scale_fill_continuous(low=&quot;white&quot;, high=&quot;navy&quot;) It would probably look better if we had more than 4 points. Let's try with the entire US. library(ggplot2) plot_usmap(data=myDF, values=&quot;val&quot;, labels=T) + theme(legend.position = &quot;right&quot;) + scale_fill_continuous(low=&quot;white&quot;, high=&quot;navy&quot;) It really puts AK's area into perspective! How about if we remove AK using the exclude argument? library(ggplot2) plot_usmap(data=myDF, values=&quot;val&quot;, labels=T, exclude=c(&quot;AK&quot;)) + theme(legend.position = &quot;right&quot;) + scale_fill_continuous(low=&quot;white&quot;, high=&quot;navy&quot;) Note that if the regions argument is &quot;state&quot; or &quot;states&quot;, either the state name, abbreviation, or fips code would work to identify the state. The full 5-digit fips code is required to identify counties, however. To get a fips code for a certain county, you can do the following. usmap::fips(state = &quot;IN&quot;, county=&quot;Tippecanoe&quot;) ## [1] &quot;18157&quot; Note that the first 2 digits of the 5 digit fips code is the state fips code. usmap::fips(state = &quot;IN&quot;) ## [1] &quot;18&quot; What if we wanted to show area by the percentage of area that the state represents? First we would need to calculate it. myDF$percent_area &lt;- myDF$val/sum(myDF$val) library(ggplot2) plot_usmap(data=myDF, values=&quot;percent_area&quot;, labels=T) + theme(legend.position = &quot;right&quot;) + scale_fill_continuous(low=&quot;white&quot;, high=&quot;navy&quot;) After that, we can use the scales packages to fix the legend up. library(ggplot2) plot_usmap(data=myDF, values=&quot;percent_area&quot;, labels=T) + theme(legend.position = &quot;right&quot;) + scale_fill_continuous(low=&quot;white&quot;, high=&quot;navy&quot;, name=&quot;Percent of US area&quot;, label=scales::percent) If you were working with data that would be better represented by dollars instead of percentages, you could simply change the label argument to scales::dollars. Resources Simple examples A page with some code examples and output using usmap. More examples using usmap A page with some code examples and output using usmap. A little bit more in depth. ggplot ggmap ggmap is an excellent package that provides a suite of functions that, among other things, allows you to map spatial data on top of static maps. Important note: You must set up billing in order to use Google's APIs. Getting started To install ggmap, simply run install.packages(&quot;ggmap&quot;). To load the library, run library(ggmap). When first using this package, you may notice you need an API key to get access to certain functionality. Follow the directions here to get an API key. It should looks somethings like: mQkzTpiaLYjPqXQBotesgif3EfGL2dbrNVOrogg. Once you've acquired the API key, you have two options: Register ggmap with Google for the current session: library(ggmap) register_google(key=&quot;mQkzTpiaLYjPqXQBotesgif3EfGL2dbrNVOrogg&quot;) Register ggmap with Google, persistently through sessions: library(ggmap) register_google(key=&quot;mQkzTpiaLYjPqXQBotesgif3EfGL2dbrNVOrogg&quot;, write=TRUE) Note that if you choose option (2), your API key will be saved within your ~/.Renviron. Examples How do I get a map of West Lafayette? Click here for solution map &lt;- get_map(location=&quot;West Lafayette&quot;) ggmap(map) How do I zoom in and out on a map of West Lafayette? Click here for solution # zoom way out map &lt;- get_map(location=&quot;West Lafayette&quot;, zoom=1) ggmap(map) # zoom in map &lt;- get_map(location=&quot;West Lafayette&quot;, zoom=12) ggmap(map) How do I add Latitude and Longitude points to a map of Purdue University? Click here for solution points_to_add &lt;- data.frame(latitude=c(40.433663, 40.432104, 40.428486), longitude=c(-86.916584, -86.919610, -86.920866)) map &lt;- get_map(location=&quot;Purdue University&quot;, zoom=14) ggmap(map) + geom_point(data = points_to_add, aes(x = longitude, y = latitude)) leaflet leaflet is a popular JavaScript library to create interactive maps. The leaflet R package makes it easy to create incredible interactive maps. Examples How do I plot some longitude and latitude points on an interactive map? Click here for solution library(leaflet) points_to_plot &lt;- data.frame(latitude=c(40.433663, 40.432104, 40.428486), longitude=c(-86.916584, -86.919610, -86.920866)) map &lt;- leaflet() map &lt;- addTiles(map) map &lt;- addCircles(map, lng=points_to_plot$longitude, lat=points_to_plot$latitude) map # or another way with magrittr library(magrittr) leaflet(points_to_plot) %&gt;% addTiles() %&gt;% addCircles(lng=~longitude, lat=~latitude) magrittr is a package that adds the %&gt;% and `%&lt;% operators which allow you to pipe the output of R code to more R code, much like piping in bash. You can read more about it here. RMarkdown To install RMarkdown simply run the following: install.packages(&quot;rmarkdown&quot;) Projects in The Data Mine are all written in RMarkdown. You can download the RMarkdown file by clicking on the link at the top of each project page. Each file should end in the &quot;.Rmd&quot; which is the file extension commonly associated with RMarkdown files. You can find an exemplary RMarkdown file here: https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/rmarkdown.Rmd If you open this file in RStudio, and click on the &quot;Knit&quot; button in the upper left hand corner of IDE, you will get the resulting HTML file. Open this file in the web browser of your choice and compare and contrast the syntax in the rmarkdown.Rmd file and resulting output. Play around with the file, make modifications, and re-knit to gain a better understanding of the syntax. Note that similar input/output examples are shown in the RMarkdown Cheatsheet. Click here for video Code chunks Code chunks are sections within an RMarkdown file where you can write, display, and optionally evaluate code from a variety of languages: ## [1] &quot;awk&quot; &quot;bash&quot; &quot;coffee&quot; &quot;gawk&quot; &quot;groovy&quot; ## [6] &quot;haskell&quot; &quot;lein&quot; &quot;mysql&quot; &quot;node&quot; &quot;octave&quot; ## [11] &quot;perl&quot; &quot;psql&quot; &quot;Rscript&quot; &quot;ruby&quot; &quot;sas&quot; ## [16] &quot;scala&quot; &quot;sed&quot; &quot;sh&quot; &quot;stata&quot; &quot;zsh&quot; ## [21] &quot;highlight&quot; &quot;Rcpp&quot; &quot;tikz&quot; &quot;dot&quot; &quot;c&quot; ## [26] &quot;cc&quot; &quot;fortran&quot; &quot;fortran95&quot; &quot;asy&quot; &quot;cat&quot; ## [31] &quot;asis&quot; &quot;stan&quot; &quot;block&quot; &quot;block2&quot; &quot;js&quot; ## [36] &quot;css&quot; &quot;sql&quot; &quot;go&quot; &quot;python&quot; &quot;julia&quot; ## [41] &quot;sass&quot; &quot;scss&quot; &quot;theorem&quot; &quot;lemma&quot; &quot;corollary&quot; ## [46] &quot;proposition&quot; &quot;conjecture&quot; &quot;definition&quot; &quot;example&quot; &quot;exercise&quot; ## [51] &quot;proof&quot; &quot;remark&quot; &quot;solution&quot; The syntax is simple: ```{language, options...} code here... ``` For example: ```{r, echo=TRUE} my_variable &lt;- c(1,2,3) my_variable ``` Which will render like: my_variable &lt;- c(1,2,3) my_variable ## [1] 1 2 3 You can find a list of chunk options here. How do I run a code chunk but not display the code above the results? Click here for solution ```{r, echo=FALSE} my_variable &lt;- c(1,2,3) my_variable ``` How do I include a code chunk without evaluating the code itself? Click here for solution ```{r, eval=FALSE} my_variable &lt;- c(1,2,3) my_variable ``` How do I prevent warning messages from being displayed? Click here for solution ```{r, warning=FALSE} my_variable &lt;- c(1,2,3) my_variable ``` How do I prevent error messages from being displayed? Click here for solution ```{r, error=FALSE} my_variable &lt;- c(1,2,3) my_variable ``` How do I run a code chunk, but not include the chunk in the final output? Click here for solution ```{r, include=FALSE} my_variable &lt;- c(1,2,3) my_variable ``` How do I render a figure from a chunk? Click here for solution ```{r} my_variable &lt;- c(1,2,3) plot(my_variable) ``` How do I create a set of slides using RMarkdown? Click here for solution Please see the example Rmarkdown file here. You can change the slide format by changing the yaml header to any of: ioslides_presentation, slidy_presentation, or beamer_presentation. By default all first and second level headers (# and ##, respectively) will create a new slide. To manually create a new slide, you can use ***. Resources RMarkdown Cheatsheet An excellent quick reference for RMarkdown syntax. RMarkdown Reference A thorough reference manual showing markdown input and expected output. Gives descriptions of the various chunk options, as well as output options. RStudio RMarkdown Lessons A set of lessons detailing the ins and outs of RMarkdown. Markdown Tutorial RMarkdown uses Markdown syntax for its text. This is a good, interactive tutorial to learn the basics of Markdown. This tutorial is available in multiple languages. RMarkdown Gallery This gallery highlights a variety of reproducible and interactive RMarkdown documents. An excellent resource to see the power of RMarkdown. RMarkdown Chapter This is a chapter from Hadley Wickham's excellent R for Data Science book that details important parts of RMarkdown. RMarkdown in RStudio This is a nice article that introduces RMarkdown, and guides the user through creating their own interactive document using RMarkdown in RStudio. Reproducible Research This is another good resource that introduces RMarkdown. Plenty of helpful pictures and screenshots. Tidyverse piping glimpse filter arrange mutate group_by str_extract and str_extract_all str_extract and str_extract_all are useful functions from the stringr package. You can install the package by running: install.packages(&quot;stringr&quot;) str_extract extracts the text which matches the provided regular expression or pattern. Note that this differs from grep in a major way. grep simply returns the index in which a pattern match was found. str_extract returns the actual matching text. Note that grep typically returns the entire line where a match was found. str_extract returns only the part of the line or text that matches the pattern. For example: text &lt;- c(&quot;cat&quot;, &quot;mat&quot;, &quot;spat&quot;, &quot;spatula&quot;, &quot;gnat&quot;) # All 5 &quot;lines&quot; of text were a match. grep(&quot;.*at&quot;, text) ## [1] 1 2 3 4 5 text &lt;- c(&quot;cat&quot;, &quot;mat&quot;, &quot;spat&quot;, &quot;spatula&quot;, &quot;gnat&quot;) stringr::str_extract(text, &quot;.*at&quot;) ## [1] &quot;cat&quot; &quot;mat&quot; &quot;spat&quot; &quot;spat&quot; &quot;gnat&quot; As you can see, although all 5 words match our pattern and would be returned by grep, str_extract only returns the actual text that matches the pattern. In this case &quot;spatula&quot; is not a &quot;full&quot; match -- the pattern &quot;.*at&quot; only captures the &quot;spat&quot; part of &quot;spatula&quot;. In order to capture the rest of the word you would need to add something like &quot;.*&quot; to the end of the pattern: text &lt;- c(&quot;cat&quot;, &quot;mat&quot;, &quot;spat&quot;, &quot;spatula&quot;, &quot;gnat&quot;) stringr::str_extract(text, &quot;.*at.*&quot;) ## [1] &quot;cat&quot; &quot;mat&quot; &quot;spat&quot; &quot;spatula&quot; &quot;gnat&quot; One final note is that you must double-escape certain characters in patterns because R treats backslashes as escape values for character constants (stackoverflow). For example, to write \\( we must first escape the \\, so we write \\\\(. This is true for many character which would normally only be preceded by a single \\. Examples How can I extract the text between parenthesis in a vector of texts? Click here for solution text &lt;- c(&quot;this is easy for (you)&quot;, &quot;there (are) challenging ones&quot;, &quot;text is (really awesome) (ok?)&quot;) # Search for a literal &quot;(&quot;, followed by any amount of any text other than more parenthesis ([^()]*), followed by a literal &quot;)&quot;. stringr::str_extract(text, &quot;\\\\([^()]*\\\\)&quot;) ## [1] &quot;(you)&quot; &quot;(are)&quot; &quot;(really awesome)&quot; To get all matches, not just the first match: text &lt;- c(&quot;this is easy for (you)&quot;, &quot;there (are) challenging ones&quot;, &quot;text is (really awesome) more text (ok?)&quot;) # Search for a literal &quot;(&quot;, followed by any amount of any text (.*), followed by a literal &quot;)&quot;. stringr::str_extract_all(text, &quot;\\\\([^()]*\\\\)&quot;) ## [[1]] ## [1] &quot;(you)&quot; ## ## [[2]] ## [1] &quot;(are)&quot; ## ## [[3]] ## [1] &quot;(really awesome)&quot; &quot;(ok?)&quot; lubridate lubridate is a fantastic package that makes the typical tasks one would perform on dates, that much easier. How do I convert a string &quot;07/05/1990&quot; to a Date? Click here for solution library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union dat &lt;- &quot;07/05/1990&quot; dat &lt;- mdy(dat) class(dat) ## [1] &quot;Date&quot; How do I convert a string &quot;31-12-1990&quot; to a Date? Click here for solution my_string &lt;- &quot;31-12-1990&quot; dat &lt;- dmy(my_string) dat ## [1] &quot;1990-12-31&quot; class(dat) ## [1] &quot;Date&quot; How do I convert a string &quot;31121990&quot; to a Date? Click here for solution my_string &lt;- &quot;31121990&quot; my_date &lt;- dmy(my_string) my_date ## [1] &quot;1990-12-31&quot; class(my_date) ## [1] &quot;Date&quot; How do I extract the day, week, month, quarter, and year from a Date? Click here for solution my_date &lt;- dmy(&quot;31121990&quot;) day(my_date) ## [1] 31 week(my_date) ## [1] 53 month(my_date) ## [1] 12 quarter(my_date) ## [1] 4 year(my_date) ## [1] 1990 strrep strrep is a function that allows you to repeat the characters a given number of times. Examples How to repeat the string of characters ABC three times? Click here for solution strrep(&quot;ABC&quot;, 3) ## [1] &quot;ABCABCABC&quot; How to get a vector in which A is repeated twice B three times and C four times? Click here for solution strrep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(2,3,4)) ## [1] &quot;AA&quot; &quot;BBB&quot; &quot;CCCC&quot; nchar nchar is a function which counts the number of characters and symbols in a word or a string. Punctuation and blank spaces are counted as well. Examples How to to find the number of characters and or symbols the word &quot;Protozoa&quot;? Click here for solution nchar(&quot;Protozoa&quot;) ## [1] 8 How to to find the number of characters and or symbols forthe following strings all at once &quot;pneumonoultramicroscopicsilicovolcanoconiosis&quot;, &quot;password: DatamineRocks#stat1900@&quot;? Fun fact: &quot;pneumonoultramicroscopicsilicovolcanoconiosis&quot; is the longest word in the English dictionary. Click here for solution string_vector &lt;- c(&quot;pneumonoultramicroscopicsilicovolcanoconiosis&quot;, &quot;password: DatamineRocks#stat1900@&quot;) nchar(string_vector) ## [1] 45 33 Resources Lubridate Cheatsheet A comprehensive cheatsheet on lubridate. Excellent resource to immediately begin using lubridate. data.table SQL in R Scraping shiny Rendering images "],
["python.html", "Python Getting started Variables Printing Logical operators Lists &amp; Tuples Dicts Sets Control flow Writing functions Reading &amp; Writing data pathlib numpy scipy pandas Jupyter notebooks Writing scripts Scraping XML Plotting Classes tensorflow pytorch", " Python Getting started Python on Scholar Each year we provide students with a working Python kernel that students are able to select and use from within https://notebook.scholar.rcac.purdue.edu/ as well as within an Rmarkdown document in https://rstudio.scholar.rcac.purdue.edu/. We ask that students use this kernel when completing all Python-related questions for the course. This ensures version consistency for Python and all packages that students will use during the academic year. In addition, this enables staff to quickly modify the Python environment for all students should the need arise. Let's configure this so every time you access https://notebook.scholar.rcac.purdue.edu/ or https://rstudio.scholar.rcac.purdue.edu/, you will have access to the proper kernel, and the default version of python is correct. Navigate to https://rstudio.scholar.rcac.purdue.edu/, and login using your Purdue credentials. In the menu, click Tools &gt; Shell.... You should be presented with a shell towards the bottom left. Click within the shell, and type the following followed by pressing Enter or Return: /class/datamine/apps/runme After executing the script, in the menu, click Session &gt; Restart R. In order to run Python within https://rstudio.scholar.rcac.purdue.edu/, log in to https://rstudio.scholar.rcac.purdue.edu/ and run the following in the Console or in an R code chunk: datamine_py() install.packages(&quot;reticulate&quot;) The function datamine_py &quot;activates&quot; the Python environment we have setup for the course. Any time you want to use our environment, simply run the R function at the beginning of any R Session, prior to running anything Python code chunks. To test if the Python environment is working within https://rstudio.scholar.rcac.purdue.edu/, run the following in a Python code chunk: import sys print(sys.executable) The python executable should be located in the appropriate folder in the following path: /class/datamine/apps/python/. The runme script also adds a kernel to the list of kernels shown in https://notebook.scholar.rcac.purdue.edu/. To test if the kernel is available and working, navigate to https://notebook.scholar.rcac.purdue.edu/, login, click on New, and select the kernel matching the current year. For example, you would select f2020-s2021 for the 2020-2021 academic year. Once the notebook has launched, you can confirm the version of Python by running the following in a code cell: import sys print(sys.executable) The python executable should be located in the appropriate folder in the following path: /class/datamine/apps/python/. If you already have a a Jupyter notebook running at https://notebook.scholar.rcac.purdue.edu/, you may need to refresh in order for the kernel to appear as an option in Kernel &gt; Change Kernel. If you would like to use the Python environment that is put together for this class, from within a terminal on Scholar, run the following: source /class/datamine/apps/python.sh This will load the environment and python will launch our environment's interpreter. A note on indentation In most languages indentation is for appearance only. In Python, indentation is very important. In a language, like R, for instance, a block of code is defined by curly braces. For example, the curly braces indicate the bounds of the if statement in the following code chunk: my_val &lt;- TRUE if (my_val) { print(&quot;This is inside the bounds of the if statement, and will be evaluated because my_val is TRUE.&quot;) print(&quot;This too is inside the bounds of the if statement, and will be evaluated&quot;) } print(&quot;This is not longer inside the bounds of the if statement.&quot;) In Python, the equivalent would be: my_val = True if my_val: print(&quot;This is inside the bounds of the if statement, and will be evaluated because my_val is TRUE.&quot;) print(&quot;This too is inside the bounds of the if statement, and will be evaluated&quot;) print(&quot;This is not longer inside the bounds of the if statement.&quot;) As you can see, there are no curly braces to indicate the bounds of the if statement. Instead, the level of indentation is used. This applies for for loops as well: values = [1,2,3,4,5] for v in values: print(f&quot;v: {v}&quot;) ## v: 1 ## v: 2 ## v: 3 ## v: 4 ## v: 5 Variables Variables are declared just like in R, but rather than using &lt;- and -&gt;, Python uses a single = as is customary for most languages. You can declared variables like this: my_var = 4 Here, we declared a variable with a value of 4. Important note: Actually this is technically not true. Numbers between -5 and 256 (inclusive) are already pre-declared and exist within Python's memory before you assigned the value to my_var. The = operator simply forces my_var to point to that value that already exists! That is right, my_var is technically a pointer. One extremely important distinction between declaring variables in Python vs. in R is what is actually happening under the hood. Take the following code: my_var = 4 new_var = my_var my_var = my_var + 1 print(f&quot;my_var: {my_var}\\nnew_var: {new_var}&quot;) ## my_var: 5 ## new_var: 4 my_var = [4,] new_var = my_var my_var[0] = my_var[0] + 1 print(f&quot;my_var: {my_var}\\nnew_var: {new_var}&quot;) ## my_var: [5] ## new_var: [5] Here, the first chunk of code behaves as expected because ints are immutable, meaning the values cannot be changed. As a result, when we assign my_var = my_var + 1, my_var's value isn't changing, my_var is just being pointed to a different value of 5, which is not where new_var points. new_var still points to the value of 4. The second chunk however is dealing with a mutable list. We first assign the first value of our list to a value of 4. Then we assign my_var to new_var. This does not copy the values of my_var to new_var, but rather new_var now points to the same exact object. Then, when we increment the first value in my_var, that same change is reflected when we print the value in new_var, because new_var and my_var are the same object, i.e. new_var is my_var. An excellent article that goes into more detail can be found here. None None is a keyword used to define a null value. This would be the Python equivalent to R's NULL. If used in an if statement, None represents False. This does not mean None == False, in fact: print(None == False) ## False As you can see, although None can represent False in an if statement, they are not equivalent. bool A bool has two possible values: True and False. It is important to understand that technically: print(True == 1) ## True print(False == 0) ## True With that being said, True is not equal to numbers greater than 1: print(True == 2) ## False print(True == 3) ## False With that being said, numbers not equal to 0 evaluate to True when used in an if statement: if 3: print(&quot;3 evaluates to True&quot;) ## 3 evaluates to True if 4: print(&quot;4 evaluates to True&quot;) ## 4 evaluates to True if -1: print(&quot;-1 evaluates to True&quot;) ## -1 evaluates to True str str are strings in Python. Strings are &quot;immutable sequences of Unicode code points&quot;. Strings can be surrounded in single quotes, double quotes, or triple quoted (with either single or double quotes): print(f&quot;Single quoted text is type: {type(&#39;test&#39;)}&quot;) ## Single quoted text is type: &lt;class &#39;str&#39;&gt; print(f&#39;Double quoted text is type: {type(&quot;test&quot;)}&#39;) ## Double quoted text is type: &lt;class &#39;str&#39;&gt; print(f&quot;Triple quoted with single quotes: {type(&#39;&#39;&#39;This is some text&#39;&#39;&#39;)}&quot;) ## Triple quoted with single quotes: &lt;class &#39;str&#39;&gt; print(f&#39;Triple quoted with double quotes: {type(&quot;&quot;&quot;This is some text&quot;&quot;&quot;)}&#39;) ## Triple quoted with double quotes: &lt;class &#39;str&#39;&gt; Triple quoted strings can span multiple lines. All associated whitespace will be incorporated in the string: my_string = &quot;&quot;&quot;This text spans multiple lines.&quot;&quot;&quot; print(my_string) ## This text ## spans multiple ## lines. But, this would cause an error: my_string = &quot;This text, will throw an error&quot; print(my_string) But, you could make it span multiple lines by adding a \\, but newlines won't be maintained: my_string = &quot;This text, \\ will throw an error&quot; print(my_string) ## This text, will throw an error int int's are whole numbers. For instance: my_var = 5 print(type(my_var)) ## &lt;class &#39;int&#39;&gt; int's can be added, subtracted, and multiplied without changing types. With that being said, division of 2 int's results in a float regardless of whether or not the result of the division is a whole number or not: print(type(6+2)) ## &lt;class &#39;int&#39;&gt; print(type(6-2)) ## &lt;class &#39;int&#39;&gt; print(type(6*2)) ## &lt;class &#39;int&#39;&gt; print(type(6/2)) ## &lt;class &#39;float&#39;&gt; Similarly, any calculation between an int and float results in a float: print(type(6+2.0)) ## &lt;class &#39;float&#39;&gt; print(type(6-2.0)) ## &lt;class &#39;float&#39;&gt; print(type(6*2.0)) ## &lt;class &#39;float&#39;&gt; print(type(6/2.0)) ## &lt;class &#39;float&#39;&gt; float float's are floating point numbers, or numbers with decimals. my_var = 5.0 print(type(my_var)) ## &lt;class &#39;float&#39;&gt; float's can be converted back to int's using the int function. This coercion causes the float to be truncated, regardless of how close to the &quot;next&quot; number the float is: print(int(5.5)) ## 5 print(int(5.49)) ## 5 print(int(5.51)) ## 5 print(int(5.99999)) ## 5 complex complex's represent complex numbers. j can be used to represent an imaginary number. j must be preceded by a number, like 1j. my_var = 1j print(my_var) ## 1j print(type(my_var)) ## &lt;class &#39;complex&#39;&gt; Arithmetic with a complex always results in a complex: print(type(1j*2)) ## &lt;class &#39;complex&#39;&gt; print(type(1j*2.0)) ## &lt;class &#39;complex&#39;&gt; print(type(1j*1j)) ## &lt;class &#39;complex&#39;&gt; You cannot convert to an int or float: print(int(1j*1j)) print(float(1j*1j)) Resources Pointers in Python An excellent article explaining what happens under the hood when declaring variables in Python. Printing print is a function in Python that allows you to... well... print. Printing values and information about a program while the program is running is still to this day one of the best methods to debug your code. This is just one good reason to learn about and feel comfortable with printing. You can print simple string literals: print(&quot;This is a simple string literal being printed...&quot;) ## This is a simple string literal being printed... You can print all types of variables, not just strings: print(int(4)) ## 4 print(float(4.4)) ## 4.4 print(False) ## False You can even mix and match what you print: print(&quot;This is a string and an int:&quot;, int(4)) # notice there is a space added between the arguments to print ## This is a string and an int: 4 print(&quot;This is a string and an int and a float:&quot;, int(4), float(4.4)) ## This is a string and an int and a float: 4 4.4 print(int(4), &quot;&lt;- is an integer&quot;) ## 4 &lt;- is an integer You can even do arithmetic inside the print function: print(&quot;4 + 4 =&quot;, 4+4) ## 4 + 4 = 8 There are a series of special characters called escape characters that need to be escaped with a \\, but that represent a different symbol when processed. For example, a newline character is \\n, but when you print \\n it results in a new line: print(&quot;This is line 1.\\nThis is line 2.&quot;) ## This is line 1. ## This is line 2. Here are a couple more escape characters: print(&quot;This is a carriage return\\rAs you can see it is not a visible character.&quot;) ## This is a carriage return As you can see it is not a visible character. print(&quot;This is a .\\tAnd another.\\tAnd now two tabs.\\t\\tNice.&quot;) ## This is a . And another. And now two tabs. Nice. You may now be wondering, well what if I want to literally print \\t or \\n? There are a couple of options: print(&quot;You can escape a forward slash with another forward slash: \\\\&quot;) ## You can escape a forward slash with another forward slash: \\ print(&quot;This would then look like: \\\\t \\\\n&quot;) ## This would then look like: \\t \\n print(r&quot;You could also add an &#39;r&#39; before your string. The &#39;r&#39; represents raw and will render the text literally: \\t \\n&quot;) ## You could also add an &#39;r&#39; before your string. The &#39;r&#39; represents raw and will render the text literally: \\t \\n Similarly, if you want to use double or single quotes within double or single quotes you can escape them as well: print(&quot;This sentence has \\&quot;double quotes\\&quot;.&quot;) ## This sentence has &quot;double quotes&quot;. print(&#39;This sentence has \\&#39;single quotes\\&#39;.&#39;) ## This sentence has &#39;single quotes&#39;. Of course, you can mix and match quotes to avoid needing to escape: print(&#39;Now it is easy to print &quot;double quotes&quot;.&#39;) ## Now it is easy to print &quot;double quotes&quot;. print(&quot;Now it is easy to print &#39;single quotes&#39;.&quot;) ## Now it is easy to print &#39;single quotes&#39;. f-strings f-strings are extremely straightforward, useful, and fast. I would highly recommend using f-strings when the need arrives to print something more than simple text. f-string stands for &quot;format string&quot;. An f-string is a string literal that starts with an f or an F: print(f&#39;This is an f-string.&#39;) ## This is an f-string. print(F&#39;This is an f-string.&#39;) ## This is an f-string. Of course, you can use double or single quotes, like normal: print(f&quot;This still works.&quot;) ## This still works. print(F&quot;So does this.&quot;) ## So does this. What do f-strings do? They allow you to print expressions inline: print(f&quot;4+4={4+4}&quot;) ## 4+4=8 They allow you to call functions: def sumthis(a, b): return(a+b) print(f&quot;4+4={sumthis(4,4)}&quot;) ## 4+4=8 Overall, they are just a really nice feature that makes printing a pleasure. You can even write multi-line f-strings: first = &#39;First&#39; second = &#39;Second&#39; multiline_string = f&quot;First line {first}.&quot; \\ &quot;Second line {second}.&quot; print(multiline_string) ## First line First.Second line {second}. But make sure you put an f before each line. first = &#39;First&#39; second = &#39;Second&#39; multiline_string = f&quot;First line {first}.&quot; \\ f&quot;Second line {second}.&quot; print(multiline_string) ## First line First.Second line Second. Better yet, use triple quotes with the f-string to handle multiline f-strings: multiline_string = f&quot;&quot;&quot;First line {first}. Second line {second}.&quot;&quot;&quot; print(multiline_string) ## First line First. ## Second line Second. Of course, this is not all f-strings are capable of. The &quot;format&quot; comes from somewhere. We can format our dates and times: import datetime dt = datetime.datetime.now() print(f&#39;This is the datetime: {dt: %Y/%m/%d %H:%M}&#39;) ## This is the datetime: 2021/03/14 20:54 As you can see, the content following the : is used to specify the format. For numbers, you can specify the number of decimals: my_float = 444.44444445 print(f&#39;My float: {my_float:.3f}&#39;) ## My float: 444.444 print(f&#39;My float: {my_float:.5f}&#39;) ## My float: 444.44444 Or if you desire leading zeros: my_float = 444.44444445 print(f&#39;My float: {my_float:010.3f}&#39;) ## My float: 000444.444 print(f&#39;My float: {my_float:010.5f}&#39;) ## My float: 0444.44444 print(f&#39;My float: {my_float:10.5f}&#39;) ## My float: 444.44444 Note that the first 0 means &quot;zero pad&quot;, and the following 10 represents the total width of the result. In this case it means zero pad until the full number takes up 10 characters (including the decimal place). You could remove the intial 0 if you want to make numbers line up neatly: my_float = 444.44444445 print(f&#39;My float: {555.55}&#39;) ## My float: 555.55 print(f&#39;My float: {22}&#39;) ## My float: 22 print(f&#39;My float: {1234.5}&#39;) # vs ## My float: 1234.5 print(&quot;\\nvs.\\n&quot;) ## ## vs. print(f&#39;My float: {555.55:7.02f}&#39;) ## My float: 555.55 print(f&#39;My float: {22:7.02f}&#39;) ## My float: 22.00 print(f&#39;My float: {1234.5:7.02f}&#39;) ## My float: 1234.50 Resources RealPython f-strings A good walkthrough on f-strings. Logical operators Logical operators are symbols that can be used within Python to make comparisons. Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == equal to != not equal to not x negation, not x x or y x OR y x and y x AND y x is y x and y both point to the same objects in memory x == y x and y have the same values It may be important to give a quick example of the difference between == and is: x = -5 y = -5 print(x==y) # True ## True print(x is y) # True ## True x = 256 y = 256 print(x==y) # True ## True print(x is y) # True ## True x = 257 y = 257 print(x==y) # True ## True print(x is y) # False ## False This may be a surprising result for some of you. What is going on here? Well, Python makes an optimization where numbers between -5 and 256 (inclusive) are already declared internally. When you assign one of those pre-declared values to a variable, the variable points to the already declared object, rather than re-declaring the object. This is why the is operator is True for the first two numbers, and False for 257 -- x and y literally point to the same object when is results in True and does not when is results in False. x = -5 y = -5 print(x==y) # True ## True print(x is y) # True ## True print(id(x)) ## 139665752471664 print(id(y)) ## 139665752471664 x = 256 y = 256 print(x==y) # True ## True print(x is y) # True ## True print(id(x)) ## 139665752668560 print(id(y)) ## 139665752668560 x = 257 y = 257 print(x==y) # True ## True print(x is y) # False ## False print(id(x)) ## 139665244286608 print(id(y)) ## 139665244286640 There are a variety of interesting behaviors highlighted in this excellent article. It would be well worthwhile to read it. Lists &amp; Tuples Lists and tuples are two of the primary data types in Python. Lists are declared using square brackets, and contain any data type: my_list = [1,2,3,4.4,&quot;some_string&quot;] Tuples are declared using parentheses, and can contain any data type: my_tuple = (1,2,3,4.4,&quot;some_string&quot;) In general, when you think &quot;parentheses&quot;, you should think about tuples. One exception to this is when using tuple comprehensions. One may think that the following code would produce a tuple containing the values 0, 2, 4, 6, however, it instead creates a generator: not_a_tuple = (i*2 for i in range(4)) print(not_a_tuple) ## &lt;generator object &lt;genexpr&gt; at 0x7f06596bb510&gt; You would have to explicitly say it is a tuple to get the expected result: my_tuple = tuple(i*2 for i in range(4)) print(my_tuple) ## (0, 2, 4, 6) Lists are mutable, meaning you can modify the values once they've been declared: my_list[1] = 100 print(my_list) ## [1, 100, 3, 4.4, &#39;some_string&#39;] Tuples, are immutable (not mutable). You will get an error if you try to modify a tuple: # TypeError: &#39;tuple&#39; object does not support item assignment my_tuple[1] = 100 You can convert to and from lists and tuples easily: now_a_list = list(my_tuple) print(type(now_a_list)) ## &lt;class &#39;list&#39;&gt; now_a_tuple = tuple(my_list) print(type(now_a_tuple)) ## &lt;class &#39;tuple&#39;&gt; Indexing Indexing in R and Python are similar, but have a couple of key differences. The first, and most apparent difference is Python is 0-indexed, whereas R is 1-indexed. What does this mean? In R, if we want the first item in a list, we do the following: my_list &lt;- c(&quot;first&quot;, &quot;second&quot;, &quot;third&quot;, &quot;fourth&quot;, &quot;fifth&quot;) my_list[1] ## [1] &quot;first&quot; Take a look at what happens in Python: my_list = [&quot;first&quot;, &quot;second&quot;, &quot;third&quot;, &quot;fourth&quot;, &quot;fifth&quot;] my_list[1] ## &#39;second&#39; As you can see, my_list[1] actually accesses the second value. To access the first value, we do this: my_list[0] ## &#39;first&#39; When using the : this continues to hold. In R: my_list[1:2] ## [1] &quot;first&quot; &quot;second&quot; But, to achieve this in Python: my_list[0:2] ## [&#39;first&#39;, &#39;second&#39;] Additionally, Python can support a second : that defines a &quot;jump&quot;. For example: my_list[0:5:2] ## [&#39;first&#39;, &#39;third&#39;, &#39;fifth&#39;] One last major difference is how negative indexes work. In R, they remove the value at the given position: # remove the first and second values my_list[c(-1, -2)] ## [1] &quot;third&quot; &quot;fourth&quot; &quot;fifth&quot; # remove the first through third values my_list[-1:-3] ## [1] &quot;fourth&quot; &quot;fifth&quot; In Python, negative indexes just mean &quot;start from the back&quot; instead of &quot;start from the front&quot;. For example: my_list[-1] # last value ## &#39;fifth&#39; my_list[-2] # second to last value ## &#39;fourth&#39; my_list[-5] # first value ## &#39;first&#39; Its important to be careful when using negative indexes as the its not necessarily intuitive as when my_list[-5] is the first value, we would expect my_list[5] to be the last value, when, in fact, it produces an IndexError because the last value is my_list[4]. my_list[5] # causes an error! List methods A method is a function for a particular object. When you hear or read method just think function. A list is one example of an object. In Python, the most common objects like lists, dicts, tuples, sets, etc., all have extremely useful methods built right in! The following is a table of list methods from w3schools.com. Method Description append() Adds an element at the end of the list clear() Removes all the elements from the list copy() Returns a copy of the list count() Returns the number of elements with the specified value extend() Add the elements of a list (or any iterable), to the end of the current list index() Returns the index of the first element with the specified value insert() Adds an element at the specified position pop() Removes the element at the specified position remove() Removes the item with the specified value reverse() Reverses the order of the list sort() Sorts the list Examples Let's start by creating a couple of lists: list_one = [&quot;first&quot;, &quot;second&quot;, &quot;third&quot;, &quot;Fourth&quot;, &quot;fifth&quot;] list_two = [&quot;sixth&quot;, &quot;seventh&quot;, &quot;eighth&quot;, &quot;ninth&quot;] How do I add the string &quot;tenth&quot; to list_two? Click here for solution list_two.append(&quot;tenth&quot;) print(list_two) ## [&#39;sixth&#39;, &#39;seventh&#39;, &#39;eighth&#39;, &#39;ninth&#39;, &#39;tenth&#39;] How do I remove &quot;Fourth&quot; from list_one and then add &quot;fourth&quot; back? Click here for solution list_one.remove(&quot;Fourth&quot;) print(list_one) ## [&#39;first&#39;, &#39;second&#39;, &#39;third&#39;, &#39;fifth&#39;] list_one.append(&quot;fourth&quot;) print(list_one) ## [&#39;first&#39;, &#39;second&#39;, &#39;third&#39;, &#39;fifth&#39;, &#39;fourth&#39;] How do I remove the first element, and save the value in a new variable? Click here for solution new_variable = list_one.pop(0) print(f&#39;The new variable: {new_variable}&#39;) ## The new variable: first print(f&#39;The old list: {list_one}&#39;) ## The old list: [&#39;second&#39;, &#39;third&#39;, &#39;fifth&#39;, &#39;fourth&#39;] How do I combine list_one and list_two into one big list? Click here for solution list_one.extend(list_two) print(list_one) ## [&#39;second&#39;, &#39;third&#39;, &#39;fifth&#39;, &#39;fourth&#39;, &#39;sixth&#39;, &#39;seventh&#39;, &#39;eighth&#39;, &#39;ninth&#39;, &#39;tenth&#39;] Tuple methods The following is a table of tuple methods from w3schools.com. Method Description count() Returns the number of times a specified value occurs in a tuple index() Searches the tuple for a specified value and returns the position of where it was found Dicts Dictionaries, commonly referred to as dicts, are used to store key:value pairs. Under the hood, dicts are hash tables (or hash maps). Even with extremely large sets of data, dicts are able to very quickly add, remove, and search for data on average. Dicts are able to accomplish this at the expense of space. There are two ways to declare a dict, you can either use an empty or populated set of curly braces {}, or the dict keyword. # Declaring dicts my_dict_01 = {} print(type(my_dict_01)) ## &lt;class &#39;dict&#39;&gt; my_dict_02 = dict() print(type(my_dict_02)) ## &lt;class &#39;dict&#39;&gt; my_dict_03 = {&quot;first_names&quot;: [&quot;John&quot;, &quot;Jill&quot;,], &quot;last_names&quot;: [&quot;Smith&quot;, &quot;Johnson&quot;, &quot;Chen&quot;]} print(type(my_dict_03)) ## &lt;class &#39;dict&#39;&gt; my_dict_04 = dict(first_names=[&quot;John&quot;, &quot;Jill&quot;,], last_names=[&quot;Smith&quot;, &quot;Johnson&quot;, &quot;Chen&quot;]) print(type(my_dict_04)) ## &lt;class &#39;dict&#39;&gt; Be careful! Dicts are not the only data type that utilizes the curly braces. The following is not a dict, but rather a set. not_a_dict = {&quot;John&quot;, &quot;Jill&quot;, &quot;Ellen&quot;,} print(type(not_a_dict)) ## &lt;class &#39;set&#39;&gt; There are two primary ways to &quot;get&quot; information from a dict. One is to use the get method, the other is to use square brackets and strings. my_dict = {&quot;fruits&quot;: [&quot;apple&quot;, &quot;orange&quot;, &quot;pear&quot;], &quot;person&quot;: &quot;John&quot;, &quot;vegetables&quot;: [&quot;carrots&quot;, &quot;peas&quot;]} # If &quot;person&quot; is indeed a key, they will function the same way my_dict[&quot;person&quot;] my_dict.get(&quot;person&quot;) # If the key does not exist, like below, they will not # function the same way. my_dict.get(&quot;height&quot;) # Returns None when key doesn&#39;t exist my_dict[&quot;height&quot;] # Throws a KeyError exception because the key, &quot;height&quot; doesn&#39;t exist The following is a table of dict methods from w3schools.com. Method Description clear() Removes all the elements from the dictionary copy() Returns a copy of the dictionary fromkeys() Returns a dictionary with the specified keys and value get() Returns the value of the specified key, or None if the key doesn't exist items() Returns a list containing a tuple for each key value pair keys() Returns a list containing the dictionary's keys pop() Removes and returns the element with the specified key popitem() Removes the last inserted key-value pair setdefault() Returns the value of the specified key. If the key does not exist: insert the key, with the specified value update() Updates the dictionary with the specified key-value pairs values() Returns a list of all the values in the dictionary Sets The following is a table of set methods from w3schools.com. Method Description add() Adds an element to the set clear() Removes all the elements from the set copy() Returns a copy of the set difference() Returns a set containing the difference between two or more sets difference_update() Removes the items in this set that are also included in another, specified set discard() Remove the specified item intersection() Returns a set, that is the intersection of two other sets intersection_update() Removes the items in this set that are not present in other, specified set(s) isdisjoint() Returns whether two sets have an intersection or not issubset() Returns whether another set contains this set or not issuperset() Returns whether this set contains another set or not pop() Removes an element from the set remove() Removes the specified element symmetric_difference() Returns a set with the symmetric differences of two sets symmetric_difference_update() Inserts the symmetric differences from this set and another union() Return a set containing the union of sets update() Update the set with the union of this set and others Control flow Control flow in Python is pretty straightforward and simple. Whereas in R you may be less likely to use loops, in Python, loops are used all over the place! If/else statements If/else statements work just like they do in R, but syntax is different. For example, in R we have: value &lt;- 44 if (value &gt; 44) { print(&quot;Value is greater than 44.&quot;) } else { print(&quot;Value is not greater than 44.&quot;) } ## [1] &quot;Value is not greater than 44.&quot; In Python, the equivalent would be: value = 44 if value &gt; 44: print(&quot;Value is greater than 44.&quot;) else: print(&quot;Value is not greater than 44.&quot;) ## Value is not greater than 44. Very similar! On big difference is there is no need for curly braces around the if statement in Python. In addition, you do not need to have parentheses around the if statement in Python. With that being said, you can: value = 44 if (value &gt; 44): print(&quot;Value is greater than 44.&quot;) else: print(&quot;Value is not greater than 44.&quot;) ## Value is not greater than 44. One important point is that instead of using curly braces, Python has strict &quot;tab&quot; rules. The tabs are what indicate whether or not the following lines are inside the if statement or not. For example: if True: print(&quot;Inside the if statement.&quot;) ## Inside the if statement. print(&quot;Not inside the if statement.&quot;) ## Not inside the if statement. As you can see, the spacing is critical. If you have nested if statements (an if statement inside an if statement), you just continue to add tabs to indicate inside which statement we are. if True: print(&quot;Inside the first if statement.&quot;) if True: print(&quot;Inside the second if statement.&quot;) print(&quot;Back inside the first if statement.&quot;) ## Inside the first if statement. ## Inside the second if statement. ## Back inside the first if statement. print(&quot;Outside both if statements.&quot;) ## Outside both if statements. Let's come back to this example: value = 44 if value &gt; 44: print(&quot;Value is greater than 44.&quot;) else: print(&quot;Value is not greater than 44.&quot;) ## Value is not greater than 44. This is a very common and repeated pattern where we declare a variable some way, and then immediately use it in an if statement. Python has something called a walrus operator := (aptly named due to its appearance if you use your imagination). This operator allows you to declare variables inside of an expression: if value := 44 &gt; 44: print(&quot;Value is greater than 44.&quot;) else: print(&quot;Value is not greater than 44.&quot;) ## Value is not greater than 44. For loops For loops are extremely common in Python. Unlike in R where you tend to see more apply family functions, in Python loops are common. Here is an example: my_list = [&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumped&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;] for item in my_list: print(item) ## the ## quick ## brown ## fox ## jumped ## over ## the ## lazy ## dog Here, item, represents the value from my_list we are currently iterating on. In the first loop, item is &quot;the&quot;, in the second loop, item is &quot;quick&quot;, etc. item is not a special keyword. You can call the variable that holds the current value anything you want (except a keyword like &quot;if&quot;, &quot;else&quot;, &quot;def&quot;, etc.). For example: my_list = [&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumped&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;] for my_special_variable in my_list: print(my_special_variable) ## the ## quick ## brown ## fox ## jumped ## over ## the ## lazy ## dog As you can see, the name you give the variable is of no consequence. You can loop over any iterable in Python. An iterable is any object that implements both the __iter__() and __next__() dunder methods. Some example of iterables include: lists, strings, tuples, sets, dicts, etc. To test if an object is iterable, you could do the following: this_is_not_iterable = 43 try: test = iter(this_is_not_iterable) except: print(f&#39;Variable is not iterable&#39;) ## Variable is not iterable You can iterate over multiple iterables at once, for example, a list of lists, or a tuple of tuples: tuple_of_tuples = ((&quot;first&quot;, 1), (&quot;second&quot;, 2), (&quot;third&quot;, 3)) for my_string, my_value in tuple_of_tuples: print(f&#39;my_string: {my_string}, my_value: {my_value}&#39;) ## my_string: first, my_value: 1 ## my_string: second, my_value: 2 ## my_string: third, my_value: 3 There are a few ways to iterate through the keys and values of a dict: my_dict = {&quot;first&quot;:1, &quot;second&quot;:2, &quot;third&quot;:3} for my_key in my_dict: print(f&#39;key: {my_key}, value: {my_dict[my_key]}&#39;) ## key: first, value: 1 ## key: second, value: 2 ## key: third, value: 3 Alternatively you could use the items method: my_dict = {&quot;first&quot;:1, &quot;second&quot;:2, &quot;third&quot;:3} for a_tuple in my_dict.items(): print(a_tuple) ## (&#39;first&#39;, 1) ## (&#39;second&#39;, 2) ## (&#39;third&#39;, 3) As you can see items produces a tuple at each iteration. Rather than indexing each tuple to get your desired value, Python can automatically unpack the tuple like this: my_dict = {&quot;first&quot;:1, &quot;second&quot;:2, &quot;third&quot;:3} for first_value, second_value in my_dict.items(): print(f&#39;firstval: {first_value}, secondval: {second_value}&#39;) ## firstval: first, secondval: 1 ## firstval: second, secondval: 2 ## firstval: third, secondval: 3 Note that this is similar to this, but in a loop: first_value, second_value = (&quot;first&quot;, 1) print(f&#39;firstval: {first_value}, secondval: {second_value}&#39;) ## firstval: first, secondval: 1 Although it is not commonly used, for loops also &quot;go with&quot; else statements in Python. For example, this example from the official Python docs. for n in range(2, 10): for x in range(2, n): if n % x == 0: print(n, &#39;equals&#39;, x, &#39;*&#39;, n/x) break else: print(n, &#39;is a prime number&#39;) ## 2 is a prime number ## 3 is a prime number ## 4 equals 2 * 2.0 ## 5 is a prime number ## 6 equals 2 * 3.0 ## 7 is a prime number ## 8 equals 2 * 4.0 ## 9 equals 3 * 3.0 As you can see the else statement is aligned with the for loop, not the if statement. enumerate A frequent pattern is needing to access an index as you are looping through an iterable. In other languages, this value is often ready for you. For example, in C: int i; for (i = 0; i &lt;= 10; i++) { printf(&quot;%d&quot;, i); } Here, i can be used to access a value in an array. In Python, we can automatically add an index using the enumerate function. For example, my_list = [&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumped&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;] for idx, val in enumerate(my_list): print(f&#39;The index of {val} is {idx}.&#39;) ## The index of the is 0. ## The index of quick is 1. ## The index of brown is 2. ## The index of fox is 3. ## The index of jumped is 4. ## The index of over is 5. ## The index of the is 6. ## The index of lazy is 7. ## The index of dog is 8. If by any chance you'd like the idx portion (which, again, can be named anything you'd like) to start counting at 1 instead of 0, you can do that: my_list = [&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumped&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;] for count, val in enumerate(my_list, start=1): print(f&#39;There have been {count} words so far.&#39;) ## There have been 1 words so far. ## There have been 2 words so far. ## There have been 3 words so far. ## There have been 4 words so far. ## There have been 5 words so far. ## There have been 6 words so far. ## There have been 7 words so far. ## There have been 8 words so far. ## There have been 9 words so far. break break is a keyword in Python that stops execution and immediately jumps out of the loop, continuing execution of code immediately following the end of the loop. my_list = list(range(1, 11)) # this will only print &quot;1&quot; as # the loop is immediately escaped when # break is executed for i in my_list: print(i) break ## 1 In the following example, we exit the loop once we get to number &quot;5&quot;. my_list = list(range(1, 11)) for i in my_list: print(i) if i == 5: break ## 1 ## 2 ## 3 ## 4 ## 5 continue continue is a keyword in Python that prevents any of the remaining statements in the current iteration of the loop from running, and continues from the &quot;top&quot; of the loop on the next iteration. For example, my_list = (1,2,&#39;a&#39;,3,4,&#39;b&#39;,5) count = 0 for i in my_list: if type(i) == str: continue count += 1 print(count) ## 5 Here, since 'a' and 'b' are strings, we execute the continue which then skips the count += 1 part of the loop and begins at the next iteration. Finding a median There are many ways to find a median in Python. Here is one way to do it, without having a separate sort function according to: www.programmersought.com/article/45573563620/ import os import sys def partition(arr, low, high): end = high key = arr[high] while low &lt; high: while low &lt; high and arr[low] &lt; key: low = low + 1 while low &lt; high and arr[high] &gt;= key: high = high - 1 if low &lt; high: arr[low], arr[high] = arr[high], arr[low] arr[high], arr[end] = arr[end], arr[high] return high def getMid(arr): pos = 0 if (len(arr) &gt; 1): start = 0 end = len(arr) - 1 if len(arr) % 2 == 1: mid = len(arr)/2 else: mid = len(arr)/2 - 1 while start &lt; end: pos = partition(arr, start, end) if pos == mid: break elif pos &gt; mid: end = pos - 1 else: start = pos + 1 return arr[pos] List comprehensions List comprehensions are a nice syntactic trick that Python allows you to use. Here is an example of a list comprehension. my_list = [1,2,3,4,5] my_squares = [i**2 for i in my_list] print(my_squares) ## [1, 4, 9, 16, 25] This is the equivalent of: my_list = [1,2,3,4,5] my_squares = [] for i in my_list: my_squares.append(i**2) print(my_squares) ## [1, 4, 9, 16, 25] The former is much more concise than the latter. You can do something similar with tuples, but be careful. For example: my_list = (1,2,3,4,5) my_squares = (i**2 for i in my_list) print(my_squares) ## &lt;generator object &lt;genexpr&gt; at 0x7f06596bb660&gt; It didn't work as expected! This is because ( and ) are reserved for generator expressions. For example, to create a generator that keeps generating the next square: my_list = (1,2,3,4,5) my_generator = (i**2 for i in my_list) # you can either loop through the generator for i in my_generator: print(i) ## 1 ## 4 ## 9 ## 16 ## 25 my_generator = (i**2 for i in my_list) # or get the next value print(next(my_generator)) ## 1 print(next(my_generator)) ## 4 In order to get the desired result, we need to specify that we aren't desiring a generator as output, but rather a tuple: my_list = (1,2,3,4,5) my_squares = tuple(i**2 for i in my_list) print(my_squares) ## (1, 4, 9, 16, 25) You can do something similar with dicts as well. For example: my_dict = {&quot;first&quot;: 1, &quot;second&quot;: 2, &quot;third&quot;: 3} my_squares = {key:value**2 for key, value in my_dict.items()} print(my_squares) ## {&#39;first&#39;: 1, &#39;second&#39;: 4, &#39;third&#39;: 9} You can add simple if in the comprehensions as well. my_list = [1,2,3,4,5] my_odds = [v for v in my_list if v % 2 == 1] print(my_odds) ## [1, 3, 5] You can also work through layers of nested objects. my_list = [[1,2,3,4,5], [1,2,3], [1,2,3,4,5,6,7,8], [1,2,3,4,5,6,7,8,9]] my_odds = [number for a_list in my_list for number in a_list if number % 2 == 1] print(my_odds) ## [1, 3, 5, 1, 3, 1, 3, 5, 7, 1, 3, 5, 7, 9] As you can imagine, this is probably not the best idea most of the time as it is probably less clear to most people than, say: my_list = [[1,2,3,4,5], [1,2,3], [1,2,3,4,5,6,7,8], [1,2,3,4,5,6,7,8,9]] my_odds = [] for li in my_list: for number in li: if number % 2 == 1: my_odds.append(number) print(my_odds) ## [1, 3, 5, 1, 3, 1, 3, 5, 7, 1, 3, 5, 7, 9] While list comprehensions are a great feature, it is careful to not always use them. Clear code is (nearly) always better code. Writing functions In a nutshell, a function is a set of instructions or actions packaged together in a single definition or unit. Typically, function accept 0 or more arguments as input, and returns 0 or more results as output. The following is an example of a function in Python: def word_count(sentence: str) -&gt; int: &quot;&quot;&quot; word_count is a function that accepts a sentence as an argument, and returns the number of words in the sentence. Args: sentence (str): The sentence for which we are counting the words. Returns: int: The number of words in the sentence &quot;&quot;&quot; result = len(sentence.split()) return result test_sentence = &quot;this is a sentence, with 7 words.&quot; word_count(test_sentence) ## 7 The function is named word_count. The function has a single parameter named sentence. The function returns a single value, result, which is the number of words in the provided sentence. test_sentence is the argument to word_count. An argument is the actual value passed to the function. We pass values to functions -- this just means we use the values as arguments to the function. The parameter, sentence, is the name shown in the function definition. Functions can have helper functions. A helper function is a function defined and used within another function in order to reduce complexity or make the task at hand more clear. For example, let's say we wanted our function to strip all punctuation before counting the words: import string def word_count(sentence: str) -&gt; int: &quot;&quot;&quot; word_count is a function that accepts a sentence as an argument, and returns the number of words in the sentence. Args: sentence (str): The sentence for which we are counting the words. Returns: int: The number of words in the sentence &quot;&quot;&quot; def _strip_punctuation(sentence: str): &quot;&quot;&quot; helper function to strip punctuation. &quot;&quot;&quot; return sentence.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) sentence_no_punc = _strip_punctuation(sentence) result = len(sentence_no_punc.split()) return result test_sentence = &quot;this is a sentence, with 7 words.&quot; word_count(test_sentence) ## 7 Here, our helper function is named _strip_punctuation. The preceding _ is a hint to the programmer that the function is just for internal use (just for inside the function). If you try to call _strip_punctuation outside of word_count, you will get an error. _strip_punctuation is defined within the scope of word_count and is not available outside that scope. In this example, word_count is the caller, the function that calls the other function, _strip_punctuation. The other function, _strip_punctuation, can be referred to as the callee. In Python functions can be passed to other functions as arguments. In general, functions that accept another function as an argument or return functions, are called higher order functions. Some examples of higher order functions in Python are map, filter, and reduce. The function passed as an argument, is often referred to as a callback function, as the caller is expected to call back (execute) the argument at a later point in time. packing and unpacking To pass any number of n arguments to a function, we can use argument tuple packing. For example: def sum_then_multiply_by_x(x = 0, *args): print(args) return sum(args)*x print(sum_then_multiply_by_x(2, 1, 2, 3)) ## (1, 2, 3) ## 12 print(sum_then_multiply_by_x(2, 1, 2, 3, 4)) ## (1, 2, 3, 4) ## 20 print(sum_then_multiply_by_x(2, 1, 2, 3, 4, 5)) ## (1, 2, 3, 4, 5) ## 30 Here, every argument passed after the x argument is packed into a tuple called args. As you can see, you can pass any number of arguments. Okay, great. Then what is argument tuple unpacking? Unpacking is the process of unpacking a tuple containing many values to be passed as separate values to a function. For example: def print_foo_BAR(foo, bar): print(f&#39;{foo}{bar.upper()}&#39;) # normally we would call this function like so: print_foo_BAR(&quot;first&quot;, &quot;second&quot;) # but, we can unpack a tuple of values to pass the arguments positionally ## firstSECOND my_tuple_to_unpack = (&quot;first&quot;, &quot;second&quot;) print_foo_BAR(*my_tuple_to_unpack) ## firstSECOND Just like we have tuple packing and unpacking for positional arguments, we have dict packing and unpacking for keyword arguments. For example: def print_arguments(**kwargs): for key, value in kwargs.items(): print(f&#39;key: {key}, value: {value}&#39;) print_arguments(arg1=&quot;pretty&quot;, arg2=&quot;princess&quot;) ## key: arg1, value: pretty ## key: arg2, value: princess print_arguments(arg1=&quot;pretty&quot;, arg2=&quot;pretty&quot;, arg3=&quot;princess&quot;) ## key: arg1, value: pretty ## key: arg2, value: pretty ## key: arg3, value: princess Here, instead of kwargs being a tuple internally, it is a dict. Likewise, we have argument dictionary unpacking as well: def wild_animals(lions, tigers, bears): print(f&#39;lions: {lions}&#39;) print(f&#39;tigers: {tigers}&#39;) print(f&#39;bears: {bears}&#39;) print(&#39;oh my!&#39;) # normally we would call this function like so: wild_animals([&quot;bernice&quot;, &quot;sandra&quot;, &quot;arnold&quot;], [&quot;janice&quot;,], (&#39;paul&#39;, &#39;jim&#39;, &#39;dwight&#39;)) # but, we can unpack a tuple of values to pass the arguments positionally ## lions: [&#39;bernice&#39;, &#39;sandra&#39;, &#39;arnold&#39;] ## tigers: [&#39;janice&#39;] ## bears: (&#39;paul&#39;, &#39;jim&#39;, &#39;dwight&#39;) ## oh my! my_dict_to_unpack = {&quot;lions&quot;:[&quot;bernice&quot;, &quot;sandra&quot;, &quot;arnold&quot;], &quot;tigers&quot;:[&quot;janice&quot;,], &quot;bears&quot;:(&#39;paul&#39;, &#39;jim&#39;, &#39;dwight&#39;)} wild_animals(**my_dict_to_unpack) ## lions: [&#39;bernice&#39;, &#39;sandra&#39;, &#39;arnold&#39;] ## tigers: [&#39;janice&#39;] ## bears: (&#39;paul&#39;, &#39;jim&#39;, &#39;dwight&#39;) ## oh my! arguments When calling a function, arguments are not all the same. In Python, there are positional and keyword arguments. For example: def add_x_multiply_by_y(value: int, x: int, y: int) -&gt; int: return (value+x)*y add_x_multiply_by_y(2, 3, 4) ## 20 Here, 2, 3, and 4 are positional arguments. It is using the order in which the arguments are passed in order to determine to which parameter the argument belongs. If we were to rearrange the order in which we passed our values, it would change the result: def add_x_multiply_by_y(value: int, x: int, y: int) -&gt; int: return (value+x)*y add_x_multiply_by_y(2, 4, 3) ## 18 With that being said, we can use keyword arguments to specify where we are passing our values to. For example: def add_x_multiply_by_y(value: int, x: int, y: int) -&gt; int: return (value+x)*y add_x_multiply_by_y(2, y=4, x=3) ## 20 Now, since we specified that we are passing 4 to y, we get the same result from our first example. Be careful when mixing and matching keyword and positional arguments. Positional arguments must come before keyword arguments. For example: # Error: positional argument follows keyword argument (&lt;string&gt;, line 5) def add_x_multiply_by_y(value: int, x: int, y: int) -&gt; int: return (value+x)*y add_x_multiply_by_y(2, x=4, 3) Just like in R, in Python, arguments can have default values. For example: def add_x_multiply_by_y(value: int, x: int, y: int = 5) -&gt; int: return (value+x)*y add_x_multiply_by_y(1, 2) ## 15 Here, 1 is a positional argument for value and 2 is a positional argument for x. When not provided, y has a default value of 5. Again, you must be careful when giving an argument a default value. You cannot have a non-default argument follow a default argument. For example: # Error: non-default argument follows default argument (&lt;string&gt;, line 1) def add_x_multiply_by_y(value: int = 0, x: int, y: int) -&gt; int: return (value+x)*y add_x_multiply_by_y(x=1, y=3) By default, you can pass arguments as either positional or keyword arguments. With that being said, if you want to, you can create arguments that are only positional or only keyword. The way to specify an argument as only keyword, is to use tuple packing before a keyword argument. For example: def sum_then_multiply_by_x(*args, x) -&gt; int: return sum(args)*x sum_then_multiply_by_x(1,2,3,4, x=5) ## 50 Here, if you were to try and run the following, it would fail: # TypeError: sum_then_multiply_by_x() missing 1 required keyword-only argument: &#39;x&#39; sum_then_multiply_by_x(1,2,3,4,5) This should make sense. We do not know how many arguments will be passed to *args. Therefore it is required that any following arguments are required to be keyword arguments, otherwise the function wouldn't know where *args stops and the next argument begins. You may then think, but can't we put the keyword argument before *args and then we know the first argument, x, is the first argument and the remaining arguments are *args? No, because then x could also be positional, then, and this would work. def sum_then_multiply_by_x(x, *args) -&gt; int: return sum(args)*x sum_then_multiply_by_x(1,2,3,4,5) ## 14 To create one or more positional only arguments, simply add a / as a standalone argument after all of the arguments which you would like to be only positional. For example: def sum_then_multiply_by_x(one, two, /, three, x) -&gt; int: return sum([one, two, three])*x sum_then_multiply_by_x(1,2,3,4) # all positional, will work sum_then_multiply_by_x(1,2,three=3,x=4) # two keyword, two positional, will work sum_then_multiply_by_x(1,two=2,three=3,x=4) # a positional only argument was passed as a keyword argument, error docstrings docstrings are the strings inside the function immediately following the function declaration. docstrings provide documentation for the function. You can put any information you'd like in a docstring, however, it is best to carefully describe what the function does, and stay consistent in style from docstring to docstring. You can access a function's docstring in various ways: print(word_count.__doc__) ## ## word_count is a function that accepts a sentence as an argument, ## and returns the number of words in the sentence. ## ## Args: ## sentence (str): The sentence for which we are counting the words. ## ## Returns: ## int: The number of words in the sentence ## help(word_count) ## Help on function word_count in module __main__: ## ## word_count(sentence: str) -&gt; int ## word_count is a function that accepts a sentence as an argument, ## and returns the number of words in the sentence. ## ## Args: ## sentence (str): The sentence for which we are counting the words. ## ## Returns: ## int: The number of words in the sentence In addition, if you are coding in a tool like VSCode, for example, you may gain the ability to hover over a function and see its docstring and other information. For example: It is good practice to write docstrings for every function you write. annotations Another &quot;thing&quot; you may have noticed from our word_count function if you've ever used Python in the past. In the signature of our function def word_count(sentence: str) -&gt; int, we have some extra, not required information in the form of function annotations. Specifically, you could write the word_count function like this, but it would function just the same: def word_count(sentence): &quot;&quot;&quot; word_count is a function that accepts a sentence as an argument, and returns the number of words in the sentence. Args: sentence (str): The sentence for which we are counting the words. Returns: int: The number of words in the sentence &quot;&quot;&quot; def _strip_punctuation(sentence: str): &quot;&quot;&quot; helper function to strip punctuation. &quot;&quot;&quot; return sentence.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) sentence_no_punc = _strip_punctuation(sentence) result = len(sentence_no_punc.split()) return result Here, we do not specify that sentence should be a str or that the returned result should be an int. When we do annotate functions, it is purely a way to add metadata to our function. In large projects, function annotations are recommended. Although Python does not strictly enforce type annotations, packages like mypy can be added to a deployment scheme to strictly enforce it. decorators Write a function called get_filename_from_url that, given a url to a file, like https://image.shutterstock.com/image-vector/cute-dogs-line-art-border-260nw-1079902403.jpg returns the filename with the extension. Click here for solution import os from urllib.parse import urlparse def get_filename_from_url(url: str) -&gt; str: &quot;&quot;&quot; Given a link to a file, return the filename with extension. Args: url (str): The url of the file. Returns: str: A string with the filename, including the file extension. &quot;&quot;&quot; return os.path.basename(urlparse(url).path) Write a function that, given a url to an image, and a full path to a directory, saves the image to the provided directory. By default, have the function save the images to the user's home directory in a unix-like operating system. Click here for solution import requests from pathlib import Path import getpass def scrape_image(from_url: str, to_dir: str = f&#39;/home/{getpass.getuser()}&#39;): &quot;&quot;&quot; Given a url to an image, scrape the image and save the image to the provided directory. If no directory is provided, by default, save to the user&#39;s home directory. Args: from_url (str): U to_dir (str, optional): [description]. Defaults to f&#39;/home/{getpass.getuser()}&#39;. &quot;&quot;&quot; resp = requests.get(from_url) # this function is from the previous example filename = get_filename_from_url(from_url) # Make directory if doesn&#39;t already exist Path(to_dir).mkdir(parents=True, exist_ok=True) file = open(f&#39;{to_dir}/{filename}&#39;, &quot;wb&quot;) file.write(resp.content) file.close() Reading &amp; Writing data read_csv Please see here. csv csv is a Python module that is useful for reading and writing tabular data. Much like the read.csv function in R, the csv module is useful for data that is like csv but not necessarily comma-separated. To use the csv module, simply import it: import csv Examples How do you print each row of a csv flights_sample.csv? Click here for solution # my_csv_file is the variable holding the file with open(&#39;flights_sample.csv&#39;) as my_csv_file: my_reader = csv.reader(my_csv_file) # each &quot;row&quot; here is a list where each # value in the list is an element in the row for row in my_reader: print(row) # you can change the word &quot;row&quot; to anything you # would like, just make sure to change it everywhere! # first, we need to &quot;reset&quot; the file so it starts at the beginning my_csv_file.seek(0) for my_row in my_reader: print(my_row) ## [&#39;Year&#39;, &#39;Month&#39;, &#39;DayofMonth&#39;, &#39;DayOfWeek&#39;, &#39;DepTime&#39;, &#39;CRSDepTime&#39;, &#39;ArrTime&#39;, &#39;CRSArrTime&#39;, &#39;UniqueCarrier&#39;, &#39;FlightNum&#39;, &#39;TailNum&#39;, &#39;ActualElapsedTime&#39;, &#39;CRSElapsedTime&#39;, &#39;AirTime&#39;, &#39;ArrDelay&#39;, &#39;DepDelay&#39;, &#39;Origin&#39;, &#39;Dest&#39;, &#39;Distance&#39;, &#39;TaxiIn&#39;, &#39;TaxiOut&#39;, &#39;Cancelled&#39;, &#39;CancellationCode&#39;, &#39;Diverted&#39;, &#39;CarrierDelay&#39;, &#39;WeatherDelay&#39;, &#39;NASDelay&#39;, &#39;SecurityDelay&#39;, &#39;LateAircraftDelay&#39;] ## [&#39;1987&#39;, &#39;10&#39;, &#39;14&#39;, &#39;3&#39;, &#39;741&#39;, &#39;730&#39;, &#39;912&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;91&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;23&#39;, &#39;11&#39;, &#39;SAN&#39;, &#39;SFO&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1990&#39;, &#39;10&#39;, &#39;15&#39;, &#39;4&#39;, &#39;729&#39;, &#39;730&#39;, &#39;903&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;94&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;14&#39;, &#39;-1&#39;, &#39;SAN&#39;, &#39;SFO&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1990&#39;, &#39;10&#39;, &#39;17&#39;, &#39;6&#39;, &#39;741&#39;, &#39;730&#39;, &#39;918&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;97&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;29&#39;, &#39;11&#39;, &#39;SAN&#39;, &#39;SFO&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1990&#39;, &#39;10&#39;, &#39;18&#39;, &#39;7&#39;, &#39;729&#39;, &#39;730&#39;, &#39;847&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;78&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;-2&#39;, &#39;-1&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;19&#39;, &#39;1&#39;, &#39;749&#39;, &#39;730&#39;, &#39;922&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;93&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;33&#39;, &#39;19&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;21&#39;, &#39;3&#39;, &#39;728&#39;, &#39;730&#39;, &#39;848&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;80&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;-1&#39;, &#39;-2&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;22&#39;, &#39;4&#39;, &#39;728&#39;, &#39;730&#39;, &#39;852&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;84&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;3&#39;, &#39;-2&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;23&#39;, &#39;5&#39;, &#39;731&#39;, &#39;730&#39;, &#39;902&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;91&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;13&#39;, &#39;1&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;24&#39;, &#39;6&#39;, &#39;744&#39;, &#39;730&#39;, &#39;908&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;84&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;19&#39;, &#39;14&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## 0 ## [&#39;Year&#39;, &#39;Month&#39;, &#39;DayofMonth&#39;, &#39;DayOfWeek&#39;, &#39;DepTime&#39;, &#39;CRSDepTime&#39;, &#39;ArrTime&#39;, &#39;CRSArrTime&#39;, &#39;UniqueCarrier&#39;, &#39;FlightNum&#39;, &#39;TailNum&#39;, &#39;ActualElapsedTime&#39;, &#39;CRSElapsedTime&#39;, &#39;AirTime&#39;, &#39;ArrDelay&#39;, &#39;DepDelay&#39;, &#39;Origin&#39;, &#39;Dest&#39;, &#39;Distance&#39;, &#39;TaxiIn&#39;, &#39;TaxiOut&#39;, &#39;Cancelled&#39;, &#39;CancellationCode&#39;, &#39;Diverted&#39;, &#39;CarrierDelay&#39;, &#39;WeatherDelay&#39;, &#39;NASDelay&#39;, &#39;SecurityDelay&#39;, &#39;LateAircraftDelay&#39;] ## [&#39;1987&#39;, &#39;10&#39;, &#39;14&#39;, &#39;3&#39;, &#39;741&#39;, &#39;730&#39;, &#39;912&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;91&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;23&#39;, &#39;11&#39;, &#39;SAN&#39;, &#39;SFO&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1990&#39;, &#39;10&#39;, &#39;15&#39;, &#39;4&#39;, &#39;729&#39;, &#39;730&#39;, &#39;903&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;94&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;14&#39;, &#39;-1&#39;, &#39;SAN&#39;, &#39;SFO&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1990&#39;, &#39;10&#39;, &#39;17&#39;, &#39;6&#39;, &#39;741&#39;, &#39;730&#39;, &#39;918&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;97&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;29&#39;, &#39;11&#39;, &#39;SAN&#39;, &#39;SFO&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1990&#39;, &#39;10&#39;, &#39;18&#39;, &#39;7&#39;, &#39;729&#39;, &#39;730&#39;, &#39;847&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;78&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;-2&#39;, &#39;-1&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;19&#39;, &#39;1&#39;, &#39;749&#39;, &#39;730&#39;, &#39;922&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;93&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;33&#39;, &#39;19&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;21&#39;, &#39;3&#39;, &#39;728&#39;, &#39;730&#39;, &#39;848&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;80&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;-1&#39;, &#39;-2&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;22&#39;, &#39;4&#39;, &#39;728&#39;, &#39;730&#39;, &#39;852&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;84&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;3&#39;, &#39;-2&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;23&#39;, &#39;5&#39;, &#39;731&#39;, &#39;730&#39;, &#39;902&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;91&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;13&#39;, &#39;1&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] ## [&#39;1991&#39;, &#39;10&#39;, &#39;24&#39;, &#39;6&#39;, &#39;744&#39;, &#39;730&#39;, &#39;908&#39;, &#39;849&#39;, &#39;PS&#39;, &#39;1451&#39;, &#39;NA&#39;, &#39;84&#39;, &#39;79&#39;, &#39;NA&#39;, &#39;19&#39;, &#39;14&#39;, &#39;SAN&#39;, &#39;ABC&#39;, &#39;447&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;0&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;, &#39;NA&#39;] # my_csv_file is the variable holding the file with open(&#39;flights_sample.csv&#39;) as my_csv_file: my_reader = csv.reader(my_csv_file) # instead of printing a list, you can use the &quot;join&quot; # string method to neatly format the output for this_row in my_reader: print(&#39;, &#39;.join(this_row)) ## Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay ## 1987, 10, 14, 3, 741, 730, 912, 849, PS, 1451, NA, 91, 79, NA, 23, 11, SAN, SFO, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1990, 10, 15, 4, 729, 730, 903, 849, PS, 1451, NA, 94, 79, NA, 14, -1, SAN, SFO, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1990, 10, 17, 6, 741, 730, 918, 849, PS, 1451, NA, 97, 79, NA, 29, 11, SAN, SFO, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1990, 10, 18, 7, 729, 730, 847, 849, PS, 1451, NA, 78, 79, NA, -2, -1, SAN, ABC, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1991, 10, 19, 1, 749, 730, 922, 849, PS, 1451, NA, 93, 79, NA, 33, 19, SAN, ABC, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1991, 10, 21, 3, 728, 730, 848, 849, PS, 1451, NA, 80, 79, NA, -1, -2, SAN, ABC, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1991, 10, 22, 4, 728, 730, 852, 849, PS, 1451, NA, 84, 79, NA, 3, -2, SAN, ABC, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1991, 10, 23, 5, 731, 730, 902, 849, PS, 1451, NA, 91, 79, NA, 13, 1, SAN, ABC, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA ## 1991, 10, 24, 6, 744, 730, 908, 849, PS, 1451, NA, 84, 79, NA, 19, 14, SAN, ABC, 447, NA, NA, 0, NA, 0, NA, NA, NA, NA, NA How do you print each row of a csv grades_semi.csv, where instead of being comma-separated, values are semi-colon-separated? Click here for solution with open(&#39;grades_semi.csv&#39;) as my_csv_file: my_reader = csv.reader(my_csv_file, delimiter=&#39;;&#39;) for row in my_reader: print(row) ## [&#39;grade&#39;, &#39;year&#39;] ## [&#39;100&#39;, &#39;junior&#39;] ## [&#39;99&#39;, &#39;sophomore&#39;] ## [&#39;75&#39;, &#39;sophomore&#39;] ## [&#39;74&#39;, &#39;sophomore&#39;] ## [&#39;44&#39;, &#39;senior&#39;] ## [&#39;69&#39;, &#39;junior&#39;] ## [&#39;88&#39;, &#39;junior&#39;] ## [&#39;99&#39;, &#39;senior&#39;] ## [&#39;90&#39;, &#39;freshman&#39;] ## [&#39;92&#39;, &#39;junior&#39;] pathlib Path Examples How do I get the size of a file in bytes? Megabytes? Gigabytes? Important note: This example will fail unless you have a file called 5000_products.csv in the same directory as you are working in. The . represents the current working directory. You can read more about this here. from pathlib import Path p = Path(&quot;./5000_products.csv&quot;) size_in_bytes = p.stat().st_size print(f&#39;Size in bytes: {size_in_bytes}&#39;) ## Size in bytes: 15416485 print(f&#39;Size in megabytes: {size_in_bytes/1_000_000}&#39;) ## Size in megabytes: 15.416485 print(f&#39;Size in gigabytes: {size_in_bytes/1_000_000_000}&#39;) ## Size in gigabytes: 0.015416485 numpy scipy pandas read_csv read_csv is a function from the pandas library that allows you to read tabular data into a pandas DataFrame. Examples How do I read a csv file called grades.csv into a DataFrame? Click here for solution Note that the &quot;.&quot; means the current working directory. So, if we were in &quot;/home/john/projects&quot;, &quot;./grades.csv&quot; would be the same as &quot;/home/john/projects/grades.csv&quot;. This is called a relative path. Read this for a better understanding. import pandas as pd myDF = pd.read_csv(&quot;./grades.csv&quot;) myDF.head() ## grade year ## 0 100 junior ## 1 99 sophomore ## 2 75 sophomore ## 3 74 sophomore ## 4 44 senior How do I read a csv file called grades_semi.csv where instead of being comma-separated, it is semi-colon-separated, into a DataFrame? Click here for solution import pandas as pd myDF = pd.read_csv(&quot;./grades_semi.csv&quot;, sep=&quot;;&quot;) myDF.head() ## grade year ## 0 100 junior ## 1 99 sophomore ## 2 75 sophomore ## 3 74 sophomore ## 4 44 senior How do I specify the type of 1 or more columns when reading in a csv file? Click here for solution import pandas as pd myDF = pd.read_csv(&quot;./grades.csv&quot;) myDF.dtypes # as you can see, year is of dtype &quot;object&quot; # object dtype can hold any Python object # we know that this column should hold strings # so let&#39;s specify this as we read in the data ## grade int64 ## year object ## dtype: object myDF = pd.read_csv(&quot;./grades.csv&quot;, dtype={&quot;year&quot;: &quot;string&quot;}) myDF.dtypes # if we wanted to specify that the &quot;grade&quot; # column should be float64 instead of int64 # we could do that too ## grade int64 ## year string ## dtype: object myDF = pd.read_csv(&quot;./grades.csv&quot;, dtype={&quot;year&quot;: &quot;string&quot;, &quot;grade&quot;: &quot;float64&quot;}) myDF.dtypes # and you can see that they are indeed floats now ## grade float64 ## year string ## dtype: object myDF.head() ## grade year ## 0 100.0 junior ## 1 99.0 sophomore ## 2 75.0 sophomore ## 3 74.0 sophomore ## 4 44.0 senior Given a list of csv files with the same columns, how can I read them in and combine them into a single dataframe? Click here for solution my_csv_files = [&quot;./grades.csv&quot;, &quot;./grades2.csv&quot;] data = [] for file in my_csv_files: myDF = pd.read_csv(file) data.append(myDF) final_result = pd.concat(data, axis=0) final_result ## grade year ## 0 100 junior ## 1 99 sophomore ## 2 75 sophomore ## 3 74 sophomore ## 4 44 senior ## 5 69 junior ## 6 88 junior ## 7 99 senior ## 8 90 freshman ## 9 92 junior ## 0 100 junior ## 1 99 sophomore ## 2 75 sophomore ## 3 74 sophomore ## 4 44 senior ## 5 69 junior ## 6 88 junior ## 7 99 senior ## 8 90 freshman ## 9 92 junior ## 10 45 senior to_csv to_csv is a function from the pandas library that allows you to write/save a pandas DataFrame into a csv file on your computer. Examples How do I save the pandas DataFrame called myDF as a cvs file called grades.csv in my scratch directory? Click here for solution import pandas as pd # This is the data we loaded using read_csv myDF = pd.read_csv(&quot;./grades.csv&quot;) # The code below saves this DataFrame as a csv file called grades.csv into your personal scratch folder myDF.to_csv(&quot;/scratch/scholar/UserName/grades.csv&quot;) read_parquet read_parquet is a function from the pandas library that allows you to read tabular data into a pandas DataFrame. Examples How do I read a parquet file called grades.parquet into a DataFrame? Click here for solution Note that the &quot;.&quot; means the current working directory. So, if we were in &quot;/home/john/projects&quot;, &quot;./grades.parquet&quot; would be the same as &quot;/home/john/projects/grades.parquet&quot;. This is called a relative path. Read this for a better understanding. import pandas as pd myDF = pd.read_parquet(&quot;./grades.parquet&quot;) myDF.head() DataFrame The DataFrame is one of the primary classes used from the pandas package. Much like data.frames in R, DataFrames in pandas store tabular, two-dimensional datasets. Most operations involve reading a dataset into a DataFrame, accessing the DataFrame's attributes, and using the DataFrame's methods to perform operations on the underlying data or with other DataFrames. Examples How do I get the number of rows and columns of a DataFrame, myDF? Click here for solution import pandas as pd myDF = pd.read_csv(&quot;./flights_sample.csv&quot;) # returns a tuple where the first value is the number of rows # and the second value is the number of columns myDF.shape # number of rows ## (9, 29) myDF.shape[0] # number of columns ## 9 myDF.shape[1] ## 29 How do I get the column names of a DataFrame, myDF? Click here for solution import pandas as pd myDF = pd.read_csv(&quot;./flights_sample.csv&quot;) myDF.columns ## Index([&#39;Year&#39;, &#39;Month&#39;, &#39;DayofMonth&#39;, &#39;DayOfWeek&#39;, &#39;DepTime&#39;, &#39;CRSDepTime&#39;, ## &#39;ArrTime&#39;, &#39;CRSArrTime&#39;, &#39;UniqueCarrier&#39;, &#39;FlightNum&#39;, &#39;TailNum&#39;, ## &#39;ActualElapsedTime&#39;, &#39;CRSElapsedTime&#39;, &#39;AirTime&#39;, &#39;ArrDelay&#39;, ## &#39;DepDelay&#39;, &#39;Origin&#39;, &#39;Dest&#39;, &#39;Distance&#39;, &#39;TaxiIn&#39;, &#39;TaxiOut&#39;, ## &#39;Cancelled&#39;, &#39;CancellationCode&#39;, &#39;Diverted&#39;, &#39;CarrierDelay&#39;, ## &#39;WeatherDelay&#39;, &#39;NASDelay&#39;, &#39;SecurityDelay&#39;, &#39;LateAircraftDelay&#39;], ## dtype=&#39;object&#39;) How do I change the name of a column &quot;Year&quot; to &quot;year&quot;? Click here for solution import pandas as pd myDF = pd.read_csv(&quot;./flights_sample.csv&quot;) # You must set myDF equal to the result # otherwise, myDF will remain unchanged myDF = myDF.rename(columns={&quot;Year&quot;: &quot;year&quot;}) # Alternatively, you can use the inplace # argument to make the change directly # to myDF myDF.rename(columns={&quot;year&quot;: &quot;YEAR&quot;}, inplace=True) # As you can see, since we used inplace=True # the change has been made to myDF without # setting myDF equal to the result of our # operation myDF.columns ## Index([&#39;YEAR&#39;, &#39;Month&#39;, &#39;DayofMonth&#39;, &#39;DayOfWeek&#39;, &#39;DepTime&#39;, &#39;CRSDepTime&#39;, ## &#39;ArrTime&#39;, &#39;CRSArrTime&#39;, &#39;UniqueCarrier&#39;, &#39;FlightNum&#39;, &#39;TailNum&#39;, ## &#39;ActualElapsedTime&#39;, &#39;CRSElapsedTime&#39;, &#39;AirTime&#39;, &#39;ArrDelay&#39;, ## &#39;DepDelay&#39;, &#39;Origin&#39;, &#39;Dest&#39;, &#39;Distance&#39;, &#39;TaxiIn&#39;, &#39;TaxiOut&#39;, ## &#39;Cancelled&#39;, &#39;CancellationCode&#39;, &#39;Diverted&#39;, &#39;CarrierDelay&#39;, ## &#39;WeatherDelay&#39;, &#39;NASDelay&#39;, &#39;SecurityDelay&#39;, &#39;LateAircraftDelay&#39;], ## dtype=&#39;object&#39;) How do I display the first n rows of a DataFrame? Click here for solution import pandas as pd myDF = pd.read_csv(&quot;./flights_sample.csv&quot;) # By default, this returns 5 rows myDF.head() # Use the &quot;n&quot; parameter to return a different number of rows myDF.head(n=10) How can I convert a list of dicts to a DataFrame? Click here for solution list_of_dicts = [] list_of_dicts.append({&#39;columnA&#39;: 1, &#39;columnB&#39;: 2}) list_of_dicts.append({&#39;columnB&#39;: 4, &#39;columnA&#39;: 1}) myDF = pd.DataFrame(list_of_dicts) myDF.head() ## columnA columnB ## 0 1 2 ## 1 1 4 Resources DataFrame Reference A list of DataFrame attributes and methods, with links to detailed docs. Series Resources 10 minute intro to pandas A great introduction to pandas. Very quick. Indexing The primary ways to index a pandas DataFrame or Series is using the loc and iloc methods. The loc method is primarily label based, and iloc is primarily integer position based. For example, given the following DataFrame: import pandas as pd myDF = pd.read_csv(&quot;./flights_sample.csv&quot;) myDF.head() ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 0 1987 10 14 ... NaN NaN NaN ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## 3 1990 10 18 ... NaN NaN NaN ## 4 1991 10 19 ... NaN NaN NaN ## ## [5 rows x 29 columns] If we wanted to get only the rows where Year is 1990, we could do this: # all rows where the column &quot;Year&quot; is equal to 1990 myDF.loc[myDF.loc[:, &#39;Year&#39;] == 1990, :] ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## 3 1990 10 18 ... NaN NaN NaN ## ## [3 rows x 29 columns] Or this: # all rows where the column &quot;Year&quot; is equal to 1990 myDF.loc[myDF[&#39;Year&#39;] == 1990, :] ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## 3 1990 10 18 ... NaN NaN NaN ## ## [3 rows x 29 columns] The : simply means &quot;all rows&quot; if it is before the comma, and &quot;all columns&quot; if it is after the comma. In the same way, having myDF['Year'] == 1990 before the first comma, means to filter rows where myDF['Year'] == 1990 results in True. Note that you could use iloc instead of loc for any of the previous examples. Both iloc and loc allow for boolean (logical) based indexing. Since myDF['Year'] == 1990 or myDF.loc[:, 'Year'] == 1990 both result in a Series of True and False values, this works. To isolate a single column/Series using loc, we can do the following: myDF.loc[:, &#39;Year&#39;] # or ## 0 1987 ## 1 1990 ## 2 1990 ## 3 1990 ## 4 1991 ## 5 1991 ## 6 1991 ## 7 1991 ## 8 1991 ## Name: Year, dtype: int64 myDF.iloc[:, 0] # since Year is the first column ## 0 1987 ## 1 1990 ## 2 1990 ## 3 1990 ## 4 1991 ## 5 1991 ## 6 1991 ## 7 1991 ## 8 1991 ## Name: Year, dtype: int64 You can isolate multiple columns just as easily: myDF.loc[:, (&#39;Year&#39;, &#39;Month&#39;)] # or ## Year Month ## 0 1987 10 ## 1 1990 10 ## 2 1990 10 ## 3 1990 10 ## 4 1991 10 ## 5 1991 10 ## 6 1991 10 ## 7 1991 10 ## 8 1991 10 myDF.iloc[:, 0:1] ## Year ## 0 1987 ## 1 1990 ## 2 1990 ## 3 1990 ## 4 1991 ## 5 1991 ## 6 1991 ## 7 1991 ## 8 1991 Or, multiple rows: print(myDF.loc[0:2,:]) # or ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 0 1987 10 14 ... NaN NaN NaN ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## ## [3 rows x 29 columns] print(myDF.iloc[0:2,:]) ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 0 1987 10 14 ... NaN NaN NaN ## 1 1990 10 15 ... NaN NaN NaN ## ## [2 rows x 29 columns] Note that myDF.loc[0:1, :] is very different than myDF.iloc[0:1,:] even though you get the same result in this example. The former looks at the row index of the DataFrame, and the latter looks at the position. So, if we changed the row index of the DataFrame, this will become clear. myDF2 = myDF.copy(deep=True) # create a new column to be used as index myDF2[&#39;idx&#39;] = range(2,len(myDF2[&#39;Year&#39;])+2) myDF2 = myDF2.set_index(&quot;idx&quot;) myDF2.head() ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## idx ... ## 2 1987 10 14 ... NaN NaN NaN ## 3 1990 10 15 ... NaN NaN NaN ## 4 1990 10 17 ... NaN NaN NaN ## 5 1990 10 18 ... NaN NaN NaN ## 6 1991 10 19 ... NaN NaN NaN ## ## [5 rows x 29 columns] print(myDF2.loc[0:2,:]) # or ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## idx ... ## 2 1987 10 14 ... NaN NaN NaN ## ## [1 rows x 29 columns] print(myDF2.iloc[0:2,:]) ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## idx ... ## 2 1987 10 14 ... NaN NaN NaN ## 3 1990 10 15 ... NaN NaN NaN ## ## [2 rows x 29 columns] As you can see, loc gets all of the rows where the row index is 0, 1, or 2 (in this case, just 2, because there is no 0 or 1 row index). iloc gets the rows in position 0, and 1, which happen to have row index 2, and 3 respectively. You can also index on both rows and columns: myDF.iloc[0:2, 0:2] # or ## Year Month ## 0 1987 10 ## 1 1990 10 myDF.loc[0:1, (&quot;Year&quot;, &quot;Month&quot;)] ## Year Month ## 0 1987 10 ## 1 1990 10 If you want to index using multiple logic statements, for example, if you wanted to get all rows where Year is 1990 OR Month is 10, you could do the following: myDF.loc[(myDF.loc[:, &quot;Year&quot;]==1990) | (myDF.loc[:, &quot;Month&quot;]==10), :] ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 0 1987 10 14 ... NaN NaN NaN ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## 3 1990 10 18 ... NaN NaN NaN ## 4 1991 10 19 ... NaN NaN NaN ## 5 1991 10 21 ... NaN NaN NaN ## 6 1991 10 22 ... NaN NaN NaN ## 7 1991 10 23 ... NaN NaN NaN ## 8 1991 10 24 ... NaN NaN NaN ## ## [9 rows x 29 columns] Note that the parentheses are critical here. Without the parentheses, you will get an error. The following will not work: # ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). myDF.loc[myDF.loc[:, &quot;Year&quot;]==1990 | myDF.loc[:, &quot;Month&quot;]==10, :] You can use &amp; for logical AND just like you can use | for logical OR: myDF.loc[(myDF.loc[:, &quot;Year&quot;]==1990) &amp; (myDF.loc[:, &quot;Month&quot;]==10), :] ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## 3 1990 10 18 ... NaN NaN NaN ## ## [3 rows x 29 columns] Resources 10 minute intro to pandas A great introduction to pandas. Very quick. loc/iloc/indexing The official user guide for indexing. dates and times groupby groupby is a powerful method of both DataFrames and Series. For a DataFrame, groupby groups by each unique value in a given column, and allows you to perform operations on those groups. For example, given a sample of the flights data, what if you wanted to calculate the average departure delay, DepDelay, for each Year of data in the Year column? import pandas as pd myDF = pd.read_csv(&quot;./flights_sample.csv&quot;) myDF.head() ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 0 1987 10 14 ... NaN NaN NaN ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## 3 1990 10 18 ... NaN NaN NaN ## 4 1991 10 19 ... NaN NaN NaN ## ## [5 rows x 29 columns] myDF.groupby(&quot;Year&quot;).mean() ## Month DayofMonth DayOfWeek ... NASDelay SecurityDelay LateAircraftDelay ## Year ... ## 1987 10.0 14.000000 3.000000 ... NaN NaN NaN ## 1990 10.0 16.666667 5.666667 ... NaN NaN NaN ## 1991 10.0 21.800000 3.800000 ... NaN NaN NaN ## ## [3 rows x 25 columns] As you can see, for each column with numeric values, the average is calculated for each of the 3 Years, 1987, 1990, and 1991. You can isolate the DepDelay, before or after the calculation. # before myDF.groupby(&quot;Year&quot;)[&#39;DepDelay&#39;].mean() ## Year ## 1987 11 ## 1990 3 ## 1991 6 ## Name: DepDelay, dtype: int64 or # after myDF.groupby(&quot;Year&quot;).mean()[&#39;DepDelay&#39;] You can group by multiple variables as well. For example, you could find the mean DepDelay for each day of the week for each year. myDF.groupby([&quot;Year&quot;, &quot;DayOfWeek&quot;])[&#39;DepDelay&#39;].mean() ## Year DayOfWeek ## 1987 3 11 ## 1990 4 -1 ## 6 11 ## 7 -1 ## 1991 1 19 ## 3 -2 ## 4 -2 ## 5 1 ## 6 14 ## Name: DepDelay, dtype: int64 You may notice that the result is a Series, not a DataFrame. To move the Year and DayOfWeek indexes back to be columns of a DataFrame, you can use the reset_index() method. myDF.groupby([&quot;Year&quot;, &quot;DayOfWeek&quot;])[&#39;DepDelay&#39;].mean().reset_index() ## Year DayOfWeek DepDelay ## 0 1987 3 11 ## 1 1990 4 -1 ## 2 1990 6 11 ## 3 1990 7 -1 ## 4 1991 1 19 ## 5 1991 3 -2 ## 6 1991 4 -2 ## 7 1991 5 1 ## 8 1991 6 14 Examples How do I calculate the number of rows for each Year? Click here for solution myDF.groupby(&quot;Year&quot;)[&quot;DepDelay&quot;].count() How do I calculate the median DepDelay by Year? Click here for solution myDF.groupby(&quot;Year&quot;)[&quot;DepDelay&quot;].median() pivot pivot is a powerful method from the pandas library used to reshape data. pivot takes 3 main arguments: index, columns, and values. index is the column of the dataframe that you would like to become your new index. columns is the column of your dataframe where you would like each unique value to be mapped to a column. values is the column of your dataframe for which you'd like the values to be. For example, what if we wanted to reshape our sample of flights data to have the departure delay displayed by day of week, by year, where there is one column for each day of the week, and each index is a year? import pandas as pd myDF = pd.read_csv(&quot;./flights_sample.csv&quot;) myDF.head() ## Year Month DayofMonth ... NASDelay SecurityDelay LateAircraftDelay ## 0 1987 10 14 ... NaN NaN NaN ## 1 1990 10 15 ... NaN NaN NaN ## 2 1990 10 17 ... NaN NaN NaN ## 3 1990 10 18 ... NaN NaN NaN ## 4 1991 10 19 ... NaN NaN NaN ## ## [5 rows x 29 columns] myDF.pivot(index=&#39;Year&#39;, columns=&#39;DayOfWeek&#39;, values=&#39;DepDelay&#39;) ## DayOfWeek 1 3 4 5 6 7 ## Year ## 1987 NaN 11.0 NaN NaN NaN NaN ## 1990 NaN NaN -1.0 NaN 11.0 -1.0 ## 1991 19.0 -2.0 -2.0 1.0 14.0 NaN Alternatively, you could swap the columns and indexes. myDF.pivot(index=&#39;DayOfWeek&#39;, columns=&#39;Year&#39;, values=&#39;DepDelay&#39;) ## Year 1987 1990 1991 ## DayOfWeek ## 1 NaN NaN 19.0 ## 3 11.0 NaN -2.0 ## 4 NaN -1.0 -2.0 ## 5 NaN NaN 1.0 ## 6 NaN 11.0 14.0 ## 7 NaN -1.0 NaN Like before, you can always use reset_index() to bring the index back in as a column. myDF.pivot(index=&#39;DayOfWeek&#39;, columns=&#39;Year&#39;, values=&#39;DepDelay&#39;).reset_index() ## Year DayOfWeek 1987 1990 1991 ## 0 1 NaN NaN 19.0 ## 1 3 11.0 NaN -2.0 ## 2 4 NaN -1.0 -2.0 ## 3 5 NaN NaN 1.0 ## 4 6 NaN 11.0 14.0 ## 5 7 NaN -1.0 NaN Jupyter notebooks Writing scripts A script is a file containing a sequence of logic to be executed. In Python, this is just a simple .py file like my_script.py. shebang The first line in a script is typically the shebang. You can read a little bit about this here. The shebang dictates which Python interpreter and environment is used when executing the script without the preceding python or python3. For example, if you were to run: python ~/my_script.py my_script.py would be executed using the python interpreter located at the result of: which python If no preceding python or python3 is provided, the operating system will try to execute the script with various shells (depending on the shell you use), and will fail (because the shells aren't meant to execute Python code). If you add a shebang to the top of your script, you can remove the preceding python or python3 as the system will use the shebang to determine which interpreter to use. Code like this will work: $HOME/my_script.py When it comes to Python, there are two common shebangs: #!/usr/bin/python #!/usr/bin/env python The first will use the system default Python interpreter located at /usr/bin/python. The second, will use whatever Python interpreter is currently what would be executed if you just ran python. What does this mean? If you were to activate a custom conda or virtual environment, the #!/usr/bin/env python shebang would choose to execute your script using the interpreter and environment that you've activated. When you want a script to run with a certain set of Python packages present in a given environment, it may not be convenient for you to run a command like conda activate my_environment prior to executing the script. In these situations, you can simply use your shebang to point to the interpreter associated with my_environment. For example, the interpreter used with our course environment is located: /class/datamine/apps/python/f2020-s2021/env/bin/python. If we wanted our script to run using all of the packages we've installed into our environment, we can simply use this shebang: #!/class/datamine/apps/python/f2020-s2021/env/bin/python. arguments Arguments are the values passed to the script. For example, in the following command, -i, special word, and my_file.txt are arguments being passed to grep. grep -i &#39;special word&#39; my_file.txt Similarly, we can pass arguments to Python scripts: $HOME/my_script.py -i &#39;okay, sounds good!&#39; To access and utilize arguments in your Python script, you can use the sys package. For example, if you were to run the following script: import sys def main(): print(sys.argv) if __name__ == &#39;__main__&#39;: main() Like: $HOME/my_script.py -i &#39;okay, sounds good!&#39; The results would be: [&#39;$HOME/my_script.py&#39;, &#39;-i&#39;, &#39;okay, sounds good!&#39;] sys.argv returns a list of arguments where sys.argv[0] is equal to the script name. You could then use sys.argv to perform certain operations based on the arguments provided. Arguments can be positional, which means the order in which they are given to a script is critical, and how the script is able to discern what the arguments are. For example, if you were to switch the positions of the pattern and file arguments to grep, it would fail: grep my_file.txt &#39;my_pattern&#39; # fails Positional arguments tend to be required, but aren't required to be required. Arguments can also be optional, which means they aren't required. Typically, optional arguments are preceded by at least one -. Most of the time optional arguments, or options, can have multiple forms. Short form (for example -i), or long form (for example -i is the same as --ignore-case). Sometimes options can get values. If options don't have values, you can assume that the presence of the flag means TRUE and the lack means FALSE. When using short form, the value for the option is separated by a space (for example grep -f my_file.txt). When using long form, the value for the option is separated by an equals sign (for example grep --file=my_file.txt). argparse argparse is a Python package that helps build consistent CLI's and handles much of the tedious logic involved in making the various arguments passed to your script function properly. Adding a positional argument is simple: import argparse def main(): parser = argparse.ArgumentParser() parser.add_argument(&quot;echo&quot;) args = parser.parse_args() if __name__ == &#39;__main__&#39;: main() You can then access the value for that positional argument like: args.echo. If your argument has hyphens in it (but not at the beginning of the word): import argparse def main(): parser = argparse.ArgumentParser() parser.add_argument(&quot;echo-value&quot;) args = parser.parse_args() if __name__ == &#39;__main__&#39;: main() You can access that argument like: args.echo_value. The interior hyphens are converted to underscores. To add useful &quot;help&quot; messages, you can do the following: import argparse def main(): parser = argparse.ArgumentParser() parser.add_argument(&quot;echo&quot;, help=&#39;repeats the text from echo here&#39;) args = parser.parse_args() if __name__ == &#39;__main__&#39;: main() The help message can be shown by running the script with the -h or --help options: $HOME/my_script.py --help To add optional arguments, you can do the following: import argparse def main(): parser = argparse.ArgumentParser() parser.add_argument(&quot;echo&quot;, help=&#39;repeats the text from echo here&#39;) parser.add_arguments(&quot;-l&quot;, &quot;--loud&quot;, help=&quot;makes our text print in caps&quot;) args = parser.parse_args() if __name__ == &#39;__main__&#39;: main() Then, if you run: $HOME/my_script.py &#39;text to echo&#39; By default, since the option -l or --loud is not present, nothing happens and the value of args.loud is None. If you want the default value for args.loud to be False instead, you could do the following: import argparse def main(): parser = argparse.ArgumentParser() parser.add_argument(&quot;echo&quot;, help=&#39;repeats the text from echo here&#39;) parser.add_arguments(&quot;-l&quot;, &quot;--loud&quot;, help=&quot;makes our text print in caps&quot;, action=&#39;store_true&#39;) args = parser.parse_args() if __name__ == &#39;__main__&#39;: main() Then, by default if -l or --loud is not present, args.loud is False. Resources Official documentation The official documentation for the argparse library. Official tutorial The official tutorial for the argparse library. Scraping Web scraping is the process of programmatically scraping or downloading web content online and processing it into the desired format. It can roughly be broken into two steps: Scraping: The process of copying content from online. Typically the scraped content is HTML, but it can vary. Both the requests package and selenium package have the capability of scraping data. Parsing: The process of programmatically extracting the desired information from the scraped content. Xpath expressions, css selectors, and packages like beautifulsoup4 are particularly useful for this task. lxml, selenium, and beautifulsoup4 are all excellent packages to parse scraped content. With lxml and selenium we can use xpath expressions to locate the information we want. With that being said, that information is often returned in a Python class with some sort of name like Element (representing an HTML element). Learning how to access the information is important. The following is an example that strives to demonstrate how to: Access the raw HTML of an element in lxml, selenium, and beautifulsoup4. Access attributes of an element in lxml, selenium, and beautifulsoup4. Access the values (the text between tags) of an element in lxml, selenium, and beautifulsoup4. Access the tag name of an element in lxml, selenium, and beautifulsoup4. First, let's setup the the tools to each be at the same stage -- with an element ready to go. # SELENIUM SETUP from selenium import webdriver from selenium.webdriver.firefox.options import Options firefox_options = Options() firefox_options.add_argument(&quot;window-size=1920,1080&quot;) # Headless mode means no GUI firefox_options.add_argument(&quot;--headless&quot;) firefox_options.add_argument(&quot;start-maximized&quot;) firefox_options.add_argument(&quot;disable-infobars&quot;) firefox_options.add_argument(&quot;--disable-extensions&quot;) firefox_options.add_argument(&quot;--no-sandbox&quot;) firefox_options.add_argument(&quot;--disable-dev-shm-usage&quot;) # Set the location of the executable Firefox program on Scholar firefox_options.binary_location = &#39;/class/datamine/apps/firefox/firefox&#39; # Set the location of the executable geckodriver program on Scholar driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://datamine.purdue.edu&quot;) # LXML SETUP import requests import lxml.html # note that without this header, a website may give you a puzzle to solve my_headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} # scrape the webpage response = requests.get(&quot;https://datamine.purdue.edu&quot;, headers=my_headers) # load the webpage into an lxml tree tree = lxml.html.fromstring(response.text) # BEAUTIFULSOUP4 SETUP from bs4 import BeautifulSoup as bsoup my_headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} html = requests.get(&#39;https://datamine.purdue.edu&#39;, headers=my_headers) soup = bsoup(html.text) At this point in time, when you see us using the soup object, we are using beautifulsoup4. When you see us using the tree object, we are using lxml. When you see us using the driver object, we are using selenium. Scrape the entire webpage: print(soup) import lxml.html print(lxml.html.tostring(tree)) print(driver.page_source) Get and print the hero on https://datamine.purdue.edu: &lt;section class=&quot;office__hero&quot;&gt; &lt;h2&gt;The Data Mine&lt;/h2&gt; &lt;p&gt;Advancing data science for undergraduates through collaboration, learning, research, innovation, and entrepreneurship. &lt;b&gt;Open to all undergraduates!&lt;/b&gt;&lt;/p&gt; &lt;/section&gt; soup_element = soup.find(&#39;section&#39;, attrs={&#39;class&#39;: &#39;office__hero&#39;}) # or soup_element = soup.find(&#39;section&#39;, class_=&#39;office__hero&#39;) print(soup_element) lxml_element = tree.xpath(&quot;//section[@class=&#39;office__hero&#39;]&quot;)[0] print(lxml.html.tostring(lxml_element)) selenium_element = driver.find_element_by_xpath(&quot;//section[@class=&#39;office__hero&#39;]&quot;) print(selenium_element.get_attribute(&quot;outerHTML&quot;)) Get the tag of our elements: print(soup_element.name) print(lxml_element.tag) print(selenium_element.tag_name) Get the &quot;class&quot; attribute of our elements: print(soup_element.attrs.get(&quot;class&quot;)[0]) print(lxml_element.attrib.get(&quot;class&quot;)) print(selenium_element.get_attribute(&quot;class&quot;)) Get and print the nested h2 element: nested_soup_element = soup_element.find(&#39;h2&#39;) print(nested_soup_element) nested_lxml_element = lxml_element.xpath(&quot;.//h2&quot;)[0] print(lxml.html.tostring(nested_lxml_element)) nested_selenium_element = selenium_element.find_element_by_xpath(&quot;.//h2&quot;) print(nested_selenium_element.get_attribute(&quot;outerHTML&quot;)) Get the contents or values of the nested element: print(nested_soup_element.text) print(nested_lxml_element.text) print(nested_selenium_element.text) requests requests is a Python package used to make web requests over HTTP. To summarize, HTTP is a protocol for communication between servers and clients. An example of a server would be a fancy computer running in an Amazon AWS warehouse. An example of a client would be your laptop when you are web surfing! HTTP has requests and responses. A client (your browser, for example), sends a request to a server. The server then returns a response to the client. HTTP has the following methods: GET, POST, PUT, HEAD, DELETE, PATCH, and OPTIONS. You can read more about those here. The important thing is to realize that the requests package enables you to easily &quot;use&quot; these methods in Python. For example, the GET method is used to get data from a server: import requests response = requests.get(&quot;https://datamine.purdue.edu/&quot;) print(response) ## &lt;Response [200]&gt; For simpler tasks, using requests to scrape the data, and a package like lxml or beautifulsoup4 to parse the data, is appropriate. Examples How do I scrape the HTML from https://datamine.purdue.edu/? Click here for solution response = requests.get(&quot;https://datamine.purdue.edu/&quot;) print(response.text[:500]) ## &lt;!DOCTYPE html&gt; ## &lt;html lang=&quot;en&quot;&gt; ## ## &lt;head&gt; ## &lt;!-- general meta data Here --&gt; ## &lt;title&gt;The Data Mine&lt;/title&gt; ## &lt;meta content=&quot;The first large-scale living learning community for undergraduates from all majors, focused on Data Science for All.&quot; name=&quot;description&quot; /&gt; ## &lt;meta content=&quot;Marketing and Media&quot; name=&quot;author&quot; /&gt; ## &lt;meta charset=&quot;utf-8&quot; /&gt; ## &lt;meta content=&quot;IE=edge&quot; http-equiv=&quot;X-UA-Compatible&quot; /&gt; ## &lt;meta content=&quot;width=device-width, initial-scale=1&quot; name=&quot;viewport&quot; /&gt; ## ## ## &lt;lin How do I get the status code from the requests response? Click here for solution response = requests.get(&quot;https://datamine.purdue.edu/&quot;) print(response.status_code) ## 200 Resources Official documentation The official documentation for the requests library. lxml lxml is a package used for processing XML in Python. See here selenium selenium is an extremely powerful browser automation tool with official wrappers in Ruby, Java, Python, C#, Javascript. It is a tool that is used extensively in industry, and definitely worth while learning the basics. For most websites, the tool combination of requests and lxml or beautifulsoup4 will be more than adequate, and easier to jump right in. Where selenium really shines is the ability to interact with the browser before, during, and after scraping and parsing data. Many websites use javascript to load various pieces of HTML as the user interacts with the browser. For example, if you browse on https://pinterest.com, you will find that if you scroll very quickly, images will take a second or so to completely load. If you were to scrape these web pages with a tool like requests, the content you would be scraping would just be the content that was loaded in the original state. In the case of pinterest, this would mean that the other 20 pictures you wanted to scrape wouldn't be present within the scraped content. To get by this limitation, we can use selenium to emulate a human browsing the web page. We can make the program &quot;scroll down&quot; before scraping the web page, so the pictures would all be present. Other examples that could change the web page's state, hence changing the content we scrape could be: clicking on filters, using a search bar, hovering, etc. One other major way selenium differs from the other tools mentioned is the ability to interact with the browser, scrape, and parse the data. It can do it all. Unfortunately selenium requires more setup than other packages. To get started with selenium you must first choose a browser: Firefox, Chrome, or Safari. In addition to your chosen browser, you will need an accompanying web driver. For Firefox, the web driver is geckodriver. For Chrome, the web driver is ChromeDriver, and for Safari you need to enable SafariDriver. For simplicity, we will demonstrate with Firefox and geckodriver on Scholar. Here are some sane configurations pre-made for you. We already have compatible Firefox and geckodriver versions available on Scholar. Note that if you were to comment out the --headless option, Firefox would literally launch and you could watch your program in action as it is being executed. If you wrote a program to scrape images off of a website, you'd be able to see the images load and the browser slowly scroll. Note that you would need to log in via ThinLinc to do this. By enabling headless mode, you prevent this, and just need to imagine how you would interact with a given web page. Feel free to copy and paste this code in your work. from selenium import webdriver from selenium.webdriver.firefox.options import Options firefox_options = Options() firefox_options.add_argument(&quot;window-size=1920,1080&quot;) # Headless mode means no GUI firefox_options.add_argument(&quot;--headless&quot;) firefox_options.add_argument(&quot;start-maximized&quot;) firefox_options.add_argument(&quot;disable-infobars&quot;) firefox_options.add_argument(&quot;--disable-extensions&quot;) firefox_options.add_argument(&quot;--no-sandbox&quot;) firefox_options.add_argument(&quot;--disable-dev-shm-usage&quot;) # Set the location of the executable Firefox program on Scholar firefox_options.binary_location = &#39;/class/datamine/apps/firefox/firefox&#39; # Set the location of the executable geckodriver program on Scholar driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) Examples How do I scrape a website using selenium? Click here for solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://datamine.purdue.edu&quot;) print(driver.page_source[:500]) How do I scrape the office &quot;hero&quot; on https://datamine.purdue.edu, with all of its contents? Click here for solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://datamine.purdue.edu&quot;) my_element = driver.find_element_by_xpath(&quot;//section[@class=&#39;office__hero&#39;]&quot;) print(my_element.get_attribute(&quot;outerHTML&quot;)) How do I scrape the office &quot;hero&quot; on https://datamine.purdue.edu, with all of its contents, but without the outermost HTML? Click here for solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://datamine.purdue.edu&quot;) my_element = driver.find_element_by_xpath(&quot;//section[@class=&#39;office__hero&#39;]&quot;) print(my_element.get_attribute(&quot;innerHTML&quot;)) How do I use a search bar like Google with selenium? Search for &quot;mdw&quot; at https://purdue.edu/directory and scrape and print the data. Click here for solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) # get the webpage driver.get(&quot;https://www.purdue.edu/directory&quot;) # isolate the search bar &quot;input&quot; element element = driver.find_element_by_xpath(&quot;//input[@id=&#39;basicSearchInput&#39;]&quot;) # use &quot;send_keys&quot; to type in the search bar element.send_keys(&quot;mdw&quot;) # just like when you use a browser, you either need to push &quot;enter&quot; or click on the search button # this time, we will press enter from selenium.webdriver.common.keys import Keys element.send_keys(Keys.RETURN) # just how a browser can take a few seconds to fully load, let&#39;s wait for the page to load completely import time time.sleep(5) # get the table(s) elements = driver.find_elements_by_xpath(&quot;//table[@class=&#39;more&#39;]&quot;) # how many tables are there? print(len(elements)) Alternatively, we could click on the search button instead of pressing RETURN/enter: driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) # get the webpage driver.get(&quot;https://www.purdue.edu/directory&quot;) # isolate the search bar &quot;input&quot; element element = driver.find_element_by_xpath(&quot;//input[@id=&#39;basicSearchInput&#39;]&quot;) # use &quot;send_keys&quot; to type in the search bar element.send_keys(&quot;mdw&quot;) # find the button to execute the search button = driver.find_element_by_xpath(&quot;//a[@id=&#39;glass&#39;]&quot;) # click the button button.click() # just how a browser can take a few seconds to fully load, let&#39;s wait for the page to load completely import time time.sleep(5) # get the table(s) elements = driver.find_elements_by_xpath(&quot;//table[@class=&#39;more&#39;]&quot;) # how many tables are there? print(len(elements)) No matter which method you choose to use (click or enter), the table looks like this: &lt;table class=&quot;more&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th scope=&quot;col&quot; colspan=&quot;2&quot;&gt;mark daniel ward&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th class=&quot;icon-key&quot; scope=&quot;row&quot;&gt;Alias&lt;/th&gt; &lt;td&gt;mdw&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th class=&quot;icon-envelope-alt&quot;&gt;Email&lt;/th&gt; &lt;td&gt;&lt;a href=&quot;mailto:mdw@purdue.edu&quot;&gt;mdw@purdue.edu&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th class=&quot;icon-library&quot; scope=&quot;row&quot;&gt;Campus&lt;/th&gt; &lt;td&gt;west lafayette&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th class=&quot;icon-sitemap&quot;&gt;Department&lt;/th&gt; &lt;td&gt;statistics&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th class=&quot;icon-briefcase&quot; scope=&quot;row&quot;&gt;Title&lt;/th&gt; &lt;td&gt;professor of statistics&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; And can be accessed by: # note since we used find_elements_by_xpath, elements is a list. # If we used find_element_by_xpath, we wouldn&#39;t need the [0] part elements[0].get_attribute(&quot;outerHTML&quot;) Then, to parse through this data we could: # first get the name # note that we now use .// -- this means search starting in the current element # if we used //, it would search the entire web page, not just our element name = elements[0].find_element_by_xpath(&quot;.//thead/tr/th&quot;).text print(name) # next, get the alias # the xpath expression here first gets the &quot;th&quot; element with class=icon-key. # we want the content of the following td element. Since the next &quot;td&quot; # element is at the same level of nesting as the &quot;th&quot; element, it is referred to # as a &quot;sibling&quot;. following-sibling::td finds the &quot;td&quot; sibling immediately following # the current &quot;th&quot; element alias = elements[0].find_element_by_xpath(&quot;.//th[@class=&#39;icon-key&#39;]/following-sibling::td&quot;).text print(alias) # next, get the email # if you dont specify what the attribute is equal to, it will # evaluate to true if there is any value, and false otherwise email = elements[0].find_element_by_xpath(&quot;.//a[@href]&quot;).text print(email) # next, get the campus campus = elements[0].find_element_by_xpath(&quot;.//th[@class=&#39;icon-sitemap&#39;]/following-sibling::td&quot;).text print(campus) # next, get the title title = elements[0].find_element_by_xpath(&quot;.//th[@class=&#39;icon-briefcase&#39;]/following-sibling::td&quot;).text print(title) How do I scrape Shutterstock images of dogs from https://www.shutterstock.com/search/dog+side+view? Click here for solution Start by opening your favorite browser and inspecting the HTML. Open the webpage https://www.shutterstock.com/search/dog+side+view, and right click on an image and select &quot;Inspect Element&quot;. This should open a help menu towards the bottom of the browser that let's you examine the HTML. You can see that the img tag contains all of the information we want. Specifically, look at the link in the src attribute: https://image.shutterstock.com/image-photo/young-labrador-retriever-4-months-260nw-97138889.jpg. We need to write a function to scrape an image given a link like that. In addition, we first need to figure out how to extract these image links from the rest of the page. It looks like the class attribute is not going to be of much value as it looks like a bunch of random numbers and letters. With that being said, it looks like the data-automation class could be useful. What if we try to extract all elements where data-automation is equal to mosaic-grid-cell-image? Let's find out. First, let's scrape the entire page using requests: import requests response = requests.get(&#39;https://www.shutterstock.com/search/dog+side+view&#39;) print(response.text[:500]) Hmm, the HTML looks like it might be missing what we want. Let's find out for sure using lxml: import lxml.html tree = lxml.html.fromstring(response.text) elements = tree.xpath(&quot;//img[@data-automation=&#39;mosaic-grid-cell-image&#39;]&quot;) print(len(elements)) Actually it looks like we found ~100 elements, great! If we had received a 406 error or some HTML that indicated we were being seen as a robot, we would try adding a header that makes our requests look like they come from a Firefox browser, like this: my_headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} html = requests.get(&#39;https://www.shutterstock.com/search/dog+side+view&#39;, headers=my_headers) Great, let's continue. We want to get the src attribute from each element, because those links contain the paths to the images we want to scrape: for element in elements: print(element.attrib.get(&quot;src&quot;)) Unfortunately, something has gone wrong. Only the first 20 or so image links have been scraped! What is going on here? This is a classic case of a website lazy loading images. What this means is the browser is waiting to fully render the images on the page until the user has the content (images) on the screen. In fact, if you load up shutterstock and then rapidly begin to scroll down, you will notice a lag where images don't load until after a few fractions of a second. requests doesn't have the capability of scraping more images from this website -- at least not easily. This is a job better suited for selenium as selenium can completely emulate human interaction with the browser. What I mean is, what if we have selenium load the page up, scroll a little bit, pause, scroll a bit more, pause, and then try scraping the content from the web page? Would this fix our issue? Let's find out. First, perform the &quot;setup&quot; steps outlined here: Important note: These settings will work on Scholar. In order to do the same on your own computer you will have to install compatible binaries for Firefox and geckodriver, and modify the paths in the code below accordingly. from selenium import webdriver from selenium.webdriver.firefox.options import Options firefox_options = Options() firefox_options.add_argument(&quot;window-size=1920,1080&quot;) # Headless mode means no GUI firefox_options.add_argument(&quot;--headless&quot;) firefox_options.add_argument(&quot;start-maximized&quot;) firefox_options.add_argument(&quot;disable-infobars&quot;) firefox_options.add_argument(&quot;--disable-extensions&quot;) firefox_options.add_argument(&quot;--no-sandbox&quot;) firefox_options.add_argument(&quot;--disable-dev-shm-usage&quot;) # Set the location of the executable Firefox program on Scholar firefox_options.binary_location = &#39;/class/datamine/apps/firefox/firefox&#39; # Set the location of the executable geckodriver program on Scholar driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) Now, let's try to scroll and see if that fixes our issues: driver.get(&quot;https://www.shutterstock.com/search/dog+side+view&quot;) # create a scroll function that emulates scrolling import time def scroll(driver, scroll_point): driver.execute_script(f&#39;window.scrollTo(0, {scroll_point});&#39;) time.sleep(5) # Needed to get the window size set right height = driver.execute_script(&#39;return document.body.scrollHeight&#39;) driver.set_window_size(900,height+100) # begin scrolling a bit, 1/4 of the page at a time, maybe scroll(driver, height/4) scroll(driver, height*2/4) scroll(driver, height*3/4) scroll(driver, height) # extract the image links elements = driver.find_elements_by_xpath(&quot;//img[@data-automation=&#39;mosaic-grid-cell-image&#39;]&quot;) for element in elements: print(element.get_attribute(&quot;src&quot;)) Excellent! Worked perfectly. Okay, so the next step would be to actually follow all of those links (or crawl them) and scrape the images themselves. You can write functions to do this, or use the examples here and here to help! For convenience: import os from urllib.parse import urlparse def get_filename_from_url(url: str) -&gt; str: &quot;&quot;&quot; Given a link to a file, return the filename with extension. Args: url (str): The url of the file. Returns: str: A string with the filename, including the file extension. &quot;&quot;&quot; return os.path.basename(urlparse(url).path) import requests from pathlib import Path import getpass def scrape_image(from_url: str, to_dir: str = f&#39;/home/{getpass.getuser()}&#39;): &quot;&quot;&quot; Given a url to an image, scrape the image and save the image to the provided directory. If no directory is provided, by default, save to the user&#39;s home directory. Args: from_url (str): U to_dir (str, optional): [description]. Defaults to f&#39;/home/{getpass.getuser()}&#39;. &quot;&quot;&quot; resp = requests.get(from_url) # this function is from the previous example filename = get_filename_from_url(from_url) # Make directory if doesn&#39;t already exist Path(to_dir).mkdir(parents=True, exist_ok=True) file = open(f&#39;{to_dir}/{filename}&#39;, &quot;wb&quot;) file.write(resp.content) file.close() Let's cycle through and scrape each image, now: for element in elements: scrape_image(element.get_attribute(&quot;src&quot;)) XML XML stands for Extensible Markup Language. To read more about XML see here. lxml lxml is a package used for processing XML in Python. To get started, simply import the package: from lxml import etree To load XML from a string, do the following: my_string = f&quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;My Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;div class=&quot;abc123 sktoe-sldjkt dkjfg3-dlgsk&quot;&gt; &lt;div class=&quot;glkjr-slkd dkgj-0 dklfgj-00&quot;&gt; &lt;a class=&quot;slkdg43lk dlks&quot; href=&quot;https://example.com/123456&quot;&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;div class=&quot;ldskfg4&quot;&gt; &lt;span class=&quot;slktjoe&quot; aria-label=&quot;123 comments, 43 Retweets, 4000 likes&quot;&gt;Love it.&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-amount=&quot;12&quot;&gt;13&lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;div class=&quot;abc123 sktoe-sls dkjfg-dlgsk&quot;&gt; &lt;div class=&quot;glkj-slkd dkgj-0 dklfj-00&quot;&gt; &lt;a class=&quot;slkd3lk dls&quot; href=&quot;https://example.com/123456&quot;&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;div class=&quot;ldg4&quot;&gt; &lt;span class=&quot;sktjoe&quot; aria-label=&quot;1000 comments, 455 Retweets, 40000 likes&quot;&gt;Love it.&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-amount=&quot;122&quot;&gt;133&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;&quot;&quot;&quot; tree = etree.fromstring(my_string) Or, to load an XML file called example.xml do the following: tree = etree.parse(&quot;example.xml&quot;) From there, you can use xpath expressions to parse the dataset. Examples How do I load a webpage I scraped using requests into an lxml tree? Click here for solution import requests import lxml.html # note that without this header, a website may give you a puzzle to solve my_headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} # scrape the webpage response = requests.get(&quot;https://www.reddit.com/r/puppies/&quot;, headers=my_headers) # load the webpage into an lxml tree tree = lxml.html.fromstring(response.text) How do I get the name of the root node from my lxml tree called tree? Click here for solution # remember &quot;/&quot; gets the node starting at the root node and &quot;*&quot; is a # wildcard that means &quot;anything&quot; tree.xpath(&quot;/*&quot;)[0].tag ## &#39;html&#39; If the root node is named &quot;html&quot;, how do I get the name of all nested tags? Click here for solution list_of_tags = [x.tag for x in tree.xpath(&quot;/html/*&quot;)] print(list_of_tags) # remember, this odd syntax is just a &quot;list comprehension&quot;. It is # essentially a nice short-hand way of writing a loop in Python. # It is the exact same as: ## [&#39;head&#39;, &#39;body&#39;] for element in tree.xpath(&quot;/html/*&quot;): print(element.tag) ## head ## body How do I get the attributes of an element? Click here for solution import pandas as pd # as you can see, this prints the attributes in a dict-like object for each div element # in the node. for element in tree.xpath(&quot;//div&quot;): print(element.attrib) # Note, if you ever want to convert a list of dicts to a pandas dataframe # you will need to convert to a dict. ## {} ## {&#39;class&#39;: &#39;abc123 sktoe-sldjkt dkjfg3-dlgsk&#39;} ## {&#39;class&#39;: &#39;glkjr-slkd dkgj-0 dklfgj-00&#39;} ## {} ## {&#39;class&#39;: &#39;ldskfg4&#39;} ## {&#39;data-amount&#39;: &#39;12&#39;} ## {} ## {&#39;class&#39;: &#39;abc123 sktoe-sls dkjfg-dlgsk&#39;} ## {&#39;class&#39;: &#39;glkj-slkd dkgj-0 dklfj-00&#39;} ## {} ## {&#39;class&#39;: &#39;ldg4&#39;} ## {&#39;data-amount&#39;: &#39;122&#39;} list_of_dicts = [] for element in tree.xpath(&quot;//div&quot;): list_of_dicts.append(element.attrib) myDF = pd.DataFrame(list_of_dicts) myDF.head() # unexpected ## 0 ## 0 None ## 1 class ## 2 class ## 3 None ## 4 class list_of_dicts = [] for element in tree.xpath(&quot;//div&quot;): list_of_dicts.append(dict(element.attrib)) myDF = pd.DataFrame(list_of_dicts) myDF.head() # fixed ## class data-amount ## 0 NaN NaN ## 1 abc123 sktoe-sldjkt dkjfg3-dlgsk NaN ## 2 glkjr-slkd dkgj-0 dklfgj-00 NaN ## 3 NaN NaN ## 4 ldskfg4 NaN How do I get the div elements with attribute &quot;data-amount&quot;? Click here for solution for element in tree.xpath(&quot;//div[@data-amount]&quot;): print(element.attrib) ## {&#39;data-amount&#39;: &#39;12&#39;} ## {&#39;data-amount&#39;: &#39;122&#39;} How do I get the div elements where data-amount is greater than 50? Click here for solution for element in tree.xpath(&quot;//div[@data-amount &gt; 50]&quot;): print(element.attrib) ## {&#39;data-amount&#39;: &#39;122&#39;} How do I get the values of the span tags? Click here for solution for element in tree.xpath(&quot;//span&quot;): print(element.text) ## Love it. ## Love it. Plotting There are a variety of very powerful plotting tools in Python. Some of the primary ones include: matplotlib, plotly, bokeh, seaborn, plotnine, and pygal. In R, ggplot tends to dominate. In Python, it would be more difficult to choose &quot;if you have to learn one library&quot;, but that library would most likely be matplotlib. matplotlib To begin with matplotlib is easy, and straightforward. import matplotlib.pyplot as plt From the documentation, &quot;matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., create a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.&quot;. You essentially create a plot, call functions that modify the plot (and the plot is preserved and modified after each change), and when you are done, close a plot. barplot barplot is a function that creates a barplot. Barplots are used to display categorical data. The following is an example of plotting some data from the precip dataset. import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10]) ## &lt;BarContainer object of 10 artists&gt; plt.show() plt.close() As you can see, the x-axis labels are bad. What if we turn the labels to be vertical? import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10]) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(myDF[&#39;place&#39;].iloc[:10], rotation=&#39;vertical&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064cb26040&gt;, &lt;matplotlib.axis.XTick object at 0x7f064cb26730&gt;, &lt;matplotlib.axis.XTick object at 0x7f064cb2b790&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8e7280&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8e7a00&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8e7d90&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8f1100&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8f1610&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8f1b20&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8f7070&gt;], &lt;a list of 10 Text major ticklabel objects&gt;) plt.show() plt.close() Much better, however, some of the longer names go off of the plot. Let's fix this. subplots_adjust adjusts the position of one of the edges of the plot, as a fraction of the figure width or height. Options other than bottom include: left, right, top, as well as wspace and hspace. import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10]) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(myDF[&#39;place&#39;].iloc[:10], rotation=&#39;vertical&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c8aa1c0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8a6d30&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8a6130&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8641c0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c864940&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c864af0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c86c040&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c86c550&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c86ca60&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c86cf70&gt;], &lt;a list of 10 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.2) plt.show() plt.close() This is even better, however, it would be nice to have a title and axis label(s). import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10]) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(myDF[&#39;place&#39;].iloc[:10], rotation=&#39;vertical&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064d076130&gt;, &lt;matplotlib.axis.XTick object at 0x7f064e60c220&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8cc430&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8c32b0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8c3760&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8c3c70&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8211f0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c821700&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8c33d0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c8218b0&gt;], &lt;a list of 10 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.3) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) plt.show() plt.close() We are getting there. Let's change the color. import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10], color=&quot;#FF826B&quot;) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(myDF[&#39;place&#39;].iloc[:10], rotation=&#39;vertical&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c84be50&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c84ba00&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c838dc0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c7920a0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c792430&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c792910&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c792e20&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c795370&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c795880&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c795d90&gt;], &lt;a list of 10 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.3) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) plt.show() plt.close() What if we want different colors for the different cities? import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) colors = (&quot;#8DD3C7&quot;, &quot;#FFFFB3&quot;, &quot;#BEBADA&quot;, &quot;#FB8072&quot;, &quot;#80B1D3&quot;, &quot;#FDB462&quot;, &quot;#B3DE69&quot;, &quot;#FCCDE5&quot;, &quot;#D9D9D9&quot;, &quot;#BC80BD&quot;,) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10], color=colors) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(myDF[&#39;place&#39;].iloc[:10], rotation=&#39;vertical&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c7c4fd0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c7c4b80&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c7aaf10&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c7851c0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c785730&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c785c40&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c70d190&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c70d6a0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c70dbb0&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c713100&gt;], &lt;a list of 10 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.3) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) plt.show() plt.close() What if instead of x-axis labels, we want to use a legend? import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) colors = (&quot;#8DD3C7&quot;, &quot;#FFFFB3&quot;, &quot;#BEBADA&quot;, &quot;#FB8072&quot;, &quot;#80B1D3&quot;, &quot;#FDB462&quot;, &quot;#B3DE69&quot;, &quot;#FCCDE5&quot;, &quot;#D9D9D9&quot;, &quot;#BC80BD&quot;, &quot;#E7FFAC&quot;, &quot;#AFF8DB&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10], color=colors) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(&#39;&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c741c10&gt;], &lt;a list of 1 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.3) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) labels = {place:color for place, color in zip(myDF[&#39;place&#39;].iloc[:10].to_list(), colors[:10])} print(labels) ## {&#39;Mobile&#39;: &#39;#8DD3C7&#39;, &#39;Juneau&#39;: &#39;#FFFFB3&#39;, &#39;Phoenix&#39;: &#39;#BEBADA&#39;, &#39;Little Rock&#39;: &#39;#FB8072&#39;, &#39;Los Angeles&#39;: &#39;#80B1D3&#39;, &#39;Sacramento&#39;: &#39;#FDB462&#39;, &#39;San Francisco&#39;: &#39;#B3DE69&#39;, &#39;Denver&#39;: &#39;#FCCDE5&#39;, &#39;Hartford&#39;: &#39;#D9D9D9&#39;, &#39;Wilmington&#39;: &#39;#BC80BD&#39;} handles = [plt.Rectangle((0,0),1,1, color=color) for label,color in labels.items()] plt.legend(handles=handles, labels=labels.keys()) plt.show() plt.close() Pretty good, but now we don't need so much space at the bottom, and we need to make space for that legend. We use subplots_adjust to modify the space at the bottom, and loc to move the position of the legend to the upper right (see here to see other loc options). import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) colors = (&quot;#8DD3C7&quot;, &quot;#FFFFB3&quot;, &quot;#BEBADA&quot;, &quot;#FB8072&quot;, &quot;#80B1D3&quot;, &quot;#FDB462&quot;, &quot;#B3DE69&quot;, &quot;#FCCDE5&quot;, &quot;#D9D9D9&quot;, &quot;#BC80BD&quot;, &quot;#E7FFAC&quot;, &quot;#AFF8DB&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10], color=colors) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(&#39;&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c8c3f70&gt;], &lt;a list of 1 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.15) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) labels = {place:color for place, color in zip(myDF[&#39;place&#39;].iloc[:10].to_list(), colors[:10])} print(labels) ## {&#39;Mobile&#39;: &#39;#8DD3C7&#39;, &#39;Juneau&#39;: &#39;#FFFFB3&#39;, &#39;Phoenix&#39;: &#39;#BEBADA&#39;, &#39;Little Rock&#39;: &#39;#FB8072&#39;, &#39;Los Angeles&#39;: &#39;#80B1D3&#39;, &#39;Sacramento&#39;: &#39;#FDB462&#39;, &#39;San Francisco&#39;: &#39;#B3DE69&#39;, &#39;Denver&#39;: &#39;#FCCDE5&#39;, &#39;Hartford&#39;: &#39;#D9D9D9&#39;, &#39;Wilmington&#39;: &#39;#BC80BD&#39;} handles = [plt.Rectangle((0,0),1,1, color=color) for label,color in labels.items()] plt.legend(handles=handles, labels=labels.keys(), loc=1) plt.show() plt.close() Okay, but the legend is still covering our plot. Let's use bbox_to_anchor to move it. import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) colors = (&quot;#8DD3C7&quot;, &quot;#FFFFB3&quot;, &quot;#BEBADA&quot;, &quot;#FB8072&quot;, &quot;#80B1D3&quot;, &quot;#FDB462&quot;, &quot;#B3DE69&quot;, &quot;#FCCDE5&quot;, &quot;#D9D9D9&quot;, &quot;#BC80BD&quot;, &quot;#E7FFAC&quot;, &quot;#AFF8DB&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10], color=colors) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(&#39;&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c65e5b0&gt;], &lt;a list of 1 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.15) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) labels = {place:color for place, color in zip(myDF[&#39;place&#39;].iloc[:10].to_list(), colors[:10])} print(labels) ## {&#39;Mobile&#39;: &#39;#8DD3C7&#39;, &#39;Juneau&#39;: &#39;#FFFFB3&#39;, &#39;Phoenix&#39;: &#39;#BEBADA&#39;, &#39;Little Rock&#39;: &#39;#FB8072&#39;, &#39;Los Angeles&#39;: &#39;#80B1D3&#39;, &#39;Sacramento&#39;: &#39;#FDB462&#39;, &#39;San Francisco&#39;: &#39;#B3DE69&#39;, &#39;Denver&#39;: &#39;#FCCDE5&#39;, &#39;Hartford&#39;: &#39;#D9D9D9&#39;, &#39;Wilmington&#39;: &#39;#BC80BD&#39;} handles = [plt.Rectangle((0,0),1,1, color=color) for label,color in labels.items()] plt.legend(handles=handles, labels=labels.keys(), bbox_to_anchor=(1.3, 1)) plt.show() plt.close() Let's use subplots_adjust to add some space on the right side. import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) colors = (&quot;#8DD3C7&quot;, &quot;#FFFFB3&quot;, &quot;#BEBADA&quot;, &quot;#FB8072&quot;, &quot;#80B1D3&quot;, &quot;#FDB462&quot;, &quot;#B3DE69&quot;, &quot;#FCCDE5&quot;, &quot;#D9D9D9&quot;, &quot;#BC80BD&quot;, &quot;#E7FFAC&quot;, &quot;#AFF8DB&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10], color=colors) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(&#39;&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c5e16a0&gt;], &lt;a list of 1 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.15, right=.75) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) labels = {place:color for place, color in zip(myDF[&#39;place&#39;].iloc[:10].to_list(), colors[:10])} print(labels) ## {&#39;Mobile&#39;: &#39;#8DD3C7&#39;, &#39;Juneau&#39;: &#39;#FFFFB3&#39;, &#39;Phoenix&#39;: &#39;#BEBADA&#39;, &#39;Little Rock&#39;: &#39;#FB8072&#39;, &#39;Los Angeles&#39;: &#39;#80B1D3&#39;, &#39;Sacramento&#39;: &#39;#FDB462&#39;, &#39;San Francisco&#39;: &#39;#B3DE69&#39;, &#39;Denver&#39;: &#39;#FCCDE5&#39;, &#39;Hartford&#39;: &#39;#D9D9D9&#39;, &#39;Wilmington&#39;: &#39;#BC80BD&#39;} handles = [plt.Rectangle((0,0),1,1, color=color) for label,color in labels.items()] plt.legend(handles=handles, labels=labels.keys(), bbox_to_anchor=(1, 1)) plt.show() plt.close() Let's remove or make the legend border white. import pandas as pd myDF = pd.read_csv(&quot;./files/precip.csv&quot;) colors = (&quot;#8DD3C7&quot;, &quot;#FFFFB3&quot;, &quot;#BEBADA&quot;, &quot;#FB8072&quot;, &quot;#80B1D3&quot;, &quot;#FDB462&quot;, &quot;#B3DE69&quot;, &quot;#FCCDE5&quot;, &quot;#D9D9D9&quot;, &quot;#BC80BD&quot;, &quot;#E7FFAC&quot;, &quot;#AFF8DB&quot;) plt.bar(myDF[&#39;place&#39;].iloc[:10], myDF[&#39;precip&#39;].iloc[:10], color=colors) ## &lt;BarContainer object of 10 artists&gt; plt.xticks(&#39;&#39;) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c5bc5e0&gt;], &lt;a list of 1 Text major ticklabel objects&gt;) plt.subplots_adjust(bottom=0.15, right=.75) plt.title(&quot;Average Precipitation&quot;) plt.ylabel(&quot;Inches of rain&quot;) labels = {place:color for place, color in zip(myDF[&#39;place&#39;].iloc[:10].to_list(), colors[:10])} print(labels) ## {&#39;Mobile&#39;: &#39;#8DD3C7&#39;, &#39;Juneau&#39;: &#39;#FFFFB3&#39;, &#39;Phoenix&#39;: &#39;#BEBADA&#39;, &#39;Little Rock&#39;: &#39;#FB8072&#39;, &#39;Los Angeles&#39;: &#39;#80B1D3&#39;, &#39;Sacramento&#39;: &#39;#FDB462&#39;, &#39;San Francisco&#39;: &#39;#B3DE69&#39;, &#39;Denver&#39;: &#39;#FCCDE5&#39;, &#39;Hartford&#39;: &#39;#D9D9D9&#39;, &#39;Wilmington&#39;: &#39;#BC80BD&#39;} handles = [plt.Rectangle((0,0),1,1, color=color) for label,color in labels.items()] plt.legend(handles=handles, labels=labels.keys(), bbox_to_anchor=(1, 1), edgecolor=&#39;white&#39;) plt.show() plt.close() boxplot boxplot is a function that creates a boxplot. Boxplots and side-by-side boxplots are very good for understanding fundamental information about your data, like the quartiles, upper and lower bounds, potential outliers, etc. First, we need to import matplotlib, and get our dataset. import matplotlib.pyplot as plt from rdatasets import data myDF = data(&quot;trees&quot;) myDF.head() ## Girth Height Volume ## 0 8.3 70 10.3 ## 1 8.6 65 10.3 ## 2 8.8 63 10.2 ## 3 10.5 72 16.4 ## 4 10.7 81 18.8 Next, let's say a tree is tall if it is above 75, and short otherwise. We can use the pandas method cut to create a new column called Size. import pandas as pd myDF[&#39;Size&#39;] = pd.cut(myDF[&#39;Height&#39;], bins = [0, 76, 100], labels = [&#39;short&#39;, &#39;tall&#39;]) myDF.head() ## Girth Height Volume Size ## 0 8.3 70 10.3 short ## 1 8.6 65 10.3 short ## 2 8.8 63 10.2 short ## 3 10.5 72 16.4 short ## 4 10.7 81 18.8 tall Okay, great! Let's take a look at the girth of the two groups of trees. import matplotlib.pyplot as plt plt.boxplot(myDF[&#39;Girth&#39;]) ## {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c8002e0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c800850&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c7998e0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c799d90&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c800250&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c7992e0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c799160&gt;], &#39;means&#39;: []} plt.show() Uh oh, that looks like a boxplot for all of the trees girth. If we look at the official documentation, we can see that boxplot makes a plot for each column of x or each vector in sequence x where x is our first argument (the data). Let's get our data into the correct format first. import matplotlib.pyplot as plt tmp = myDF.pivot(columns=&quot;Size&quot;, values=&quot;Girth&quot;) plt.boxplot([tmp[&#39;short&#39;], tmp[&#39;tall&#39;]]) ## {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c8f1be0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c8f1a00&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c901f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c891df0&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c8a1a00&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c8a1a60&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c891be0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f0659410910&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c8f1c40&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064c9016a0&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c901610&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cb496a0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c9016d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cb49700&gt;], &#39;means&#39;: []} plt.show() plt.close() Well, that didn't work. It looks like the NA's in each column are preventing matplotlib from working. Let's remove the NA's. import matplotlib.pyplot as plt tmp = myDF.pivot(columns=&quot;Size&quot;, values=&quot;Girth&quot;) plt.boxplot([tmp[&#39;short&#39;].dropna(), tmp[&#39;tall&#39;].dropna()]) ## {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064cb2bbb0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cb2b5e0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cdd1df0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cdd1cd0&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064cb2bee0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cb2b100&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cdad130&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cdad220&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064c8d7ac0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cdd17c0&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064cdd1bb0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cdad4f0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064cdd12b0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cdadb50&gt;], &#39;means&#39;: []} ## ## /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray ## return array(a, dtype, copy=False, order=order) plt.show() plt.close() Okay, great! Now, let's add some proper labels. import matplotlib.pyplot as plt tmp = myDF.pivot(columns=&quot;Size&quot;, values=&quot;Girth&quot;) plt.boxplot([tmp[&#39;short&#39;].dropna(), tmp[&#39;tall&#39;].dropna()]) ## {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064cb13b50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064ce04a30&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cd84790&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cd84400&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064ce04be0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064ce04880&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cd84a90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cd67820&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f064cb13190&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cd84cd0&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f0659406c70&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cd67070&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7f065940d2b0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f064cd67670&gt;], &#39;means&#39;: []} ## ## /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray ## return array(a, dtype, copy=False, order=order) plt.title(&quot;Tree girth&quot;) plt.xticks([1,2], [&#39;short&#39;, &#39;tall&#39;]) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c6bda00&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c6bd430&gt;], [Text(0, 0, &#39;short&#39;), Text(0, 0, &#39;tall&#39;)]) plt.ylabel(&quot;Girth in inches&quot;) plt.show() plt.close() Let's add some color. import matplotlib.pyplot as plt tmp = myDF.pivot(columns=&quot;Size&quot;, values=&quot;Girth&quot;) boxes = plt.boxplot([tmp[&#39;short&#39;].dropna(), tmp[&#39;tall&#39;].dropna()], patch_artist=True) ## /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray ## return array(a, dtype, copy=False, order=order) plt.title(&quot;Tree girth&quot;) plt.xticks([1,2], [&#39;short&#39;, &#39;tall&#39;]) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c6b8610&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c6b8f70&gt;], [Text(0, 0, &#39;short&#39;), Text(0, 0, &#39;tall&#39;)]) plt.ylabel(&quot;Girth in inches&quot;) for box in boxes[&#39;boxes&#39;]: box.set(facecolor=&#39;#90ee90&#39;) ## [None] ## [None] plt.show() plt.close() Better, but that sure isn't too intuitive. Here is an eclectic color scheme to highlight how to change other components. import matplotlib.pyplot as plt tmp = myDF.pivot(columns=&quot;Size&quot;, values=&quot;Girth&quot;) boxes = plt.boxplot([tmp[&#39;short&#39;].dropna(), tmp[&#39;tall&#39;].dropna()], patch_artist=True) ## /opt/hostedtoolcache/Python/3.9.1/x64/lib/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray ## return array(a, dtype, copy=False, order=order) plt.title(&quot;Tree girth&quot;) plt.xticks([1,2], [&#39;short&#39;, &#39;tall&#39;]) ## ([&lt;matplotlib.axis.XTick object at 0x7f064c628430&gt;, &lt;matplotlib.axis.XTick object at 0x7f064c7b9f70&gt;], [Text(0, 0, &#39;short&#39;), Text(0, 0, &#39;tall&#39;)]) plt.ylabel(&quot;Girth in inches&quot;) plt.setp(boxes[&quot;boxes&quot;], color=&quot;red&quot;) ## [None, None] for box in boxes[&#39;boxes&#39;]: box.set(facecolor=&#39;#90ee90&#39;) ## [None] ## [None] plt.setp(boxes[&#39;whiskers&#39;], color=&quot;pink&quot;) ## [None, None, None, None] plt.setp(boxes[&#39;fliers&#39;], color=&quot;blue&quot;) # outliers not present in this graph ## [None, None] plt.setp(boxes[&#39;medians&#39;], color=&quot;purple&quot;) ## [None, None] plt.setp(boxes[&#39;caps&#39;], color=&quot;grey&quot;) ## [None, None, None, None] plt.show() plt.close() piechart dotchart scatterplot lineplot plot is a function that creates a lineplot. Lineplots are used to display numeric data. The following is an example of plotting some data from the Orange dataset. First we need to import matplotlib, and get our dataset. import matplotlib.pyplot as plt from rdatasets import data myDF = data(&quot;Orange&quot;) Let's peek at our dataset. myDF.head() ## Tree age circumference ## 0 1 118 30 ## 1 1 484 58 ## 2 1 664 87 ## 3 1 1004 115 ## 4 1 1231 120 Then, we plot age on our x-axis, and circumference on our y-axis. plt.plot(myDF[&#39;age&#39;], myDF[&#39;circumference&#39;]) Okay, that's a start, but a lineplot doesn't look quite right unless the x-axis, age is sorted. myDF.sort_values([&#39;age&#39;, &#39;circumference&#39;], inplace=True) plt.plot(myDF[&#39;age&#39;], myDF[&#39;circumference&#39;]) Okay, that is better. Let's add a title and axis labels. myDF.sort_values([&#39;age&#39;, &#39;circumference&#39;], inplace=True) plt.plot(myDF[&#39;age&#39;], myDF[&#39;circumference&#39;]) plt.title(&quot;Orange circumference by Age&quot;) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Circumference&quot;) What if we wanted to change the color of the plot? myDF.sort_values([&#39;age&#39;, &#39;circumference&#39;], inplace=True) plt.plot(myDF[&#39;age&#39;], myDF[&#39;circumference&#39;], color=&quot;#FB8072&quot;) plt.title(&quot;Orange circumference by Age&quot;) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Circumference&quot;) How about adding another line on top of the current line? import numpy as np myDF.sort_values([&#39;age&#39;, &#39;circumference&#39;], inplace=True) plt.plot(myDF[&#39;age&#39;], myDF[&#39;circumference&#39;], color=&quot;#FB8072&quot;) plt.plot(myDF[&#39;age&#39;], np.sin(myDF[&#39;age&#39;])*myDF[&#39;circumference&#39;]) plt.title(&quot;Orange circumference by Age&quot;) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Circumference&quot;) Of course, we can change the texture of lines as well. Textures include: solid, dashed, dotted, dashdot. import numpy as np myDF.sort_values([&#39;age&#39;, &#39;circumference&#39;], inplace=True) plt.plot(myDF[&#39;age&#39;], myDF[&#39;circumference&#39;], color=&quot;#FB8072&quot;, linestyle=&quot;dashed&quot;) plt.plot(myDF[&#39;age&#39;], np.sin(myDF[&#39;age&#39;])*myDF[&#39;circumference&#39;], linestyle=&quot;dotted&quot;) plt.title(&quot;Orange circumference by Age&quot;) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Circumference&quot;) We could zoom in on the chart by changing the axis limits using xlim and ylim. import numpy as np myDF.sort_values([&#39;age&#39;, &#39;circumference&#39;], inplace=True) plt.plot(myDF[&#39;age&#39;], myDF[&#39;circumference&#39;], color=&quot;#FB8072&quot;, linestyle=&quot;dashed&quot;) plt.plot(myDF[&#39;age&#39;], np.sin(myDF[&#39;age&#39;])*myDF[&#39;circumference&#39;], linestyle=&quot;dotted&quot;) plt.title(&quot;Orange circumference by Age&quot;) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Circumference&quot;) plt.xlim(950, 1250) ## (950.0, 1250.0) plt.show() Resources Official introduction to pyplot A decent introduction to pyplot. Official usage guide A detailed explanation of the anatomy of a figure, terminology used with matplotlib, etc. Probably more detail than most people would want. plotly plotnine pygal seaborn bokeh Classes Attributes Methods tensorflow pytorch "],
["tools.html", "Tools Docker Tableau GitHub VPNs", " Tools Docker Tableau GitHub Overview GitHub is a git repository hosting service. There are other, less well known repository hosting services such as: GitLab, Bitbucket, and Gitea. git itself is a free and open source version-control system for tracking changes in source code during software development.1 git Install Follow the instructions here to install git onto your machine. Configure git Run the following commands: git config --global user.name &quot;You name here&quot; git config --global user.email &quot;your_email@example.com&quot; Next, you need to authenticate with GitHub. Create a public/private keypair: ssh-keygen -t rsa -C &quot;your_email@example.com&quot; This creates two files: ~/.ssh/id_rsa --your private key and ~/.ssh/id_rsa.pub --your public key Copy your public key to your clipboard. Navigate and sign in to https://github.com. Go here, and click &quot;New SSH key&quot;. Name the key whatever you'd like in the &quot;Title&quot; field. Usually, I put the name of the computer I'm using. Paste the key in the &quot;Key&quot; field, and click &quot;Add SSH key&quot;. At this point in time you should be good to go. Verify by running the following in your terminal: ssh -T git@github.com You should receive a message like: Hi username! You&#39;ve successfully authenticated, but Github does not provide shell access. Clone a repository If you've followed the directions here to configure git with SSH: Open a terminal and navigate into the folder in which you'd like to clone the repository. For example, let's say I would like to clone this book's repository into my ~/projects folder: cd ~/projects Next, run the following command: git clone git@github.com:TheDataMine/the-examples-book.git At this point in time, you should have a new folder called the-examples-book inside your ~/projects folder. Commit changes to a repository Creating a commit is simple: Navigate into your project repository folder. For example, let's assume our repository lives: ~/projects/the-examples-book. cd ~/projects/the-examples-book Modify the repository files as you would like, saving the changes. Create your commit, with an accompanying message: git commit -m &quot;Fixed minor spelling error.&quot; Fetch remote changes Navigate to the local repository. For example, let's assume our repository lives: ~/projects/the-examples-book. cd ~/projects/the-examples-book Fetch and pull the changes: git fetch git pull Push local commits to the remote origin First fetch any remote changes. Then run the following commands: git push Create a new branch To create a new branch based off of the master branch do the following. Checkout the master branch: git checkout master Create a new branch named fix-spelling-errors-01 based off of the master branch and check the new fix-spelling-errors-01 branch out: git checkout -b fix-spelling-errors-01 Publish your branch to GitHub If your current local branch is not present on its remote origin, git push will publish the branch to GitHub. Create a pull request After publishing a local branch to GitHub, in order to create a pull request, simply navigate to the following link: https://github.com/my_organization/my_repo/pull/new/my_branch_name Replace my_organization with the username or organization name. For example: thedatamine. Replace my_repo with the name of the repository. For example: the-examples-book. Replace my_branch_name with the name of the branch you would like to have merged into the master branch. For example: fix-spelling-errors-01. So at the end, using our examples, you would navigate to: https://github.com/TheDataMine/the-examples-book/pull/new/fix-spelling-errors-01 Fill out the information, and click &quot;Create pull request&quot;. GitHub Desktop GitHub Desktop is the official graphical user interface (GUI) for GitHub. It is extremely user friendly. It can help new users and more experienced users alike manage their repositories on GitHub. Install Follow the directions here to install GitHub Desktop. Upon the launch of the application, you should be presented with a screen similar to this: 3. Click on &quot;Sign in to GitHub.com&quot;. This should launch a browser and ask you to &quot;Sign in to GitHub to continue to GitHub Desktop&quot;. 4. Enter your GitHub credentials and click on &quot;Sign in&quot;. If your browser asks if you'd like to open something up in the GitHub Desktop application, click okay. When you see the following screen, enter your name and email address you'd like to associate with your commits. Anyone will be able to see this information if you publish commits. Click continue. Select whether or not you'd like to send usage statistics to Microsoft, and click &quot;Finish&quot;. Commit changes to a repository First, make a change to to a file within the repository. In this example, I added a contributor named John Smith: 2. In the lower left-hand corner of the GUI, add a Commit title and description. Concise and detailed titles and descriptions are best. Click &quot;Commit to name-of-branch&quot; in this case, our branch name is fix-spelling-errors-01. 3. At this point in time the Commit is only local (on your machine). In order to update the remote respository (on GitHub), you'll need to publish your branch. If your branch is already published (present on github.com), you'll need to push your local commits to the remote origin (which is the remote fix-spelling-errors-01 branch in this case) by clicking on the &quot;Push origin&quot; button: Push local commits to the remote origin If you have commits that are ready to be pushed to the remote origin (github.com), you'll be presented with a screen similar to this: Simply click on the &quot;Push origin&quot; button in order to push your local commits to the remote origin (which is in this case, a remote branch called fix-spelling-errors-01): You can verify that the changes have been made by navigating to the branch on github.com, and checking the commit history. Create a new branch In GitHub Desktop, click on the &quot;Current Branch&quot; dropdown: 2. Click on the &quot;New Branch&quot; button: 3. When presented with the following screen, ensure that your new branch will be based on the master branch: 4. Type whatever name you'd like to give the new branch. In this case, we are calling it fix-spelling-errors-01. Click &quot;Create Branch&quot;. 5. Your current branch should now be fix-spelling-errors-01 or whatever name you entered in step (4). You can see this in the dropdown: Publish your branch to GitHub If the branch you created is not already present remotely, you'll have a button available to you that says &quot;Publish Branch&quot;. Clicking this button will push the branch to the remote repository (on github.com): 2. You can confirm that the branch has been successfully pushed to github.com by navigating to the repository on github, and clicking on the &quot;branches&quot; tab: Create a pull request If the branch you are working on is already published remotely, and the remote repository and local repository are both up to date, you will be presented with a screen similar to this: Note that if your local repository is ahead of the remote repository, you will instead be presented with a screen similar to this: You will first need to push your local commits to the origin (which is the remote fix-spelling-errors-01 branch in this case) by clicking on the &quot;Push origin&quot; button. Click the &quot;Create Pull Request&quot; button. This will open up a tab in your browser: Leave a detailed comment about what you've modified or added to the book. You can click on &quot;Preview&quot; to see what your comment will look like. GitHub's markdown applies here. Once satisfied, click &quot;Create pull request&quot;. Resources GitHub glossary: An excellent resource to understand git and GitHub specific terminology. Learn git branching: An interactive game that teaches you about git branching. VPNs https://en.wikipedia.org/wiki/Git↩ "],
["faqs.html", "FAQs Cannot open the connection. No such file or directory. How do I connect to Scholar from off-campus? In Scholar, on RStudio, my font size looks weird or my cursor is offset. I'm unable to type into the terminal in RStudio. I'm unable to connect to RStudio Server. RStudio is taking a long time to open. How do I delete a file from my RStudio directory? (asked by Karthik Uppuluri) How do I rename a file from my RStudio directory? How can you run a line of R code in RStudio without clicking the &quot;Run&quot; button? My R session freezes. Scholar is slow. How to transfer files between your computer and Scholar. My password will not work. Jupyter Notebook download error with IE. Jupyter Notebook kernel dying. Python kernel not working, Jupyter Notebook won't save. Installing my_package for Python. Displaying multiple images after a single Jupyter Notebook Python code cell. RMarkdown “Error: option error has NULL value” when knitting&quot;. How do you create an RMarkdown file? Problems building an RMarkdown document on Scholar. How can I use SQL in RMarkdown? Copy/paste from terminal (not a console) inside RStudio to RMarkdown. How do I render an image in a shiny app? The package my_package is not found. Problems installing ggmap. Error: object_name is not found. Zoom in on ggmap. Find the latitude and longitude of a location. Problems saving work as a PDF in R on Scholar. What is a good resource to better understand HTML? Is there a style guide for R code? Is there a guide for best practices using R? Tips for using Jupyter notebooks. What is my username on Scholar? How and why would I need to &quot;escape a character&quot;? How can I fix the error &quot;Illegal byte sequence&quot; when using a UNIX utility like cut? Unicode character error when Knitting an RMarkdown file to PDF. My tab key will not auto-complete anymore in RStudio. How can I fix this?", " FAQs Cannot open the connection. No such file or directory. Copy/paste from terminal (not a console) inside RStudio to RMarkdown. Displaying multiple images after a single Jupyter Notebook Python code cell. Error: object_name is not found. Find the latitude and longitude of a location. How and why would I need to &quot;escape a character&quot;? How can I fix the error &quot;Illegal byte sequence&quot; when using a UNIX utility like cut? How can I use SQL in RMarkdown? How can you run a line of R code in RStudio without clicking the &quot;Run&quot; button? How do I connect to Scholar from off-campus? How do I delete a file from my RStudio directory? How do I rename a file from my RStudio directory? How do I render an image in a shiny app? How do you create an RMarkdown file? How to transfer files between your computer and Scholar. I'm unable to connect to RStudio Server. I'm unable to type into the terminal in RStudio. In Scholar, on RStudio, my font size looks weird or my cursor is offset. Installing my_package for Python. Is there a guide for best practices using R? Is there a style guide for R code? Jupyter Notebook download error with IE. Jupyter Notebook kernel dying. My password will note work. My R session freezes. Problems building an RMarkdown document on Scholar. Problems installing ggmap. Problems saving work as a PDF in R on Scholar. Python kernel not working, Jupyter Notebook won't save. RMarkdown “Error: option error has NULL value” when knitting&quot;. RStudio is taking a long time to open. Scholar is slow. The package my_package is not found. Tips for using Jupyter notebooks. Unicode character error when Knitting an RMarkdown file to PDF. What is a good resource to better understand HTML? What is my username on Scholar? Zoom in on ggmap. Cannot open the connection. No such file or directory. If you receive an error similar to: Error in file(file, &quot;rt&quot;) : cannot open the connection In addition: Warning message: In file(file, &quot;rt&quot;) : cannot open file &#39;/class/datamine/data/goodreads_books.csv&#39;: No such file or directory This error means that the path to the dataset does not exist. In this case, the path should be /class/datamine/data/csv/goodreads_books.csv. When you receive an error like this, usually during a call to read.csv, you should double check that the path to your dataset is actually correct. How do I connect to Scholar from off-campus? There are a variety of ways to connect to Scholar from off-campus. If you just want to use Jupyter notebooks (e.g., for Python), you can use JupyterHub. If you just want to use RStudio, you can use RStudio Server. In Scholar, on RStudio, my font size looks weird or my cursor is offset. In scholar, navigate to Tools &gt; Global Options &gt; Appearance. You can change your font, including the size and the color scheme. The default font in RStudio Server Pro is Modern (font size 10), and the default Editor theme is Textmate. Make your desired changes, and then click the Apply button. I'm unable to type into the terminal in RStudio. Try opening a new terminal, try clearing the terminal buffer, or interrupting the current terminal. All these options come from a menu that will pop up when you hit the small down arrow next to the words &quot;Terminal 1&quot; (it might be another number depending on how many terminals are open) which is on the left side right above the terminal in RStudio. I'm unable to connect to RStudio Server. Try closing your browser, clearing your cookies, and using the original link: https://rstudio.scholar.rcac.purdue.edu/ for RStudio Server Pro. RStudio is taking a long time to open. Click here for video In general, you do NOT want to save your .RData file when you close RStudio. These files will make RStudio take a long time to open, next time you use RStudio. It is possible that you (previously) saved a large .RData file the last time that you closed RStudio. If you did save your .RData file, and your RStudio is very slow to open, then you might want to remove the .RData file now. You can do the following: Inside RStudio, select the Terminal (located near the Console; do not use the Console itself). Inside the Terminal, type: cd (and hit Enter/Return) so that you will be working in your home directory. You can double-check this by typing: pwd and it should show you that you are working in /home/mdw (but of course mdw will be whatever your username is). Type: rm .RData (be sure to put a space between rm and .RData) and then hit Enter/Return. Now your R workspace should be fresh when you log out of RStudio (by clicking the little orange &quot;log out&quot; button, in the upper-right-hand corner of RStudio). In other words, next time, you will not have old variables hanging around, from a previous session. Now your RStudio should load more quickly at the start. How do I delete a file from my RStudio directory? (asked by Karthik Uppuluri) In the lower-right-hand corner of your RStudio, you have a panel with 5 tabs: Files, Plots, Packages, Help, Viewer Choose the Files tab. That will give you a listing of files in your home directory. You can click on any of them (i.e., put a checkbox beside the name of the file) and hit the Delete button. Screenshot provided by Hilda Somnooma Marie Bernadette Ibriga: How do I rename a file from my RStudio directory? In the lower-right-hand corner of your RStudio, you have a panel with 5 tabs: Files, Plots, Packages, Help, Viewer Choose the Files tab. That will give you a listing of files in your home directory. You can click on any of them (i.e., put a checkbox beside the name of the file) and hit the Rename button. How can you run a line of R code in RStudio without clicking the &quot;Run&quot; button? Click anywhere on the line (you do not need to highlight the line, and you do not need to click at the start or end of the line; anywhere on the line is ok). Type the &quot;Control&quot; and the &quot;Return (or Enter)&quot; keys together, at the same time, to run that line. This will save you a great deal of time, in the long run. My R session freezes. Log out of RStudio Server Pro, using either the &quot;Sign Out&quot; under the File Menu, or using the little orange &quot;log out&quot; button, in the upper-right-hand corner of RStudio. If neither option works, you can try closing your browser window manually. Scholar is slow. Possibility one: Some of the files we use in this class require a few minutes to load, if we use the read.csv() function in R. Here is a method that can save you some time in data import: Read only the first, say, 10000 rows of data (see instructions below), and complete your code using the smaller dataset. The code works for the subset of data should also work for the complete data. This output is not your final answer! Once you complete the code, read in the entire dataset, and run the code to RStudio. You may even close the ThinLinc after submitting the code as long as you do not close your RStudio window. Closing RStudio will stop your code from running. It is also highly recommended to save your code prior to running it. Some time (e.g., a few hours) later, you can come back and check your output. Scholar is a computing facility that is always on, and thus you can leave it do the work. How do you read the first 10000 rows then? For example, we usually use the following line of code to read all of the election data: myDF &lt;- read.csv(&#39;/class/datamine/data/election/itcont2020.txt&#39;) Now, with an additional parameter nrows, you can decide how many rows to read: myDF_short &lt;- read.csv(&#39;/class/datamine/data/election/itcont2020.txt&#39;, nrows = 10000) Possibility two: You could be close to using 100% of your quota on scholar. Use the Terminal (not the Console), and run the following command: myquota. If your quota is near 100% in your /home directory (25 GB), you will need to delete some files. How to transfer files between your computer and Scholar. Solution 1: use a file transfer client There are many specialized file transfer clients. On Windows, we recommend WinSCP: https://winscp.net/eng/download.php (There are frequently advertisements on this page, but look for the green button that says something like DOWNLOAD WINSCP 5.17.7 (10.6 MB)) On a Mac, we recommand Fetch: https://fetchsoftworks.com/ (Education users can apply for a free license: https://fetchsoftworks.com/fetch/free) The server hostname that you want to connect to is: scholar.rcac.purdue.edu FileZilla is another good client, which works on all platforms. Download and install the FileZilla Client onto your personal computer. FileZilla uses sftp ([S]SH [F]ile [T]ransfer [P]rotocol) to transfer files to and from Scholar. To connect to Scholar from FileZilla, enter the following information and click &quot;Quickconnect&quot;: Host: scholar.rcac.purdue.edu Username: &lt;your_scholar_username&gt; (For example, Dr. Ward's would be mdw. See here.) Password: &lt;your_scholar_password&gt; Port: 22 After clicking &quot;Quickconnect&quot; you may be asked something similar to the following: Select &quot;OK&quot; and establish the connection. The files on the left-hand side are your local computer's files. The files on the right-hand side are the files in Scholar. To download files from Scholar, right click the file(s) on the Scholar side (right-hand side) and click &quot;Download&quot;. To upload files to Scholar, right click the file(s) on your local machine (left-hand side) and click &quot;Upload&quot;. Solution 2: use SFTP On windows: Open your start menu and click on cmd. Type: sftp username@scholar.rcac.purdue.edu (replace &quot;username&quot; with your username). Once connected, follow the documentation from RCAC to transfer files. On mac: Open a terminal. Type: sftp username@scholar.rcac.purdue.edu (replace &quot;username&quot; with your username). Once connected, follow the documentation from RCAC to transfer files. My password will not work. Remember that you need to use your BoilerKey to log into most resources on Scholar this year: https://www.purdue.edu/boilerkey You typically type your 4-digit PIN, then a comma, and then your randomly generated BoilerKey code. There is still one Scholar tool that uses the Career password: Jupyter Notebooks, located at https://notebook.scholar.rcac.purdue.edu/ If your Career password has expired and you need to log onto Jupyter Notebooks, you can use these steps to reset your password: Go to Secure Purdue. Click on the option &quot;Change your password&quot;. After logging in, search for the link &quot;Change Password&quot; that &quot;Allows you to change your Purdue Career Account password&quot;. Jupyter Notebook download error with IE. Please note that Internet Explorer is not a recommended browser. If still want to use Explorer, make sure you download the notebook as &quot;All Files&quot; (or something similar). That is, we need to allow the browser to save in its natural format, and not to convert the notebook when it downloads the file. Jupyter Notebook kernel dying. Make sure you are using the R 3.6 (Scholar) kernel. Make sure you are using https://notebook.scholar.rcac.purdue.edu and not https://notebook.brown.rcac.purdue.edu. (Use Scholar instead of Brown.) Try clicking Kernel &gt; Shutdown, and then reconnect the kernel. If one particular Jupyter Notebook template gives you this error, then create a new R 3.6 (Scholar) file. Try re-running the code from an earlier project that you had set up and working using Jupyter Notebooks. One student needed to re-run the setup command one time in the terminal: /class/datamine/apps/runme.sh You could be close to using 100% of your quota on scholar. Use the Terminal (not the Console), and run the following command: myquota. If your quota is near 100% in your /home directory (25 GB), you will need to delete some files. Python kernel not working, Jupyter Notebook won't save. You probably have a package conflict. Navigate to Jupyter Notebook: https://notebook.scholar.rcac.purdue.edu/, and login. Click on the &quot;Running&quot; tab and shutdown all running kernels. Then navigate to RStudio: https://rstudio.scholar.rcac.purdue.edu/, and login. Open a terminal, and run the following commands: pip uninstall mypackagenamehere /class/datamine/apps/runme.sh Go back to https://notebook.scholar.rcac.purdue.edu/, click on &quot;Control Panel&quot; in the upper right hand corner. Click the &quot;Stop My Server&quot; button, followed by the green &quot;My Server&quot; button. Installing my_package for Python. Do not install packages in Scholar using: pip install my_package or pip install my_package --user We've tried to provide you with a ready-made kernel with every package you would want or need. If you need a newer version of some package, or need a package not available in the kernel, please send us a message indicating what you need. Depending on the situation we may point you to create your own kernel. Displaying multiple images after a single Jupyter Notebook Python code cell. Sometimes it may be convenient to have several images displayed after a single Jupyter cell. For example, if you want to have side-by-side images or graphs for comparison. The following code allows you to place figures side-by-side or in a grid. Note you will need the included import statement at the very top of the notebook. import matplotlib.pyplot as plt number_of_plots = 2 fig, axs = plt.subplots(number_of_plots) fig.suptitle(&#39;Vertically stacked subplots&#39;, fontsize=12) axs[0].plot(x, y) axs[1].imshow(img) plt.show() number_of_plots = 3 fig, axs = plt.subplots(1,number_of_plots) fig.suptitle(&#39;Horizontally stacked subplots&#39;, fontsize=12) axs[0].plot(x, y) axs[1].imshow(img) axs[2].imshow(img2) plt.show() number_of_plots_vertical = 2 number_of_plots_horizontal = 2 # 2 x 2 = 4 total plots fig, axs = plt.subplots(number_of_plots_vertical,number_of_plots_horizontal) fig.suptitle(&#39;Grid of subplots&#39;, fontsize=12) axs[0][0].plot(x, y) # top left axs[0][1].imshow(img) # top right axs[1][0].imshow(img2) # bottom left axs[1][1].plot(a, b) # bottom right plt.show() RMarkdown “Error: option error has NULL value” when knitting&quot;. This error message occurs when running a code chunk in RMarkdown by clicking the green &quot;play&quot; button (Run Current Chunk). Do not click on the green triangle &quot;play&quot; button. Instead, knit the entire document, using the &quot;knit&quot; button that looks like a ball of yarn with a knitting needle on it. How do you create an RMarkdown file? Any text file with the .Rmd file extension can be opened and knitted into a PDF (or other format). If you'd like to create an RMarkdown file in RStudio, you can do so. Open an RStudio session. Click on File &gt; New File &gt; RMarkdown.... You may put R code into the R blocks (the grey sections of the document), and put any comments into the white sections in between. This is an excellent guide to RMarkdown, and this is a cheatsheet to get you up and running quickly. Problems building an RMarkdown document on Scholar. If you are having problems building an RMarkdown document on Scholar, try the following: Remove your R directory: Open up a terminal (not a console) in RStudio. Run the following commands: cd ~ rm -rf R This will force the removal of your R directory. It will remove your old R libraries. They will reload the newest versions if you install them again, and as you use them. This is recommended, especially at the start of the academic year. If your R is taking a long time to open, see here. How can I use SQL in RMarkdown? When you use SQL in RMarkdown you can highlight the code in code chunks just like R by writing &quot;sql&quot; instead of &quot;r&quot; in the brackets: SELECT * FROM table; You will notice that all the SQL code chunks provided in the template have the option eval=F. The option eval=F or eval=FALSE means that the SQL statements would be shown in your knitted document, but without being executed. To actually run SQL inside RMarkdown see here. You can read about the different languages that can be displayed in RMarkdown here: https://bookdown.org/yihui/rmarkdown/language-engines.html. Copy/paste from terminal (not a console) inside RStudio to RMarkdown. If you're using the terminal inside the Scholar RStudio at https://rstudio.scholar.rcac.purdue.edu, then right clicking won't work. A trick that does work (and often works in other situations as well) is the keyboard shortcut ctrl-insert for copy and shift-insert for paste. Alternatively, use the Edit/Copy from the menu in the terminal. How do I render an image in a shiny app? There are a variety of ways to render an image in an RShiny app. See here. The package my_package is not found. The package might not be installed. Try running: install.packages(&quot;ggmap&quot;) Note that if you have already run this on ThinLinc, there is no need to do it again. Another possibility is that the library is not loaded, try running: library(ggmap) Problems installing ggmap. Two possible fixes: Open a terminal (not the console) in RStudio and run: rm -rf ~/R After that, re-open RStudio and re-install ggmap: install.packages(&quot;ggmap&quot;) # Don&#39;t forget to load the package as well library(ggmap) Open a terminal (not the console) and run: module load gcc/5.2.0 After that, restart all RStudio processes. Error: object_name is not found. In R if you try to reference an object that does not yet exist, you will receive this error. For example: my_list &lt;- c(1, 2, 3) mylist In this example you will receive the error Error: object 'mylist' not found. The reason is mylist doesn't exist, we only created my_list. Zoom in on ggmap. Run the following code in R: ?get_googlemap Under the arguments section you will see the argument zoom and can read about what values it can accept. For the zoom level , a map with zoom=9 would not even show the entire state of California. Try different integers. Larger integers &quot;zoom in&quot; and smaller integers &quot;zoom out&quot;. Find the latitude and longitude of a location. Install the ggmap package. Run the following lines of code to retrieve latitude and longitude of a location: as.numeric(geocode(&quot;London&quot;)) Replace &quot;London&quot; with the name of your chosen location. Problems saving work as a PDF in R on Scholar. Make sure you are saving to your own working directory: getwd() This should result in something like: /home/&lt;username&gt;/... where &lt;username&gt; is your username. Read this to find your username. If you don't see your username anywhere the the resulting path, instead try: Specifying a different directory: dev.print(pdf, &quot;/home/&lt;username&gt;/project4map.pdf&quot;) Make sure you replace &lt;username&gt; with your username. Try setting your working directory before saving: setwd(&quot;/home/&lt;username&gt;&quot;) Make sure you replace &lt;username&gt; with your username. What is a good resource to better understand HTML? https://www.geeksforgeeks.org/html-course-structure-of-an-html-document/ Is there a style guide for R code? https://style.tidyverse.org/ Is there a guide for best practices using R? https://www.r-bloggers.com/r-code-best-practices/ Comment what you are going to do. Code -- what did you do? Comment on the output -- what did you get? Tips for using Jupyter notebooks. See here. What is my username on Scholar? To find your username on Scholar: Open a terminal (not the console). Execute the following code: echo $USER How and why would I need to &quot;escape a character&quot;? You would need to escape a character any time when you have a command or piece of code where you would like to represent a character literally, but that character has been reserved for some other use. For example, if I wanted to use grep to search for the $ character, literally, I would need to escape that character as its purpose has been reserved as an indicator or anchor for the end of the line. grep -i &quot;\\$50.00&quot; some_file.txt Without the \\ this code would not work as intended. In this case, if you chose to use single quotes instead, this would work, because single quotes are taken literally by the shell and aren't expanded like with double quotes: grep -i &#39;$50.00&#39; some_file.txt Another example would be searching for &quot;a&quot; or &quot;b&quot;, notice we need to escape (, ), and |: grep -i &#39;\\(a\\|b\\)&#39; some_file.txt Alternatively, we could use the -E option which uses extended regular expressions and doesn't need to be escaped as much: grep -Ei &#39;(a|b)&#39; some_file.txt Another example would be if you wanted to write out 10*10*10 = 1000 in markdown. If you don't escape the asterisks, the result may be rendered as 101010 = 1000, which is clearly not what was intended. For this reason, we would type out: 10\\*10\\*10 = 1000 Which would then have its intended effect. Resources Basic matches Last paragraph here How can I fix the error &quot;Illegal byte sequence&quot; when using a UNIX utility like cut? Often times this is due to your input having illegal, non-utf-8 values. You can find all lines with illegal values by running: grep -axv &#39;.*&#39; file To fix this issue, you can remove the illegal values by running: iconv -c -t UTF-8 &lt; old_file &gt; new_file Unicode character error when Knitting an RMarkdown file to PDF. If you get the following error when trying to Knit an RMarkdown file to PDF: ! Package inputenc Error: Unicode character &lt;somecharacter&gt; (U+0195) (inputenc) not set up for use with LaTeX. You are probably trying to print a unicode character. If you don't think you are trying to print a unicode character, it could be that part of some dataset which you are printing is. To fix this error, print a different slice of the dataset. Alternatively, try using xelatex to compile your PDF, by modifying your YAML header to look something like: --- title: &quot;Title&quot; output: pdf_document: latex_engine: xelatex --- Important note: Make sure you verify that the PDF contents are what you expect if testing xelatex. My tab key will not auto-complete anymore in RStudio. How can I fix this? In the Terminal (not the Console) in RStudio, type: cd ~/.config mv rstudio rstudio.old mv RStudio RStudio.old and then log out of RStudio using the little orange button, and log back in. "],
["projects.html", "Spring 2021 projects Templates Submissions STAT 19000 STAT 29000 STAT 39000", " Spring 2021 projects Templates Our course project template can be found here, or on Scholar: /class/datamine/apps/templates/project_template.Rmd Important note: We've updated the template to allow a code chunk option that prevents content from running off the page. Simply add linewidth=80 to any code chunk that creates output that runs off the page. This video demonstrates: opening a browser (emphasizing Firefox as the best choice), opening RStudio Server Pro (https://rstudio.scholar.rcac.purdue.edu), introducing (basics) about what RStudio looks like, checking to see that the students are using R 4.0, running the initial (one-time) setup script, opening the project template, knitting the template into a PDF file, and finally handling the popup blocker, which can potentially block the PDF. Click here for video Students in STAT 19000, 29000, and 39000 are to use this as a template for all project submissions. The template includes a code chunk that &quot;activates&quot; our Python environment, and adjusts some default settings. In addition, it provides examples on how to include solutions for Python, R, Bash, and SQL. Every question should be clearly marked with a third-level header (using 3 #s) followed by Question 1, Question 2, etc. Sections for solutions should be added or removed, based on the number of questions in the given project. All code chunks are to be run and solutions displayed for the compiled PDF submission. Any format or template related questions should be asked in Piazza. Submissions Unless otherwise specified, all projects will need 2-4 submitted files: A compiled PDF file (built using the template), with all code and output. The .Rmd file (based off of the template), used to Knit the final PDF. If it is a project containing R code, a .R file containing all of the R code with comments explaining what the code does. Note: This is not an .Rmd file. If it is a project containing Python code, a .py file containing all of the Python code. See here to learn how to transfer files to and from Scholar. STAT 19000 Topics The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration. Language Project # Name Topics Python 1 Intro to Python: part I declaring variables, printing, running cells, exporting to different formats, etc. Python 2 Intro to Python: part II lists, tuples, if statements, opening files, pandas, matplotlib, etc. Python 3 Intro to Python: part III sets, dicts, pandas, matplotlib, lists, tuples, etc. Python 4 Control flow in Python if statements, for loops, dicts, lists, matplotlib, etc. Python 5 Scientific computing/Data wrangling: part I timing, I/O, indexing in pandas, pandas functions, matplotlib, etc. Python 6 Functions: part I writing functions, docstrings, pandas, etc. Python 7 Functions: part II writing functions, docstrings, pandas, etc. Python 8 Scientific computing/Data wrangling: part II building a recommendation system Python 9 Scientific computing/Data wrangling: part III building a recommendation system, continued... Python 10 Packages Learn more about Python packaging, importing, etc. Python 11 Python Classes: part I writing classes in Python to build a game, dunder methods, attributes, methods, etc. Python 12 Python Classes: part II writing classes in Python to build a game, dunder methods, attributes, methods, etc., continued... Python 13 Data wrangling &amp; matplotlib: part I more pandas, more matplotlib, wrangling with increased difficulty, etc. Python 14 Data wrangling &amp; matplotlib: part II more pandas, more matplotlib, wrangling with increased difficulty, etc. Project 1 Motivation: In this course we require the majority of project submissions to include a compiled PDF, a .Rmd file based off of our template, and a code file (a .R file if the project is in R, a .py file if the project is in Python). Although RStudio makes it easy to work with both Python and R, there are occasions where working out a Python problem in a Jupyter Notebook could be convenient. For that reason, we will introduce Jupyter Notebook in this project. Context: This is the first in a series of projects that will introduce Python and its tooling to students. Scope: jupyter notebooks, rstudio, python Learning objectives: Use Jupyter Notebook to run Python code and create Markdown text. Use RStudio to run Python code and compile your final PDF. Gain exposure to Python control flow and reading external data. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/open_food_facts/openfoodfacts.tsv Questions 1. Navigate to https://notebook.scholar.rcac.purdue.edu/ and sign in with your Purdue credentials (without BoilerKey). This is an instance of Jupyter Notebook. The main screen will show a series of files and folders that are in your $HOME directory. Create a new notebook by clicking on New &gt; f2020-s2021. Change the name of your notebook to &quot;LASTNAME_FIRSTNAME_project01&quot; where &quot;LASTNAME&quot; is your family name, and &quot;FIRSTNAME&quot; is your given name. Try to export your notebook (using the File dropdown menu, choosing the option Download as), what format options (for example, .pdf) are available to you? Important note: f2020-s2021 is the name of our course notebook kernel. A notebook kernel is an engine that runs code in a notebook. ipython kernels run Python code. f2020-s2021 is an ipython kernel that we've created for our course Python environment, which contains a variety of compatible, pre-installed packages for you to use. When you select f2020-s2021 as your kernel, all of the packages in our course environment are automatically made available to you. Click here for video If the kernel f2020-s2021 does not appear in Jupyter Notebooks, you can make it appear as follows: Login to https://rstudio.scholar.rcac.purdue.edu Click on Tools &gt; Shell... (in the menu) In the shell (terminal looking thing that should say something like: bash-4.2$), type the following followed by Enter/Return: /class/datamine/apps/runme Then click on Session &gt; Restart R (in the menu) You should now have access to the course kernel named f2020-s2021 in https://notebook.scholar.rcac.purdue.edu Item(s) to submit: A list of export format options. Solution .ipynb .py .html .md .rst .tex .pdf 2. Each &quot;box&quot; in a Jupyter Notebook is called a cell. There are two primary types of cells: code, and markdown. By default, a cell will be a code cell. Place the following Python code inside the first cell, and run the cell. What is the output? from thedatamine import hello_datamine hello_datamine() Hint: You can run the code in the currently selected cell by using the GUI (the buttons), as well as by pressing Ctrl+Return/Enter. Item(s) to submit: Output from running the provided code. Solution &quot;Hello student! Welcome to The Data Mine!&quot; 3. Jupyter Notebooks allow you to easily pull up documentation, similar to ?function in R. To do so, use the help function, like this: help(my_function). What is the output from running the help function on hello_datamine? Can you modify the code from question (2) to print a customized message? Create a new markdown cell and explain what you did to the code from question (2) to make the message customized. Important note: Some Jupyter-only methods to do this are: Click on the function of interest and type Shift+Tab or Shift+Tab+Tab. Run function?, for example, print?. Important note: You can also see the source code of a function in a Jupyter Notebook by typing function??, for example, print??. Item(s) to submit: Output from running the help function on hello_datamine. Modified code from question (2) that prints a customized message. Solution help(hello_datamine) Help on function hello_datamine in module thedatamine.core: hello_datamine(name: str = 'student') -&gt; None Prints a hello message to a Data Mine student. Args: str (name, optional): The name of a student. Defaults to &#39;student&#39;. hello_datamine(&quot;Kevin&quot;) 4. At this point in time, you've now got the basics of running Python code in Jupyter Notebooks. There is really not a whole lot more to it. For this class, however, we will continue to create RMarkdown documents in addition to the compiled PDFs. You are welcome to use Jupyter Notebooks for personal projects or for testing things out, however, we will still require an RMarkdown file (.Rmd), PDF (generated from the RMarkdown file), and .py file (containing your python code). For example, please move your solutions from Questions 1, 2, 3 from Jupyter Notebooks over to RMarkdown (we discuss RMarkdown below). Let's learn how to run Python code chunks in RMarkdown. Sign in to https://rstudio.scholar.rcac.purdue.edu (with BoilerKey). Projects in The Data Mine should all be submitted using our template found here or on Scholar (/class/datamine/apps/templates/project_template.Rmd). Open the project template and save it into your home directory, in a new RMarkdown file named project01.Rmd. Prior to running any Python code, run datamine_py() in the R console, just like you did at the beginning of every project from the first semester. Code chunks are parts of the RMarkdown file that contains code. You can identify what type of code a code chunk contains by looking at the engine in the curly braces &quot;{&quot; and &quot;}&quot;. As you can see, it is possible to mix and match different languages just by changing the engine. Move the solutions for questions 1-3 to your project01.Rmd. Make sure to place all Python code in python code chunks. Run the python code chunks to ensure you get the same results as you got when running the Python code in a Jupyter Notebook. Important note: Make sure to run datamine_py() in the R console prior to attempting to run any Python code. Hint: The end result of the project01.Rmd should look similar to this. Click here for video Click here for video Item(s) to submit: project01.Rmd with the solutions from questions 1-3 (including any Python code in python code chunks). Solution Done. 5. It is not a Data Mine project without data! Here are some examples of reading in data line by line using the csv package. How many columns are in the following dataset: /class/datamine/data/open_food_facts/openfoodfacts.tsv? Print the first row, the number of columns, and then exit the loop after the first iteration using the break keyword. Hint: You can get the number of elements in a list by using the len method. For example: len(my_list). Hint: You can use the break keyword to exit a loop. As soon as break is executed, the loop is exited and the code immediately following the loop is run. for my_row in my_csv_reader: print(my_row) break print(&quot;Exited loop as soon as &#39;break&#39; was run.&quot;) Hint: '\\t' represents a tab in Python. Click here for video Important note: If you get a Dtype warning, feel free to just ignore it. Relevant topics: for loops, break, print Item(s) to submit: Python code used to solve this problem. The first row printed, and the number of columns printed. Solution import csv with open(&#39;/class/datamine/data/open_food_facts/openfoodfacts.tsv&#39;) as my_file: my_reader = csv.reader(my_file, delimiter=&#39;\\t&#39;) for row in my_reader: print(row) print(len(row)) break # prematurely leave the loop 6 (optional). Unlike in R, where many of the tools you need are built-in (read.csv, data.frames, etc.), in Python, you will need to rely on packages like numpy and pandas to do the bulk of your data science work. In R it would be really easy to find the mean of the 151st column, caffeine_100g: myDF &lt;- read.csv(&quot;/class/datamine/data/open_food_facts/openfoodfacts.tsv&quot;, sep=&quot;\\t&quot;, quote=&quot;&quot;) mean(myDF$caffeine_100g, na.rm=T) # 2.075503 If you were to try to modify our loop from question (5) to do the same thing, you will run into a myriad of issues, just to try and get the mean of a column. Luckily, it is easy to do using pandas: import pandas as pd myDF = pd.read_csv(&quot;/class/datamine/data/open_food_facts/openfoodfacts.tsv&quot;, sep=&quot;\\t&quot;) myDF[&quot;caffeine_100g&quot;].mean() # 2.0755028571428573 Take a look at some of the methods you can perform using pandas here. Perform an interesting calculation in R, and replicate your work using pandas. Which did you prefer, Python or R? Click here for video Item(s) to submit: R code used to solve the problem. Python code used to solve the problem. Solution # could be anything. Project 2 Motivation: In Python it is very important to understand some of the data types in a little bit more depth than you would in R. Many of the data types in Python will seem very familiar. A character in R is similar to a str in Python. An integer in R is an int in Python. A numeric in R is similar to a float in Python. A logical in R is similar to a bool in Python. In addition to all of that, there are some very popular classes that packages like numpy and pandas introduces. On the other hand, there are some data types in Python like tuples, lists, sets, and dicts that diverge from R a little bit more. It is integral to understand some basic concepts before jumping too far into everything. Context: This is the second project introducing some basic data types, and demonstrating some familiar control flow concepts, all while digging right into a dataset. Scope: tuples, lists, if statements, opening files Learning objectives: List the differences between lists &amp; tuples and when to use each. Gain familiarity with string methods, list methods, and tuple methods. Demonstrate the ability to read and write data of various formats using various packages. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/craigslist/vehicles.csv Questions 1. Read in the dataset /class/datamine/data/craigslist/vehicles.csv into a pandas DataFrame called myDF. pandas is an integral tool for various data science tasks in Python. You can read a quick intro here. We will be slowly introducing bits and pieces of this package throughout the semester. Similarly, we will try to introduce byte-sized (ha!) portions of plotting packages to slowly build up your skills. How big is the dataset (in Mb or Gb)? Click here for video Important note: If you didn't do optional question 6 in project 1, we would recommend taking a look. Hint: Remember to check out a question's relevant topics. We try very hard to link you to content and examples that will get you up and running as quickly as possible. Relevant topics: pandas read_csv, get filesize in Python Item(s) to submit: Python code used to solve the problem. Solution import pandas as pd from pathlib import Path myDF = pd.read_csv(&quot;/class/datamine/data/craigslist/vehicles.csv&quot;) p = Path(&quot;/class/datamine/data/craigslist/vehicles.csv&quot;) size_in_mb = p.stat().st_size/1000 print(size_in_mb) 2. In question (1) we read in our data into a pandas DataFrame. Use one of the pandas DataFrame attributes to get the number of columns and rows of our dataset. How many columns and rows are there? Use f-strings to print a message, for example: There are 123 columns in the DataFrame! There are 321 rows in the DataFrame! In project 1, we learned how to read a csv file in, line-by-line, and print values. Use the csv package to print just the first row, which should contain the names of the columns, OR instead of using the csv package, use one of the pandas attributes from myDF (to print the column names). Click here for video Relevant topics: csv read csv, pandas DataFrame, f-strings, break Item(s) to submit: The output from printing the f-strings. Python code used to solve the problem. Solution print(f&#39;There are {myDF.shape[1]} columns in the DataFrame!&#39;) print(f&#39;There are {myDF.shape[0]} columns in the DataFrame!&#39;) import csv our_row = [] with open(&quot;/class/datamine/data/craigslist/vehicles.csv&quot;) as my_file: my_reader = csv.reader(my_file) for row in my_reader: our_row = row break print(our_row) print(myDF.columns.to_list()) 3. Use the csv or pandas package to get a list called our_columns that contains the column names. Add a string, &quot;extra&quot;, to the end of our_columns. Print the second value in the list. Without using a loop, print the 1st, 3rd, 5th, etc. elements of the list. Print the last four elements of the list ( &quot;state&quot;, &quot;lat&quot;, &quot;long&quot;, and &quot;extra&quot;) by accessing their negative index. &quot;extra&quot; doesn't belong in our list, you can easily remove this value from our list by doing the following: our_columns.pop(25) # or even this, as pop removes the last value by default our_columns.pop() BUT the problem with this solution is that you must know the index of the value you'd like to remove, and sometimes you do not know the index of the value. Instead, please show how to use a list method to remove &quot;extra&quot; by value rather than by index. Click here for video Relevant topics: csv read csv, break, append, indexing Item(s) to submit: Python code used to solve the problem. The output from running your code. Solution our_columns = [] with open(&quot;/class/datamine/data/craigslist/vehicles.csv&quot;) as my_file: my_reader = csv.reader(my_file) for row in my_reader: our_columns = row break our_columns.append(&quot;extra&quot;) print(our_columns[1]) print(our_columns[::2]) print(our_columns[-4:]) print(our_columns.remove(&quot;extra&quot;)) 4. matplotlib is one of the primary plotting packages in Python. You are provided with the following code: my_values = tuple(myDF.loc[:, &#39;odometer&#39;].dropna().to_list()) The result is a tuple containing the odometer readings from all of the vehicles in our dataset. Create a lineplot of the odometer readings. Well, that plot doesn't seem too informative. Let's first sort the values in our tuple: my_values.sort() What happened? A tuple is immutable. What this means is that once the contents of a tuple are declared they cannot be modified. For example: # This will fail because tuples are immutable my_values[0] = 100 You can read a good article about this here. In addition, here is a great post that gives you an idea when using a tuple might be a good idea. Okay, so let's go back to our problem. We know that lists are mutable (and therefore sortable), so convert my_values to a list and then sort, and re-plot. It looks like there are some (potential) outliers that are making our plot look a little wonky. For the sake of seeing how the plot would look, use negative indexing to plot the sorted values minus the last 50 values (the 50 highest values). New new plot may not look that different, that is okay. Hint: To prevent plotting values on the same plot, close your plot with the close method, for example: import matplotlib.pyplot as plt my_values = [1,2,3,4,5] plt.plot(my_values) plt.show() plt.close() Relevant topics: list methods, indexing, matplotlib lineplot Item(s) to submit: Python code used to solve the problem. The output from running your code. Solution import matplotlib.pyplot as plt my_values = tuple(myDF.loc[:, &#39;odometer&#39;].dropna().to_list()) plt.plot(my_values) plt.close() my_values = list(my_values) my_values.sort() plt.plot(my_values) plt.show() plt.close() plt.plot(my_values[:-50]) plt.show() plt.close() 5. We've covered a lot in this project! Use what you've learned so far to do one (or more) of the following tasks: - Create a cool graphic using matplotlib, that summarizes some data from our dataset. - Use pandas and your investigative skills to sift through the dataset and glean an interesting factoid. - Create some commented coding examples that highlight the differences between lists and tuples. Include at least 3 examples. Relevant topics: pandas, indexing, matplotlib Item(s) to submit: Python code used to solve the problem. The output from running your code. Solution # Could be anything. Project 3 Motivation: A dictionary (referred to as a dict) is one of the most useful data structures in Python. You can think about them as a data structure containing key: value pairs. Under the hood, a dict is essentially a data structure called a hash table. Hash tables are a data structure with a useful set of properties. The time needed for searching, inserting, or removing a piece of data has a constant average lookup time, meaning that no matter how big your hash table grows to be, inserting, searching, or deleting a piece of data will usually take about the same amount of time. (The worst case time increases linearly.) Dictionaries (dict) are used a lot, so it is worthwhile to understand them. Although not used quite as often, another important data type called a set, is also worthwhile learning about. Dictionaries, often referred to as dicts, are really powerful. There are two primary ways to &quot;get&quot; information from a dict. One is to use the get method, the other is to use square brackets and strings. Test out the following to understand the differences between the two: my_dict = {&quot;fruits&quot;: [&quot;apple&quot;, &quot;orange&quot;, &quot;pear&quot;], &quot;person&quot;: &quot;John&quot;, &quot;vegetables&quot;: [&quot;carrots&quot;, &quot;peas&quot;]} # If &quot;person&quot; is indeed a key, they will function the same way my_dict[&quot;person&quot;] my_dict.get(&quot;person&quot;) # If the key does not exist, like below, they will not # function the same way. my_dict.get(&quot;height&quot;) # Returns None when key doesn&#39;t exist print(my_dict.get(&quot;height&quot;)) # By printing, we can see None in this case my_dict[&quot;height&quot;] # Throws a KeyError exception because the key, &quot;height&quot; doesn&#39;t exist Context: In our third project, we introduce some basic data types, and we demonstrate some familiar control flow concepts, all while digging right into a dataset. Throughout the course, we will slowly introduce concepts from pandas, and popular plotting packages. Scope: dicts, sets, if/else statements, opening files, tuples, lists Learning objectives: Explain what is a dict is and why it is useful. Understand how a set works and when it could be useful. List the differences between lists &amp; tuples and when to use each. Gain familiarity with string methods, list methods, and tuple methods. Gain familiarity with dict methods. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/craigslist/vehicles.csv Questions 1. In project 2 we learned how to read in data using pandas. Read in the (/class/datamine/data/craigslist/vehicles.csv) dataset into a DataFrame called myDF using pandas. In R we can get a sneak peek at the data by doing something like: head(myDF) # where myDF is a data.frame There is a very similar (and aptly named method) in pandas that allows us to do the exact same thing with a pandas DataFrame. Get the head of myDF, and take a moment to consider how much time it would take to get this information if we didn't have this nice head method. Click here for video Relevant topics: pandas read_csv, head Item(s) to submit: Python code used to solve the problem. The head of the dataset. Solution import pandas as pd myDF = pd.read_csv(&quot;/class/datamine/data/craigslist/vehicles.csv&quot;) myDF.head() 2. Dictionaries, often referred to as dicts, are really powerful. There are two primary ways to &quot;get&quot; information from a dict. One is to use the get method, the other is to use square brackets and strings. Test out the following to understand the differences between the two: my_dict = {&quot;fruits&quot;: [&quot;apple&quot;, &quot;orange&quot;, &quot;pear&quot;], &quot;person&quot;: &quot;John&quot;, &quot;vegetables&quot;: [&quot;carrots&quot;, &quot;peas&quot;]} # If &quot;person&quot; is indeed a key, they will function the same way my_dict[&quot;person&quot;] my_dict.get(&quot;person&quot;) # If the key does not exist, like below, they will not # function the same way. my_dict.get(&quot;height&quot;) # Returns None when key doesn&#39;t exist print(my_dict.get(&quot;height&quot;)) # By printing, we can see None in this case my_dict[&quot;height&quot;] # Throws a KeyError exception because the key, &quot;height&quot; doesn&#39;t exist Look at the dataset. Create a dict called my_dict that contains key:value pairs where the keys are years, and the values are a single int representing the number of vehicles from that year on craigslist. Use the year column, a loop, and a dict to accomplish this. Print the dictionary. You can use the following code to extract the year column as a list. In the next project we will learn how to loop over pandas DataFrames. Hint: If you get a KeyError, remember, you must declare each key value pair just like any other variable. Use the following code to initialize each year key to the value 0. myyears = myDF[&#39;year&#39;].dropna().to_list() # get a list containing each unique year unique_years = list(set(myyears)) # for each year (key), initialize the value (value) to 0 my_dict = {} for year in unique_years: my_dict[year] = 0 Hint: Here are some of the results you should get: print(my_dict[1912]) # 5 print(my_dict[1982]) # 185 print(my_dict[2014]) # 31703 Note: There is a special kind of dict called a defaultdict, that allows you to give default values to a dict, giving you the ability to &quot;skip&quot; initialization. We will show you this when we release the solutions to this project! It is not required, but it is interesting to know about! Click here for video Relevant topics: dicts Item(s) to submit: Python code used to solve the problem. my_dict printed. Solution years = myDF[&#39;year&#39;].dropna().to_list() unique_years = list(set(years)) my_dict = {} for year in unique_years: my_dict[year] = 0 for year in years: my_dict[year] += 1 print(my_dict) OR from collections import defaultdict years = myDF[&#39;year&#39;].dropna().to_list() my_dict = defaultdict(int) for year in years: my_dict[year] += 1 print(my_dict) 3. After completing question (2) you can easily access the number of vehicles from a given year. For example, to get the number of vehicles on craigslist from 1912, just run: my_dict[1912] # or my_dict.get(1912) A dict stores its data in key:value pairs. Identify a &quot;key&quot; from my_dict, as well as the associated &quot;value&quot;. As you can imagine, having data in this format can be very beneficial. One benefit is the ability to easily create a graphic using matplotlib. Use matplotlib to create a bar graph with the year on the x-axis, and the number of vehicles from that year on the y-axis. Important note: If when you end up seeing something like &lt;BarContainer object of X artists&gt;, you should probably end the code chunk with plt.show() instead. What is happening is Python is trying to print the plot object. That text is the result. To instead display the plot you need to call plt.show(). Hint: To use matplotlib, first import it: import matplotlib.pyplot as plt # now you can use it, for example plt.plot([1,2,3,1]) plt.show() plt.close() Hint: The keys method and values method from dict could be useful here. Click here for video Relevant topics: dicts, matplotlib, barplot Item(s) to submit: Python code used to solve the problem. The resulting plot. A sentence giving an example of a &quot;key&quot; and associated &quot;value&quot; from my_dict (e.g., a sentence explaining the 1912 example above). Solution In my_dict a key is 1912, and the associated value is 5. import matplotlib.pyplot as plt plt.bar(my_dict.keys(), my_dict.values()) 4. In the hint in question (2), we used a set to quickly get a list of unique years in a list. Some other common uses of a set are when you want to get a list of values that are in one list but not another, or get a list of values that are present in both lists. Examine the following code. You'll notice that we are looping over many values. Replace the code for each of the three examples below with code that uses no loops whatsoever. listA = [1, 2, 3, 4, 5, 6, 12, 12] listB = [2, 1, 7, 7, 7, 2, 8, 9, 10, 11, 12, 13] # 1. values in list A but not list B # values in list A but not list B onlyA = [] for valA in listA: if valA not in listB and valA not in onlyA: onlyA.append(valA) print(onlyA) # [3, 4, 5, 6] # 2. values in listB but not list A onlyB = [] for valB in listB: if valB not in listA and valB not in onlyB: onlyB.append(valB) print(onlyB) # [7, 8, 9, 10, 11, 13] # 3. values in both lists # values in both lists in_both_lists = [] for valA in listA: if valA in listB and valA not in in_both_lists: in_both_lists.append(valA) print(in_both_lists) # [1,2,12] Hint: You should use a set. Note: In addition to being easier to read, using a set is much faster than loops! Note: A set is a group of values that are unordered, unchangeable, and no duplicate values are allowed. While they aren't used a lot, they can be useful for a few common tasks like: removing duplicate values efficiently, efficiently finding values in one group of values that are not in another group of values, etc. Relevant topics: sets Item(s) to submit: Python code used to solve the problem. The output from running the code. Solution listA = [1, 2, 3, 4, 5, 6, 12, 12] listB = [2, 1, 7, 7, 7, 2, 8, 9, 10, 11, 12, 13] onlyA = list(set(listA) - set(listB)) print(onlyA) onlyB = list(set(listB) - set(listA)) print(onlyB) in_both_lists = list(set.intersection(set(listA), set(listB))) print(in_both_lists) 5. The value of a dictionary does not have to be a single value (like we've shown so far). It can be anything. Observe that there is latitude and longitude data for each row in our DataFrame (lat and long, respectively). Wouldn't it be useful to be able to quickly &quot;get&quot; pairs of latitude and longitude data for a given state? First, run the following code to get a list of tuples where the first value is the state, the second value is the lat, and the third value is the long. states_list = list(myDF.loc[:, [&quot;state&quot;, &quot;lat&quot;, &quot;long&quot;]].dropna().to_records(index=False)) states_list[0:3] # [(&#39;az&#39;, 34.4554, -114.269), (&#39;or&#39;, 46.1837, -123.824), (&#39;sc&#39;, 34.9352, -81.9654)] # to get the first tuple states_list[0] # (&#39;az&#39;, 34.4554, -114.269) # to get the first value in the first tuple states_list[0][0] # az # to get the second tuple states_list[1] # (&#39;or&#39;, 46.1837, -123.824) # to get the first value in the second tuple states_list[1][0] # or Hint: If you have an issue where you cannot append values to a specific key, make sure to first initialize the specific key to an empty list so the append method is available to use. Now, organize the latitude and longitude data in a dictionary called geoDict such that each state from the state column is a key, and the respective value is a list of tuples, where the first value in each tuple is the latitude (lat) and the second value is the longitude (long). For example, the first 2 (lat,long) pairs in Indiana (&quot;in&quot;) are: geoDict.get(&quot;in&quot;)[0:2] # [(39.0295, -86.8675), (38.8585, -86.4806)] len(geoDict.get(&quot;in&quot;)) # 5687 Click here for video Now that you can easily access latitude and longitude pairs for a given state, run the following code to plot the points for Texas (the state value is &quot;tx&quot;). Include the the graphic produced below in your solution, but feel free to experiment with other states. NOTE: You do NOT need to include this portion of Question 5 in your Markdown .Rmd file. We cannot get this portion to build in Markdown, but please do include it in your Python .py file. from shapely.geometry import Point import geopandas as gpd from geopandas import GeoDataFrame usa = gpd.read_file(&#39;/class/datamine/data/craigslist/cb_2018_us_state_20m.shp&#39;) usa.crs = {&#39;init&#39;: &#39;epsg:4269&#39;} pts = [Point(y,x) for x, y in geoDict.get(&quot;tx&quot;)] gdf = gpd.GeoDataFrame(geometry=pts, crs = 4269) fig, gax = plt.subplots(1, figsize=(10,10)) base = usa[usa[&#39;NAME&#39;].isin([&#39;Hawaii&#39;, &#39;Alaska&#39;, &#39;Puerto Rico&#39;]) == False].plot(ax=gax, color=&#39;white&#39;, edgecolor=&#39;black&#39;) gdf.plot(ax=base, color=&#39;darkred&#39;, marker=&quot;*&quot;, markersize=10) plt.show() plt.close() # to save to jpg: plt.savefig(&#39;q5.jpg&#39;) Relevant topics: dicts, lists and tuples Item(s) to submit: Python code used to solve the problem. Graphic file (q5.jpg) produced for the given state. Solution geoDict = dict() for val in states_list: geoDict[val[0]] = [] for val in states_list: geoDict[val[0]].append((val[1],val[2])) geoDict.get(&quot;in&quot;)[0:2] len(geoDict.get(&quot;in&quot;)) from shapely.geometry import Point import geopandas as gpd from geopandas import GeoDataFrame usa = gpd.read_file(&#39;/class/datamine/data/craigslist/cb_2018_us_state_20m.shp&#39;) usa.crs = {&#39;init&#39;: &#39;epsg:4269&#39;} pts = [Point(y,x) for x, y in geoDict.get(&quot;tx&quot;)] gdf = gpd.GeoDataFrame(geometry=pts, crs = 4269) fig, gax = plt.subplots(1, figsize=(10,10)) base = usa[usa[&#39;NAME&#39;].isin([&#39;Hawaii&#39;, &#39;Alaska&#39;, &#39;Puerto Rico&#39;]) == False].plot(ax=gax, color=&#39;white&#39;, edgecolor=&#39;black&#39;) gdf.plot(ax=base, color=&#39;darkred&#39;, marker=&quot;*&quot;, markersize=10) plt.show() # to save to jpg: plt.savefig(&#39;q5.jpg&#39;) 6. Use your new skills to extract some sort of information from our dataset and create a graphic. This can be as simple or complicated as you are comfortable with! Relevant topics: dicts, lists and tuples Item(s) to submit: Python code used to solve the problem. The graphic produced using the code. Solution Could be anything. Project 4 Motivation: We've now been introduced to a variety of core Python data structures. Along the way we've touched on a bit of pandas, matplotlib, and have utilized some control flow features like for loops and if statements. We will continue to touch on pandas and matplotlib, but we will take a deeper dive in this project and learn more about control flow, all while digging into the data! Context: We just finished a project where we were able to see the power of dictionaries and sets. In this project we will take a step back and make sure we are able to really grasp control flow (if/else statements, loops, etc.) in Python. Scope: python, dicts, lists, if/else statements, for loops Learning objectives: List the differences between lists &amp; tuples and when to use each. Explain what is a dict and why it is useful. Demonstrate a working knowledge of control flow in python: if/else statements, while loops, for loops, etc. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/craigslist/vehicles.csv Questions 1. Unlike in R, where traditional loops are rare and typically accomplished via one of the apply functions, in Python, loops are extremely common and important to understand. In Python, any iterator can be looped over. Some common iterators are: tuples, lists, dicts, sets, pandas Series, and pandas DataFrames. In the previous project we had some examples of looping over lists, let's learn how to loop over pandas Series and Dataframes! Load up our dataset /class/datamine/data/craigslist/vehicles.csv into a DataFrame called myDF. In project (3), we organized the latitude and longitude data in a dictionary called geoDict such that each state from the state column is a key, and the respective value is a list of tuples, where the first value in each tuple is the latitude (lat) and the second value is the longitude (long). Repeat this question, but do not use lists, instead use pandas to accomplish this. Hint: The data frame has 435,849 rows, and it takes forever to accomplish this with pandas. We just want you to do this one time, to see how slow this is. Try it first with only 10 rows, and then with 100 rows, and once you are sure it is working, try it with (say) 20,000 rows. You do not need to do this with the entire data frame. It takes too long! Click here for video Here is a video about the new feature to reset your RStudio session if you make a big mistake or if your session is very slow Relevant topics: DataFrame.iterrows(), Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution import pandas as pd myDF = pd.read_csv(&quot;/class/datamine/data/craigslist/vehicles.csv&quot;) geoDict = dict() for my_index, my_row in myDF.iterrows(): geoDict[my_row[&quot;state&quot;]] = [] for my_index, my_row in myDF.iterrows(): geoDict[my_row[&quot;state&quot;]].append((my_row[&quot;lat&quot;], my_row[&quot;long&quot;])) geoDict.get(&quot;in&quot;)[0:2] len(geoDict.get(&quot;in&quot;)) 2. Wow! The solution to question (1) was slow. In general, you'll want to avoid looping over large DataFrames. Here is a pretty good explanation of why, as well as a good system on what to try when computing something. In this case, we could have used indexing to get latitude and longitude values for each state, and would have no need to build this dict. The method we learned in Project 3, Question 5 is faster and easier! Just in case you did not solve Project 3, Question 5, here is a fast way to build geoDict: import pandas as pd myDF = pd.read_csv(&quot;/class/datamine/data/craigslist/vehicles.csv&quot;) states_list = list(myDF.loc[:, [&quot;state&quot;, &quot;lat&quot;, &quot;long&quot;]].dropna().to_records(index=False)) geoDict = {} for mytriple in states_list: geoDict[mytriple[0]] = [] for mytriple in states_list: geoDict[mytriple[0]].append( (mytriple[1],mytriple[2]) ) Now we will practice iterating over a dictionary, list, and tuple, all at once! Loop through geoDict and use f-strings to print the state abbreviation. Print the first latitude and longitude pair, as well as every 5000th latitude and longitude pair for each state. Round values to the hundreths place. For example, if the state was &quot;pu&quot;, and it had 12000 latitude and longitude pairs, we would print the following: pu: Lat: 41.41, Long: 41.41 Lat: 22.21, Long: 21.21 Lat: 11.11, Long: 10.22 In the above example, Lat: 41.41, Long: 41.41 would be the 0th pair, Lat: 22.21, Long: 21.21 would be the 5000th pair, and Lat: 11.11, Long: 10.22 would be the 10000th pair. Make sure to use f-strings to round the latitude and longitude values to two decimal places. There are several ways to solve this question. You can use whatever method is easiest for you, but please be sure (as always) to add comments to explain your method of solution. Click here for video Hint: Enumerate is a useful function that adds an index to our loop. Hint: Using an if statement and the modulo operator could be useful. Note: Whenever we have a loop within another loop, the &quot;inner&quot; loop is called a &quot;nested&quot; loop, as it is &quot;nested&quot; inside of the other. Relevant topics: dicts, modulus operator, f-strings, if/else, for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution for key, value in geoDict.items(): print(f&#39;{key}:&#39;) for idx, triplet in enumerate(value): if (idx % 5000 == 0): print(f&#39;Lat: {triplet[0]:.2f}, Long: {triplet[1]:.2f}&#39;) 3. We are curious about how the year of the car (year) effects the price (price). In R, we could get the median price by year easily, using tapply: tapply(myDF$price, myDF$year, median, na.rm=T) Using pandas, we would do this: res = myDF.groupby([&#39;year&#39;], dropna=True).median() These are very convenient functions that do a lot of work for you. If we were to take a look at the median price of cars by year, it would look like: import matplotlib.pyplot as plt res = myDF.groupby([&#39;year&#39;], dropna=True).median()[&quot;price&quot;] plt.bar(res.index, res.values) Using the content of the variable my_list provided in the code below, calculate the median car price per year without using the median function and without using a sort function. Use only dictionaries, for loops and if statements. Replicate the plot generated by running the code above (you can use the plot to make sure it looks right). my_list = list(myDF.loc[:, [&quot;year&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) Click here for video Hint: If you do not want to write your own median function to find the median, then it is OK to just use the getMid function found here or to use a median function from elsewhere on the web. Just be sure to cite your source, if you do use a median function that someone else provides or that you use from the internet. There are many small variations on median functions, especially when it comes to (for instance) lists with even length. Hint: It is also OK to use: import statistics and the function statistics.median Relevant topics: dicts, for loops, if/else Item(s) to submit: Python code used to solve the problem. Output from running your code. The barplot. Solution from collections import defaultdict import matplotlib.pyplot as plt my_list = list(myDF.loc[:, [&quot;year&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) my_dict = defaultdict(list) for (year, price) in my_list: my_dict[year].append(float(price)) my_years, my_prices = [],[] for year, prices in sorted(my_dict.items()): if len(prices) % 2 == 0: lower_mid = int(len(prices)/2 - 1) upper_mid = int(len(prices)/2) prices_sorted = sorted(prices) median_price = (prices_sorted[lower_mid] + prices_sorted[upper_mid]) / 2 else: median_price = sorted(prices)[int(len(prices)/2)] print(f&#39;{year}: {median_price}&#39;) my_years.append(year) my_prices.append(median_price) plt.bar(my_years, my_prices) plt.show() 4. Now calculate the mean price by year(still not using pandas code), and create a barplot with the price on the y-axis and year on the x-axis. Whoa! Something is odd here. Explain what is happening. Modify your code to use an if statement to &quot;weed out&quot; the likely erroneous value. Re-plot your values. Click here for video (same as in Question 3) Click here for another video Click here for one more video Hint: It is also OK to use a built-in mean function, for instace: import statistics and the function statistics.mean Relevant topics: sorted, for loops, if/else, list methods Item(s) to submit: Python code used to solve the problem. Output from running your code. The barplot. Solution from collections import defaultdict my_dict = defaultdict(list) for (year, price) in my_list: my_dict[year].append(price) my_years, my_prices = [],[] for year, prices in sorted(my_dict.items()): print(f&#39;{year}: {sum(prices)/len(prices)}&#39;) my_years.append(year) my_prices.append(sum(prices)/len(prices)) plt.bar(my_years, my_prices) plt.show() from collections import defaultdict my_dict = defaultdict(list) for (year, price) in my_list: if price &gt; 1_000_000: continue my_dict[year].append(price) my_years, my_prices = [],[] for year, prices in sorted(my_dict.items()): avg_price = sum(prices)/len(prices) print(f&#39;{year}: {avg_price}&#39;) my_years.append(year) my_prices.append(avg_price) plt.bar(my_years, my_prices) plt.show() 5. List comprehensions are a neat feature of Python that allows for a more concise syntax for smaller loops. While at first they may seem difficult and more confusing, eventually they grow on you. For example, say you wanted to capitalize every state in a list full of states: my_states = myDF[&#39;state&#39;].to_list() my_states = [state.upper() for state in my_states] Or, maybe you wanted to find the average price of cars in &quot;excellent&quot; condition (without pandas): my_list = list(myDF.loc[:, [&quot;condition&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) my_list = [price for (condition, price) in my_list if condition == &quot;excellent&quot;] sum(my_list)/len(my_list) Do the following using list comprehensions, and the provided code: my_list = list(myDF.loc[:, [&quot;state&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) Calculate the average price of vehicles from Indiana (in). Calculate the average price of vehicles from Indiana (in), Michigan (mi), and Illinois (il) combined. my_list = list(myDF.loc[:, [&quot;manufacturer&quot;, &quot;year&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) Calculate the average price of a &quot;honda&quot; (manufacturer) that is 2010 or newer (year). Click here for video Relevant topics: sorted, for loops, if/else, list methods, sum, len, defaultdict, matplotlib Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution my_list = list(myDF.loc[:, [&quot;state&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) my_list = [price for (state,price) in my_list if state == &quot;in&quot;] print(sum(my_list)/len(my_list)) my_list = list(myDF.loc[:, [&quot;state&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) my_list = [price for (state,price) in my_list if state in (&#39;in&#39;, &#39;il&#39;, &#39;mi&#39;)] print(sum(my_list)/len(my_list)) my_list = list(myDF.loc[:, [&quot;manufacturer&quot;, &quot;year&quot;, &quot;price&quot;,]].dropna().to_records(index=False)) my_list = [p for (m, y, p) in my_list if m==&#39;honda&#39; and y &gt;= 2010] print(sum(my_list)/len(my_list)) 6. Let's use a package called spacy to try and parse phone numbers out of the description column. First, simply loop through and print the text and the label. What is the label of the majority of the phone numbers you can see? import spacy # get list of descriptions my_list = list(myDF.loc[:, [&quot;description&quot;,]].dropna().to_records(index=False)) my_list = [m[0] for m in my_list] # load the pre-built spacy model nlp = spacy.load(&quot;en_core_web_lg&quot;) # apply the model to a description doc = nlp(my_list[0]) # print the text and label of each &quot;entity&quot; for entity in doc.ents: print(entity.text, entity.label_) Use an if statement to filter out all entities that are not the label you see. Loop through again and see what our printed data looks like. There is still a lot of data there that we don't want to capture, right? Phone numbers in the US are usually 7 (5555555), 8 (555-5555), 10 (5555555555), 11 (15555555555), 12 (555-555-5555), or 14 (1-555-555-5555) digits. In addition to your first &quot;filter&quot;, add another &quot;filter&quot; that keeps only text where the text is one of those lengths. That is starting to look better, but there are still some erroneous values. Come up with another &quot;filter&quot;, and loop through our data again. Explain what your filter does and make sure that it does a better job on the first 10 documents than when we don't use your filter. Note: If you get an error when trying to knit that talks about &quot;unicode&quot; characters, this is caused by trying to print special characters (non-ascii). An easy fix is just to remove all non-ascii text. You can do this with the encode string method. For example: Instead of: for entity in doc.ents: print(entity.text, entity.label_) Do: for entity in doc.ents: print(entity.text.encode(&#39;ascii&#39;, errors=&#39;ignore&#39;), entity.label_) Click here for video Note: It can be fun to utilize machine learning and natural language processing, but that doesn't mean it is always the best solution! We could get rid of all of our filters and use regular expressions with much better results! We will demonstrate this in our solution. Relevant topics: for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. 1-2 sentences explaining what your filter does. Solution import spacy my_list = list(myDF.loc[:, [&quot;description&quot;,]].dropna().to_records(index=False)) my_list = [m[0] for m in my_list] # load the pre-built spacy model nlp = spacy.load(&quot;en_core_web_lg&quot;) for doc in my_list[:10]: d = nlp(doc) for entity in d.ents: print(entity.text.encode(&#39;ascii&#39;, errors=&#39;ignore&#39;), entity.label_) for doc in my_list[:10]: d = nlp(doc) for entity in d.ents: if entity.label_ == &quot;CARDINAL&quot;: print(entity.text.encode(&#39;ascii&#39;, errors=&#39;ignore&#39;), entity.label_) for doc in my_list[:10]: d = nlp(doc) for entity in d.ents: if entity.label_ == &quot;CARDINAL&quot; and len(entity.text) in [7, 8, 10, 11, 12, 14]: print(entity.text.encode(&#39;ascii&#39;, errors=&#39;ignore&#39;), entity.label_) for doc in my_list[:10]: d = nlp(doc) for entity in d.ents: if entity.label_ == &quot;CARDINAL&quot; and len(entity.text) in [7, 8, 10, 11, 12, 14]: print(entity.text.encode(&#39;ascii&#39;, errors=&#39;ignore&#39;), entity.label_) import re pattern = &#39;\\(?([0-9]{3})?\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})&#39; for doc in my_list[:10]: if matches := re.finditer(pattern, doc): for match in matches: print(match.group()) Project 5 Motivation: Up until this point we've utilized bits and pieces of the pandas library to perform various tasks. In this project we will formally introduce pandas and numpy, and utilize their capabilities to solve data-driven problems. Context: By now you'll have had some limited exposure to pandas. This is the first in a three project series that covers some of the main components of both the numpy and pandas libraries. We will take a two project intermission to learn about functions, and then continue. Scope: python, pandas, numpy, DataFrames, Series, ndarrays, indexing Learning objectives: Distinguish the differences between numpy, pandas, DataFrames, Series, and ndarrays. Use numpy, scipy, and pandas to solve a variety of data-driven problems. Demonstrate the ability to read and write data of various formats using various packages. View and access data inside DataFrames, Series, and ndarrays. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/stackoverflow/unprocessed/2018.csv /class/datamine/data/stackoverflow/unprocessed/2018.parquet /class/datamine/data/stackoverflow/unprocessed/2018.feather Questions 1. Take a look at the pandas docs. There are a lot of formats that pandas has the ability to read. The most popular formats in this course are: csv (with commas or some other separator), excel, json, or some database. CSV is very prevalent, but it was not designed to work well with large amounts of data. Newer formats like parquet and feather are designed from the ground up to be efficient, and take advantage of special processor instruction set called SIMD. The benefits of using these formats can be significant. Let's do some experiments! How much space do each of the following files take up on Scholar: 2018.csv, 2018.parquet, and 2018.feather? How much smaller (as a percentage) is the parquet file than the csv? How much smaller (as a percentage) is the feather file than the csv? Use f-strings to format the percentages. Time reading in the following files: 2018.csv, 2018.parquet, and 2018.feather. How much faster (as a percentage) is reading the parquet file than the csv? How much faster (as a percentage) is reading the feather file than the csv? Use f-strings to format the percentages. To time a piece of code, you can use the block-timer package: from block_timer.timer import Timer with Timer(title=&quot;Using dict to declare a dict&quot;) as t1: my_dict = dict() with Timer(title=&quot;Using {} to declare a dict&quot;) as t2: my_dict = {} # or if you need more fine-tuned values print(t1.elapsed) print(t2.elapsed) Read the 2018.csv file into a pandas DataFrame called my2018. Time writing the contents of my2018 to the following files: 2018.csv, 2018.parquet, and 2018.feather. Write the files to your scratch directory: /scratch/scholar/&lt;username&gt;, where &lt;username&gt; is your username. How much faster (as a percentage) is writing the parquet file than the csv? How much faster (as a percentage) is writing the feather file than the csv? Use f-strings to format the percentages. Click here for video Click here for video Relevant topics: pandas read_csv, pandas to_csv Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution import pandas as pd from block_timer.timer import Timer with Timer(title=&quot;csv&quot;) as csv: myDF = pd.read_csv(&quot;/class/datamine/data/stackoverflow/unprocessed/2018.csv&quot;) with Timer(title=&quot;parquet&quot;) as parquet: myDF = pd.read_parquet(&quot;/class/datamine/data/stackoverflow/unprocessed/2018.parquet&quot;) with Timer(title=&quot;feather&quot;) as feather: myDF = pd.read_feather(&quot;/class/datamine/data/stackoverflow/unprocessed/2018.feather&quot;) print(f&#39;Reading parquet is {csv.elapsed/parquet.elapsed-1.0:.2%} faster than csv.&#39;) print(f&#39;Reading feather is {csv.elapsed/feather.elapsed-1.0:.2%} faster than csv.&#39;) from block_timer.timer import Timer myDF = pd.read_csv(&quot;/class/datamine/data/stackoverflow/unprocessed/2018.csv&quot;) with Timer(title=&quot;csv&quot;) as csv: myDF.to_csv(&quot;/scratch/scholar/kamstut/2018.csv&quot;) with Timer(title=&quot;parquet&quot;) as parquet: myDF.to_parquet(&quot;/scratch/scholar/kamstut/2018.parquet&quot;) with Timer(title=&quot;feather&quot;) as feather: myDF.to_feather(&quot;/scratch/scholar/kamstut/2018.feather&quot;) print(f&#39;Writing parquet is {csv.elapsed/parquet.elapsed-1.0:.2%} faster than csv.&#39;) print(f&#39;Writing feather is {csv.elapsed/feather.elapsed-1.0:.2%} faster than csv.&#39;) from pathlib import Path csv = Path(&#39;/class/datamine/data/stackoverflow/unprocessed/2018.csv&#39;).stat().st_size parquet = Path(&#39;/class/datamine/data/stackoverflow/unprocessed/2018.parquet&#39;).stat().st_size feather = Path(&#39;/class/datamine/data/stackoverflow/unprocessed/2018.feather&#39;).stat().st_size print(f&#39;The parquet file is {csv/parquet:.2%} smaller than csv.&#39;) print(f&#39;Writing feather is {csv/feather:.2%} smaller than csv.&#39;) 2. A method is just a function associated with an object or class. For example, mean is just a method of the pandas DataFrame: # myDF is an object of class DataFrame # mean is a method of the DataFrame class myDF.mean() In pandas there are two main methods used for indexing: loc and iloc. Use the column Student and indexing in pandas to calculate what percentage of respondents are students and not students. Consider the respondent to be a student if the Student column is anything but &quot;No&quot;. Create a new DataFrame called not_students that is a subset of the original dataset without students. Click here for video Relevant topics: loc/iloc/indexing Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution not_students = myDF.loc[myDF.loc[:,&quot;Student&quot;]==&quot;No&quot;,:] not_student_percent = len(not_students.loc[:, &quot;Respondent&quot;])/len(myDF.loc[:, &quot;Respondent&quot;]) student_percent = 1-not_student_percent print(f&quot;Percent not students: {not_student_percent}&quot;) print(f&quot;Percent students: {student_percent}&quot;) 3. In pandas, if you were to isolate a single column using indexing, like this: myDF.loc[:, &quot;Student&quot;] The result would be a pandas Series. A Series is the 1-dimensional equivalent of a DataFrame. type(myDF.loc[:, &quot;Student&quot;]) # pandas.core.series.Series pandas and numpy make it very easy to convert between a Series, ndarray, and list. Here is a very useful graphic to highlight how to do this. Look at the DevType column in not_students. As you can see, a single value may contain a list of semi-colon-separated professions. Create a list with a unique group of all the possible professions. Consider each semi-colon-separated value a profession. How many professions are there? It looks like somehow the profession &quot;Student&quot; got in there even though we filtered by the Student column. Use not_students to get a subset of our data for which the respondents replied &quot;No&quot; to Student, yet put &quot;Student&quot; as one of many possible DevTypes. How many respondents are in that subset? Hint: If you have a column containing strings in pandas, and would like to use string methods on every string in the column, you can use .str. For example: # this would use the `strip` string method on each value in myColumn, and compare them to &#39;&#39; # `contains` is another useful string method... myDF.loc[myDF.loc[:, &quot;myColumn&quot;].str.strip() == &#39;&#39;, :] Hint: See here. Click here for video Relevant topics: list comprehensions, for loops, loc/iloc/indexing Item(s) to submit: Python code used to solve the problem. Output from running your code. The number of professions there are. The number of respondents that replied &quot;No&quot; to Student, yet put &quot;Student&quot; as the DevType. Solution professions = [p.split(&quot;;&quot;) for p in not_students.loc[:, &quot;DevType&quot;].dropna().tolist()] # unnest the nested lists professions = [p for li in professions for p in li] professions = list(set(professions)) print(professions) print(len(professions)) result = myDF.loc[(myDF.loc[:, &quot;Student&quot;]==&quot;No&quot;) &amp; (myDF.loc[:, &quot;DevType&quot;].str.contains(&quot;Student&quot;)),:] len(result) 4. As you can see, while perhaps a bit more strict, indexing in pandas is not that much more difficult than indexing in R. While not always necessary, remembering to put &quot;:&quot; to indicate &quot;all columns&quot; or &quot;all rows&quot; makes life easier. In addition, remembering to put parentheses around logical groupings is also a good thing. Practice makes perfect! Randomly select 100 females and 100 males. How many of each sample is in each Age category? (Do not use the sample method yet, but instead use numeric indexing and random) import random print(f&quot;A random integer between 1 and 100 is {random.randint(1, 101)}&quot;) ## A random integer between 1 and 100 is 50 It would be nice to visualize these results. pandas Series have some built in methods to create plots. Use [this] method to generate a bar plot for both females and males. How do they compare? Hint: You may need to import matplotlib in order to display the graphic: import matplotlib.pyplot as plt # female barplot code here plt.show() # male barplot code here plt.show() Click here for video Hint: Once you have your female and male DataFrames, the value_counts method found here may be particularly useful. Relevant topics: list comprehensions, for loops, loc/iloc/indexing, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution import random import matplotlib.pyplot as plt males = myDF.loc[(myDF.loc[:, &quot;Gender&quot;]==&quot;Male&quot;), :] random_indices = [random.randint(0, len(males)) for i in range(0,100)] males = males.iloc[random_indices, ] print(males.loc[:,&quot;Age&quot;].value_counts()) females = myDF.loc[(myDF.loc[:, &quot;Gender&quot;]==&quot;Female&quot;), :] random_indices = [random.randint(0, len(females)) for i in range(0,100)] females = females.iloc[random_indices] print(females.loc[:,&quot;Age&quot;].value_counts()) males.loc[:,&quot;Age&quot;].value_counts().plot.bar() plt.show() females.loc[:,&quot;Age&quot;].value_counts().plot.bar() plt.show() 5. pandas really helps out when it comes to working with data in Python. This is a really cool dataset, use your newfound skills to do a mini-analysis. Your mini-analysis should include 1 or more graphics, along with some interesting observation you made while exploring the data. Relevant topics: Item(s) to submit: Python code used to solve the problem. Output from running your code. A graphic. 1-2 sentences explaining your interesting observation and graphic. Solution Could be anything. Project 6 Motivation: Being able to analyze and create good visualizations is a skill that is invaluable in many fields. It can be pretty fun too! In this project, we are going to take a small hiatus from the regular stream of projects to do some data visualizations. Context: We've been working hard all semester and learning valuable skills. In this project we are going to ask you to examine some plots, write a little bit, and use your creative energies to create good visualizations about the flight data. Scope: python, r, visualizing data Learning objectives: Demostrate the ability to create basic graphs with default settings. Demonstrate the ability to modify axes labels and titles. Demonstrate the ability to customize a plot (color, shape/linetype). Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/flights/*.csv (all csv files) Questions 1. Here are the results from the 2009 Data Expo poster competition. The object of the competition was to visualize interesting information from the flights dataset. Examine all 8 posters and write a single sentence for each poster with your first impression(s). An example of an impression that will not get full credit would be: &quot;My first impression is that this poster is bad and doesn't look organized.&quot;. An example of an impression that will get full credit would be: &quot;My first impression is that the author had a good visualization-to-text ratio and it seems easy to follow along.&quot;. Click here for video Item(s) to submit: 8 bullets, each containing a sentence with the first impression of the 8 visualizations. Order should be &quot;first place&quot;, to &quot;honourable mention&quot;, followed by &quot;other posters&quot; in the given order. Or, label which graphic each sentence is about. 2. Creating More Effective Graphs by Dr. Naomi Robbins and The Elements of Graphing Data by Dr. William Cleveland at Purdue University, are two excellent books about data visualization. Read the following excerpts from the books (respectively), and list 2 things you learned, or found interesting from each book. Excerpt 1 Excerpt 2 Click here for video Item(s) to submit: Two bullets for each book with items you learned or found interesting. 3. Of the 7 posters with at least 3 plots and/or maps, choose 1 poster that you think you could improve upon or &quot;out plot&quot;. Create 4 plots/maps that either: Improve upon a plot from the poster you chose, or Show a completely different plot that does a good job of getting an idea or observation across, or Ruin a plot. Purposefully break the best practices you've learned about in order to make the visualization misleading. (limited to 1 of the 4 plots) For each plot/map where you choose to do (1), include 1-2 sentences explaining what exactly you improved upon and how. Point out some of the best practices from the 2 provided texts that you followed. For each plot/map where you choose to do (2), include 1-2 sentences explaining your graphic and outlining the best practices from the 2 texts that you followed. For each plot/map where you choose to do (3), include 1-2 sentences explaining what you changed, what principle it broke, and how it made the plot misleading or worse. While we are not asking you to create a poster, please use RMarkdown to keep your plots, code, and text nicely formatted and organized. The more like a story your project reads, the better. You are free to use either R or Python or both to complete this project. Please note that it would be unadvisable to use an interactive plotting package like plotly, as these packages will not render plots from within RMarkdown in RStudio. Some useful R packages: base R functions: bar, plot, lines, etc. usmap ggplot Some useful Python packages: matplotlib plotnine Click here for video Item(s) to submit: All associated R/Python code you used to wrangling the data and create your graphics. 4 plots, with at least 4 associated RMarkdown code chunks. 1-2 sentences per plot explaining what exactly you improved upon, what best practices from the texts you used, and how. If it is a brand new visualization, describe and explain your graphic, outlining the best practices from the 2 texts that you followed. If it is the ruined plot you chose, explain what you changed, what principle it broke, and how it made the plot misleading or worse. 4. Now that you've been exploring data visualization, copy, paste, and update your first impressions from question (1) with your updated impressions. Which impression changed the most, and why? Click here for video Item(s) to submit: 8 bullets with updated impressions (still just a sentence or two) from question (1). A sentence explaining which impression changed the most and why. Project 7 Motivation: There is one pretty major topic that we have yet to explore in Python -- functions! A key component to writing efficient code is writing functions. Functions allow us to repeat and reuse coding steps that we used previously, over and over again. If you find you are repeating code over and over, a function may be a good way to reduce lots of lines of code. Context: We are taking a small hiatus from our pandas and numpy focused series to learn about and write our own functions in Python! Scope: python, functions, pandas Learning objectives: Comprehend what a function is, and the components of a function in Python. Differentiate between positional and keyword arguments. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/yelp/data/parquet Questions 1. You've been given a path to a folder for a dataset. Explore the files. Give a brief description of the files and what each file contains. Note: Take a look at the size of each of the files. If you are interested in experimenting, try using pandas read_json function to read the yelp_academic_dataset_user.json file in the json folder /class/datamine/data/yelp/data/json/yelp_academic_dataset_user.json. Even with the large amount of memory available to you, this should fail. In order to make it work you would need to use the chunksize option to read the data in bit by bit. Now consider that the reviews.parquet file is .3gb larger than the yelp_academic_dataset_user.json file, but can be read in with no problem. That is seriously impressive! Click here for video Click here for another video Relevant topics: read_parquet Item(s) to submit: Python code used to solve the problem. Output from running your code. The name of each dataset and a brief summary of each dataset. No more than 1-2 sentences about each dataset. Solution 2. Read the businesses.parquet file into a pandas DataFrame called businesses. Take a look to the hours and attributes columns. If you look closely, you'll observe that both columns contain a lot more than a single feature. In fact, the attributes column contains 39 features and the hours column contains 7! len(businesses.loc[:, &quot;attributes&quot;].iloc[0].keys()) # 39 len(businesses.loc[:, &quot;hours&quot;].iloc[0].keys()) # 7 Let's start by writing a simple function. Create a function called has_attributes that takes a business_id as an argument, and returns True if the business has any attributes and False otherwise. Test it with the following code: print(has_attributes(&#39;f9NumwFMBDn751xgFiRbNA&#39;)) # True print(has_attributes(&#39;XNoUzKckATkOD1hP6vghZg&#39;)) # False print(has_attributes(&#39;Yzvjg0SayhoZgCljUJRF9Q&#39;)) # True print(has_attributes(&#39;7uYJJpwORUbCirC1mz8n9Q&#39;)) # False While this is useful to get whether or not a single business has any attributes, if you wanted to apply this function to the entire attributes column/Series, you would just use the notna method: businesses.loc[:, &quot;attributes&quot;].notna() Important note: Make sure your return value is of type bool. To check this: type(True) # bool type(&quot;True&quot;) # str Click here for video Click here for another video Relevant topics: pandas indexing, functions Item(s) to submit: Python code used to solve the problem. Output from running the provided &quot;test&quot; code. 3. Take a look at the attributes of the first business: businesses.loc[:, &quot;attributes&quot;].iloc[0] What is the type of the value? Let's assume the company you work for gets data formatted like businesses each week, but your boss wants the 39 features in attributes and the 7 features in hours to become their own columns. Write a function called fix_businesses_data that accepts an argument called data_path (of type str) that is a full path to a parquet file that is in the exact same format as businesses.parquet. In addition to the data_path argument, fix_businesses_data should accept another argument called output_dir (of type str). output_dir should contain the path where you want your &quot;fixed&quot; parquet file to output. fix_businesses_data should return None. The result of your function, should be a new file called new_businesses.parquet saved in the output_dir, the data in this file should no longer contain either the attributes or hours columns. Instead, each row should contain 39+7 new columns. Test your function out: from pathlib import Path my_username = &quot;kamstut&quot; # replace &quot;kamstut&quot; with YOUR username fix_businesses_data(data_path=&quot;/class/datamine/data/yelp/data/parquet/businesses.parquet&quot;, output_dir=f&quot;/scratch/scholar/{my_username}&quot;) # see if output exists p = Path(f&quot;/scratch/scholar/{my_username}&quot;).glob(&#39;**/*&#39;) files = [x for x in p if x.is_file()] print(files) Important note: Make sure that either /scratch/scholar/{my_username} or /scratch/scholar/{my_username}/ will work as arguments to output_dir. If you use the pathlib library, as shown in the provided function &quot;skeleton&quot; below, both will work automatically! from pathlib import Path def fix_businesses_data(data_path: str, output_dir: str) -&gt; None: &quot;&quot;&quot; fix_data accepts a parquet file that contains data in a specific format. fix_data &quot;explodes&quot; the attributes and hours columns into 39+7=46 new columns. Args: data_path (str): Full path to a file in the same format as businesses.parquet. output_dir (str): Path to a directory where new_businesses.parquet should be output. &quot;&quot;&quot; # read in original parquet file businesses = pd.read_parquet(data_path) # unnest the attributes column # unnest the hours column # output new file businesses.to_parquet(str(Path(f&quot;{output_dir}&quot;).joinpath(&quot;new_businesses.parquet&quot;))) return None Click here for video Hint: Check out the code below, notice how using pathlib handles whether or not we have the trailing /. from pathlib import Path print(Path(&quot;/class/datamine/data/&quot;).joinpath(&quot;my_file.txt&quot;)) print(Path(&quot;/class/datamine/data&quot;).joinpath(&quot;my_file.txt&quot;)) Hint: You can test out your function on /class/datamine/data/yelp/data/parquet/businesses_sample.parquet to not waste as much time. Hint: If we were using R and the tidyverse package, this sort of behavior is called &quot;unnesting&quot;. You can read more about it here. Hint: This stackoverflow post should be very useful! Specifically, run this code and take a look at the output: businesses businesses.loc[0:4, &quot;attributes&quot;].apply(pd.Series) Notice that some rows have json, and others have None: businesses.loc[0, &quot;attributes&quot;] # has json businesses.loc[2, &quot;attributes&quot;] # has None This method allows us to handle both cases. If the row has json it converts the values, if it has None it just puts each column with a value of None. Hint: Here is an example that shows you how to concatenate (combine) dataframes. Relevant topics: pandas indexing, functions, concat, apply Item(s) to submit: Python code used to solve the problem. Output from running your code. 4. That's a pretty powerful function, and could definitely be useful. What if, instead of working on just our specifically formatted parquet file, we wrote a function that worked for any pandas DataFrame? Write a function called unnest that accepts a pandas DataFrame as an argument (let's call this argument myDF), and a list of columns (let's call this argument columns), and returns a DataFrame where the provided columns are unnested. Click here for video Important note: You may write unnest so that the resulting dataframe contains the original dataframe and the unnested columns, or you may return just the unnested columns -- both will be accepted solutions. Hint: The following should work: businesses = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/businesses.parquet&quot;) new_businesses_df = unnest(businesses, [&quot;attributes&quot;, ]) new_businesses_df.shape # (209393, 39) new_businesses_df.head() new_businesses_df = unnest(businesses, [&quot;attributes&quot;, &quot;hours&quot;]) new_businesses_df.shape # (209393, 46) new_businesses_df.head() Relevant topics: pandas indexing, functions, apply, drop Item(s) to submit: Python code used to solve the problem. Output from running the provided code. 5. Try out the code below. If a provided column isn't already nested, the column name is ruined and the data is changed. If the column doesn't already exist, a KeyError is thrown. Modify our function from question (4) to skip unnesting if the column doesn't exist. In addition, modify the function from question (4) to skip the column if the column isn't nested. Let's consider a column nested if the value of the column is a dict, and not nested otherwise. businesses = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/businesses.parquet&quot;) new_businesses_df = unnest(businesses, [&quot;doesntexist&quot;,]) # KeyError new_businesses_df = unnest(businesses, [&quot;postal_code&quot;,]) # not nested To test your code, run the following. The result should be a DataFrame where attributes has been unnested, and that is it. businesses = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/businesses.parquet&quot;) results = unnest(businesses, [&quot;doesntexist&quot;, &quot;postal_code&quot;, &quot;attributes&quot;]) results.shape # (209393, 39) results.head() Click here for video Hint: To see if a variable is a dict you could use type: my_variable = {&#39;key&#39;: &#39;value&#39;} type(my_variable) ## &lt;class &#39;dict&#39;&gt; Relevant topics: pandas indexing, functions, apply, drop Item(s) to submit: Python code used to solve the problem. Output from running the provided code. Project 8 Motivation: A key component to writing efficient code is writing functions. Functions allow us to repeat and reuse coding steps that we used previously, over and over again. If you find you are repeating code over and over, a function may be a good way to reduce lots of lines of code. There are some pretty powerful features of functions that we have yet to explore that aren't necessarily present in R. In this project we will continue to learn about and harness the power of functions to solve data-driven problems. Context: We are taking a small hiatus from our pandas and numpy focused series to learn about and write our own functions in Python! Scope: python, functions, pandas Learning objectives: Comprehend what a function is, and the components of a function in Python. Differentiate between positional and keyword arguments. Learn about packing and unpacking variables and arguments. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/yelp/data/parquet Questions 1. The company you work for is assigning you to the task of building out some functions for the new API they've built. For simplicity assume you have access to the following pandas DataFrame's at all times: photos = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/photos.parquet&quot;) businesses = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/businesses.parquet&quot;) checkins = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/checkins.parquet&quot;) tips = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/tips.parquet&quot;) users = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/users.parquet&quot;) reviews = pd.read_parquet(&quot;/class/datamine/data/yelp/data/parquet/reviews.parquet&quot;) You would expect that friends may have a similar taste in restaurants or businesses. Write a function called get_friends_data that accepts a user_id as an argument, and returns a pandas DataFrame with the information in the users DataFrame for each friend of user_id. Look at the solutions from the previous project, as well as this page. Add type hints for your function. You should have a type hint for our argument, user_id, as well as a type hint for the returned data. In addition to type hints, make sure to document your function with a docstring. Hint: Every function in the solutions for last week's projects has a docstring. You can use this as a reference. Hint: You should get the same number of friends for the following code: Click here for video Click here for another video print(get_friends_data(&quot;ntlvfPzc8eglqvk92iDIAw&quot;).shape) # (13,22) print(get_friends_data(&quot;AY-laIws3S7YXNl_f_D6rQ&quot;).shape) # (1, 22) print(get_friends_data(&quot;xvu8G900tezTzbbfqmTKvA&quot;).shape) # (193,22) Relevant topics: pandas indexing, functions Item(s) to submit: Python code used to solve the problem. Output from running your code. 2. Write a function called calculate_avg_business_stars that accepts a business_id and returns the average number of stars that business received in reviews. Like in question (1) make sure to add type hints and docstrings. In addition, add comments when (and if) necessary. There is a really cool method that gives us the same &quot;powers&quot; that tapply gives us in R. Use the groupby method from pandas to calculate the average stars for all businesses. Index the result to confirm that your calculate_avg_business_stars function worked properly. Hint: You should get the same average number of start value for the following code: print(calculate_avg_business_stars(&quot;f9NumwFMBDn751xgFiRbNA&quot;)) # 3.1025641025641026 Click here for video Relevant topics: pandas indexing, functions, groupby, numpy.mean() Item(s) to submit: Python code used to solve the problem. Output from running your code. 3. Write a function called visualize_stars_over_time that accepts a business_id and returns a line plot that shows the average number of stars for each year the business has review data. Like in previous questions, make sure to add type hints and docstrings. In addition, add comments when (and if) necessary. You can test your function with some of these: visualize_stars_over_time(&#39;RESDUcs7fIiihp38-d6_6g&#39;) Click here for video Relevant topics: pandas indexing, functions, matplotlib lineplot Item(s) to submit: Python code used to solve the problem. Output from running your code. 4. Modify question (3), and add an argument called granularity that dictates whether the plot will show the average rating over years, or months. granularity should accept one of two strings: &quot;years&quot;, or &quot;months&quot;. By default, if granularity isn't specified, it should be &quot;years&quot;. visualize_stars_over_time(&#39;RESDUcs7fIiihp38-d6_6g&#39;, &quot;months&quot;) Click here for video Relevant topics: pandas indexing, functions, matplotlib lineplot Item(s) to submit: Python code used to solve the problem. Output from running your code. 5. Modify question (4) to accept multiple business_id's, and create a line for each id. Each of the following should work: visualize_stars_over_time(&quot;RESDUcs7fIiihp38-d6_6g&quot;, &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;, &quot;months&quot;) visualize_stars_over_time(&quot;RESDUcs7fIiihp38-d6_6g&quot;, &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;, &quot;K7lWdNUhCbcnEvI0NhGewg&quot;, &quot;months&quot;) visualize_stars_over_time(&quot;RESDUcs7fIiihp38-d6_6g&quot;, &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;, &quot;K7lWdNUhCbcnEvI0NhGewg&quot;, granularity=&quot;years&quot;) Hint: Use plt.show to decide when to show your complete plot and start anew. Click here for video Relevant topics: pandas indexing, functions, matplotlib lineplot Item(s) to submit: Python code used to solve the problem. Output from running your code. 6. After some thought, your boss decided that using the function from question (5) would get pretty tedious when there are a lot of businesses to include in the plot. You disagree. You think there is a way to pass a list of business_ids without modifying your function, but rather how you pass the arguments to the function. Demonstrate how to do this with the list provided: our_businesses = [&quot;RESDUcs7fIiihp38-d6_6g&quot;, &quot;4JNXUYY8wbaaDmk3BPzlWw&quot;, &quot;K7lWdNUhCbcnEvI0NhGewg&quot;] # modify something below to make this work: visualize_stars_over_time(our_businesses, granularity=&quot;years&quot;) Hint: Google &quot;python packing unpacking arguments&quot;. Click here for video Relevant topics: pandas indexing, functions, matplotlib lineplot Item(s) to submit: Python code used to solve the problem. Output from running your code. STAT 29000 Topics The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration. Language Project # Name Topics Python 1 Web scraping: part I xml, lxml, pandas, etc. Python 2 Web scraping: part II requests, functions, xml, loops, if statements, etc. Python 3 Web scraping: part III selenium, lxml, lists, pandas, etc. Python 4 Web scraping: part IV requests, beautifulsoup4, lxml, selenium, xml, cronjobs, loops, etc. Python 5 Web scraping: part V web scraping + related topics Python 6 Plotting in Python: part I ways to plot in Python, more work with pandas, etc. Python 7 Plotting in Python: part II ways to plot in Python, more work with pandas, etc. Python 8 Writing scripts: part I how to write scripts in Python, more work with pandas, matplotlib, etc. Python 9 Writing scripts: part II how to write scripts in Python, argparse, more work with pandas, matplotlib, etc. R 10 ggplot: part I ggplot basics R 11 ggplot: part II more ggplot R 12 tidyverse &amp; data.table: part I data wrangling and computation using tidyverse packages and data.table R 13 tidyverse &amp; data.table: part II data wrangling and computation using tidyverse packages and data.table R 14 tidyverse &amp; data.table: part III data wrangling and computation using tidyverse packages and data.table Project 1 Motivation: Extensible Markup Language or XML is a very important file format for storing structured data. Even though formats like JSON, and csv tend to be more prevalent, many, many legacy systems still use XML, and it remains an appropriate format for storing complex data. In fact, JSON and csv are quickly becoming less relevant as new formats and serialization methods like parquet and protobufs are becoming more common. Context: In previous semesters we've explored XML. In this project we will refresh our skills and, rather than exploring XML in R, we will use the lxml package in Python. This is the first project in a series of 5 projects focused on web scraping in R and Python. Scope: python, XML Learning objectives: Review and summarize the differences between XML and HTML/CSV. Match XML terms to sections of XML demonstrating working knowledge. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/apple/health/watch_dump.xml Resources We realize that for many of you this is a big &quot;jump&quot; right into Python. Don't worry! Python is a very intuitive language with a clean syntax. It is easy to read and write. We will do our very best to keep things as straightforward as possible, especially in the early learning stages of the class. We will be actively updating the examples book with videos and more examples throughout the semester. Ask a question in Piazza and perhaps we will add an example straight to the book to help out. Some potentially useful resources for the semester include: The STAT 19000 projects. We are easing 19000 students into Python and will post solutions each week. It would be well worth 10 minutes to look over the questions and solutions each week. Here is a decent cheat sheet that helps you quickly get an idea of how to do something you know how to do in R, in Python. The Examples Book -- updating daily with more examples and videos. Be sure to click on the &quot;relevant topics&quot; links as we try to point you to topics with examples that should be particularly useful to solve the problems we assign. Questions Important note: It would be well worth your time to read through the xml section of the book, as well as take the time to work through pandas 10 minute intro. 1. A good first step when working with XML is to get an idea how your document is structured. Normally, there should be good documentation that spells this out for you, but it is good to know what to do when you don't have the documentation. Start by finding the &quot;root&quot; node. What is the name of the root node of the provided dataset? Hint: Make sure to import the lxml package first: from lxml import etree Here are two videos about running Python in RStudio: Click here for video Click here for video and here is a video about XML scraping in Python: Click here for video Relevant topics: lxml, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from lxml import etree tree = etree.parse(&quot;/class/datamine/data/apple/health/watch_dump.xml&quot;) tree.xpath(&quot;/*&quot;)[0].tag 2. Remember, XML can be nested. In question (1) we figured out what the root node was called. What are the names of the next &quot;tier&quot; of elements? Hint: Now that we know the root node, you could use the root node name as a part of your xpath expression. Hint: As you may have noticed in question (1) the xpath method returns a list. Sometimes this list can contain many repeated tag names. Since our goal is to see the names of the second &quot;tier&quot; elements, you could convert the resulting list to a set to quickly see the unique list as set's only contain unique values. Relevant topics: for loops, lxml, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution set([x.tag for x in tree.xpath(&quot;/HealthData/*&quot;)]) 3. Continue to explore each &quot;tier&quot; of data until there isn't any left. Name the &quot;full paths&quot; of all of the &quot;last tier&quot; tags. Hint: Let's say a &quot;last tier&quot; tag is just a path where there are no more nested elements. For example, /HealthData/Workout/WorkoutRoute/FileReference is a &quot;last tier&quot; tag. If you try and get the nested elements for it, they don't exist: tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/FileReference/*&quot;) Hint: Here are 3 of the 7 &quot;full paths&quot;: /HealthData/Workout/WorkoutRoute/FileReference /HealthData/Record/MetadataEntry /HealthData/ActivitySummary Relevant topics: lxml, xml, for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution print(set([x.tag for x in tree.xpath(&quot;/HealthData/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/ActivitySummary/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/HeartRateVariabilityMetadataList/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/MetadataEntry/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutEvent/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutEntry/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/FileReference/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/MetadataEntry/*&quot;)])) /HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute /HealthData/Workout/WorkoutRoute/FileReference /HealthData/Workout/WorkoutRoute/MetadataEntry /HealthData/Record/MetadataEntry /HealthData/Workout/WorkoutEvent /HealthData/Workout/WorkoutEntry /HealthData/ActivitySummary 4. At this point in time you may be asking yourself &quot;but where is the data&quot;? Depending on the structure of the XML file, the data could either be between tags like: &lt;some_tag&gt;mydata&lt;/some_tag&gt; Or, it could be in an attribute: &lt;question answer=&quot;tac&quot;&gt;What is cat spelled backwards?&lt;/question&gt; Collect the &quot;ActivitySummary&quot; data, and convert the list of dicts to a pandas DataFrame. The following is an example of converting a list of dicts to a pandas DataFrame called myDF: import pandas as pd list_of_dicts = [] list_of_dicts.append({&#39;columnA&#39;: 1, &#39;columnB&#39;: 2}) list_of_dicts.append({&#39;columnB&#39;: 4, &#39;columnA&#39;: 1}) myDF = pd.DataFrame(list_of_dicts) Hint: It is important to note that an element's &quot;attrib&quot; attribute looks and feels like a dict, but it is actually a lxml.etree._Attrib. If you try to convert a list of lxml.etree._Attrib to a pandas DataFrame, it will not work out as you planned. Make sure to first convert each lxml.etree._Attrib to a dict before converting to a DataFrame. You can do so like: # this will convert a single `lxml.etree._Attrib` to a dict my_dict = dict(my_lxml_etree_attrib) Relevant topics: dicts, lists, lxml, xml, for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution dat = tree.xpath(&quot;/HealthData/ActivitySummary&quot;) list_of_dicts = [] for e in dat: list_of_dicts.append(dict(e.attrib)) myDF = pd.DataFrame(data=list_of_dicts) myDF.sort_values([&#39;activeEnergyBurned&#39;], ascending=False).head() 5. pandas is a Python package that provides the DataFrame and Series classes. A DataFrame is very similar to a data.frame in R and can be used to manipulate the data within very easily. A Series is the class that handles a single column of a DataFrame. Go through the pandas in 10 minutes page from the official documentation. Sort, find, and print the top 5 rows of data based on the &quot;activeEnergyBurned&quot; column. Relevant topics: pandas, dicts, lists, lxml, xml, for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution # could be anything Project 2 Motivation: Web scraping is is the process of taking content off of the internet. Typically this goes hand-in-hand with parsing or processing the data. Depending on the task at hand, web scraping can be incredibly simple. With that being said, it can quickly become difficult. Typically, students find web scraping fun and empowering. Context: In the previous project we gently introduced XML and xpath expressions. In this project, we will learn about web scraping, scrape data from The New York Times, and parse through our newly scraped data using xpath expressions. Scope: python, web scraping, xml Learning objectives: html Review and summarize the differences between XML and HTML/CSV. Use the requests package to scrape a web page. Use the lxml package to filter and parse data from a scraped web page. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset You will be extracting your own data from online in this project. There is no base dataset. Questions 1. The New York Times is one of the most popular newspapers in the United States. Open a modern browser (preferably Firefox or Chrome), and navigate to https://nytimes.com. By the end of this project you will be able to scrape some data from this website! The first step is to explore the structure of the website. You can either right click and click on &quot;view page source&quot;, which will pull up a page full of HTML used to render the page. Alternatively, if you want to focus on a single element, an article title, for example, right click on the article title and click on &quot;inspect element&quot;. This will pull up an inspector that allows you to see portions of the HTML. Click around the website and explore the HTML however you see fit. Open a few front page articles and notice how most articles start with a bunch of really important information, namely: an article title, summary, picture, picture caption, picture source, author portraits, authors, and article datetime. For example: https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html Copy and paste the h1 element (in its entirety) containing the article title (for the article provided) in an HTML code chunk. Do the same for the same article's summary. Click here for video Relevant topics: html Item(s) to submit: 2 code chunks containing the HTML requested. Solution &lt;h1 id=&quot;link-4686dc8b&quot; class=&quot;css-rsa88z e1h9rw200&quot; data-test-id=&quot;headline&quot;&gt;U.S. Says China’s Repression of Uighurs Is ‘Genocide’&lt;/h1&gt; &lt;p id=&quot;article-summary&quot; class=&quot;css-w6ymp8 e1wiw3jv0&quot;&gt;The finding by the Trump administration is the strongest denunciation by any government of China’s actions and follows a Biden campaign statement with the same declaration.&lt;/p&gt; 2. In question (1) we copied two elements of an article. When scraping data from a website, it is important to continually consider the patterns in the structure. Specifically, it is important to consider whether or not the defining characteristics you use to parse the scraped data will continue to be in the same format for new data. What do I mean by defining characterstic? I mean some combination of tag, attribute, and content from which you can isolate the data of interest. For example, given a link to a new nytimes article, do you think you could isolate the article title by using the id=&quot;link-4686dc8b&quot; attribute of the h1 tag? Maybe, or maybe not, but it sure seems like &quot;link-4686dc8b&quot; might be unique to the article and not able to be used given a new article. Write an xpath expression to isolate the article title, and another xpath expression to isolate the article summary. Important note: You do not need to test your xpath expression yet, we will be doing that shortly. Relevant topics: html, xml, xpath expressions Item(s) to submit: Two xpath expressions in an HTML code chunk. Solution //h1[@data-test-id=&quot;headline&quot;] //p[@id=&quot;article-summary&quot;] 3. Use the requests package to scrape the webpage containing our article from questions (1) and (2). Use the lxml.html package and the xpath method to test out your xpath expressions from question (2). Did they work? Print the content of the elements to confirm. Click here for video Relevant topics: html, xml, xpath expressions, lxml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution import lxml.html import requests url = &quot;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&quot; response = requests.get(url, stream=True) response.raw.decode_content = True tree = lxml.html.parse(response.raw) print(tree.xpath(&#39;//p[@id=&quot;article-summary&quot;]&#39;)[0].text) print(tree.xpath(&#39;//h1[@data-test-id=&quot;headline&quot;]&#39;)[0].text) 4. Here are a list of article links from https://nytimes.com: https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html Write a function called get_article_and_summary that accepts a string called link as an argument, and returns both the article title and summary. Test get_article_and_summary out on each of the provided links: title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) Hint: The first line of your function should look like this: def get_article_and_summary(myURL: str) -&gt; (str, str): Click here for video Click here for video Relevant topics: html, xml, xpath expressions, lxml, functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from typing import Tuple import lxml.html import requests def get_article_and_summary(link: str) -&gt; Tuple[str, str]: &quot;&quot;&quot; Given a link to a new york times article, return the article title and summary. Args: link (str): The link to the new york times article. Returns: Tuple[str, str]: A tuple first containing the article title, and then the article summary. &quot;&quot;&quot; # scrape the web page response = requests.get(link, stream=True) response.raw.decode_content = True tree = lxml.html.parse(response.raw) title = tree.xpath(&#39;//p[@id=&quot;article-summary&quot;]&#39;)[0].text summary = tree.xpath(&#39;//h1[@data-test-id=&quot;headline&quot;]&#39;)[0].text return title, summary title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) 5. In question (1) we mentioned a myriad of other important information given at the top of most New York Times articles. Choose one other listed pieces of information and copy, paste, and update your solution to question (4) to scrape and return those chosen pieces of information. Important note: If you choose to scrape non-textual data, be sure to return data of an appropriate type. For example, if you choose to scrape one of the images, either print the image or return a PIL object. Relevant topics: html, xml, xpath expressions, lxml, functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from typing import Tuple import lxml.html import requests import io from PIL import Image from IPython.display import display def get_article_and_summary(link: str) -&gt; Tuple[str, str]: &quot;&quot;&quot; Given a link to a new york times article, return the article title and summary. Args: link (str): The link to the new york times article. Returns: Tuple[str, str]: A tuple first containing the article title, and then the article summary. &quot;&quot;&quot; # scrape the web page response = requests.get(link, stream=True) response.raw.decode_content = True tree = lxml.html.parse(response.raw) # parse out the title title = tree.xpath(&#39;//p[@id=&quot;article-summary&quot;]&#39;)[0].text # parse out the summary summary = tree.xpath(&#39;//h1[@data-test-id=&quot;headline&quot;]&#39;)[0].text # parse out the url to the image photo_src = tree.xpath(&#39;//picture/img&#39;)[0].attrib.get(&quot;src&quot;) # scrape image photo_content = requests.get(photo_src).content # convert image format photo_file = io.BytesIO(photo_content) photo = Image.open(photo_file).convert(&#39;RGB&#39;) # parse out photo caption caption = tree.xpath(&#39;//figcaption/span&#39;)[0].text # parse out the photo credits credits = tree.xpath(&#39;//figcaption/span/span[contains(text(), &quot;Credit&quot;)]/following-sibling::span/span&#39;) credits_list = [] for c in credits: credits_list.append(c.text) # parse author photo url # only gets if &quot;author&quot; in photo src attribute photo_src_elements = tree.xpath(&#39;//img[contains(@src, &quot;author&quot;)]&#39;) # if &quot;author&quot; in photo src attribute photo_srcs = [] for p in photo_src_elements: photo_srcs.append(p.attrib.get(&quot;src&quot;)) # scrape image author_images = [] for img in photo_srcs: photo_content = requests.get(img).content # convert image format photo_file = io.BytesIO(photo_content) photo = Image.open(photo_file).convert(&#39;RGB&#39;) author_images.append(photo) # parse out authors authors_elements = tree.xpath(&quot;//span[@class=&#39;byline-prefix&#39;]/following-sibling::a/span&quot;) authors = [] for a in authors_elements: authors.append(a.text) # parse out article publish date/time dt = tree.xpath(&quot;//time&quot;)[0].attrib.get(&quot;datetime&quot;) return title, summary, photo, caption, credits, author_images, authors, dt title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&#39;) print(f&#39;Title:\\n{title}, Summary:\\n{summary}, Caption:\\n{caption}&#39;) title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html&#39;) print(f&#39;Title:\\n{title}, Summary:\\n{summary}, Caption:\\n{caption}&#39;) title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html&#39;) print(f&#39;Title:\\n{title}, Summary:\\n{summary}, Caption:\\n{caption}&#39;) Project 3 Motivation: Web scraping takes practice, and it is important to work through a variety of common tasks in order to know how to handle those tasks when you next run into them. In this project, we will use a variety of scraping tools in order to scrape data from https://trulia.com. Context: In the previous project, we got our first taste at actually scraping data from a website, and using a parser to extract the information we were interested in. In this project, we will introduce some tasks that will require you to use a tool that let's you interact with a browser, selenium. Scope: python, web scraping, selenium Learning objectives: Review and summarize the differences between XML and HTML/CSV. Use the requests package to scrape a web page. Use the lxml package to filter and parse data from a scraped web page. Use selenium to interact with a browser in order to get a web page to a desired state for scraping. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. Visit https://trulia.com. Many websites have a similar interface, i.e. a bold and centered search bar for a user to interact with. Using selenium write Python code that that first finds the input element, and then types &quot;West Lafayette, IN&quot; followed by an emulated &quot;Enter/Return&quot;. Confirm you code works by printing the url after that process completes. Hint: You will want to use time.sleep to pause a bit after the search so the updated url is returned. Click here for video That video is already relevant for Question 2 too. Relevant topics: selenium, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.firefox.options import Options from selenium.common.exceptions import NoSuchElementException import time firefox_options = Options() firefox_options.add_argument(&quot;window-size=1920,1080&quot;) firefox_options.add_argument(&quot;--headless&quot;) # Headless mode means no GUI firefox_options.add_argument(&quot;start-maximized&quot;) firefox_options.add_argument(&quot;disable-infobars&quot;) firefox_options.add_argument(&quot;--disable-extensions&quot;) firefox_options.add_argument(&quot;--no-sandbox&quot;) firefox_options.add_argument(&quot;--disable-dev-shm-usage&quot;) firefox_options.binary_location = &#39;/class/datamine/apps/firefox/firefox&#39; driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) url = &#39;https://www.trulia.com&#39; driver.get(url) search_input = driver.find_element_by_id(&quot;banner-search&quot;) search_input.send_keys(&quot;West Lafayette, IN&quot;) search_input.send_keys(Keys.RETURN) time.sleep(3) print(driver.current_url) 2. Use your code from question (1) to test out the following queries: West Lafayette, IN (City, State) 47906 (Zip) 4505 Kahala Ave, Honolulu, HI 96816 (Full address) If you look closely you will see that there are patterns in the url. For example, the following link would probably bring up homes in Crawfordsville, IN: https://trulia.com/IN/Crawfordsville. With that being said, if you only had a zip code, like 47933, it wouldn't be easy to guess https://www.trulia.com/IN/Crawfordsville/47933/, hence, one reason why the search bar is useful. If you used xpath expressions to complete question (1), instead use a different method to find the input element. If you used a different method, use xpath expressions to complete question (1). Relevant topics: selenium, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) url = &#39;https://www.trulia.com&#39; driver.get(url) search_input = driver.find_element_by_xpath(&quot;//input[@id=&#39;banner-search&#39;]&quot;) search_input.send_keys(&quot;West Lafayette, IN&quot;) search_input.send_keys(Keys.RETURN) time.sleep(3) print(driver.current_url) 3. Let's call the page after a city/state or zipcode search a &quot;sales page&quot;. For example: Use requests to scrape the entire page: https://www.trulia.com/IN/West_Lafayette/47906/. Use lxml.html to parse the page and get all of the img elements that make up the house pictures on the left side of the website. Important note: Make sure you are actually scraping what you think you are scraping! Try printing your html to confirm it has the content you think it should have: import requests response = requests.get(...) print(response.text) Hint: Are you human? Depends. Sometimes if you add a header to your request, it won't ask you if you are human. Let's pretend we are Firefox: import requests my_headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} response = requests.get(..., headers=my_headers) Okay, after all of that work you may have discovered that only a few images have actually been scraped. If you cycle through all of the img elements and try to print the value of the src attribute, this will be clear: import lxml.html tree = lxml.html.fromstring(response.text) elements = tree.xpath(&quot;//img&quot;) for element in elements: print(element.attrib.get(&quot;src&quot;)) This is because the webpage is not immediately, completely loaded. This is a common website behavior to make things appear faster. If you pay close to when you load https://www.trulia.com/IN/Crawfordsville/47933/, and you quickly scroll down, you will see images still needing to finish rendering all of the way, slowly. What we need to do to fix this, is use selenium (instead of lxml.html) to behave like a human and scroll prior to scraping the page! Try using the following code to slowly scroll down the page before finding the elements: # driver setup and get the url # Needed to get the window size set right and scroll in headless mode myheight = driver.execute_script(&#39;return document.body.scrollHeight&#39;) driver.set_window_size(1080,myheight+100) def scroll(driver, scroll_point): driver.execute_script(f&#39;window.scrollTo(0, {scroll_point});&#39;) time.sleep(5) scroll(driver, myheight*1/4) scroll(driver, myheight*2/4) scroll(driver, myheight*3/4) scroll(driver, myheight*4/4) # find_elements_by_* Hint: At the time of writing there should be about 86 links to images of homes. Click here for video Relevant topics: selenium, xml, loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/Users/kamstut/Downloads/geckodriver&#39;) url = &#39;https://www.trulia.com/IN/West_Lafayette/47906/&#39; driver.get(url) # Needed to get the window size set right and scroll in headless mode height = driver.execute_script(&#39;return document.body.scrollHeight&#39;) driver.set_window_size(1080,height+100) def scroll(driver, scroll_point): driver.execute_script(f&#39;window.scrollTo(0, {scroll_point});&#39;) time.sleep(5) scroll(driver, height/4) scroll(driver, height/4*2) scroll(driver, height/4*3) scroll(driver, height) elements = driver.find_elements_by_xpath(&quot;//picture/img&quot;) print(len(elements)) for e in elements: print(e.get_attribute(&quot;src&quot;)) 4. Write a function called avg_house_cost that accepts a zip code as an argument, and returns the average cost of the first page of homes. Now, to make this a more meaningful statistic, filter for &quot;3+&quot; beds and then find the average. Test avg_house_cost out on the zip code 47906 and print the average costs. Important note: Use selenium to &quot;click&quot; on the &quot;3+ beds&quot; filter. Hint: If you get an error that tells you button is not clickable because it is covered by an li element, try clicking on the li element instead. Hint: You will want to wait a solid 10-15 seconds for the sales page to load before trying to select or click on anything. Hint: Your results may end up including prices for &quot;Homes Near &lt;ZIPCODE&gt;&quot;. This is okay. Even better if you manage to remove those results. If you do choose to remove those results, take a look at the data-testid attribute with value search-result-list-container. Perhaps only selecting the children of the first element will get the desired outcome. Hint: You can use the following code to remove the non-numeric text from a string, and then convert to an integer: import re int(re.sub(&quot;[^0-9]&quot;, &quot;&quot;, &quot;removenon45454_numbers$&quot;)) Click here for video Relevant topics: selenium, xml, loops, functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from selenium.webdriver.support.ui import WebDriverWait import re def avg_house_cost(zip: str) -&gt; float: firefox_options = Options() firefox_options.add_argument(&quot;window-size=1920,1080&quot;) # firefox_options.add_argument(&quot;--headless&quot;) # Headless mode means no GUI firefox_options.add_argument(&quot;start-maximized&quot;) firefox_options.add_argument(&quot;disable-infobars&quot;) firefox_options.add_argument(&quot;--disable-extensions&quot;) firefox_options.add_argument(&quot;--no-sandbox&quot;) firefox_options.add_argument(&quot;--disable-dev-shm-usage&quot;) firefox_options.binary_location = &#39;/Applications/Firefox.app/Contents/MacOS/firefox&#39; driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/Users/kamstut/Downloads/geckodriver&#39;) url = &#39;https://www.trulia.com/&#39; driver.get(url) search_input = driver.find_element_by_id(&quot;banner-search&quot;) search_input.send_keys(zip) search_input.send_keys(Keys.RETURN) time.sleep(10) allbed_button = driver.find_element_by_xpath(&quot;//button[@data-testid=&#39;srp-xl-bedrooms-filter-button&#39;]/ancestor::li&quot;) allbed_button.click() time.sleep(2) bed_button = driver.find_element_by_xpath(&quot;//button[contains(text(), &#39;3+&#39;)]&quot;) bed_button.click() time.sleep(3) price_elements = driver.find_elements_by_xpath(&quot;(//div[@data-testid=&#39;search-result-list-container&#39;])[1]//div[@data-testid=&#39;property-price&#39;]&quot;) prices = [int(re.sub(&quot;[^0-9]&quot;, &quot;&quot;, e.text)) for e in price_elements] driver.quit() return sum(prices)/len(prices) avg_house_cost(&#39;47906&#39;) 5. Get creative. Either add an interesting feature to your function from (4), or use matplotlib to generate some sort of accompanying graphic with your output. Make sure to explain what your additions do. Relevant topics: selenium, xml, loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution Could be anything. Project 4 Motivation: In this project we will continue to hone your web scraping skills, introduce you to some &quot;gotchas&quot;, and give you a little bit of exposure to a powerful tool called cron. Context: We are in the second to last project focused on web scraping. This project will introduce some supplementary tools that work well with web scraping: cron, sending emails from Python, etc. Scope: python, web scraping, selenium, cron Learning objectives: Review and summarize the differences between XML and HTML/CSV. Use the requests package to scrape a web page. Use the lxml package to filter and parse data from a scraped web page. Use the beautifulsoup4 package to filter and parse data from a scraped web page. Use selenium to interact with a browser in order to get a web page to a desired state for scraping. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. Check out the following website: https://project4.tdm.wiki Use selenium to scrape and print the 6 colors of pants offered. Click here for video for Question 1 Here is a video about the new feature to reset your RStudio session if you make a big mistake or if your session is very slow Hint: You may have to interact with the webpage for certain elements to render. Relevant topics: scraping, selenium, example clicking a button Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.firefox.options import Options from selenium.common.exceptions import NoSuchElementException import time driver.get(&quot;https://project4.tdm.wiki/&quot;) elements = driver.find_elements_by_xpath(&quot;//span[@class=&#39;ui-component-card--color-amount&#39;]&quot;) for element in elements: print(element.text) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() elements = driver.find_elements_by_xpath(&quot;//span[@class=&#39;ui-component-card--color-amount&#39;]&quot;) for element in elements: print(element.text) driver.quit() 2. Websites are updated frequently. You can imagine a scenario where a change in a website is a sign that there is more data available, or that something of note has happened. This is a fake website designed to help students emulate real changes to a website. Specifically, there is one part of the website that has two possible states (let's say, state A and state B). Upon refreshing the website, or scraping the website again, there is an \\(x\\%\\) chance that the website will be in state A and a \\(1-x\\%\\) chance the website will be in state B. Describe the two states (the thing (element or set of elements) that changes as you refresh the page), and scrape the website enough to estimate \\(x\\). Click here for video for Questions 2 and 3 Hint: You will need to interact with the website to &quot;see&quot; the change. Hint: Since we are just asking about a state, and not any specific element, you could use the page_source attribute of the selenium driver to scrape the entire page instead of trying to use xpath expressions to find a specific element. Hint: Your estimate of \\(x\\) does not need to be perfect. Relevant topics: scraping, selenium, example clicking a button Item(s) to submit: Python code used to solve the problem. Output from running your code. What state A and B represent. An estimate for x. Solution 25% in stock 75% out of stock -- anything remotely close give full credit # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() stateA = driver.page_source stateAct = 0 stateBct = 0 for i in range(10): driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() if driver.page_source == stateA: stateAct += 1 else: stateBct += 1 driver.quit() print(f&quot;State A: {stateAct}&quot;) print(f&quot;State B: {stateBct}&quot;) 3. Dig into the changing &quot;thing&quot; from question (2). What specifically is changing? Use selenium and xpath expressions to scrape and print the content. What are the two possible values for the content? Click here for video (same as above) for Questions 2 and 3 Hint: Due to the changes that occur when a button is clicked, I'd highly advice you to use the data-color attribute in your xpath expression instead of contains(text(), 'blahblah'). Hint: parent:: and following-sibling:: may be useful xpath axes to use. Relevant topics: scraping, selenium, example using following-sibling:: Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() element = driver.find_element_by_xpath(&quot;//span[@data-color=&#39;Chartreuse&#39;]/parent::div/following-sibling::div/span&quot;) print(element.text) driver.quit() 4. The following code allows you to send an email using Python from your Purdue email account. Replace the username and password with your own information and send a test email to yourself to ensure that it works. Click here for video for Questions 4 and 5 Important note: Do NOT include your password in your homework submission. Any time you need to type your password in you final submission just put something like &quot;SUPERSECRETPASSWORD&quot; or &quot;MYPASSWORD&quot;. Hint: To include an image (or screenshot) in RMarkdown, try ![](./my_image.png) where my_image.png is inside the same folder as your .Rmd file. Hint: The spacing and tabs near the message variable are very important. Make sure to copy the code exactly. Otherwise, your subject may not end up in the subject of your email, or the email could end up being blank when sent. Hint: Questions 4 and 5 were inspired by examples and borrowed from the code found at the Real Python website. def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message): import smtplib, ssl from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart message = MIMEMultipart(&quot;alternative&quot;) message[&quot;Subject&quot;] = my_subject message[&quot;From&quot;] = my_purdue_email message[&quot;To&quot;] = to # Create the plain-text and HTML version of your message text = f&#39;&#39;&#39;\\ Subject: {my_subject} To: {to} From: {my_purdue_email} {my_message}&#39;&#39;&#39; html = f&#39;&#39;&#39;\\ &lt;html&gt; &lt;body&gt; {my_message} &lt;/body&gt; &lt;/html&gt; &#39;&#39;&#39; # Turn these into plain/html MIMEText objects part1 = MIMEText(text, &quot;plain&quot;) part2 = MIMEText(html, &quot;html&quot;) # Add HTML/plain-text parts to MIMEMultipart message # The email client will try to render the last part first message.attach(part1) message.attach(part2) context = ssl.create_default_context() with smtplib.SMTP(&quot;smtp.purdue.edu&quot;, 587) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(my_purdue_email, my_password) server.sendmail(my_purdue_email, to, message.as_string()) # this sends an email from kamstut@purdue.edu to mdw@purdue.edu # replace supersecretpassword with your own password # do NOT include your password in your homework submission. send_purdue_email(&quot;kamstut@purdue.edu&quot;, &quot;supersecretpassword&quot;, &quot;mdw@purdue.edu&quot;, &quot;put subject here&quot;, &quot;put message body here&quot;) Relevant topics: functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Screenshot showing your received the email. Solution import smtplib, ssl port = 587 # For starttls smtp_server = &quot;smtp.purdue.edu&quot; sender_email = &quot;kamstut@purdue.edu&quot; receiver_email = &quot;kamstut@purdue.edu&quot; password = &quot;MYPASSWORD&quot; message = &quot;&quot;&quot;\\ Subject: Hi there This message is sent from Python.&quot;&quot;&quot; context = ssl.create_default_context() with smtplib.SMTP(smtp_server, port) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(sender_email, password) server.sendmail(sender_email, receiver_email, message) 5. The following is the content of a new Python script called is_in_stock.py: def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message): import smtplib, ssl from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart message = MIMEMultipart(&quot;alternative&quot;) message[&quot;Subject&quot;] = my_subject message[&quot;From&quot;] = my_purdue_email message[&quot;To&quot;] = to # Create the plain-text and HTML version of your message text = f&#39;&#39;&#39;\\ Subject: {my_subject} To: {to} From: {my_purdue_email} {my_message}&#39;&#39;&#39; html = f&#39;&#39;&#39;\\ &lt;html&gt; &lt;body&gt; {my_message} &lt;/body&gt; &lt;/html&gt; &#39;&#39;&#39; # Turn these into plain/html MIMEText objects part1 = MIMEText(text, &quot;plain&quot;) part2 = MIMEText(html, &quot;html&quot;) # Add HTML/plain-text parts to MIMEMultipart message # The email client will try to render the last part first message.attach(part1) message.attach(part2) context = ssl.create_default_context() with smtplib.SMTP(&quot;smtp.purdue.edu&quot;, 587) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(my_purdue_email, my_password) server.sendmail(my_purdue_email, to, message.as_string()) def main(): # scrape element from question 3 # does the text indicate it is in stock? # if yes, send email to yourself telling you it is in stock. # otherwise, gracefully end script using the &quot;pass&quot; Python keyword if __name__ == &quot;__main__&quot;: main() First, make a copy of the script in your $HOME directory: cp /class/datamine/data/scraping/is_in_stock.py $HOME/is_in_stock.py If you now look in the &quot;Files&quot; tab in the lower right hand corner of RStudio, and click the refresh button, you should see the file is_in_stock.py. You can open and modify this file directly in RStudio. Before you do so, however, change the permissions of the $HOME/is_in_stock.py script so only YOU can read, write, and execute it: chmod 700 $HOME/is_in_stock.py The script should now appear in RStudio, in your home directory, with the correct permissions. Open the script (in RStudio) and fill in the main function as indicated by the comments. We want the script to scrape to see whether the pants from question 3 are in stock or not. A cron job is a task that runs at a certain interval. Create a cron job that runs your script, /class/datamine/apps/python/f2020-s2021/env/bin/python $HOME/is_in_stock.py every 5 minutes. Wait 10-15 minutes and verify that it is working properly. The long path, /class/datamine/apps/python/f2020-s2021/env/bin/python simply makes sure that our script is run with access to all of the packages in our course environment. $HOME/is_in_stock.py is the path to your script ($HOME expands or transforms to /home/&lt;my_purdue_alias&gt;. Click here for video (same as above) for Questions 4 and 5 Click here for a longer video about setting up the cronjob in Question 5 Hint: If you struggle to use the text editor used with the crontab -e command, be sure to continue reading the cron section of the book. We highlight another method that may be easier. Hint: Don't forget to copy your import statements from question (3) as well. Important note: Once you are finished with the project, if you no longer wish to receive emails every so often, follow the instructions here to remove the cron job. Relevant topics: cron, crontab guru Item(s) to submit: Python code used to solve the problem. Output from running your code. The content of your cron job in a bash code chunk. The content of your is_in_stock.py script. Solution */5 * * * * /home/kamstut/is_in_stock.py #!/usr/bin/env python3 def send_purdue_email(my_purdue_email, my_password, to): import smtplib, ssl port = 587 # For starttls smtp_server = &quot;smtp.purdue.edu&quot; sender_email = my_purdue_email receiver_email = to password = my_password message = &quot;&quot;&quot;\\ Subject: Test subject This is the email body.&quot;&quot;&quot; context = ssl.create_default_context() with smtplib.SMTP(smtp_server, port) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(sender_email, password) server.sendmail(sender_email, receiver_email, message) def main(): # scrape element from question 3 # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() element = driver.find_element_by_xpath(&quot;//span[@data-color=&#39;Chartreuse&#39;]/parent::div/following-sibling::div/span&quot;) # does the text indicate it is in stock? if element.text == &quot;In stock&quot;: # if yes, send email to yourself telling you it is in stock. send_purdue_email(&quot;kamstut@purdue.edu&quot;, &quot;MYPASSWORD&quot;, &quot;kamstut@purdue.edu&quot;) # otherwise, gracefully end script using the &quot;pass&quot; Python keyword else: pass if __name__ == &quot;__main__&quot;: main() Project 5 Motivation: One of the best things about learning to scrape data is the many applications of the skill that may pop into your mind. In this project, we want to give you some flexibility to explore your own ideas, but at the same time, add a couple of important tools to your tool set. We hope that you've learned a lot in this series, and can think of creative ways to utilize your new skills. Context: This is the last project in a series focused on scraping data. We have created a couple of very common scenarios that can be problematic when first learning to scrape data, and we want to show you how to get around them. Scope: python, web scraping, etc. Learning objectives: Use the requests package to scrape a web page. Use the lxml/selenium package to filter and parse data from a scraped web page. Learn how to step around header-based filtering. Learn how to handle rate limiting. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. It is not uncommon to be blocked from scraping a website. There are a variety of strategies that they use to do this, and in general they work well. In general, if a company wants you to extract information from their website, they will make an API (application programming interface) available for you to use. One method (that is commonly paired with other methods) is blocking your request based on headers. You can read about headers here. In general, you can think of headers as some extra data that gives the server or client context. Here is a list of headers, and some more explanation. Each header has a purpose. One common header is called the User-Agent header. A User-Agent looks something like: User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0 You can see headers if you open the console in Firefox or Chrome and load a website. It will look something like: From the mozilla link, this header is a string that &quot;lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.&quot; Basically, if you are browsing the internet with a common browser, the server will know what you are using. In the provided example, we are using Firefox 86 from Mozilla, on a Mac running Mac OS 10.16 with an Intel processor. When we send a request from a package like requests in Python, here is what the headers look like: import requests response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;) print(response.request.headers) {&#39;User-Agent&#39;: &#39;python-requests/2.25.1&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;} As you can see our User-Agent is python-requests/2.25.1. You will find that many websites block requests made from anything such user agents. One such website is: https://project5-headers.tdm.wiki. Scrape https://project5-headers.tdm.wiki from Scholar and explain what happens. What is the response code, and what does that response code mean? Can you ascertain what you would be seeing (more or less) in a browser based on the text of the response (the actual HTML)? Read this section of the documentation for the headers package, and attempt to &quot;trick&quot; https://project5-headers.tdm.wiki into presenting you with the desired information. The desired information should look something like: Hostname: c1de5faf1daa IP: 127.0.0.1 IP: 172.18.0.4 RemoteAddr: 172.18.0.2:34520 GET / HTTP/1.1 Host: project5-headers.tdm.wiki User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8 Accept-Encoding: gzip Accept-Language: en-US,en;q=0.5 Cdn-Loop: cloudflare Cf-Connecting-Ip: 107.201.65.5 Cf-Ipcountry: US Cf-Ray: 62289b90aa55f975-EWR Cf-Request-Id: 084d3f8e740000f975e0038000000001 Cf-Visitor: {&quot;scheme&quot;:&quot;https&quot;} Cookie: __cfduid=d9df5daa57fae5a4e425173aaaaacbfc91613136177 Dnt: 1 Sec-Gpc: 1 Upgrade-Insecure-Requests: 1 X-Forwarded-For: 123.123.123.123 X-Forwarded-Host: project5-headers.tdm.wiki X-Forwarded-Port: 443 X-Forwarded-Proto: https X-Forwarded-Server: 6afe64faffaf X-Real-Ip: 123.123.123.123 Click here for video Relevant topics: requests Item(s) to submit: Python code used to solve the problem. Response code received (a number), and an explanation of what that HTTP response code means. What you would (probably) be seeing in a browser if you were blocked. Python code used to &quot;trick&quot; the website into being scraped. The content of the successfully scraped site. Solution import requests response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;) print(response.status_code) # 403 # 403 is an https response code that means the content is forbidden # based on the HTML it looks like we&#39;d be presented with a CAPTCHA print(response.text) # to fix this, let&#39;s change our User-Agent header response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;, headers={&quot;User-Agent&quot;: &quot;anything&quot;}) print(response.text) # or even better, would be to &quot;fake&quot; a browser response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;, headers={&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0&quot;}) print(response.text) 2. Open a browser and navigate to: https://project5-rate-limit.tdm.wiki/. While at first glance, it will seem identical to https://project5-headers.tdm.wiki/, it is not. https://project5-rate-limit.tdm.wiki/ is rate limited based on IP address. Depending on when you are completing this project, this may or may not be obvious. If you refresh your browser fast enough, instead of receiving a bunch of information, you will receive text that says &quot;Too Many Requests&quot;. The following function tries to scrape the Cf-Request-Id header which will have a unique value each request: import requests import lxml.html def scrape_cf_request_id(url): resp = requests.get(url) tree = lxml.html.fromstring(resp.text) content = tree.xpath(&quot;//p&quot;)[0].text.split(&#39;\\n&#39;) cfid = [l for l in content if &#39;Cf-Request-Id&#39; in l][0].split()[1] return cfid You can test it out: scrape_cf_request_id(&quot;https://project5-rate-limit.tdm.wiki&quot;) Write code to scrape 10 unique Cf-Request-Ids (in a loop), and save them to a list called my_ids. What happens when you run the code? This is caused by our expected text not being present. Instead text with &quot;Too Many Requests&quot; is. While normally this error would be something that makes more sense, like an HTTPError or a Timeout Exception, it could be anything, depending on your code. One solution that might come to mind is to &quot;wait&quot; between each loop using time.sleep(). While yes, this may work, it is not a robust solution. Other users from your IP address may count towards your rate limit and cause your function to fail, the amount of sleep time may change dynamically, or even be manually adjusted to be longer, etc. The best way to handle this is to used something called exponential backoff. In a nutshell, exponential backoff is a way to increase the wait time (exponentially) until an acceptable rate is found. backoff is an excellent package to do just that. backoff, upon being triggered from a specified error or exception, will wait to &quot;try again&quot; until a certain amount of time has passed. Upon receving the same error or exception, the time to wait will increase exponentially. Use backoff to modify the provided scrape_cf_request_id function to use exponential backoff when the we alluded to occurs. Test out the modified function in a loop and print the resulting 10 Cf-Request-Ids. Click here for video Note: backoff utilizes decorators. For those interested in learning about decorators, this is an excellent article. Relevant topics: requests Item(s) to submit: Python code used to solve the problem. What happens when you run the function 10 times in a row? Fixed code that will work regardless of the rate limiting. 10 unique Cf-Request-Ids printed. Solution import requests import lxml.html # function to scrape the Cf-Request-Id def scrape_cf_request_id(url): resp = requests.get(url) tree = lxml.html.fromstring(resp.text) content = tree.xpath(&quot;//p&quot;)[0].text.split(&#39;\\n&#39;) cfid = [l for l in content if &#39;Cf-Request-Id&#39; in l][0].split()[1] print(cfid) my_list = [] for i in range(10): my_list.append(scrape_cf_request_id(&quot;https://project5-rate-limit.tdm.wiki&quot;)) This will cause an IndexError because the expected content containing the Cf-Request-Id is not present, instead &quot;Too Many Requests&quot; is. import backoff from lxml import etree @backoff.on_exception(backoff.expo, IndexError) def scrape_cf_request_id(url): resp = requests.get(url, headers={&quot;User-Agent&quot;: &quot;something&quot;}) tree = lxml.html.fromstring(resp.text) content = tree.xpath(&quot;//p&quot;)[0].text.split(&#39;\\n&#39;) cfid = [l for l in content if &#39;Cf-Request-Id&#39; in l][0].split()[1] print(cfid) my_list = [] for i in range(10): my_list.append(scrape_cf_request_id(&quot;https://project5-rate-limit.tdm.wiki&quot;)) print(my_list) 3. You now have a great set of tools to be able to scrape pretty much anything you want from the internet. Now all that is left to do is practice. Find a course appropriate website containing data you would like to scrape. Utilize the tools you've learned about to scrape at least 100 &quot;units&quot; of data. A &quot;unit&quot; is just a representation of what you are scraping. For example, a unit could be a tweet from Twitter, a basketball player's statistics from sportsreference, a product from Amazon, a blog post from your favorite blogger, etc. The hard requirements are: Documented code with thorough comments explaining what the code does. At least 100 &quot;units&quot; scraped. The data must be from multiple web pages. Write at least 1 function (with a docstring) to help you scrape. A clear explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example a head of a pandas dataframe containing the data). Item(s) to submit: Python code that scrapes 100 unites of data (with thorough comments explaining what the code does). The data must be from more than a single web page. 1 or more functions (with docstrings) used to help you scrape/parse data. Clear documentation and explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example using the head of a dataframe containing the data). Solution Python code with comments Sample of scraped data (must be from multiple pages) At least 100 &quot;units&quot; of scraped data (for example 1 tweet == 1 unit, 1 product from amazon == 1 unit, etc.) At least 1 helper function with a docstring A paragraph explaining things Project 6 Motivation: Being able to analyze and create good visualizations is a skill that is invaluable in many fields. It can be pretty fun too! In this project, we are going to dive into matplotlib with an open project. Context: We've been working hard all semester and learning a lot about web scraping. In this project we are going to ask you to examine some plots, write a little bit, and use your creative energies to create good visualizations about the flight data using the go-to plotting library for many, matplotlib. In the next project, we will continue to learn about and become comfortable using matplotlib. Scope: python, visualizing data Learning objectives: Demonstrate the ability to create basic graphs with default settings. Demonstrate the ability to modify axes labels and titles. Demonstrate the ability to customize a plot (color, shape/linetype). Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/flights/*.csv (all csv files) Questions 1. Here are the results from the 2009 Data Expo poster competition. The object of the competition was to visualize interesting information from the flights dataset. Examine all 8 posters and write a single sentence for each poster with your first impression(s). An example of an impression that will not get full credit would be: &quot;My first impression is that this poster is bad and doesn't look organized.&quot;. An example of an impression that will get full credit would be: &quot;My first impression is that the author had a good visualization-to-text ratio and it seems easy to follow along.&quot;. Click here for video Item(s) to submit: 8 bullets, each containing a sentence with the first impression of the 8 visualizations. Order should be &quot;first place&quot;, to &quot;honourable mention&quot;, followed by &quot;other posters&quot; in the given order. Or, label which graphic each sentence is about. 2. Creating More Effective Graphs by Dr. Naomi Robbins and The Elements of Graphing Data by Dr. William Cleveland at Purdue University, are two excellent books about data visualization. Read the following excerpts from the books (respectively), and list 2 things you learned, or found interesting from each book. Excerpt 1 Excerpt 2 Click here for video Item(s) to submit: Two bullets for each book with items you learned or found interesting. 3. Of the 7 posters with at least 3 plots and/or maps, choose 1 poster that you think you could improve upon or &quot;out plot&quot;. Create 4 plots/maps that either: Improve upon a plot from the poster you chose, or Show a completely different plot that does a good job of getting an idea or observation across, or Ruin a plot. Purposefully break the best practices you've learned about in order to make the visualization misleading. (limited to 1 of the 4 plots) For each plot/map where you choose to do (1), include 1-2 sentences explaining what exactly you improved upon and how. Point out some of the best practices from the 2 provided texts that you followed. For each plot/map where you choose to do (2), include 1-2 sentences explaining your graphic and outlining the best practices from the 2 texts that you followed. For each plot/map where you choose to do (3), include 1-2 sentences explaining what you changed, what principle it broke, and how it made the plot misleading or worse. While we are not asking you to create a poster, please use RMarkdown to keep your plots, code, and text nicely formatted and organized. The more like a story your project reads, the better. In this project, we are restricting you to use matplotlib in Python. While there are many interesting plotting packages like plotly and plotnine, we really want you to take the time to dig into matplotlib and learn as much as you can. Click here for video Item(s) to submit: All associated Python code you used to wrangling the data and create your graphics. 4 plots, with at least 4 associated RMarkdown code chunks. 1-2 sentences per plot explaining what exactly you improved upon, what best practices from the texts you used, and how. If it is a brand new visualization, describe and explain your graphic, outlining the best practices from the 2 texts that you followed. If it is the ruined plot you chose, explain what you changed, what principle it broke, and how it made the plot misleading or worse. 4. Now that you've been exploring data visualization, copy, paste, and update your first impressions from question (1) with your updated impressions. Which impression changed the most, and why? Click here for video Item(s) to submit: 8 bullets with updated impressions (still just a sentence or two) from question (1). A sentence explaining which impression changed the most and why. Project 7 Motivation: Being able to analyze and create good visualizations is a skill that is invaluable in many fields. It can be pretty fun too! As you probably noticed in the previous project, matplotlib can be finicky -- certain types of plots are really easy to create, while others are not. For example, you would think changing the color of a boxplot would be easy to do in matplotlib, perhaps we just need to add an option to the function call. As it turns out, this isn't so straightforward (as illustrated at the end of this section). Occasionally this will happen and that is when packages like seaborn or plotnine (both are packages built using matplotlib) can be good. In this project we will explore this a little bit, and learn about some useful pandas functions to help shape your data in a format that any given package requires. Context: In the next project, we will continue to learn about and become comfortable using matplotlib, seaborn, and plotnine. Scope: python, visualizing data Learning objectives: Demonstrate the ability to create basic graphs with default settings. Demonstrate the ability to modify axes labels and titles. Demonstrate the ability to customize a plot (color, shape/linetype). Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/apple/health/watch_dump.xml Questions 1. In an earlier project we explored some XML data in the form of an Apple Watch data dump. Most health-related apps give you some sort of graph or set of graphs as an output. Use any package you want to parse the XML data. There are a lot of Records in this dataset. Each Record has an attribute called creationDate. Create a barplot of the number of Records per day. Make sure your plot is polished, containing proper labels and good colors. Hint: You could start by parsing out the required data into a pandas dataframe or series. Hint: The groupby method is one of the most useful pandas methods. It allows you to quickly perform operations on groups of data. Click here for video Click here for another video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code (including the graphic). 2. The plot in question 1 should look bimodal. Let's focus only on the first apparent group of readings. Create a new dataframe containing only the readings for the time period from 9/1/2017 to 5/31/2019. How many Records are there in that time period? Click here for video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code. 3. It is hard to discern weekly patterns (if any) based on the graphics created so far. For the period of time in question 2, create a labeled bar plot for the count of Records by day of the week. What (if any) discernable patterns are there? Make sure to include the labels provided below: labels = [&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;] Click here for video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code (including the graphic). 4. Create a pandas dataframe containing the following data from watch_dump.xml: A column called bpm with the bpm (beats per minute) of the InstantaneousBeatsPerMinute. A column called time with the time of each individual bpm reading in InstantaneousBeatsPerMinute. A column called date with the date. A column called dayofweek with the day of the week. Hint: You may want to use pd.to_numeric to convert the bpm column to a numeric type. Hint: This is one way to convert the numbers 0-6 to days of the week: myDF[&#39;dayofweek&#39;] = myDF[&#39;dayofweek&#39;].map({0:&quot;Mon&quot;, 1:&quot;Tue&quot;, 2:&quot;Wed&quot;, 3:&quot;Thu&quot;, 4:&quot;Fri&quot;, 5: &quot;Sat&quot;, 6: &quot;Sun&quot;}) Click here for video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code. 5. Create a heatmap using seaborn, where the y-axis shows the day of the week (&quot;Mon&quot; - &quot;Sun&quot;), the x-axis shows the hour, and the values on the interior of the plot are the average bpm by hour by day of the week. Click here for video Relevant topics: lxml, groupby, pivot Item(s) to submit: Python code used to solve the problem. Output from running your code (including the graphic). Project 8 Motivation: Python is an interpreted language (as opposed to a compiled language). In a compiled language, you are (mostly) unable to run and evaluate a single instruction at a time. In Python (and R -- also an interpreted language), we can run and evaluate a line of code easily using a repl. In fact, this is the way you've been using Python to date -- selecting and running pieces of Python code. Other ways to use Python include creating a package (like numpy, pandas, and pytorch), and creating scripts. You can create powerful CLI's (command line interface) tools using Python. In this project, we will explore this in detail and learn how to create scripts that accept options and input and perform tasks. Context: This is the first (of two) projects where we will learn about creating and using Python scripts. Scope: python Learning objectives: Write a python script that accepts user inputs and returns something useful. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. Often times the deliverable part of a project isn’t custom built packages or modules, but a script. A script is a .py file with python code written inside to perform action(s). Python scripts are incredibly easy to run, for example, if you had a python script called question01.py, you could run it by opening a terminal and typing: python3 /path/to/question01.py The python interpreter then looks for the scripts entrypoint, and starts executing. You should read this article about the main function and python scripts. In addition, read this section, paying special attention to the shebang. Create a Python script called question01.py in your $HOME directory. Use the second shebang from the article: #!/usr/bin/env python3. When run, question01.py should use the sys package to print the location of the interpreter being used to run the script. For example, if we started a Python interpreter in RStudio using the following code: datamine_py() reticulate::repl_python() Then, we could print the interpreter by running the following Python code one line at a time: import sys print(sys.executable) Since we are using our Python environment, you should see this result: /class/datamine/apps/python/f2020-s2021/env/bin/python3. This is the fully qualified path of the Python interpreter we've been using for this course. Restart your R session by clicking Session &gt; Restart R, navigate to the &quot;Terminal&quot; tab in RStudio, and run the following lines in the terminal. What is the output? # this command gives execute permissions to your script -- this only needs to be run once chmod +x $HOME/question01.py # execute your script $HOME/question01.py Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Click here for another video Relevant topics: shebang Item(s) to submit: The entire question01.py script's contents in a Python code chunk with chunk option &quot;eval=F&quot;. Output from running your code copy and pasted as text. 2. Was your output in question (1) expected? Why or why not? When we restarted the R session, our datamine_py's effects were reversed, and the default Python interpreter is no longer our default when running python3. It is very common to have a multitude of Python environments available to use. But, when we are running a Python script it is not convenient to have to run various commands (in our case, the single datamine_py command) in order to get our script to run the way we want it to run. In addition, if our script used a set of packages that were not installed outside of our course environment, the script would fail. In this project, since our focus is more on how to write scripts and make them work as expected, we will have some fun and experiment with some pre-trained state of the art machine learning models. The following function accepts a string called sentence as an input and returns the sentiment of the sentence, &quot;POSITIVE&quot; or &quot;NEGATIVE&quot;. from transformers import pipeline def get_sentiment(model, sentence: str) -&gt; str: result = model(sentence) return result[0].get(&#39;label&#39;) model = pipeline(&#39;sentiment-analysis&#39;) print(get_sentiment(model, &#39;This is really great!&#39;)) print(get_sentiment(model, &#39;Oh no! Thats horrible!&#39;)) Include get_sentiment (including the import statement) in a new script, question02.py script. Note that you do not have to use get_sentiment anywhere, just include it for now. Go to the terminal in RStudio and execute your script. What happens? Remember, since our current shebang is #!/usr/bin/env python3, if our script uses one or more packages that are not installed in the current environment environment, the script will fail. This is what is happening. The transformers package that we use is not installed in the current environment. We do, however, have an environment that does have it installed, and it is located on Scholar at: /class/datamine/apps/python/pytorch2021/env/bin/python. Update the script's shebang and try to run it again. Does it work now? Explanation: Depending on the state of your current environment, the original shebang, #!/usr/bin/env python3 will use the same Python interpreter and environment that is currently set to python3 (run which python3 to see). If you haven't run datamine_py, this will be something like: /apps/spack/scholar/fall20/apps/anaconda/2020.11-py38-gcc-4.8.5-djkvkvk/bin/python or /usr/bin/python, if you have run datamine_py, this will be: /class/datamine/apps/python/f2020-s2021/env/bin/python. Both environments lack the transformers package. Our other environment whose interpreter lives here: /class/datamine/apps/python/pytorch2021/env/bin/python does have this package. The shebang is then critically important for any scripts that want to utilize packages from a specific environment. Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Relevant topics: writing scripts Item(s) to submit: Sentence explaining why or why not the output from question (1) was expected. Sentence explaining what happens when you include get_sentiment in your script and try to execute it. The entirety of the updated (working) script's content in a Python code chunk with chunk option &quot;eval=F&quot;. 3. Okay, great. We now understand that if we want to use packages from a specific environment, we need to modify our shebang accordingly. As it currently stands, our script is pretty useless. Modify the script, in a new script called question03.py to accept a single argument. This argument should be a sentence. Your script should then print the sentence, and whether or not the sentence is &quot;POSITIVE&quot; or &quot;NEGATIVE&quot;. Use sys.argv to accomplish this. Make sure the script functions in the following way: $HOME/question03.py This is a happy sentence, yay! Too many arguments. $HOME/question03.py &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE $HOME/question03.py ./question03.py requires at least 1 argument, &quot;sentence&quot;. Hint: One really useful way to exit the script and print a message is like this: import sys sys.exit(f&quot;{__file__} requires at least 1 argument, &#39;sentence&#39;&quot;) Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Click here for another video Relevant topics: writing scripts Item(s) to submit: The entirety of the updated (working) script's content in a Python code chunk with chunk option &quot;eval=F&quot;. Output from running your script with the given examples. 4. If you look at the man pages for a command line tool like awk or grep (you can get these by running man awk or man grep in the terminal), you will see that typically CLI's have a variety of options. Options usually follow the following format: grep -i &#39;ok&#39; some_file.txt However, often times you have 2 ways you can use an option -- either with the short form (for example -i), or long form (for example -i is the same as --ignore-case). Sometimes options can get values. If options don't have values, you can assume that the presence of the flag means TRUE and the lack means FALSE. When using short form, the value for the option is separated by a space (for example grep -f my_file.txt). When using long form, the value for the option is separated by an equals sign (for example grep --file=my_file.txt). Modify your script (as a new question04.py) to include an option called score. When active (question04.py --score or question04.py -s), the script should return both the sentiment, &quot;POSITIVE&quot; or &quot;NEGATIVE&quot; and the probability of being accurate. Make sure that you modify your checks from question 3 to continue to work whenever we use --score or -s. Some examples below: $HOME/question04.py &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE $HOME/question04.py --score &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py -s &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py &#39;This is a happy sentence, yay!&#39; -s Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py &#39;This is a happy sentence, yay!&#39; --score Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py &#39;This is a happy sentence, yay!&#39; --value Unknown option(s): [&#39;--value&#39;] $HOME/question04.py &#39;This is a happy sentence, yay!&#39; --value --score Too many arguments. $HOME/question04.py question04.py requires at least 1 argument, &quot;sentence&quot; $HOME/question04.py --score ./question04.py requires at least 1 argument, &quot;sentence&quot;. No sentence provided. $HOME/question04.py &#39;This is one sentence&#39; &#39;This is another&#39; ./question04.py requires only 1 sentence, but 2 were provided. Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Hint: Experiment with the provided function. You will find the probability of being accurate is already returned by the model. Click here for video Click here for another video Relevant topics: writing scripts Item(s) to submit: The entirety of the updated (working) script's content in a Python code chunk with chunk option &quot;eval=F&quot;. Output from running your script with the given examples. 5. Wow, that is an extensive amount of logic for for a single option. Luckily, Python has the argparse package to help you build CLI's and handle situations like this. You can find the documentation for argparse here and a nice little tutorial here. Update your script (as a new question05.py) using argparse instead of custom logic. Specifically, add 1 positional argument called &quot;sentence&quot;, and 1 optional argument &quot;--score&quot; or &quot;-s&quot;. You should handle the following scenarios: $HOME/question05.py &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE $HOME/question05.py --score &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py -s &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py &#39;This is a happy sentence, yay!&#39; -s Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py &#39;This is a happy sentence, yay!&#39; --score Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py &#39;This is a happy sentence, yay!&#39; --value usage: question05.py [-h] [-s] sentence question05.py: error: unrecognized arguments: --value $HOME/question05.py &#39;This is a happy sentence, yay!&#39; --value --score usage: question05.py [-h] [-s] sentence question05.py: error: unrecognized arguments: --value $HOME/question05.py usage: question05.py [-h] [-s] sentence positional arguments: sentence optional arguments: -h, --help show this help message and exit -s, --score display the probability of accuracy $HOME/question05.py --score usage: question05.py [-h] [-s] sentence question05.py: error: too few arguments $HOME/question05.py &#39;This is one sentence&#39; &#39;This is another&#39; usage: question05.py [-h] [-s] sentence question05.py: error: unrecognized arguments: This is another Hint: A good way to print the help information if no arguments are provided is: if len(sys.argv) == 1: parser.print_help() parser.exit() Important note: Include the bash code chunk option error=T to enable RMarkdown to knit and output errors. Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Relevant topics: writing scripts, argparse Item(s) to submit: Python code used to solve the problem. Output from running your code. STAT 39000 Topics The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration. Language Project # Name Topics Python 1 Web scraping: part I xml, lxml, pandas, etc. Python 2 Web scraping: part II requests, functions, xml, loops, if statements, etc. Python 3 Web scraping: part III selenium, lxml, lists, pandas, etc. Python 4 Web scraping: part IV requests, beautifulsoup4, lxml, selenium, xml, cronjobs, loops, etc. Python 5 Web scraping: part V web scraping + related topics Python 6 Plotting in Python: part I ways to plot in Python, more work with pandas, etc. Python 7 Plotting in Python: part II ways to plot in Python, more work with pandas, etc. Python 8 Writing scripts: part I how to write scripts in Python, more work with pandas, matplotlib, etc. Python 9 Writing scripts: part II how to write scripts in Python, argparse, more work with pandas, matplotlib, etc. R 10 ggplot: part I ggplot basics R 11 ggplot: part II more ggplot R 12 tidyverse &amp; data.table: part I data wrangling and computation using tidyverse packages and data.table, maybe some slurm? R 13 tidyverse &amp; data.table: part II data wrangling and computation using tidyverse packages and data.table, maybe some slurm? R 14 tidyverse &amp; data.table: part III data wrangling and computation using tidyverse packages and data.table, maybe some slurm? Project 1 Motivation: Extensible Markup Language or XML is a very important file format for storing structured data. Even though formats like JSON, and csv tend to be more prevalent, many, many legacy systems still use XML, and it remains an appropriate format for storing complex data. In fact, JSON and csv are quickly becoming less relevant as new formats and serialization methods like parquet and protobufs are becoming more common. Context: In previous semesters we've explored XML. In this project we will refresh our skills and, rather than exploring XML in R, we will use the lxml package in Python. This is the first project in a series of 5 projects focused on web scraping in R and Python. Scope: python, XML Learning objectives: Review and summarize the differences between XML and HTML/CSV. Match XML terms to sections of XML demonstrating working knowledge. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/apple/health/watch_dump.xml Resources We realize that it may be a while since you've used Python. That's okay! We are going to be taking things at a much more reasonable pace than Spring 2020. Some potentially useful resources for the semester include: The STAT 19000 projects. We are easing 19000 students into Python and will post solutions each week. It would be well worth 10 minutes to look over the questions and solutions each week. Here is a decent cheat sheet that helps you quickly get an idea of how to do something you know how to do in R, in Python. The Examples Book -- updating daily with more examples and videos. Be sure to click on the &quot;relevant topics&quot; links as we try to point you to topics with examples that should be particularly useful to solve the problems we assign. Questions Important note: It would be well worth your time to read through the xml section of the book, as well as take the time to work through pandas 10 minute intro. 1. A good first step when working with XML is to get an idea how your document is structured. Normally, there should be good documentation that spells this out for you, but it is good to know what to do when you don't have the documentation. Start by finding the &quot;root&quot; node. What is the name of the root node of the provided dataset? Hint: Make sure to import the lxml package first: from lxml import etree Here are two videos about running Python in RStudio: Click here for video Click here for video and here is a video about XML scraping in Python: Click here for video Relevant topics: lxml, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from lxml import etree tree = etree.parse(&quot;/class/datamine/data/apple/health/watch_dump.xml&quot;) tree.xpath(&quot;/*&quot;)[0].tag 2. Remember, XML can be nested. In question (1) we figured out what the root node was called. What are the names of the next &quot;tier&quot; of elements? Hint: Now that we know the root node, you could use the root node name as a part of your xpath expression. Hint: As you may have noticed in question (1) the xpath method returns a list. Sometimes this list can contain many repeated tag names. Since our goal is to see the names of the second &quot;tier&quot; elements, you could convert the resulting list to a set to quickly see the unique list as set's only contain unique values. Relevant topics: for loops, lxml, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution set([x.tag for x in tree.xpath(&quot;/HealthData/*&quot;)]) 3. Continue to explore each &quot;tier&quot; of data until there isn't any left. Name the &quot;full paths&quot; of all of the &quot;last tier&quot; tags. Hint: Let's say a &quot;last tier&quot; tag is just a path where there are no more nested elements. For example, /HealthData/Workout/WorkoutRoute/FileReference is a &quot;last tier&quot; tag. If you try and get the nested elements for it, they don't exist: tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/FileReference/*&quot;) Hint: Here are 3 of the 7 &quot;full paths&quot;: /HealthData/Workout/WorkoutRoute/FileReference /HealthData/Record/MetadataEntry /HealthData/ActivitySummary Relevant topics: lxml, xml, for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution print(set([x.tag for x in tree.xpath(&quot;/HealthData/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/ActivitySummary/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/HeartRateVariabilityMetadataList/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/MetadataEntry/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutEvent/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutEntry/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/FileReference/*&quot;)])) print(set([x.tag for x in tree.xpath(&quot;/HealthData/Workout/WorkoutRoute/MetadataEntry/*&quot;)])) /HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute /HealthData/Workout/WorkoutRoute/FileReference /HealthData/Workout/WorkoutRoute/MetadataEntry /HealthData/Record/MetadataEntry /HealthData/Workout/WorkoutEvent /HealthData/Workout/WorkoutEntry /HealthData/ActivitySummary 4. At this point in time you may be asking yourself &quot;but where is the data&quot;? Depending on the structure of the XML file, the data could either be between tags like: &lt;some_tag&gt;mydata&lt;/some_tag&gt; Or, it could be in an attribute: &lt;question answer=&quot;tac&quot;&gt;What is cat spelled backwards?&lt;/question&gt; Collect the &quot;ActivitySummary&quot; data, and convert the list of dicts to a pandas DataFrame. The following is an example of converting a list of dicts to a pandas DataFrame called myDF: import pandas as pd list_of_dicts = [] list_of_dicts.append({&#39;columnA&#39;: 1, &#39;columnB&#39;: 2}) list_of_dicts.append({&#39;columnB&#39;: 4, &#39;columnA&#39;: 1}) myDF = pd.DataFrame(list_of_dicts) Hint: It is important to note that an element's &quot;attrib&quot; attribute looks and feels like a dict, but it is actually a lxml.etree._Attrib. If you try to convert a list of lxml.etree._Attrib to a pandas DataFrame, it will not work out as you planned. Make sure to first convert each lxml.etree._Attrib to a dict before converting to a DataFrame. You can do so like: # this will convert a single `lxml.etree._Attrib` to a dict my_dict = dict(my_lxml_etree_attrib) Relevant topics: dicts, lists, lxml, xml, for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution dat = tree.xpath(&quot;/HealthData/ActivitySummary&quot;) list_of_dicts = [] for e in dat: list_of_dicts.append(dict(e.attrib)) myDF = pd.DataFrame(data=list_of_dicts) myDF.sort_values([&#39;activeEnergyBurned&#39;], ascending=False).head() 5. pandas is a Python package that provides the DataFrame and Series classes. A DataFrame is very similar to a data.frame in R and can be used to manipulate the data within very easily. A Series is the class that handles a single column of a DataFrame. Go through the pandas in 10 minutes page from the official documentation. Sort, find, and print the top 5 rows of data based on the &quot;activeEnergyBurned&quot; column. Relevant topics: pandas, dicts, lists, lxml, xml, for loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution # could be anything Project 2 Motivation: Web scraping is is the process of taking content off of the internet. Typically this goes hand-in-hand with parsing or processing the data. Depending on the task at hand, web scraping can be incredibly simple. With that being said, it can quickly become difficult. Typically, students find web scraping fun and empowering. Context: In the previous project we gently introduced XML and xpath expressions. In this project, we will learn about web scraping, scrape data from The New York Times, and parse through our newly scraped data using xpath expressions. Scope: python, web scraping, xml Learning objectives: html Review and summarize the differences between XML and HTML/CSV. Use the requests package to scrape a web page. Use the lxml package to filter and parse data from a scraped web page. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset You will be extracting your own data from online in this project. There is no base dataset. Questions 1. The New York Times is one of the most popular newspapers in the United States. Open a modern browser (preferably Firefox or Chrome), and navigate to https://nytimes.com. By the end of this project you will be able to scrape some data from this website! The first step is to explore the structure of the website. You can either right click and click on &quot;view page source&quot;, which will pull up a page full of HTML used to render the page. Alternatively, if you want to focus on a single element, an article title, for example, right click on the article title and click on &quot;inspect element&quot;. This will pull up an inspector that allows you to see portions of the HTML. Click around the website and explore the HTML however you see fit. Open a few front page articles and notice how most articles start with a bunch of really important information, namely: an article title, summary, picture, picture caption, picture source, author portraits, authors, and article datetime. For example: https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html Copy and paste the h1 element (in its entirety) containing the article title (for the article provided) in an HTML code chunk. Do the same for the same article's summary. Click here for video Relevant topics: html Item(s) to submit: 2 code chunks containing the HTML requested. Solution &lt;h1 id=&quot;link-4686dc8b&quot; class=&quot;css-rsa88z e1h9rw200&quot; data-test-id=&quot;headline&quot;&gt;U.S. Says China’s Repression of Uighurs Is ‘Genocide’&lt;/h1&gt; &lt;p id=&quot;article-summary&quot; class=&quot;css-w6ymp8 e1wiw3jv0&quot;&gt;The finding by the Trump administration is the strongest denunciation by any government of China’s actions and follows a Biden campaign statement with the same declaration.&lt;/p&gt; 2. In question (1) we copied two elements of an article. When scraping data from a website, it is important to continually consider the patterns in the structure. Specifically, it is important to consider whether or not the defining characteristics you use to parse the scraped data will continue to be in the same format for new data. What do I mean by defining characterstic? I mean some combination of tag, attribute, and content from which you can isolate the data of interest. For example, given a link to a new nytimes article, do you think you could isolate the article title by using the id=&quot;link-4686dc8b&quot; attribute of the h1 tag? Maybe, or maybe not, but it sure seems like &quot;link-4686dc8b&quot; might be unique to the article and not able to be used given a new article. Write an xpath expression to isolate the article title, and another xpath expression to isolate the article summary. Important note: You do not need to test your xpath expression yet, we will be doing that shortly. Relevant topics: html, xml, xpath expressions Item(s) to submit: Two xpath expressions in an HTML code chunk. Solution //h1[@data-test-id=&quot;headline&quot;] //p[@id=&quot;article-summary&quot;] 3. Use the requests package to scrape the webpage containing our article from questions (1) and (2). Use the lxml.html package and the xpath method to test out your xpath expressions from question (2). Did they work? Print the content of the elements to confirm. Click here for video Relevant topics: html, xml, xpath expressions, lxml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution import lxml.html import requests url = &quot;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&quot; response = requests.get(url, stream=True) response.raw.decode_content = True tree = lxml.html.parse(response.raw) print(tree.xpath(&#39;//p[@id=&quot;article-summary&quot;]&#39;)[0].text) print(tree.xpath(&#39;//h1[@data-test-id=&quot;headline&quot;]&#39;)[0].text) 4. Here are a list of article links from https://nytimes.com: https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html Write a function called get_article_and_summary that accepts a string called link as an argument, and returns both the article title and summary. Test get_article_and_summary out on each of the provided links: title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) Hint: The first line of your function should look like this: def get_article_and_summary(myURL: str) -&gt; (str, str): Click here for video Click here for video Relevant topics: html, xml, xpath expressions, lxml, functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from typing import Tuple import lxml.html import requests def get_article_and_summary(link: str) -&gt; Tuple[str, str]: &quot;&quot;&quot; Given a link to a new york times article, return the article title and summary. Args: link (str): The link to the new york times article. Returns: Tuple[str, str]: A tuple first containing the article title, and then the article summary. &quot;&quot;&quot; # scrape the web page response = requests.get(link, stream=True) response.raw.decode_content = True tree = lxml.html.parse(response.raw) title = tree.xpath(&#39;//p[@id=&quot;article-summary&quot;]&#39;)[0].text summary = tree.xpath(&#39;//h1[@data-test-id=&quot;headline&quot;]&#39;)[0].text return title, summary title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) title, summary = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html&#39;) print(f&#39;Title: {title}, Summary: {summary}&#39;) 5. In question (1) we mentioned a myriad of other important information given at the top of most New York Times articles. Choose two other listed pieces of information and copy, paste, and update your solution to question (4) to scrape and return those chosen pieces of information. Important note: If you choose to scrape non-textual data, be sure to return data of an appropriate type. For example, if you choose to scrape one of the images, either print the image or return a PIL object. Relevant topics: html, xml, xpath expressions, lxml, functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from typing import Tuple import lxml.html import requests import io from PIL import Image from IPython.display import display def get_article_and_summary(link: str) -&gt; Tuple[str, str]: &quot;&quot;&quot; Given a link to a new york times article, return the article title and summary. Args: link (str): The link to the new york times article. Returns: Tuple[str, str]: A tuple first containing the article title, and then the article summary. &quot;&quot;&quot; # scrape the web page response = requests.get(link, stream=True) response.raw.decode_content = True tree = lxml.html.parse(response.raw) # parse out the title title = tree.xpath(&#39;//p[@id=&quot;article-summary&quot;]&#39;)[0].text # parse out the summary summary = tree.xpath(&#39;//h1[@data-test-id=&quot;headline&quot;]&#39;)[0].text # parse out the url to the image photo_src = tree.xpath(&#39;//picture/img&#39;)[0].attrib.get(&quot;src&quot;) # scrape image photo_content = requests.get(photo_src).content # convert image format photo_file = io.BytesIO(photo_content) photo = Image.open(photo_file).convert(&#39;RGB&#39;) # parse out photo caption caption = tree.xpath(&#39;//figcaption/span&#39;)[0].text # parse out the photo credits credits = tree.xpath(&#39;//figcaption/span/span[contains(text(), &quot;Credit&quot;)]/following-sibling::span/span&#39;) credits_list = [] for c in credits: credits_list.append(c.text) # parse author photo url # only gets if &quot;author&quot; in photo src attribute photo_src_elements = tree.xpath(&#39;//img[contains(@src, &quot;author&quot;)]&#39;) # if &quot;author&quot; in photo src attribute photo_srcs = [] for p in photo_src_elements: photo_srcs.append(p.attrib.get(&quot;src&quot;)) # scrape image author_images = [] for img in photo_srcs: photo_content = requests.get(img).content # convert image format photo_file = io.BytesIO(photo_content) photo = Image.open(photo_file).convert(&#39;RGB&#39;) author_images.append(photo) # parse out authors authors_elements = tree.xpath(&quot;//span[@class=&#39;byline-prefix&#39;]/following-sibling::a/span&quot;) authors = [] for a in authors_elements: authors.append(a.text) # parse out article publish date/time dt = tree.xpath(&quot;//time&quot;)[0].attrib.get(&quot;datetime&quot;) return title, summary, photo, caption, credits, author_images, authors, dt title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html&#39;) print(f&#39;Title:\\n{title}, Summary:\\n{summary}, Caption:\\n{caption}&#39;) title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html&#39;) print(f&#39;Title:\\n{title}, Summary:\\n{summary}, Caption:\\n{caption}&#39;) title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary(&#39;https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html&#39;) print(f&#39;Title:\\n{title}, Summary:\\n{summary}, Caption:\\n{caption}&#39;) Project 3 Motivation: Web scraping takes practice, and it is important to work through a variety of common tasks in order to know how to handle those tasks when you next run into them. In this project, we will use a variety of scraping tools in order to scrape data from https://trulia.com. Context: In the previous project, we got our first taste at actually scraping data from a website, and using a parser to extract the information we were interested in. In this project, we will introduce some tasks that will require you to use a tool that let's you interact with a browser, selenium. Scope: python, web scraping, selenium Learning objectives: Review and summarize the differences between XML and HTML/CSV. Use the requests package to scrape a web page. Use the lxml package to filter and parse data from a scraped web page. Use selenium to interact with a browser in order to get a web page to a desired state for scraping. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. Visit https://trulia.com. Many websites have a similar interface, i.e. a bold and centered search bar for a user to interact with. Using selenium write Python code that that first finds the input element, and then types &quot;West Lafayette, IN&quot; followed by an emulated &quot;Enter/Return&quot;. Confirm you code works by printing the url after that process completes. Hint: You will want to use time.sleep to pause a bit after the search so the updated url is returned. Click here for video That video is already relevant for Question 2 too. Relevant topics: selenium, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.firefox.options import Options from selenium.common.exceptions import NoSuchElementException import time firefox_options = Options() firefox_options.add_argument(&quot;window-size=1920,1080&quot;) firefox_options.add_argument(&quot;--headless&quot;) # Headless mode means no GUI firefox_options.add_argument(&quot;start-maximized&quot;) firefox_options.add_argument(&quot;disable-infobars&quot;) firefox_options.add_argument(&quot;--disable-extensions&quot;) firefox_options.add_argument(&quot;--no-sandbox&quot;) firefox_options.add_argument(&quot;--disable-dev-shm-usage&quot;) firefox_options.binary_location = &#39;/class/datamine/apps/firefox/firefox&#39; driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) url = &#39;https://www.trulia.com&#39; driver.get(url) search_input = driver.find_element_by_id(&quot;banner-search&quot;) search_input.send_keys(&quot;West Lafayette, IN&quot;) search_input.send_keys(Keys.RETURN) time.sleep(3) print(driver.current_url) 2. Use your code from question (1) to test out the following queries: West Lafayette, IN (City, State) 47906 (Zip) 4505 Kahala Ave, Honolulu, HI 96816 (Full address) If you look closely you will see that there are patterns in the url. For example, the following link would probably bring up homes in Crawfordsville, IN: https://trulia.com/IN/Crawfordsville. With that being said, if you only had a zip code, like 47933, it wouldn't be easy to guess https://www.trulia.com/IN/Crawfordsville/47933/, hence, one reason why the search bar is useful. If you used xpath expressions to complete question (1), instead use a different method to find the input element. If you used a different method, use xpath expressions to complete question (1). Relevant topics: selenium, xml Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) url = &#39;https://www.trulia.com&#39; driver.get(url) search_input = driver.find_element_by_xpath(&quot;//input[@id=&#39;banner-search&#39;]&quot;) search_input.send_keys(&quot;West Lafayette, IN&quot;) search_input.send_keys(Keys.RETURN) time.sleep(3) print(driver.current_url) 3. Let's call the page after a city/state or zipcode search a &quot;sales page&quot;. For example: Use requests to scrape the entire page: https://www.trulia.com/IN/West_Lafayette/47906/. Use lxml.html to parse the page and get all of the img elements that make up the house pictures on the left side of the website. Important note: Make sure you are actually scraping what you think you are scraping! Try printing your html to confirm it has the content you think it should have: import requests response = requests.get(...) print(response.text) Hint: Are you human? Depends. Sometimes if you add a header to your request, it won't ask you if you are human. Let's pretend we are Firefox: import requests my_headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;} response = requests.get(..., headers=my_headers) Okay, after all of that work you may have discovered that only a few images have actually been scraped. If you cycle through all of the img elements and try to print the value of the src attribute, this will be clear: import lxml.html tree = lxml.html.fromstring(response.text) elements = tree.xpath(&quot;//img&quot;) for element in elements: print(element.attrib.get(&quot;src&quot;)) This is because the webpage is not immediately, completely loaded. This is a common website behavior to make things appear faster. If you pay close to when you load https://www.trulia.com/IN/Crawfordsville/47933/, and you quickly scroll down, you will see images still needing to finish rendering all of the way, slowly. What we need to do to fix this, is use selenium (instead of lxml.html) to behave like a human and scroll prior to scraping the page! Try using the following code to slowly scroll down the page before finding the elements: # driver setup and get the url # Needed to get the window size set right and scroll in headless mode myheight = driver.execute_script(&#39;return document.body.scrollHeight&#39;) driver.set_window_size(1080,myheight+100) def scroll(driver, scroll_point): driver.execute_script(f&#39;window.scrollTo(0, {scroll_point});&#39;) time.sleep(5) scroll(driver, myheight*1/4) scroll(driver, myheight*2/4) scroll(driver, myheight*3/4) scroll(driver, myheight*4/4) # find_elements_by_* Hint: At the time of writing there should be about 86 links to images of homes. Click here for video Relevant topics: selenium, xml, loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/Users/kamstut/Downloads/geckodriver&#39;) url = &#39;https://www.trulia.com/IN/West_Lafayette/47906/&#39; driver.get(url) # Needed to get the window size set right and scroll in headless mode height = driver.execute_script(&#39;return document.body.scrollHeight&#39;) driver.set_window_size(1080,height+100) def scroll(driver, scroll_point): driver.execute_script(f&#39;window.scrollTo(0, {scroll_point});&#39;) time.sleep(5) scroll(driver, height/4) scroll(driver, height/4*2) scroll(driver, height/4*3) scroll(driver, height) elements = driver.find_elements_by_xpath(&quot;//picture/img&quot;) print(len(elements)) for e in elements: print(e.get_attribute(&quot;src&quot;)) 4. Write a function called avg_house_cost that accepts a zip code as an argument, and returns the average cost of the first page of homes. Now, to make this a more meaningful statistic, filter for &quot;3+&quot; beds and then find the average. Test avg_house_cost out on the zip code 47906 and print the average costs. Important note: Use selenium to &quot;click&quot; on the &quot;3+ beds&quot; filter. Hint: If you get an error that tells you button is not clickable because it is covered by an li element, try clicking on the li element instead. Hint: You will want to wait a solid 10-15 seconds for the sales page to load before trying to select or click on anything. Hint: Your results may end up including prices for &quot;Homes Near &lt;ZIPCODE&gt;&quot;. This is okay. Even better if you manage to remove those results. If you do choose to remove those results, take a look at the data-testid attribute with value search-result-list-container. Perhaps only selecting the children of the first element will get the desired outcome. Hint: You can use the following code to remove the non-numeric text from a string, and then convert to an integer: import re int(re.sub(&quot;[^0-9]&quot;, &quot;&quot;, &quot;removenon45454_numbers$&quot;)) Click here for video Relevant topics: selenium, xml, loops, functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from selenium.webdriver.support.ui import WebDriverWait import re def avg_house_cost(zip: str) -&gt; float: firefox_options = Options() firefox_options.add_argument(&quot;window-size=1920,1080&quot;) # firefox_options.add_argument(&quot;--headless&quot;) # Headless mode means no GUI firefox_options.add_argument(&quot;start-maximized&quot;) firefox_options.add_argument(&quot;disable-infobars&quot;) firefox_options.add_argument(&quot;--disable-extensions&quot;) firefox_options.add_argument(&quot;--no-sandbox&quot;) firefox_options.add_argument(&quot;--disable-dev-shm-usage&quot;) firefox_options.binary_location = &#39;/Applications/Firefox.app/Contents/MacOS/firefox&#39; driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/Users/kamstut/Downloads/geckodriver&#39;) url = &#39;https://www.trulia.com/&#39; driver.get(url) search_input = driver.find_element_by_id(&quot;banner-search&quot;) search_input.send_keys(zip) search_input.send_keys(Keys.RETURN) time.sleep(10) allbed_button = driver.find_element_by_xpath(&quot;//button[@data-testid=&#39;srp-xl-bedrooms-filter-button&#39;]/ancestor::li&quot;) allbed_button.click() time.sleep(2) bed_button = driver.find_element_by_xpath(&quot;//button[contains(text(), &#39;3+&#39;)]&quot;) bed_button.click() time.sleep(3) price_elements = driver.find_elements_by_xpath(&quot;(//div[@data-testid=&#39;search-result-list-container&#39;])[1]//div[@data-testid=&#39;property-price&#39;]&quot;) prices = [int(re.sub(&quot;[^0-9]&quot;, &quot;&quot;, e.text)) for e in price_elements] driver.quit() return sum(prices)/len(prices) avg_house_cost(&#39;47906&#39;) 5. Get creative. Either add an interesting feature to your function from (4), or use matplotlib to generate some sort of accompanying graphic with your output. Make sure to explain what your additions do. Relevant topics: selenium, xml, loops Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution Could be anything. Project 4 Motivation: In this project we will continue to hone your web scraping skills, introduce you to some &quot;gotchas&quot;, and give you a little bit of exposure to a powerful tool called cron. Context: We are in the second to last project focused on web scraping. This project will introduce some supplementary tools that work well with web scraping: cron, sending emails from Python, etc. Scope: python, web scraping, selenium, cron Learning objectives: Review and summarize the differences between XML and HTML/CSV. Use the requests package to scrape a web page. Use the lxml package to filter and parse data from a scraped web page. Use the beautifulsoup4 package to filter and parse data from a scraped web page. Use selenium to interact with a browser in order to get a web page to a desired state for scraping. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. Check out the following website: https://project4.tdm.wiki Use selenium to scrape and print the 6 colors of pants offered. Click here for video for Question 1 Here is a video about the new feature to reset your RStudio session if you make a big mistake or if your session is very slow Hint: You may have to interact with the webpage for certain elements to render. Relevant topics: scraping, selenium, example clicking a button Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.firefox.options import Options from selenium.common.exceptions import NoSuchElementException import time driver.get(&quot;https://project4.tdm.wiki/&quot;) elements = driver.find_elements_by_xpath(&quot;//span[@class=&#39;ui-component-card--color-amount&#39;]&quot;) for element in elements: print(element.text) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() elements = driver.find_elements_by_xpath(&quot;//span[@class=&#39;ui-component-card--color-amount&#39;]&quot;) for element in elements: print(element.text) driver.quit() 2. Websites are updated frequently. You can imagine a scenario where a change in a website is a sign that there is more data available, or that something of note has happened. This is a fake website designed to help students emulate real changes to a website. Specifically, there is one part of the website that has two possible states (let's say, state A and state B). Upon refreshing the website, or scraping the website again, there is an \\(x\\%\\) chance that the website will be in state A and a \\(1-x\\%\\) chance the website will be in state B. Describe the two states (the thing (element or set of elements) that changes as you refresh the page), and scrape the website enough to estimate \\(x\\). Click here for video for Questions 2 and 3 Hint: You will need to interact with the website to &quot;see&quot; the change. Hint: Since we are just asking about a state, and not any specific element, you could use the page_source attribute of the selenium driver to scrape the entire page instead of trying to use xpath expressions to find a specific element. Hint: Your estimate of \\(x\\) does not need to be perfect. Relevant topics: scraping, selenium, example clicking a button Item(s) to submit: Python code used to solve the problem. Output from running your code. What state A and B represent. An estimate for x. Solution 25% in stock 75% out of stock -- anything remotely close give full credit # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() stateA = driver.page_source stateAct = 0 stateBct = 0 for i in range(10): driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() if driver.page_source == stateA: stateAct += 1 else: stateBct += 1 driver.quit() print(f&quot;State A: {stateAct}&quot;) print(f&quot;State B: {stateBct}&quot;) 3. Dig into the changing &quot;thing&quot; from question (2). What specifically is changing? Use selenium and xpath expressions to scrape and print the content. What are the two possible values for the content? Click here for video (same as above) for Questions 2 and 3 Hint: Due to the changes that occur when a button is clicked, I'd highly advice you to use the data-color attribute in your xpath expression instead of contains(text(), 'blahblah'). Hint: parent:: and following-sibling:: may be useful xpath axes to use. Relevant topics: scraping, selenium, example using following-sibling:: Item(s) to submit: Python code used to solve the problem. Output from running your code. Solution # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() element = driver.find_element_by_xpath(&quot;//span[@data-color=&#39;Chartreuse&#39;]/parent::div/following-sibling::div/span&quot;) print(element.text) driver.quit() 4. The following code allows you to send an email using Python from your Purdue email account. Replace the username and password with your own information and send a test email to yourself to ensure that it works. Click here for video for Questions 4 and 5 Important note: Do NOT include your password in your homework submission. Any time you need to type your password in you final submission just put something like &quot;SUPERSECRETPASSWORD&quot; or &quot;MYPASSWORD&quot;. Hint: To include an image (or screenshot) in RMarkdown, try ![](./my_image.png) where my_image.png is inside the same folder as your .Rmd file. Hint: The spacing and tabs near the message variable are very important. Make sure to copy the code exactly. Otherwise, your subject may not end up in the subject of your email, or the email could end up being blank when sent. Hint: Questions 4 and 5 were inspired by examples and borrowed from the code found at the Real Python website. def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message): import smtplib, ssl from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart message = MIMEMultipart(&quot;alternative&quot;) message[&quot;Subject&quot;] = my_subject message[&quot;From&quot;] = my_purdue_email message[&quot;To&quot;] = to # Create the plain-text and HTML version of your message text = f&#39;&#39;&#39;\\ Subject: {my_subject} To: {to} From: {my_purdue_email} {my_message}&#39;&#39;&#39; html = f&#39;&#39;&#39;\\ &lt;html&gt; &lt;body&gt; {my_message} &lt;/body&gt; &lt;/html&gt; &#39;&#39;&#39; # Turn these into plain/html MIMEText objects part1 = MIMEText(text, &quot;plain&quot;) part2 = MIMEText(html, &quot;html&quot;) # Add HTML/plain-text parts to MIMEMultipart message # The email client will try to render the last part first message.attach(part1) message.attach(part2) context = ssl.create_default_context() with smtplib.SMTP(&quot;smtp.purdue.edu&quot;, 587) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(my_purdue_email, my_password) server.sendmail(my_purdue_email, to, message.as_string()) # this sends an email from kamstut@purdue.edu to mdw@purdue.edu # replace supersecretpassword with your own password # do NOT include your password in your homework submission. send_purdue_email(&quot;kamstut@purdue.edu&quot;, &quot;supersecretpassword&quot;, &quot;mdw@purdue.edu&quot;, &quot;put subject here&quot;, &quot;put message body here&quot;) Relevant topics: functions Item(s) to submit: Python code used to solve the problem. Output from running your code. Screenshot showing your received the email. Solution import smtplib, ssl port = 587 # For starttls smtp_server = &quot;smtp.purdue.edu&quot; sender_email = &quot;kamstut@purdue.edu&quot; receiver_email = &quot;kamstut@purdue.edu&quot; password = &quot;MYPASSWORD&quot; message = &quot;&quot;&quot;\\ Subject: Hi there This message is sent from Python.&quot;&quot;&quot; context = ssl.create_default_context() with smtplib.SMTP(smtp_server, port) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(sender_email, password) server.sendmail(sender_email, receiver_email, message) 5. The following is the content of a new Python script called is_in_stock.py: def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message): import smtplib, ssl from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart message = MIMEMultipart(&quot;alternative&quot;) message[&quot;Subject&quot;] = my_subject message[&quot;From&quot;] = my_purdue_email message[&quot;To&quot;] = to # Create the plain-text and HTML version of your message text = f&#39;&#39;&#39;\\ Subject: {my_subject} To: {to} From: {my_purdue_email} {my_message}&#39;&#39;&#39; html = f&#39;&#39;&#39;\\ &lt;html&gt; &lt;body&gt; {my_message} &lt;/body&gt; &lt;/html&gt; &#39;&#39;&#39; # Turn these into plain/html MIMEText objects part1 = MIMEText(text, &quot;plain&quot;) part2 = MIMEText(html, &quot;html&quot;) # Add HTML/plain-text parts to MIMEMultipart message # The email client will try to render the last part first message.attach(part1) message.attach(part2) context = ssl.create_default_context() with smtplib.SMTP(&quot;smtp.purdue.edu&quot;, 587) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(my_purdue_email, my_password) server.sendmail(my_purdue_email, to, message.as_string()) def main(): # scrape element from question 3 # does the text indicate it is in stock? # if yes, send email to yourself telling you it is in stock. # otherwise, gracefully end script using the &quot;pass&quot; Python keyword if __name__ == &quot;__main__&quot;: main() First, make a copy of the script in your $HOME directory: cp /class/datamine/data/scraping/is_in_stock.py $HOME/is_in_stock.py If you now look in the &quot;Files&quot; tab in the lower right hand corner of RStudio, and click the refresh button, you should see the file is_in_stock.py. You can open and modify this file directly in RStudio. Before you do so, however, change the permissions of the $HOME/is_in_stock.py script so only YOU can read, write, and execute it: chmod 700 $HOME/is_in_stock.py The script should now appear in RStudio, in your home directory, with the correct permissions. Open the script (in RStudio) and fill in the main function as indicated by the comments. We want the script to scrape to see whether the pants from question 3 are in stock or not. A cron job is a task that runs at a certain interval. Create a cron job that runs your script, /class/datamine/apps/python/f2020-s2021/env/bin/python $HOME/is_in_stock.py every 5 minutes. Wait 10-15 minutes and verify that it is working properly. The long path, /class/datamine/apps/python/f2020-s2021/env/bin/python simply makes sure that our script is run with access to all of the packages in our course environment. $HOME/is_in_stock.py is the path to your script ($HOME expands or transforms to /home/&lt;my_purdue_alias&gt;. Click here for video (same as above) for Questions 4 and 5 Click here for a longer video about setting up the cronjob in Question 5 Hint: If you struggle to use the text editor used with the crontab -e command, be sure to continue reading the cron section of the book. We highlight another method that may be easier. Hint: Don't forget to copy your import statements from question (3) as well. Important note: Once you are finished with the project, if you no longer wish to receive emails every so often, follow the instructions here to remove the cron job. Relevant topics: cron, crontab guru Item(s) to submit: Python code used to solve the problem. Output from running your code. The content of your cron job in a bash code chunk. The content of your is_in_stock.py script. Solution */5 * * * * /home/kamstut/is_in_stock.py #!/usr/bin/env python3 def send_purdue_email(my_purdue_email, my_password, to): import smtplib, ssl port = 587 # For starttls smtp_server = &quot;smtp.purdue.edu&quot; sender_email = my_purdue_email receiver_email = to password = my_password message = &quot;&quot;&quot;\\ Subject: Test subject This is the email body.&quot;&quot;&quot; context = ssl.create_default_context() with smtplib.SMTP(smtp_server, port) as server: server.ehlo() # Can be omitted server.starttls(context=context) server.ehlo() # Can be omitted server.login(sender_email, password) server.sendmail(sender_email, receiver_email, message) def main(): # scrape element from question 3 # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() element = driver.find_element_by_xpath(&quot;//span[@data-color=&#39;Chartreuse&#39;]/parent::div/following-sibling::div/span&quot;) # does the text indicate it is in stock? if element.text == &quot;In stock&quot;: # if yes, send email to yourself telling you it is in stock. send_purdue_email(&quot;kamstut@purdue.edu&quot;, &quot;MYPASSWORD&quot;, &quot;kamstut@purdue.edu&quot;) # otherwise, gracefully end script using the &quot;pass&quot; Python keyword else: pass if __name__ == &quot;__main__&quot;: main() 6. Take a look at the byline of each pair of pants (the sentences starting with &quot;Perfect for...&quot;). Inspect the HTML. Try and scrape the text using xpath expressions like you normally would. What happens? Are you able to scrape it? Google around and come up with your best explanation of what is happening. Relevant topics: pseudo elements Item(s) to submit: Python code used to solve the problem. Output from running your code. An explanation of what is happening. Solution If you try and scrape using an xpath expression and the class attribute, you get no text: # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() element = driver.find_element_by_xpath(&quot;//span[@class=&#39;ui-component-card--desc-amount ui-component-card--desc-amount-2&#39;]&quot;) print(element.text) driver.quit() What is happening is pseudo elements are not part of the DOM -- they are for styling (in this case, using &quot;content&quot; is css). To scrape, you'd need to use a script and css: # The state of the Chartreuse pants, sold out (A) or in stock (B) driver = webdriver.Firefox(options=firefox_options, executable_path=&#39;/class/datamine/apps/geckodriver&#39;) driver.get(&quot;https://project4.tdm.wiki/&quot;) # click the &quot;wild colors&quot; button label = driver.find_element_by_xpath(&quot;//label[@for=&#39;ui-component-toggle__wild&#39;]&quot;) label.click() script = &quot;return window.getComputedStyle(document.querySelector(&#39;.ui-component-card--desc-amount.ui-component-card--desc-amount-2&#39;),&#39;:before&#39;).getPropertyValue(&#39;content&#39;)&quot; print(driver.execute_script(script).strip()) driver.quit() Project 5 Motivation: One of the best things about learning to scrape data is the many applications of the skill that may pop into your mind. In this project, we want to give you some flexibility to explore your own ideas, but at the same time, add a couple of important tools to your tool set. We hope that you've learned a lot in this series, and can think of creative ways to utilize your new skills. Context: This is the last project in a series focused on scraping data. We have created a couple of very common scenarios that can be problematic when first learning to scrape data, and we want to show you how to get around them. Scope: python, web scraping, etc. Learning objectives: Use the requests package to scrape a web page. Use the lxml/selenium package to filter and parse data from a scraped web page. Learn how to step around header-based filtering. Learn how to handle rate limiting. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. It is not uncommon to be blocked from scraping a website. There are a variety of strategies that they use to do this, and in general they work well. In general, if a company wants you to extract information from their website, they will make an API (application programming interface) available for you to use. One method (that is commonly paired with other methods) is blocking your request based on headers. You can read about headers here. In general, you can think of headers as some extra data that gives the server or client context. Here is a list of headers, and some more explanation. Each header has a purpose. One common header is called the User-Agent header. A User-Agent looks something like: User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0 You can see headers if you open the console in Firefox or Chrome and load a website. It will look something like: From the mozilla link, this header is a string that &quot;lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.&quot; Basically, if you are browsing the internet with a common browser, the server will know what you are using. In the provided example, we are using Firefox 86 from Mozilla, on a Mac running Mac OS 10.16 with an Intel processor. When we send a request from a package like requests in Python, here is what the headers look like: import requests response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;) print(response.request.headers) {&#39;User-Agent&#39;: &#39;python-requests/2.25.1&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;} As you can see our User-Agent is python-requests/2.25.1. You will find that many websites block requests made from anything such user agents. One such website is: https://project5-headers.tdm.wiki. Scrape https://project5-headers.tdm.wiki from Scholar and explain what happens. What is the response code, and what does that response code mean? Can you ascertain what you would be seeing (more or less) in a browser based on the text of the response (the actual HTML)? Read this section of the documentation for the headers package, and attempt to &quot;trick&quot; https://project5-headers.tdm.wiki into presenting you with the desired information. The desired information should look something like: Hostname: c1de5faf1daa IP: 127.0.0.1 IP: 172.18.0.4 RemoteAddr: 172.18.0.2:34520 GET / HTTP/1.1 Host: project5-headers.tdm.wiki User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8 Accept-Encoding: gzip Accept-Language: en-US,en;q=0.5 Cdn-Loop: cloudflare Cf-Connecting-Ip: 107.201.65.5 Cf-Ipcountry: US Cf-Ray: 62289b90aa55f975-EWR Cf-Request-Id: 084d3f8e740000f975e0038000000001 Cf-Visitor: {&quot;scheme&quot;:&quot;https&quot;} Cookie: __cfduid=d9df5daa57fae5a4e425173aaaaacbfc91613136177 Dnt: 1 Sec-Gpc: 1 Upgrade-Insecure-Requests: 1 X-Forwarded-For: 123.123.123.123 X-Forwarded-Host: project5-headers.tdm.wiki X-Forwarded-Port: 443 X-Forwarded-Proto: https X-Forwarded-Server: 6afe64faffaf X-Real-Ip: 123.123.123.123 Click here for video Relevant topics: requests Item(s) to submit: Python code used to solve the problem. Response code received (a number), and an explanation of what that HTTP response code means. What you would (probably) be seeing in a browser if you were blocked. Python code used to &quot;trick&quot; the website into being scraped. The content of the successfully scraped site. Solution import requests response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;) print(response.status_code) # 403 # 403 is an https response code that means the content is forbidden # based on the HTML it looks like we&#39;d be presented with a CAPTCHA print(response.text) # to fix this, let&#39;s change our User-Agent header response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;, headers={&quot;User-Agent&quot;: &quot;anything&quot;}) print(response.text) # or even better, would be to &quot;fake&quot; a browser response = requests.get(&quot;https://project5-headers.tdm.wiki&quot;, headers={&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0&quot;}) print(response.text) 2. Open a browser and navigate to: https://project5-rate-limit.tdm.wiki/. While at first glance, it will seem identical to https://project5-headers.tdm.wiki/, it is not. https://project5-rate-limit.tdm.wiki/ is rate limited based on IP address. Depending on when you are completing this project, this may or may not be obvious. If you refresh your browser fast enough, instead of receiving a bunch of information, you will receive text that says &quot;Too Many Requests&quot;. The following function tries to scrape the Cf-Request-Id header which will have a unique value each request: import requests import lxml.html def scrape_cf_request_id(url): resp = requests.get(url) tree = lxml.html.fromstring(resp.text) content = tree.xpath(&quot;//p&quot;)[0].text.split(&#39;\\n&#39;) cfid = [l for l in content if &#39;Cf-Request-Id&#39; in l][0].split()[1] return cfid You can test it out: scrape_cf_request_id(&quot;https://project5-rate-limit.tdm.wiki&quot;) Write code to scrape 10 unique Cf-Request-Ids (in a loop), and save them to a list called my_ids. What happens when you run the code? This is caused by our expected text not being present. Instead text with &quot;Too Many Requests&quot; is. While normally this error would be something that makes more sense, like an HTTPError or a Timeout Exception, it could be anything, depending on your code. One solution that might come to mind is to &quot;wait&quot; between each loop using time.sleep(). While yes, this may work, it is not a robust solution. Other users from your IP address may count towards your rate limit and cause your function to fail, the amount of sleep time may change dynamically, or even be manually adjusted to be longer, etc. The best way to handle this is to used something called exponential backoff. In a nutshell, exponential backoff is a way to increase the wait time (exponentially) until an acceptable rate is found. backoff is an excellent package to do just that. backoff, upon being triggered from a specified error or exception, will wait to &quot;try again&quot; until a certain amount of time has passed. Upon receving the same error or exception, the time to wait will increase exponentially. Use backoff to modify the provided scrape_cf_request_id function to use exponential backoff when the we alluded to occurs. Test out the modified function in a loop and print the resulting 10 Cf-Request-Ids. Click here for video Note: backoff utilizes decorators. For those interested in learning about decorators, this is an excellent article. Relevant topics: requests Item(s) to submit: Python code used to solve the problem. What happens when you run the function 10 times in a row? Fixed code that will work regardless of the rate limiting. 10 unique Cf-Request-Ids printed. Solution import requests import lxml.html # function to scrape the Cf-Request-Id def scrape_cf_request_id(url): resp = requests.get(url) tree = lxml.html.fromstring(resp.text) content = tree.xpath(&quot;//p&quot;)[0].text.split(&#39;\\n&#39;) cfid = [l for l in content if &#39;Cf-Request-Id&#39; in l][0].split()[1] print(cfid) my_list = [] for i in range(10): my_list.append(scrape_cf_request_id(&quot;https://project5-rate-limit.tdm.wiki&quot;)) This will cause an IndexError because the expected content containing the Cf-Request-Id is not present, instead &quot;Too Many Requests&quot; is. import backoff from lxml import etree @backoff.on_exception(backoff.expo, IndexError) def scrape_cf_request_id(url): resp = requests.get(url, headers={&quot;User-Agent&quot;: &quot;something&quot;}) tree = lxml.html.fromstring(resp.text) content = tree.xpath(&quot;//p&quot;)[0].text.split(&#39;\\n&#39;) cfid = [l for l in content if &#39;Cf-Request-Id&#39; in l][0].split()[1] print(cfid) my_list = [] for i in range(10): my_list.append(scrape_cf_request_id(&quot;https://project5-rate-limit.tdm.wiki&quot;)) print(my_list) 3. You now have a great set of tools to be able to scrape pretty much anything you want from the internet. Now all that is left to do is practice. Find a course appropriate website containing data you would like to scrape. Utilize the tools you've learned about to scrape at least 100 &quot;units&quot; of data. A &quot;unit&quot; is just a representation of what you are scraping. For example, a unit could be a tweet from Twitter, a basketball player's statistics from sportsreference, a product from Amazon, a blog post from your favorite blogger, etc. The hard requirements are: Documented code with thorough comments explaining what the code does. At least 100 &quot;units&quot; scraped. The data must be from multiple web pages. Write at least 1 function (with a docstring) to help you scrape. A clear explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example a head of a pandas dataframe containing the data). Item(s) to submit: Python code that scrapes 100 unites of data (with thorough comments explaining what the code does). The data must be from more than a single web page. 1 or more functions (with docstrings) used to help you scrape/parse data. Clear documentation and explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example using the head of a dataframe containing the data). Solution Python code with comments Sample of scraped data (must be from multiple pages) At least 100 &quot;units&quot; of scraped data (for example 1 tweet == 1 unit, 1 product from amazon == 1 unit, etc.) At least 1 helper function with a docstring A paragraph explaining things Project 6 Motivation: Being able to analyze and create good visualizations is a skill that is invaluable in many fields. It can be pretty fun too! In this project, you can pick and choose if a couple of different plotting projects. Context: We've been working hard all semester, learning a lot about web scraping. In this project, you are given the choice between a project designed to go through some matplotlib basics, and a project that has you replicate plots from a book using plotly (an interactive plotting package) inside a Jupyter Notebook (which you would submit instead of an RMarkdown file). Scope: python, visualizing data Learning objectives: Demostrate the ability to create basic graphs with default settings. Demonstrate the ability to modify axes labels and titles. Demonstrate the ability to customize a plot (color, shape/linetype). Click here for video Make sure to read about, and use the template found here, and the important information about projects submissions here. Option 1 A maybe-too-familiar-to-you 29000 project Option 2 1. Here are a variety of interesting graphics from the popular book Displaying time series, spatial and space-time data with R by Oscar Perpinan Lamigueiro. You can replicate the graphics using data found here. Choose 3 graphics from the book to replicate using plotly. The replications do not need to be perfect -- a strong effort to get as close as possible is fine. Feel free to change colors as you please. If you have the desire to improve the graphic, please feel free to do so and explain how it is an improvement. Use https://notebook.scholar.rcac.purdue.edu and the f2020-s2021 kernel to complete this project. The only thing you need to submit for this project is the downloaded .ipynb file. Make sure that the grader will be able to click &quot;run all&quot; (using the same kernel, f2020-s2021), and have everything run properly. Important note: The object of this project is to challenge yourself (as much as you want), learn about and mess around with plotly, and be creative. If you have an idea for a cool plot, graphic, or modification, please include it! Item(s) to submit: Python code used to solve the problem. Output from running your code. Project 7 Motivation: Being able to analyze and create good visualizations is a skill that is invaluable in many fields. It can be pretty fun too! As you probably noticed in the previous project, matplotlib can be finicky -- certain types of plots are really easy to create, while others are not. For example, you would think changing the color of a boxplot would be easy to do in matplotlib, perhaps we just need to add an option to the function call. As it turns out, this isn't so straightforward (as illustrated at the end of this section). Occasionally this will happen and that is when packages like seaborn or plotnine (both are packages built using matplotlib) can be good. In this project we will explore this a little bit, and learn about some useful pandas functions to help shape your data in a format that any given package requires. Context: In the next project, we will continue to learn about and become comfortable using matplotlib, seaborn, and plotnine. Scope: python, visualizing data Learning objectives: Demonstrate the ability to create basic graphs with default settings. Demonstrate the ability to modify axes labels and titles. Demonstrate the ability to customize a plot (color, shape/linetype). Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/apple/health/watch_dump.xml Questions 1. In an earlier project we explored some XML data in the form of an Apple Watch data dump. Most health-related apps give you some sort of graph or set of graphs as an output. Use any package you want to parse the XML data. There are a lot of Records in this dataset. Each Record has an attribute called creationDate. Create a barplot of the number of Records per day. Make sure your plot is polished, containing proper labels and good colors. Hint: You could start by parsing out the required data into a pandas dataframe or series. Hint: The groupby method is one of the most useful pandas methods. It allows you to quickly perform operations on groups of data. Click here for video Click here for another video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code (including the graphic). 2. The plot in question 1 should look bimodal. Let's focus only on the first apparent group of readings. Create a new dataframe containing only the readings for the time period from 9/1/2017 to 5/31/2019. How many Records are there in that time period? Click here for video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code. 3. It is hard to discern weekly patterns (if any) based on the graphics created so far. For the period of time in question 2, create a labeled bar plot for the count of Records by day of the week. What (if any) discernable patterns are there? Make sure to include the labels provided below: labels = [&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;] Click here for video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code (including the graphic). 4. Create a pandas dataframe containing the following data from watch_dump.xml: A column called bpm with the bpm (beats per minute) of the InstantaneousBeatsPerMinute. A column called time with the time of each individual bpm reading in InstantaneousBeatsPerMinute. A column called date with the date. A column called dayofweek with the day of the week. Hint: You may want to use pd.to_numeric to convert the bpm column to a numeric type. Hint: This is one way to convert the numbers 0-6 to days of the week: myDF[&#39;dayofweek&#39;] = myDF[&#39;dayofweek&#39;].map({0:&quot;Mon&quot;, 1:&quot;Tue&quot;, 2:&quot;Wed&quot;, 3:&quot;Thu&quot;, 4:&quot;Fri&quot;, 5: &quot;Sat&quot;, 6: &quot;Sun&quot;}) Click here for video Relevant topics: lxml, groupby, barplot Item(s) to submit: Python code used to solve the problem. Output from running your code. 5. Create a heatmap using seaborn, where the y-axis shows the day of the week (&quot;Mon&quot; - &quot;Sun&quot;), the x-axis shows the hour, and the values on the interior of the plot are the average bpm by hour by day of the week. Click here for video Relevant topics: lxml, groupby, pivot Item(s) to submit: Python code used to solve the problem. Output from running your code (including the graphic). Project 8 Motivation: Python is an interpreted language (as opposed to a compiled language). In a compiled language, you are (mostly) unable to run and evaluate a single instruction at a time. In Python (and R -- also an interpreted language), we can run and evaluate a line of code easily using a repl. In fact, this is the way you've been using Python to date -- selecting and running pieces of Python code. Other ways to use Python include creating a package (like numpy, pandas, and pytorch), and creating scripts. You can create powerful CLI's (command line interface) tools using Python. In this project, we will explore this in detail and learn how to create scripts that accept options and input and perform tasks. Context: This is the first (of two) projects where we will learn about creating and using Python scripts. Scope: python Learning objectives: Write a python script that accepts user inputs and returns something useful. Make sure to read about, and use the template found here, and the important information about projects submissions here. Questions 1. Often times the deliverable part of a project isn’t custom built packages or modules, but a script. A script is a .py file with python code written inside to perform action(s). Python scripts are incredibly easy to run, for example, if you had a python script called question01.py, you could run it by opening a terminal and typing: python3 /path/to/question01.py The python interpreter then looks for the scripts entrypoint, and starts executing. You should read this article about the main function and python scripts. In addition, read this section, paying special attention to the shebang. Create a Python script called question01.py in your $HOME directory. Use the second shebang from the article: #!/usr/bin/env python3. When run, question01.py should use the sys package to print the location of the interpreter being used to run the script. For example, if we started a Python interpreter in RStudio using the following code: datamine_py() reticulate::repl_python() Then, we could print the interpreter by running the following Python code one line at a time: import sys print(sys.executable) Since we are using our Python environment, you should see this result: /class/datamine/apps/python/f2020-s2021/env/bin/python3. This is the fully qualified path of the Python interpreter we've been using for this course. Restart your R session by clicking Session &gt; Restart R, navigate to the &quot;Terminal&quot; tab in RStudio, and run the following lines in the terminal. What is the output? # this command gives execute permissions to your script -- this only needs to be run once chmod +x $HOME/question01.py # execute your script $HOME/question01.py Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Click here for another video Relevant topics: shebang Item(s) to submit: The entire question01.py script's contents in a Python code chunk with chunk option &quot;eval=F&quot;. Output from running your code copy and pasted as text. 2. Was your output in question (1) expected? Why or why not? When we restarted the R session, our datamine_py's effects were reversed, and the default Python interpreter is no longer our default when running python3. It is very common to have a multitude of Python environments available to use. But, when we are running a Python script it is not convenient to have to run various commands (in our case, the single datamine_py command) in order to get our script to run the way we want it to run. In addition, if our script used a set of packages that were not installed outside of our course environment, the script would fail. In this project, since our focus is more on how to write scripts and make them work as expected, we will have some fun and experiment with some pre-trained state of the art machine learning models. The following function accepts a string called sentence as an input and returns the sentiment of the sentence, &quot;POSITIVE&quot; or &quot;NEGATIVE&quot;. from transformers import pipeline def get_sentiment(model, sentence: str) -&gt; str: result = model(sentence) return result[0].get(&#39;label&#39;) model = pipeline(&#39;sentiment-analysis&#39;) print(get_sentiment(model, &#39;This is really great!&#39;)) print(get_sentiment(model, &#39;Oh no! Thats horrible!&#39;)) Include get_sentiment (including the import statement) in a new script, question02.py script. Note that you do not have to use get_sentiment anywhere, just include it for now. Go to the terminal in RStudio and execute your script. What happens? Remember, since our current shebang is #!/usr/bin/env python3, if our script uses one or more packages that are not installed in the current environment environment, the script will fail. This is what is happening. The transformers package that we use is not installed in the current environment. We do, however, have an environment that does have it installed, and it is located on Scholar at: /class/datamine/apps/python/pytorch2021/env/bin/python. Update the script's shebang and try to run it again. Does it work now? Explanation: Depending on the state of your current environment, the original shebang, #!/usr/bin/env python3 will use the same Python interpreter and environment that is currently set to python3 (run which python3 to see). If you haven't run datamine_py, this will be something like: /apps/spack/scholar/fall20/apps/anaconda/2020.11-py38-gcc-4.8.5-djkvkvk/bin/python or /usr/bin/python, if you have run datamine_py, this will be: /class/datamine/apps/python/f2020-s2021/env/bin/python. Both environments lack the transformers package. Our other environment whose interpreter lives here: /class/datamine/apps/python/pytorch2021/env/bin/python does have this package. The shebang is then critically important for any scripts that want to utilize packages from a specific environment. Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Relevant topics: writing scripts Item(s) to submit: Sentence explaining why or why not the output from question (1) was expected. Sentence explaining what happens when you include get_sentiment in your script and try to execute it. The entirety of the updated (working) script's content in a Python code chunk with chunk option &quot;eval=F&quot;. 3. Okay, great. We now understand that if we want to use packages from a specific environment, we need to modify our shebang accordingly. As it currently stands, our script is pretty useless. Modify the script, in a new script called question03.py to accept a single argument. This argument should be a sentence. Your script should then print the sentence, and whether or not the sentence is &quot;POSITIVE&quot; or &quot;NEGATIVE&quot;. Use sys.argv to accomplish this. Make sure the script functions in the following way: $HOME/question03.py This is a happy sentence, yay! Too many arguments. $HOME/question03.py &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE $HOME/question03.py ./question03.py requires at least 1 argument, &quot;sentence&quot;. Hint: One really useful way to exit the script and print a message is like this: import sys sys.exit(f&quot;{__file__} requires at least 1 argument, &#39;sentence&#39;&quot;) Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Click here for another video Relevant topics: writing scripts Item(s) to submit: The entirety of the updated (working) script's content in a Python code chunk with chunk option &quot;eval=F&quot;. Output from running your script with the given examples. 4. If you look at the man pages for a command line tool like awk or grep (you can get these by running man awk or man grep in the terminal), you will see that typically CLI's have a variety of options. Options usually follow the following format: grep -i &#39;ok&#39; some_file.txt However, often times you have 2 ways you can use an option -- either with the short form (for example -i), or long form (for example -i is the same as --ignore-case). Sometimes options can get values. If options don't have values, you can assume that the presence of the flag means TRUE and the lack means FALSE. When using short form, the value for the option is separated by a space (for example grep -f my_file.txt). When using long form, the value for the option is separated by an equals sign (for example grep --file=my_file.txt). Modify your script (as a new question04.py) to include an option called score. When active (question04.py --score or question04.py -s), the script should return both the sentiment, &quot;POSITIVE&quot; or &quot;NEGATIVE&quot; and the probability of being accurate. Make sure that you modify your checks from question 3 to continue to work whenever we use --score or -s. Some examples below: $HOME/question04.py &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE $HOME/question04.py --score &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py -s &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py &#39;This is a happy sentence, yay!&#39; -s Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py &#39;This is a happy sentence, yay!&#39; --score Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question04.py &#39;This is a happy sentence, yay!&#39; --value Unknown option(s): [&#39;--value&#39;] $HOME/question04.py &#39;This is a happy sentence, yay!&#39; --value --score Too many arguments. $HOME/question04.py question04.py requires at least 1 argument, &quot;sentence&quot; $HOME/question04.py --score ./question04.py requires at least 1 argument, &quot;sentence&quot;. No sentence provided. $HOME/question04.py &#39;This is one sentence&#39; &#39;This is another&#39; ./question04.py requires only 1 sentence, but 2 were provided. Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Hint: Experiment with the provided function. You will find the probability of being accurate is already returned by the model. Click here for video Click here for another video Relevant topics: writing scripts Item(s) to submit: The entirety of the updated (working) script's content in a Python code chunk with chunk option &quot;eval=F&quot;. Output from running your script with the given examples. 5. Wow, that is an extensive amount of logic for for a single option. Luckily, Python has the argparse package to help you build CLI's and handle situations like this. You can find the documentation for argparse here and a nice little tutorial here. Update your script (as a new question05.py) using argparse instead of custom logic. Specifically, add 1 positional argument called &quot;sentence&quot;, and 1 optional argument &quot;--score&quot; or &quot;-s&quot;. You should handle the following scenarios: $HOME/question05.py &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE $HOME/question05.py --score &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py -s &#39;This is a happy sentence, yay!&#39; Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py &#39;This is a happy sentence, yay!&#39; -s Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py &#39;This is a happy sentence, yay!&#39; --score Our sentence is: This is a happy sentence, yay! POSITIVE: 0.999848484992981 $HOME/question05.py &#39;This is a happy sentence, yay!&#39; --value usage: question05.py [-h] [-s] sentence question05.py: error: unrecognized arguments: --value $HOME/question05.py &#39;This is a happy sentence, yay!&#39; --value --score usage: question05.py [-h] [-s] sentence question05.py: error: unrecognized arguments: --value $HOME/question05.py usage: question05.py [-h] [-s] sentence positional arguments: sentence optional arguments: -h, --help show this help message and exit -s, --score display the probability of accuracy $HOME/question05.py --score usage: question05.py [-h] [-s] sentence question05.py: error: too few arguments $HOME/question05.py &#39;This is one sentence&#39; &#39;This is another&#39; usage: question05.py [-h] [-s] sentence question05.py: error: unrecognized arguments: This is another Hint: A good way to print the help information if no arguments are provided is: if len(sys.argv) == 1: parser.print_help() parser.exit() Important note: Include the bash code chunk option error=T to enable RMarkdown to knit and output errors. Important note: You can run bash code using a bash code chunk just like you would an R or Python code chunk. Simply replace &quot;python&quot; with &quot;bash&quot; in the code chunk options. Click here for video Relevant topics: writing scripts, argparse Item(s) to submit: Python code used to solve the problem. Output from running your code. "],
["fall2020-projects.html", "Fall 2020 projects STAT 19000 STAT 29000 STAT 39000", " Fall 2020 projects STAT 19000 Project 1 Motivation: In this project we are going to jump head first into The Data Mine. We will load datasets into the R environment, and introduce some core programming concepts like variables, vectors, types, etc. As we will be &quot;living&quot; primarily in an IDE called RStudio, we will take some time to learn how to connect to it, configure it, and run code. Context: This is our first project as a part of The Data Mine. We will get situated, configure the environment we will be using throughout our time with The Data Mine, and jump straight into working with data! Scope: r, rstudio, Scholar Learning objectives: Utilize other Scholar resources: rstudio.scholar.rcac.purdue.edu, notebook.scholar.rcac.purdue.edu, desktop.scholar.rcac.purdue.edu, etc. Install R and setting up a working environment. Explain and demonstrate: positional, named, and logical indexing. Read and write basic (csv) data. Make sure to read about, and use the template found here, and the important information about projects submissions here. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/disney/splash_mountain.csv Questions 1. Read the webpage here. Scholar is the computing cluster you will be using throughout the semester, and your time with The Data Mine. Each node is an individual machine with CPUs and memory (RAM). How many cores and how much memory is available, in total, for our 7 frontend nodes? How about for the sub-clusters? How much is available on your computer or laptop? Item(s) to submit: A sentence explaining how much memory and how many cores the 7 frontends have combined. A sentence explaining how much memory and how many cores the 28 sub-clusters have combined. A sentence explaining how much memory and how many cores your personal computer has. Solution The memory on the 7 frontend nodes is: 4 * 512 + 3 * 768 = 4352 GB = 4.3 TB. The memory on the 28 nodes in the sub-cluster is: 24 * 64 + 4 * 192 = 2304 GB = 2.3 TB. The memory on Dr. Ward's laptop is 8 GB. The number of cores on the 7 frontend nodes is: 4 * 20 + 3 * 20 = 140 cores. The number of cores on the 28 nodes in the sub-cluster is: 24 * 20 + 4 * 16 = 544 cores. The number of cores on Dr. Ward's laptop is 2. 2. Navigate and login to https://rstudio.scholar.rcac.purdue.edu using your Purdue Career Account credentials (and Boilerkey). This is an instance of RStudio Server running on a Scholar frontend! Frontends are labeled. So, for example, scholar-fe01.rcac.purdue.edu is frontend #1. Create a new R file (File &gt; New File &gt; R Script). Check which frontend you are logged in on by running the following in the new file: system(&quot;hostname&quot;). Press Control and Enter/Return keys at the same time, to run this line. Which frontend are you in? Relevant topics: running R code Item(s) to submit: The # of the frontend your RStudio Server session is running on. Solution The node we are working on is: system(&quot;hostname&quot;, intern=T) 3. From within RStudio, we can run every type of code that you will need to run throughout your time with The Data Mine: Python, R, Bash, SQL, etc. We've created a one-time setup script for you to run, called /class/datamine/apps/runme.sh (as seen in the video at the top of this page). After you restart R (as in the video, after 4 minutes and 16 seconds), there should be a message that is printed in your &quot;Console&quot; tab. What does the message say? Item(s) to submit: The sentence that is printed in the RStudio &quot;Console&quot;. Solution The welcome message is: &quot;You've successfully loaded The Data Mine R settings!&quot; 4. Projects in The Data Mine should all be submitted using our template found here or on Scholar (/class/datamine/apps/templates/project_template.Rmd). At the beginning of every project, the first step should be downloading and/or copying and pasting the template into a .Rmd file in RStudio. This is also demonstrated in the video at the top of this page. Open the project template and save it into your home directory, in a new RMarkdown file named project01.Rmd. Code chunks are parts of the RMarkdown file that contains code. You can identify what type of code a code chunk contains by looking at the engine in the curly braces &quot;{&quot; and &quot;}&quot;. How many of each type of code chunk are in our default template? Hint: You can read about the template here. Item(s) to submit: A list containing the type of code chunk (r, Python, sql, etc), and how many of each code chunks our default template contains. Solution There are 3 chunks of R code, 1 chunk of bash, 1 chunk of Python, 1 chunk of SQL 5. Fill out the project template, replacing the default information with your own. If a category is not applicable to you, put N/A. This template provides examples of how to run each &quot;type&quot; of code we will run in this course. Look for the second R code chunk, and run it by clicking the tiny green play button in the upper right hand corner of the code chunk. What is the output? Item(s) to submit: The output from running the R code chunk. Solution We store 1, 2, 3 into the variable my_variable, and then we display output: 1, 2, 3 my_variable &lt;- c(1,2,3) my_variable ## [1] 1 2 3 6. In question (1) we answered questions about CPUs and RAM for the Scholar cluster. To do so, we needed to perform some arithmetic. Instead of using a calculator (or paper), write these calculations using R. Replace the content of the second R code chunk in our template with your calculations. Relevant topics: templates Item(s) to submit: The R code chunk with your calculations, and output. Solution We go back to question 1 and compute directly 4 * 512 + 3 * 768 ## [1] 4352 24 * 64 + 4 * 192 ## [1] 2304 4 * 20 + 3 * 20 ## [1] 140 24 * 20 + 4 * 16 ## [1] 544 7. In (6) we got to see how you can type out arithmetic and R will calculate the result for you. One constant throughout the semester will be loading datasets into R. Load our dataset into R by running the following code: dat &lt;- read.csv(&quot;/class/datamine/data/disney/splash_mountain.csv&quot;) Confirm the dataset has been read in by running the head function on it. head prints the first few rows of data: head(dat) dat is a variable which contains our data! We can name this variable anything we want, we do not have to name it dat. Run our code to read in our dataset, this time, instead of naming our resulting dataset dat, name it splash_mountain. Place all of your code into a new R code chunk under a new level 3 header (i.e. ### Question 7). Relevant topics: reading data in R Item(s) to submit: Code used to answer this question in a code chunk in our template. Output of head. Solution We load in splash_mountain data and display the head splash_mountain &lt;- read.csv(&quot;/class/datamine/data/disney/splash_mountain.csv&quot;) head(splash_mountain) ## date datetime SACTMIN SPOSTMIN ## 1 01/01/2015 2015-01-01 07:51:12 NA 5 ## 2 01/01/2015 2015-01-01 08:02:13 NA 5 ## 3 01/01/2015 2015-01-01 08:09:12 NA 5 ## 4 01/01/2015 2015-01-01 08:16:12 NA 5 ## 5 01/01/2015 2015-01-01 08:23:12 NA 5 ## 6 01/01/2015 2015-01-01 08:29:12 NA 5 8. Let's pretend we are now done with our project. We've written some R code, maybe added some text explaining what we did, and we are ready to turn things in. For this course, we will turn in a variety of work, depending on the type of project. We will always require a PDF which contains text, code, and code output. Normally we would erase any code chunks from the template that are not used, however, for this project, it is OK to just keep the rest of the template intact. A PDF is generated by &quot;knitting&quot; a PDF (using the &quot;knit&quot; button in RStudio). In addition, if the project uses R code, you will need to also submit R code in an R script (file ending with .R). (Later this year, when submitting Python code, you will submit a Python script instead.) Let's practice. Take the code from your project01.R file and paste it (perhaps one or two lines at time) into your RMarkdown file (file ending with .Rmd). Compile your RMarkdown project into a PDF. Follow the directions in Brightspace to upload and submit your RMarkdown file, compiled PDF, and R script. Relevant topics: templates Item(s) to submit: Resulting knitted PDF. project01.R script (with all of your R code) and the analogous project01.Rmd file. How to build the R script for Project 1 in STAT 19000. In the videos below, for Question 1 and Question 6, Dr. Ward forgot to calculate the number of cores. (He only included the total amount of memory.) Dr. Ward is a human being who sometimes makes mistakes. Please remember to (also) calculate the number of cores, when you submit Question 1 and Question 6! Click here for video How to build the Rmd file and the PDF file for Project 1 in STAT 19000. Click here for video Project 2 Introduction to R using 84.51 examples Click here for video Introduction to R using NYC Yellow Taxi Cab examples Click here for video Motivation: The R environment is a powerful tool to perform data analysis. R is a tool that is often compared to Python. Both have their advantages and disadvantages, and both are worth learning. In this project we will dive in head first and learn the basics while solving data-driven problems. Context: Last project we set the stage for the rest of the semester. We got some familiarity with our project templates, and modified and ran some R code. In this project, we will continue to use R within RStudio to solve problems. Soon you will see how powerful R is and why it is often a more effective tool to use than spreadsheets. Scope: r, vectors, indexing, recycling Learning objectives: List the differences between lists, vectors, factors, and data.frames, and when to use each. Explain and demonstrate: positional, named, and logical indexing. Read and write basic (csv) data. Explain what &quot;recycling&quot; is in R and predict behavior of provided statements. Identify good and bad aspects of simple plots. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/disney/metadata.csv A public sample of the data can be found here: /class/datamine/data/disney/metadata.csv Questions 1. Use the read.csv function to load /class/datamine/data/disney/metadata.csv into a data.frame called myDF. Note that read.csv by default loads data into a data.frame. (We will learn more about the idea of a data.frame, but for now, just think of it like a spreadsheet, in which each column has the same type of data.) Print the first few rows of myDF using the head function (as in Project 1, Question 7). Relevant topics: reading data in r, head Item(s) to submit: R code used to solve the problem in an R code chunk. Solution We load the data from the disney metadata into a data frame called myDF myDF &lt;- read.csv(&quot;/class/datamine/data/disney/metadata.csv&quot;) 2. We've provided you with R code below that will extract the column WDWMAXTEMP of myDF into a vector. What is the 1st value in the vector? What is the 50th value in the vector? What type of data is in the vector? (For this last question, use the typeof function to find the type of data.) our_vec &lt;- myDF$WDWMAXTEMP Relevant topics: indexing in r, type, creating variables Item(s) to submit: R code used to solve the problem in an R code chunk. The values of the first, and 50th element in the vector. The type of data in the vector (using the typeof function). Solution We first load the column WDWMAXTEMP of the data frame myDF into a vector: our_vec &lt;- myDF$WDWMAXTEMP The 1st element is: our_vec[1] ## [1] 73.02 The 50th element is: our_vec[50] ## [1] 51.24 The type of data in the vector is: typeof(our_vec) ## [1] &quot;double&quot; 3. Use the head function to create a vector called first50 that contains the first 50 values of the vector our_vec. Use the tail function to create a vector called last50 that contains the last 50 values of the vector our_vec. You can access many elements in a vector at the same time. To demonstrate this, create a vector called mymix that contain the sum of each element of first50 being added to the analogous element of last50. Relevant topics: indexing in r, creating variables, head and tail Item(s) to submit: R code used to solve this problem. The contents of each of the three vectors. Solution We store the first 50 values of our_vec in first50: first50 &lt;- head(our_vec, n=50) Here are the first 50 values: first50 ## [1] 73.02 78.00 83.12 83.93 72.30 77.67 67.24 59.44 54.89 67.16 77.10 78.24 ## [13] 74.89 64.51 62.12 63.58 72.83 70.59 69.89 71.03 75.78 77.09 78.40 70.77 ## [25] 63.48 65.48 66.78 61.13 70.11 71.89 69.80 73.34 75.02 66.33 73.02 65.22 ## [37] 66.33 71.78 73.58 70.42 64.23 66.96 71.39 59.54 63.24 73.42 74.11 78.70 ## [49] 60.48 51.24 We store the last 50 values of our_vec in last50: last50 &lt;- tail(our_vec, n=50) Here are the last 50 values: last50 ## [1] 78.73 78.38 75.73 74.79 76.84 81.25 79.42 77.66 81.15 82.07 70.62 82.63 ## [13] 79.12 78.55 76.34 74.10 78.62 74.31 67.69 73.30 74.51 77.66 81.15 82.07 ## [25] 70.62 82.63 70.54 57.34 62.29 72.54 62.47 72.31 75.78 72.01 78.48 81.57 ## [37] 82.06 80.45 82.01 76.05 79.38 79.38 79.38 79.74 70.41 68.23 58.31 64.29 ## [49] 64.29 64.29 The sums of these two vectors, element by element, are: mymix &lt;- first50 + last50 Here are these 50 values: mymix ## [1] 151.75 156.38 158.85 158.72 149.14 158.92 146.66 137.10 136.04 149.23 ## [11] 147.72 160.87 154.01 143.06 138.46 137.68 151.45 144.90 137.58 144.33 ## [21] 150.29 154.75 159.55 152.84 134.10 148.11 137.32 118.47 132.40 144.43 ## [31] 132.27 145.65 150.80 138.34 151.50 146.79 148.39 152.23 155.59 146.47 ## [41] 143.61 146.34 150.77 139.28 133.65 141.65 132.42 142.99 124.77 115.53 4. In (3) we were able to rapidly add values together from two different vectors. Both vectors were the same size, hence, it was obvious which elements in each vector were added together. Create a new vector called hot which contains only the values of myDF$WDWMAXTEMP which are greater than or equal to 80 (our vector contains max temperatures for days at Disney World). How many elements are in hot? Calculate the sum of hot and first50. Do we get a warning? Read this and then explain what is going on. Relevant topics: logical indexing, length, recycling Item(s) to submit: R code used to solve this problem. 1-2 sentences explaining what is happening when we are adding two vectors of different lengths. Solution The values of myDF$WDWMAXTEMP that are greater than or equal to 80 are: hot &lt;- myDF$WDWMAXTEMP[myDF$WDWMAXTEMP &gt;= 80] The length of the vector hot is: length(hot) ## [1] 1255 If we calculate the sum of hot and first50, mynewsum &lt;- hot + first50 ## Warning in hot + first50: longer object length is not a multiple of shorter ## object length we get a warning that hot and first50 have lengths that are not multiples of each other (so it does not make sense to be adding them, element by element). 5. Plot the WDWMAXTEMP vector from myDF. Item(s) to submit: R code used to solve this problem. Plot of the WDWMAXTEMP vector from myDF. Relevant topics: plotting Solution We plot the max_temp vector from myDF: plot(myDF$WDWMAXTEMP) 6. The following three pieces of code each create a graphic. The first two graphics are created using only core R functions. The third graphic is created using a package called ggplot. We will learn more about all of these things later on. For now, pick your favorite graphic, and write 1-2 sentences explaining why it is your favorite, what could be improved, and include any interesting observations (if any). dat &lt;- table(myDF$SEASON) dotchart(dat, main=&quot;Seasons&quot;, xlab=&quot;Number of Days in Each Season&quot;) dat &lt;- tapply(myDF$WDWMEANTEMP, myDF$DAYOFYEAR, mean, na.rm=T) seasons &lt;- tapply(myDF$SEASON, myDF$DAYOFYEAR, function(x) unique(x)[1]) pal &lt;- c(&quot;#4E79A7&quot;, &quot;#F28E2B&quot;, &quot;#A0CBE8&quot;, &quot;#FFBE7D&quot;, &quot;#59A14F&quot;, &quot;#8CD17D&quot;, &quot;#B6992D&quot;, &quot;#F1CE63&quot;, &quot;#499894&quot;, &quot;#86BCB6&quot;, &quot;#E15759&quot;, &quot;#FF9D9A&quot;, &quot;#79706E&quot;, &quot;#BAB0AC&quot;, &quot;#1170aa&quot;, &quot;#B07AA1&quot;) colors &lt;- factor(seasons) levels(colors) &lt;- pal par(oma=c(7,0,0,0), xpd=NA) barplot(dat, main=&quot;Average Temperature&quot;, xlab=&quot;Jan 1 (Day 0) - Dec 31 (Day 365)&quot;, ylab=&quot;Degrees in Fahrenheit&quot;, col=as.factor(colors), border = NA, space=0) legend(0, -30, legend=levels(factor(seasons)), lwd=5, col=pal, ncol=3, cex=0.8, box.col=NA) library(ggplot2) library(tidyverse) summary_temperatures &lt;- myDF %&gt;% select(MONTHOFYEAR,WDWMAXTEMP:WDWMEANTEMP) %&gt;% group_by(MONTHOFYEAR) %&gt;% summarise_all(mean, na.rm=T) ggplot(summary_temperatures, aes(x=MONTHOFYEAR)) + geom_ribbon(aes(ymin = WDWMINTEMP, ymax = WDWMAXTEMP), fill = &quot;#ceb888&quot;, alpha=.5) + geom_line(aes(y = WDWMEANTEMP), col=&quot;#5D8AA8&quot;) + geom_point(aes(y = WDWMEANTEMP), pch=21,fill = &quot;#5D8AA8&quot;, size=2) + theme_classic() + labs(x = &#39;Month&#39;, y = &#39;Temperature&#39;, title = &#39;Average temperature range&#39; ) + scale_x_continuous(breaks=1:12, labels=month.abb) Solution The seasons plot makes it easy to compare the values. On the other hand, the values do not appear to be given in any particular order. The average temperature plot clearly indicates the rise and fall of the temperatures over the course of the year, but the color scheme is unusual. The colors in the plot do not seem to correspond to the colors in the legend. Additionally, the color schemes in both the plot and the legend seem to be chosen randomly. Project 3 Motivation: data.frames are the primary data structure you will work with when using R. It is important to understand how to insert, retrieve, and update data in a data.frame. Context: In the previous project we got our feet wet, and ran our first R code, and learned about accessing data inside vectors. In this project we will continue to reinforce what we've already learned and introduce a new, flexible data structure called data.frames. Scope: r, data.frames, recycling, factors Learning objectives: Explain what &quot;recycling&quot; is in R and predict behavior of provided statements. Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc. Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc. Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. List the differences between lists, vectors, factors, and data.frames, and when to use each. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/disney Questions 1. Read the dataset /class/datamine/data/disney/splash_mountain.csv into a data.frame called splash_mountain. How many columns, or features are in each dataset? How many rows or observations? Relevant topics: str, dim Item(s) to include: R code used to solve the problem. How many columns or features in each dataset? Solution We read in the Splash Mountain data set. splash_mountain &lt;- read.csv(&quot;/class/datamine/data/disney/splash_mountain.csv&quot;) We can use either the str or the dim function to see that this data frame has 223936 rows and 4 columns. str(splash_mountain) ## &#39;data.frame&#39;: 223936 obs. of 4 variables: ## $ date : chr &quot;01/01/2015&quot; &quot;01/01/2015&quot; &quot;01/01/2015&quot; &quot;01/01/2015&quot; ... ## $ datetime: chr &quot;2015-01-01 07:51:12&quot; &quot;2015-01-01 08:02:13&quot; &quot;2015-01-01 08:09:12&quot; &quot;2015-01-01 08:16:12&quot; ... ## $ SACTMIN : int NA NA NA NA NA NA NA NA NA 4 ... ## $ SPOSTMIN: int 5 5 5 5 5 5 5 5 5 NA ... dim(splash_mountain) ## [1] 223936 4 2. Splash Mountain is a fan favorite ride at Disney World's Magic Kingdom theme park. splash_mountain contains a series of dates and datetimes. For each datetime, splash_mountain contains a posted minimum wait time, SPOSTMIN, and an actual minimum wait time, SACTMIN. What is the average posted minimum wait time for Splash Mountain? What is the standard deviation? Based on the fact that SPOSTMIN represents the posted minimum wait time for our ride, does our mean and standard deviation make sense? Explain. (You might look ahead to Question 3 before writing the answer to Question 2.) Hint: If you got NA or NaN as a result, see here. Relevant topics: mean, var, NA, NaN Item(s) to submit: R code used to solve this problem. The results of running the R code. 1-2 sentences explaining why or why not the results make sense. Solution The average minimum posted wait time for Splash Mountain is mean(splash_mountain$SPOSTMIN, na.rm=T) ## [1] -71.70373 and the standard deviation is sd(splash_mountain$SPOSTMIN, na.rm=T) ## [1] 328.0586 This is strange because the average minimum posted wait time should not be a negative value, and the standard deviation seems to be very large. 3. In (2) we got some peculiar values for the mean and standard deviation. If you read the &quot;attractions&quot; tab in the file /class/datamine/data/disney/touringplans_data_dictionary.xlsx, you will find that -999 is used as a value in SPOSTMIN and SACTMIN to indicate the ride as being closed. Recalculate the mean and standard deviation of SPOSTMIN, excluding values that are -999. Does this seem to have fixed our problem? Relevant topics: NA, mean, var, indexing, which Item(s) to submit: R code used to solve this problem. The result of running the R code. A statement indicating whether or not the value look reasonable now. Solution If we remove the -999 values, then the average minimum posted wait time (with the -999 values removed) is mean(splash_mountain$SPOSTMIN[splash_mountain$SPOSTMIN != -999], na.rm=T) ## [1] 43.3892 and the standard deviation (with the -999 values removed) is sd(splash_mountain$SPOSTMIN[splash_mountain$SPOSTMIN != -999], na.rm=T) ## [1] 31.74894 This looks more reasonable! 4. SPOSTMIN and SACTMIN aren't the greatest feature/column names. An outsider looking at the data.frame wouldn't be able to immediately get the gist of what they represent. Change SPOSTMIN to posted_min_wait_time and SACTMIN to actual_wait_time. Hint: You can always use hard-coded integers to change names manually, however, if you use which, you can get the index of the column name that you would like to change. For data.frames like splash_mountain, this is a lot more efficient than manually counting which column is the one with a certain name. Relevant topics: colnames, names, which Item(s) to submit: R code used to solve the problem. The output from executing names(splash_mountain) or colnames(splash_mountain). Solution The current column names of splash_mountain are: colnames(splash_mountain) ## [1] &quot;date&quot; &quot;datetime&quot; &quot;SACTMIN&quot; &quot;SPOSTMIN&quot; We can find which column is called SPOSTMIN which(colnames(splash_mountain) == &quot;SPOSTMIN&quot;) ## [1] 4 and change this column name to posted_min_wait_time colnames(splash_mountain)[4] &lt;- &quot;posted_min_wait_time&quot; Next, we can find which column is called SACTMIN which(colnames(splash_mountain) == &quot;SACTMIN&quot;) ## [1] 3 and change this column name to actual_wait_time colnames(splash_mountain)[3] &lt;- &quot;actual_wait_time&quot; Alternatively, we could have used names instead of colnames and the effect would have been totally the same. 5. Use the cut function to create a new vector called quarter that breaks the date column up by quarter. Use the labels argument in the factor function to label the quarters &quot;q1&quot;, &quot;q2&quot;, ..., &quot;qX&quot; where X is the last quarter. Add quarter as a column named quarter in splash_mountain. How many quarters are there? Hint: If you have 2 years of data, this will result in 8 quarters: &quot;q1&quot;, ..., &quot;q8&quot;. Hint: We can generate sequential data using seq and paste0: paste0(&quot;item&quot;, seq(1, 5)) ## [1] &quot;item1&quot; &quot;item2&quot; &quot;item3&quot; &quot;item4&quot; &quot;item5&quot; or paste0(&quot;item&quot;, 1:5) ## [1] &quot;item1&quot; &quot;item2&quot; &quot;item3&quot; &quot;item4&quot; &quot;item5&quot; Relevant topics: cut, dates, factor, paste0, seq, nlevels Item(s) to submit: R code used to solve the problem. The head and tail of splash_mountain. The number of quarters in the new quarter column. Question 5 is intended to be a little more challenging, so we worked through the exact same steps, with two other data sets. That way, if you work through these, all you will need to do, to solve Question 5, is to follow the example, and change two things, namely, the data set itself (in the read.csv file) and also the format of the date. This basically steps you through everything in Question 5. We hope that these are helpful resources for you! We appreciate you very much and we are here to support you! You would not know how to solve this question on your own--because we are just getting started--but we like to sometimes put in a question like this, in which you get introduced to several new things, and we will dive deeper into these ideas as we push ahead. Click here for video Click here for video Solution We first take the date column from splash_mountain and treat it as Date values, and we break the dates according to which quarter they are in. myresults &lt;- cut((as.Date(splash_mountain$date,&quot;%m/%d/%Y&quot;)), breaks=&quot;quarter&quot;) There are 20 quarters altogether: nlevels(myresults) ## [1] 20 We set the levels to be a letter q and then a number between 1 and nlevels(myresults) levels(myresults) &lt;- paste0(&quot;q&quot;,1:nlevels(myresults)) splash_mountain$quarters &lt;- myresults head(splash_mountain) ## date datetime actual_wait_time posted_min_wait_time quarters ## 1 01/01/2015 2015-01-01 07:51:12 NA 5 q1 ## 2 01/01/2015 2015-01-01 08:02:13 NA 5 q1 ## 3 01/01/2015 2015-01-01 08:09:12 NA 5 q1 ## 4 01/01/2015 2015-01-01 08:16:12 NA 5 q1 ## 5 01/01/2015 2015-01-01 08:23:12 NA 5 q1 ## 6 01/01/2015 2015-01-01 08:29:12 NA 5 q1 6. Please include a statement in Project 3 that says, &quot;I acknowledge that the STAT 19000/29000/39000 1-credit Data Mine seminar will be recorded and posted on Piazza, for participants in this course.&quot; or if you disagree with this statement, please consult with us at datamine@purdue.edu for an alternative plan. Project 4 Motivation: Control flow is (roughtly) the order in which instructions are executed. We can execute certain tasks or code if certain requirements are met using if/else statements. In addition, we can perform operations many times in a loop using for loops. While these are important concepts to grasp, R differs from other programming languages in that operations are usually vectorized and there is little to no need to write loops. Context: We are gaining familiarity working in RStudio and writing R code. In this project we introduce and practice using control flow in R. Scope: r, data.frames, recycling, factors, if/else, for Learning objectives: Explain what &quot;recycling&quot; is in R and predict behavior of provided statements. Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc. Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc. Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. List the differences between lists, vectors, factors, and data.frames, and when to use each. Demonstrate a working knowledge of control flow in r: if/else statements, while loops, etc. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/disney Questions 1. Use read.csv to read in the /class/datamine/data/disney/splash_mountain.csv data into a data.frame called splash_mountain. In the previous project we calculated the mean and standard deviation of the SPOSTMIN (posted minimum wait time). These are vectorized operations (we will learn more about this next project). Instead of using the mean function, use a loop to calculate the mean(average), just like the previous project. Do not use sum either. Hint: Remember, if a value is NA, we don't want to include it. Hint: Remember, if a value is -999, it means the ride is closed, we don't want to include it. Note: This exercise should make you appreciate the variety of useful functions R has to offer! Relevant topics: for loops, if/else statements, is.na Item(s) to submit: R code used to solve the problem w/comments explaining what the code does. The mean posted wait time. Solution splash_mountain &lt;- read.csv(&quot;/class/datamine/data/disney/splash_mountain.csv&quot;) sum_posted_wait_time &lt;- 0 not_nas_and_nines &lt;- 0 for (i in splash_mountain$SPOSTMIN) { if (!is.na(i) &amp;&amp; i != -999) { sum_posted_wait_time &lt;- sum_posted_wait_time + i not_nas_and_nines &lt;- not_nas_and_nines + 1 } } mean_posted_wait_time &lt;- sum_posted_wait_time/not_nas_and_nines mean_posted_wait_time 2. Choose one of the .csv files containing data for a ride. Use read.csv to load the file into a data.frame named ride_name where &quot;ride_name&quot; is the name of the ride you chose. Use a for loop to loop through the ride file and add a new column called status. status should contain a string whose value is either &quot;open&quot;, or &quot;closed&quot;. If SPOSTMIN or SACTMIN is -999, classify the row as &quot;closed&quot;. Otherwise, classify the row as &quot;open&quot;. After status is added to your data.frame, convert the column to a factor. Hint: If you want to access two columns at once from a data.frame, you can do: splash_mountain[i, c(&quot;SPOSTMIN&quot;, &quot;SACTMIN&quot;)]. Relevant topics: any, for loops, if/else statements, nrow Note: For loops are often much slower (here is a video to demonstrate) than vectorized functions, as we will see in (3) below. Item(s) to submit: R code used to solve the problem w/comments explaining what the code does. The output from running str on ride_name. In this video, we basically go all the way through Question 2 using a video: Click here for video Solution splash_mountain &lt;- read.csv(&quot;/class/datamine/data/disney/splash_mountain.csv&quot;) status &lt;- c() for (i in 1:nrow(splash_mountain)) { if (any(splash_mountain[i, c(&quot;SACTMIN&quot;, &quot;SPOSTMIN&quot;)]==-999, na.rm=T)) { status[i] &lt;- &quot;closed&quot; }else { status[i] &lt;- &quot;open&quot; } } splash_mountain$status &lt;- factor(status) 3. Typically you want to avoid using for loops (or even apply functions (we will learn more about these later on, don't worry)) when they aren't needed. Instead you can use vectorized operations and indexing. Repeat (2) without using any for loops or apply functions (instead use indexing and the which function). Which method was faster? Hint: To have multiple conditions within the which statement, use | for logical OR and &amp; for logical AND. Hint: You can start by assigning every value in status as &quot;open&quot;, and then change the correct values to &quot;closed&quot;. Note: Here is a complete example (very much like question 3) with another video that shows how we can classify objects. Note: Here is a complete example with a video that makes a comparison between the concept of a for loop versus the concept for a vectorized function. Relevant topics: which Item(s) to submit: R code used to solve the problem w/comments explaining what the code does. The output from running str on ride_name. Solution splash_mountain &lt;- read.csv(&quot;/class/datamine/data/disney/splash_mountain.csv&quot;) splash_mountain$status &lt;- &quot;open&quot; splash_mountain$status[which(splash_mountain$SPOSTMIN == -999 | splash_mountain$SACTMIN == -999)] &lt;- c(&quot;closed&quot;) splash_mountain$status &lt;- factor(splash_mountain$status) 4. Create a pie chart for open vs. closed for splash_mountain.csv. First, use the table command to get a count of each status. Use the resulting table as input to the pie function. Make sure to give your pie chart a title that somehow indicates the ride to the audience. Relevant topics: pie, table Item(s) to submit: R code used to solve the problem w/comments explaining what the code does. The resulting plot displayed as output in the RMarkdown. Solution pie(table(splash_mountain$status), main=&quot;Splash Mountain&quot;) 5. Loop through the vector of files we've provided below, and create a pie chart of open vs closed for each ride. Place all 6 resulting pie charts on the same image. Make sure to give each pie chart a title that somehow indicates the ride. ride_names &lt;- c(&quot;splash_mountain&quot;, &quot;soarin&quot;, &quot;pirates_of_caribbean&quot;, &quot;expedition_everest&quot;, &quot;flight_of_passage&quot;, &quot;rock_n_rollercoaster&quot;) ride_files &lt;- paste0(&quot;/class/datamine/data/disney/&quot;, ride_names, &quot;.csv&quot;) Hint: To place all of the resulting pie charts in the same image, prior to running the for loop, run par(mfrow=c(2,3)). Relevant topics: for loop, read.csv, pie, table, par This is not exactly the same, but it is a similar example, using the campaign election data: mypiechart &lt;- function(x) { myDF &lt;- read.csv( paste0(&quot;/class/datamine/data/election/itcont&quot;, x, &quot;.txt&quot;), sep=&quot;|&quot;) mystate &lt;- rep(&quot;other&quot;, times=nrow(myDF)) mystate[myDF$STATE == &quot;CA&quot;] &lt;- &quot;California&quot; mystate[myDF$STATE == &quot;TX&quot;] &lt;- &quot;Texas&quot; mystate[myDF$STATE == &quot;NY&quot;] &lt;- &quot;New York&quot; myDF$stateclassification &lt;- factor(mystate) pie(table(myDF$stateclassification)) } myyears &lt;- c(&quot;1980&quot;,&quot;1984&quot;,&quot;1988&quot;,&quot;1992&quot;,&quot;1996&quot;,&quot;2000&quot;) par(mfrow=c(2,3)) for (i in myyears) { mypiechart(i) } Click here for video Here is another video, which guides students even more closely through Question 5. Click here for video Item(s) to submit: R code used to solve the problem w/comments explaining what the code does. The resulting plot displayed as output in the RMarkdown. Solution ride_names &lt;- c(&quot;splash_mountain&quot;, &quot;soarin&quot;, &quot;pirates_of_caribbean&quot;, &quot;flight_of_passage&quot;, &quot;expedition_everest&quot;, &quot;rock_n_rollercoaster&quot;) ride_files &lt;- paste0(c(&quot;/class/datamine/data/disney/&quot;), ride_names, &quot;.csv&quot;) par(mfrow=c(2,3)) for (i in 1:length(ride_names)) { dat &lt;- read.csv(ride_files[i]) dat$status &lt;- &quot;open&quot; dat$status[which(dat$SPOSTMIN == -999 | dat$SACTMIN == -999)] &lt;- c(&quot;closed&quot;) dat$status &lt;- factor(dat$status) pie(table(dat$status), main=ride_names[i]) } Project 5 Motivation: As briefly mentioned in project 4, R differs from other programming languages in that typically you will want to avoid using for loops, and instead use vectorized functions and the apply suite. In this project we will demonstrate some basic vectorized operations, and how they are better to use than loops. Context: While it was important to stop and learn about looping and if/else statements, in this project, we will explore the R way of doing things. Scope: r, data.frames, recycling, factors, if/else, for Learning objectives: Explain what &quot;recycling&quot; is in R and predict behavior of provided statements. Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc. Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc. Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. List the differences between lists, vectors, factors, and data.frames, and when to use each. Demonstrate a working knowledge of control flow in r: for loops . Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/fars To get more information on the dataset, see here. Questions 1. The fars dataset contains a series of folders labeled by year. In each year folder there is (at least) the files ACCIDENT.CSV, PERSON.CSV, and VEHICLE.CSV. If you take a peek at any ACCIDENT.CSV file in any year, you'll notice that the column YEAR only contains the last two digits of the year. Add a new YEAR column that contains the full year. Use the rbind function to create a data.frame called accidents that combines the ACCIDENT.CSV files from the years 1975 through 1981 (inclusive) into one big dataset. After creating that accidents data frame, change the values in the YEAR column from two digits to four digits (i.e., paste a 19 onto each year value). Relevant topics: rbind, read.csv, paste0 Here is a video to walk you through the method of solving Question 1. Click here for video Here is another video, using two functions you have not (yet) learned, namely, lapply and do.call. You do not need to understand these yet. It is just a glimpse of some powerful functions to come later in the course! Click here for video Item(s) to submit: R code used to solve the problem/comments explaining what the code does. The result of unique(accidents$YEAR). Solution accidents &lt;- data.frame() for (i in 1975:1981) { filename &lt;- paste0(&quot;/class/datamine/data/fars/&quot;, i, &quot;/ACCIDENT.CSV&quot;) dat &lt;- read.csv(filename) dat$YEAR &lt;- i accidents &lt;- rbind(accidents, dat) } 2. Using the new accidents data frame that you created in (1), how many accidents are there in which 1 or more drunk drivers were involved in an accident with a school bus? Hint: Look at the variables DRUNK_DR and SCH_BUS. Relevant topics: table, which, indexing Here is a video about a related problem with 3 fatalities (instead of considering drunk drivers). Click here for video Item(s) to submit: R code used to solve the problem/comments explaining what the code does. The result/answer itself. Solution table(accidents$DRUNK_DR, accidents$SCH_BUS) 3. Again using the accidents data frame: For accidents involving 1 or more drunk drivers and a school bus, how many happened in each of the 7 years? Which year had the largest number of these types of accidents? Relevant topics: table, which, indexing Here is a video about the related problem with 3 fatalities (instead of considering drunk drivers), tabulated according to year. Click here for video Item(s) to submit: R code used to solve the problem/comments explaining what the code does. The results. Which year had the most qualifying accidents. Solution result &lt;- table(accidents$YEAR[which(accidents$DRUNK_DR &gt; 0 &amp; accidents$SCH_BUS==1)]) result[which.max(result)] 4. Again using the accidents data frame: Calculate the mean number of motorists involved in an accident (variable PERSON) with i drunk drivers, where i takes the values from 0 through 6. Hint: It is OK that there are no accidents involving just 5 drunk drivers. Hint: You can use either a for loop or a tapply function to accomplish this question. Relevant topics: for loops, indexing, tapply, mean, Here is a video about the related problem with 3 fatalities (instead of considering drunk drivers). We calculate the mean number of fatalities for accidents with i drunk drivers, where i takes the values from 0 through 6. Click here for video Item(s) to submit: R code used to solve the problem/comments explaining what the code does. The output from running your code. Solution for (i in 0:6) { print(mean(accidents$PERSONS[accidents$DRUNK_DR == i], na.rm=T)) } 5. Again using the accidents data frame: We have a theory that there are more accidents in cold weather months for Indiana and states around Indiana. For this question, only consider the data for which STATE is one of these: Indiana (18), Illinois (17), Ohio (39), or Michigan (26). Create a barplot that shows the number of accidents by STATE and by month (MONTH) simultanously. What months have the most accidents? Are you surprised by these results? Explain why or why not? We guide students through the methodology for Question 5 in this video. We also add a legend, in case students want to distinguish which stacked barplot goes with each of the four States. Click here for video Relevant topics: in, barplot Item(s) to submit: R code used to solve the problem/comments explaining what the code does. The output (plot) from running your code. 1-2 sentences explaining which month(s) have the most accidents and whether or not this surprises you. Solution dat &lt;- accidents[accidents$STATE %in% c(17, 18, 26, 39),] barplot(table(dat$STATE, dat$MONTH)) OPTIONAL QUESTION. Spruce up your plot from (5). Do any of the following: add vibrant (and preferably colorblind friendly) colors to your plot add a title add a legend add month names or abbreviations instead of numbers Hint: Here is a resource to get you started. Item(s) to submit: R code used to solve the problem/comments explaining what the code does. The output (plot) from running your code. Solution library(RColorBrewer) dat &lt;- accidents[accidents$STATE %in% c(17, 18, 26, 39),] dat$STATE &lt;- factor(dat$STATE, labels=c(&quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Michigan&quot;, &quot;Ohio&quot;)) dat$MONTH &lt;- factor(dat$MONTH, labels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;)) color_palette &lt;- brewer.pal(5, &quot;Set2&quot;) barplot(table(dat$STATE, dat$MONTH), col=color_palette, main=&quot;Accidents by Month&quot;, legend=T, ylim=c(0, 5000), las=2) Project 6 The tapply function works like this: tapply( somedata, thewaythedataisgrouped, myfunction) myDF &lt;- read.csv(&quot;/class/datamine/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv&quot;) head(myDF) We could do four computations to compute the mean SPEND amount in each STORE_R mean(myDF$SPEND[myDF$STORE_R == &quot;CENTRAL&quot;]) mean(myDF$SPEND[myDF$STORE_R == &quot;EAST &quot;]) mean(myDF$SPEND[myDF$STORE_R == &quot;SOUTH &quot;]) mean(myDF$SPEND[myDF$STORE_R == &quot;WEST &quot;]) but it is easier to do all four of these calculations with the tapply function. We take a mean of the SPEND values, broken into groups according to the STORE_R tapply( myDF$SPEND, myDF$STORE_R, mean) We could find the total amount in the SPEND column in 2016 and then again in 2017. sum(myDF$SPEND[myDF$YEAR == &quot;2016&quot;]) sum(myDF$SPEND[myDF$YEAR == &quot;2017&quot;]) or we could do both of these calculations at once, using the tapply function. We take the sum of all SPEND amounts, broken into groups according to the YEAR tapply(myDF$SPEND, myDF$YEAR, sum) As a last example, we can calculate the amount spent on each day of purchases. We take the sum of all SPEND amounts, broken into groups according to the PURCHASE_ day tapply(myDF$SPEND, myDF$PURCHASE_, sum) It makes sense to sort the results and then look at the 20 days on which the sum of the SPEND amounts were the highest. tail(sort( tapply(myDF$SPEND, myDF$PURCHASE_, sum) ),n=20) Click here for video tapply( mydata, mygroups, myfunction, na.rm=T ) Some generic uses to explain how this would look, if we made the calculations in a naive/verbose/painful way myfunction(mydata[mygroups == 1], na.rm=T) myfunction(mydata[mygroups == 2], na.rm=T) myfunction(mydata[mygroups == 3], na.rm=T) .... myfunction(mydata[mygroups == &quot;IN&quot;], na.rm=T) myfunction(mydata[mygroups == &quot;OH&quot;], na.rm=T) myfunction(mydata[mygroups == &quot;IL&quot;], na.rm=T) .... myDF &lt;- read.csv(&quot;/class/datamine/data/flights/subset/2005.csv&quot;) head(myDF) sum the Distances of the flights, split into groups according to the airline (UniqueCarrier) sort(tapply(myDF$Distance, myDF$UniqueCarrier, sum)) Find the mean flight Distance, grouped according to the city of Origin sort(tapply(myDF$Distance, myDF$Origin, mean)) Calculate the mean departure delay (DepDelay), for each airplane (i.e., each TailNum), using na.rm=T because some of the values of the departure delays are NA tail(sort(tapply(myDF$DepDelay, myDF$TailNum, mean, na.rm=T)),n=20) Click here for video library(data.table) myDF &lt;- fread(&quot;/class/datamine/data/election/itcont2016.txt&quot;, sep=&quot;|&quot;) head(myDF) sum the amounts of all contributions made, grouped according to the STATE where the people lived sort(tapply(myDF$TRANSACTION_AMT, myDF$STATE, sum)) sum the amounts of all contributions made, grouped according to the CITY/STATE where the people lived tail(sort(tapply(myDF$TRANSACTION_AMT, paste(myDF$CITY, myDF$STATE), sum)),n=20) mylocations &lt;- paste(myDF$CITY, myDF$STATE) tail(sort(tapply(myDF$TRANSACTION_AMT, mylocations, sum)),n=20) sum the amounts of all contributions made, grouped according to the EMPLOYER where the people worked tail(sort(tapply(myDF$TRANSACTION_AMT, myDF$EMPLOYER, sum)), n=30) Click here for video Motivation: tapply is a powerful function that allows us to group data, and perform calculations on that data in bulk. The &quot;apply suite&quot; of functions provide a fast way of performing operations that would normally require the use of loops. Typically, when writing R code, you will want to use an &quot;apply suite&quot; function rather than a for loop. Context: The past couple of projects have studied the use of loops and/or vectorized operations. In this project, we will introduce a function called tapply from the &quot;apply suite&quot; of functions in R. Scope: r, for, tapply Learning objectives: Explain what &quot;recycling&quot; is in R and predict behavior of provided statements. Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc. Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc. Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. List the differences between lists, vectors, factors, and data.frames, and when to use each. Demonstrate a working knowledge of control flow in r: if/else statements, while loops, etc. Demonstrate how apply functions are generally faster than using loops. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/fars/7581.csv Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. The dataset, /class/datamine/data/fars/7581.csv contains the combined accident records from year 1975 to 1981. Load up the dataset into a data.frame named dat. In the previous project's question 4, we asked you to calculate the mean number of motorists involved in an accident (variable PERSON) with i drunk drivers where i takes the values from 0 through 6. This time, solve this question using the tapply function instead. Which method did you prefer and why? Now that you've read the data into a dataframe named dat, run the following code: # Read in data that maps state codes to state names state_names &lt;- read.csv(&quot;/class/datamine/data/fars/states.csv&quot;) # Create a vector of state names called v v &lt;- state_names$state # Set the names of the new vector to the codes names(v) &lt;- state_names$code # Create a new column in the dat dataframe with the actual names of the states dat$mystates &lt;- v[as.character(dat$STATE)] Relevant topics: tapply, mean Item(s) to submit: R code used to solve the problem. The output/solution. Solution dat &lt;- read.csv(&quot;/class/datamine/data/fars/7581.csv&quot;) state_names &lt;- read.csv(&quot;/class/datamine/data/fars/states.csv&quot;) v &lt;- state_names$state names(v) &lt;- state_names$code dat$mystates &lt;- v[as.character(dat$STATE)] tapply(dat$PERSONS, dat$DRUNK_DR, mean, na.rm=T) 2. Make a state-by-state classification of the average number of drunk drivers in an accident. Which state has the highest average number of drunk drivers per accident? Relevant topics: tapply, mean, sort Item(s) to submit: R code used to solve the problem. The entire output. Which state has the highest average number of drunk drivers per accident? Solution st &lt;- tapply(dat$DRUNK_DR, dat$mystates, mean, na.rm=T) st[which.max(st)] # or sort(tapply(dat$DRUNK_DR, dat$mystates, mean, na.rm=T), decreasing=T) 3. Add up the total number of fatalities, according to the day of the week on which they occurred. Are the numbers surprising to you? What days of the week have a higher number of fatalities? If instead you calculate the proportion of fatalities over the total number of people in the accidents, what would you expect? Calculate it and see if your expectations match. Hint: Sundays through Saturdays are days 1 through 7, respectively. Day 9 indicates that the day is unknown. This video example uses the Amazon fine food reviews dataset to make a similar calculation, in which we have two tapply statements, and we divide the results to get a ton of similar ratios all at once. Powerful stuff! It may guide you in your thinking about this question. Code from the example: double tapply examples Click here for video Relevant topics: tapply Item(s) to submit: R code used to solve the problem. What days have the highest number of fatalities? What would you expect if you calculate the proportion of fatalities over the total number of people in the accidents? Solution sort(tapply(dat$FATALS, dat$DAY_WEEK, sum)) sort(tapply(dat$FATALS, dat$DAY_WEEK, sum)/tapply(dat$PERSONS, dat$DAY_WEEK, sum)) 4. How many drunk drivers are involved, on average, in crashes that occur on straight roads? How many drunk drivers are involved, on average, in crashes that occur on curved roads? Solve the pair of questions in a single line of R code. Hint: The ALIGNMNT variable is 1 for straight, 2 for curved, and 9 for unknown. Relevant topics: tapply, sum Item(s) to submit: R code used to solve the problem. Results from running the R code. Solution tapply(dat$DRUNK_DR, dat$ALIGNMNT, mean, na.rm=T) 5. Break the day into portions, as follows: midnight to 6AM, 6AM to 12 noon, 12 noon to 6PM, 6PM to midnight, other. Find the total number of fatalities that occur during each of these time intervals. Also, find the average number of fatalities per crash that occurs during each of these time intervals. This example demonstrates a comparable calculation. In the video, I used the total number of people in the accident, and your question is (instead) about the number of fatalities, but this is essentially the only difference. I hope it helps to explain the way that the cut function works, along with the analogous breaks. Click here for video Relevant topics: tapply, cut, sum Item(s) to submit: R code used to solve the problem. Results from running the R code. Solution tapply(dat$FATALS, cut(dat$HOUR, breaks=c(0,6,12,18,24,99), include.lowest=T), sum) Project 7 Motivation: Three bread-and-butter functions that are a part of the base R are: subset, merge, and split. subset provides a more natural way to filter and select data from a data.frame. split is a useful function that splits a dataset based on one or more factors. merge brings the principals of combining data that SQL uses, to R. Context: We've been getting comfortable working with data in within the R environment. Now we are going to expand our toolset with three useful functions, all the while gaining experience and practice wrangling data! Scope: r, subset, merge, split, tapply Learning objectives: Gain proficiency using split, merge, and subset. Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc. Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Demonstrate how to use tapply to solve data-driven problems. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/goodreads/csv Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. Load up the following two datasets goodreads_books.csv and goodreads_book_authors.csvinto two data.frames books, and authors, respectively. How many columns and rows are in each of these two datasets? Relevant topics: read.csv, dim Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution books &lt;- read.csv(&quot;/class/datamine/data/goodreads/csv/books_sample.csv&quot;) authors &lt;- read.csv(&quot;/class/datamine/data/goodreads/csv/goodreads_book_authors.csv&quot;) dim(books) dim(authors) # or str(books) str(authors) 2. We want to figure out how book size (num_pages) is associated with various metrics. First, let's create a vector called book_size, that categorizes books into 4 categories based on num_pages: small (up to 250 pages), medium (250-500 pages), large (500-1000 pages), huge (1000+ pages). Note: This video and code might be helpful. Relevant topics: cut Item(s) to submit: R code used to solve the problem. The result of table(book_size). Solution book_size &lt;- cut(books$num_pages, breaks=c(-Inf, 250, 500, 1000, Inf), labels=c(&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;, &quot;huge&quot;)) table(book_size) head(books$num_pages) # We can split each element into categories, # based on how num_pages compares to 0, 250, 500, 1000, # and a huge number (say, 50000000, or Inf for infinity) head(cut(books$num_pages, breaks=c(0,250,500,1000,50000000),include.lowest=T)) # Or like this: head(cut(books$num_pages, breaks=c(0,250,500,1000,Inf),include.lowest=T)) # It is worthwhile to make sure that we included every book. # For instance, if you do not use include.lowest=T, then we # will miss the pages that are exactly equal to 0. # Or if you do not use a big enough number on the right hand side, # then you will miss the very long pages. # We know, from question 1, that we should have 1000000 books # and we can verify that we got them all here: table( cut(books$num_pages, breaks=c(0,250,500,1000,Inf),include.lowest=T), useNA=&quot;always&quot; ) sum(table( cut(books$num_pages, breaks=c(0,250,500,1000,Inf),include.lowest=T), useNA=&quot;always&quot; )) # Now we create the book_size vector: book_size &lt;- cut(books$num_pages, breaks=c(0,250,500,1000,Inf), include.lowest=T) # and we can check that the first several entries look reasonable. head(book_size) 3. Use tapply to calculate the mean average_rating, text_reviews_count, and publication_year by book_size. Did any of the result surprise you? Why or why not? Relevant topics: tapply Item(s) to submit: R code used to solve the problem. The output from running the R code. Solution tapply(books$average_rating, book_size, mean, na.rm=T) tapply(books$text_reviews_count, book_size, mean, na.rm=T) tapply(books$publication_year, book_size, mean, na.rm=T) # It might be surprising, for instance, that longer books get # more reviews and higher reviews. Answers to this part of the # question will vary. 4. Notice in (3) how we used tapply 3 times. This would get burdensome if we decided to calculate 4 or 5 or 6 columns instead. Instead of using tapply, we can use split, lapply, and colMeans to perform the same calculations. Use split to partition the data containing only the following 3 columns: average_rating, text_reviews_count, and publication_year, by book_size. Save the result as books_by_size. What class is the result? lapply is a function that allows you to loop over each item in a list and apply a function. Use lapply and colMeans to perform the same calculation as in (3). Note: This video and code and also this video and code might be helpful. Relevant topics: lapply, split, colMeans, indexing Item(s) to submit: R code used to solve the problem. The output from running the code. Solution books_by_size &lt;- split(books[,c(&#39;text_reviews_count&#39;,&#39;average_rating&#39;,&#39;publication_year&#39;)], book_size) lapply(books_by_size, colMeans, na.rm=TRUE) # We can build a temporarily data frame that has the 3 desired variables: head(data.frame(books$average_rating, books$text_reviews_count, books$publication_year)) # and split it according to the book_size. # We get a list of length 4 as a result: books_by_size &lt;- split(data.frame(books$average_rating, books$text_reviews_count, books$publication_year), book_size) class(books_by_size) length(books_by_size) str(books_by_size) # and then we can take the colMeans of each of these 4 parts of the list: lapply(books_by_size, colMeans, na.rm=T) # and the results agree with the results from question 3. 5. We are working with a lot more data than we really want right now. We've provided you with the following code to filter out non-English books and only keep columns of interest. This will create a data frame called en_books. en_books &lt;- books[books$language_code %in% c(&quot;en-US&quot;, &quot;en-CA&quot;, &quot;en-GB&quot;, &quot;eng&quot;, &quot;en&quot;, &quot;en-IN&quot;) &amp; books$publication_year &gt; 2000, c(&quot;author_id&quot;, &quot;book_id&quot;, &quot;average_rating&quot;, &quot;description&quot;, &quot;title&quot;, &quot;ratings_count&quot;, &quot;language_code&quot;, &quot;publication_year&quot;)] Now create an equivalent data frame of your own, by using the subset function (instead of indexing). Use res as the name of the data frame that you create. Do the dimensions (using dim) of en_books and res agree? Why or why not? (They should both have 8 columns, but a different number of rows.) Hint: Since the dimensions don't match, take a look at NA values for the variables used to subset our data. Note: This video and code and also this video and code might be helpful. Relevant topics: indexing, subset, NA, %in% Item(s) to submit: R code used to solve the problem. Do the dimensions match? 1-2 sentences explaining why or why not. Solution en_books_subset &lt;- subset(books, language_code %in% c(&quot;en-US&quot;, &quot;en-CA&quot;, &quot;en-GB&quot;, &quot;eng&quot;, &quot;en&quot;, &quot;en-IN&quot;) &amp; publication_year &gt; 2000, select=c(author_id, book_id, average_rating,description, title, ratings_count, language_code, publication_year)) en_books &lt;- books[books$language_code %in% c(&quot;en-US&quot;, &quot;en-CA&quot;, &quot;en-GB&quot;, &quot;eng&quot;, &quot;en&quot;, &quot;en-IN&quot;) &amp; books$publication_year &gt; 2000, c(&quot;author_id&quot;, &quot;book_id&quot;, &quot;average_rating&quot;, &quot;description&quot;, &quot;title&quot;, &quot;ratings_count&quot;, &quot;language_code&quot;, &quot;publication_year&quot;)] dim(en_books) dim(en_books_subset) sum(is.na(en_books_subset$language_code)) sum(is.na(en_books$language_code)) # you can see that without the NAs the results are the same all.equal(en_books_subset, en_books[!is.na(en_books$language_code),], check.attributes=F) # subset is automatically removing NAs en_books &lt;- books[books$language_code %in% c(&quot;en-US&quot;, &quot;en-CA&quot;, &quot;en-GB&quot;, &quot;eng&quot;, &quot;en&quot;, &quot;en-IN&quot;) &amp; books$publication_year &gt; 2000, c(&quot;author_id&quot;, &quot;book_id&quot;, &quot;average_rating&quot;, &quot;description&quot;, &quot;title&quot;, &quot;ratings_count&quot;, &quot;language_code&quot;, &quot;publication_year&quot;)] # We build a new data frame called res that takes a subset # of the data frame books that meet the two given conditions # on language_code and publication_year and that contains the 8 # desired columns. res &lt;- subset(books, subset=language_code %in% c(&quot;en-US&quot;, &quot;en-CA&quot;, &quot;en-GB&quot;, &quot;eng&quot;, &quot;en&quot;, &quot;en-IN&quot;) &amp; publication_year &gt; 2000, select=c(&quot;author_id&quot;, &quot;book_id&quot;, &quot;average_rating&quot;, &quot;description&quot;, &quot;title&quot;, &quot;ratings_count&quot;, &quot;language_code&quot;, &quot;publication_year&quot;)) class(res) dim(res) # This data frame is smaller. # The earlier data frame has some rows with missing information, # which are not included here. # (We might want to give examples/details about what is missing.) 6. We now have a nice and tidy subset of data, called res. It would be really nice to get some information on the authors. We can find that information in authors dataset loaded in question 1! In question 2 of the previous project, we had a similar issue with the states names. There is a much better and easier way to solve these types of problems. Use the merge function to combine res and authors in a way which appends all information from author when there is a match in res. Use the condition by=&quot;author_id&quot; in the merge. This is all you need to do: mymergedDF &lt;- merge(res, authors, by=&quot;author_id&quot;) Note: The resulting data frame will have all of the columns that are found in either res or authors. When we perform the merge, we only insist that the author_id should match. We do not expect that the ratings_count or average_rating should agree in res versus authors. Why? In the res data frame, the ratings_count and average_rating refer to the specific book, but in the authors data frame, the ratings_count and average_rating refer to the total works by the author. Therefore, in mymergedDF, there are columns ratings_count.x and average_rating.x from res, and there are columns ratings_count.y and average_rating.y from authors. Note: Although we provided the necessary code for this example, you might want to know more about the merge function. This video and code and also this video and code might be helpful. Relevant topics: merge Item(s) to submit: the given R code used to solve the problem. The dim of the newly merged data.frame. Solution res &lt;- merge(books, authors, by=&quot;author_id&quot;) dim(res) 7. For an author of your choice (that is in the dataset), find the author's highest rated book. Do you agree? Relevant topics: indexing, subset, which, max Item(s) to submit: R code used to solve the problem. The title of the highest rated book (from your author). 1-2 sentences explaining why or why not you agree with it being the highest rated book from that author. Solution brandons_books &lt;- res[res$name==&quot;Brandon Sanderson&quot;,] brandons_books[which.max(brandons_books$average_rating_books),] myDF &lt;- merge(res, authors, by=&quot;author_id&quot;) # Here are the books by author Sandra Cisneros: myfavoriteDF &lt;- myDF[myDF$name==&quot;Sandra Cisneros&quot;, ] myfavoriteDF # Her highest rated book has the rating 4.23: max(myfavoriteDF$average_rating.x) # It is called &quot;A House of My Own: Stories from My Life&quot;: myfavoriteDF[which.max(myfavoriteDF$average_rating.x), ]$title OPTIONAL QUESTION. Look at the column names of the new dataframe created in question 6. Notice that there are two values for ratings_count and two values for average_rating. The names that have an appended x are those values from the first argument to merge, and the names that have an appended y, are those values from the second argument to merge. Rename these columns to indicate if they refer to a book, or an author. Hint: For example, ratings_count.x could be ratings_count_book or ratings_count_author. Relevant topics: names Item(s) to submit: R code used to solve the problem. The names of the new data.frame. Solution res &lt;- merge(books, authors, by=&quot;author_id&quot;) # rename columns names(res) &lt;- gsub(&quot;\\\\.x&quot;, &quot;_books&quot;, names(res)) names(res) &lt;- gsub(&quot;\\\\.y&quot;, &quot;_author&quot;, names(res)) dim(res) Project 8 Motivation: A key component to writing efficient code is writing functions. Functions allow us to repeat and reuse coding steps that we used previously, over and over again. If you find you are repeating code over and over, a function may be a good way to reduce lots of lines of code! Context: We've been learning about and using functions all year! Now we are going to learn more about some of the terminology and components of a function, as you will certainly need to be able to write your own functions soon. Scope: r, functions Learning objectives: Gain proficiency using split, merge, and subset. Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc. Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Demonstrate how to use tapply to solve data-driven problems. Comprehend what a function is, and the components of a function in R. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/goodreads/csv Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. Read in the same data, in the same way as the previous project (with the same names). We've provided you with the function below. How many arguments does the function have? Name all of the arguments. What is the name of the function? Replace the description column in our books data.frame with the same information, but with stripped punctuation using the function provided. # A function that, given a string (myColumn), returns the string # without any punctuation. strip_punctuation &lt;- function(myColumn) { # Use regular expressions to identify punctuation. # Replace identified punctuation with an empty string &#39;&#39;. desc_no_punc &lt;- gsub(&#39;[[:punct:]]+&#39;, &#39;&#39;, myColumn) # Return the result return(desc_no_punc) } Hint: Since gsub accepts a vector of values, you can pass an entire vector to strip_punctuation. Relevant topics: functions Item(s) to submit: R code used to solve the problem. How many arguments does the function have? What are the name(s) of all of the arguments? What is the name of the function? Solution # The function has 1 argument called &quot;myColumn&quot; # The name of the function is &quot;strip_punctuation&quot; books$description &lt;- strip_punctuation(books$description) 2. Use the strsplit function to split a string by spaces. Some examples would be: strsplit(&quot;This will split by space.&quot;, &quot; &quot;) ## [[1]] ## [1] &quot;This&quot; &quot;will&quot; &quot;split&quot; &quot;by&quot; &quot;space.&quot; strsplit(&quot;This. Will. Split. By. A. Period.&quot;, &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;This&quot; &quot; Will&quot; &quot; Split&quot; &quot; By&quot; &quot; A&quot; &quot; Period&quot; An example string is: test_string &lt;- &quot;This is a test string with no punctuation&quot; Test out strsplit using the provided test_string. Make sure to copy and paste the code that declares test_string. If you counted the words shown in your results, would it be an accurate count? Why or why not? Relevant topics: strsplit, functions Item(s) to submit: R code used to solve the problem. 1-2 sentences explaining why or why not your count would be accurate. Solution test_string &lt;- &quot;This is a test string with no punctuation&quot; strsplit(test_string, &quot; &quot;) # It would not be accurate because it is counting the extra spaces as words. 3. Fix the issue in (2), using which. You may need to unlist the strsplit result first. After you've accomplished this, you can count the remaining words! Relevant topics: strsplit, sum, which Item(s) to submit: R code used to solve the problem (including counting the words). Solution test &lt;- unlist(strsplit(test_string, &quot; &quot;)) length(test[test!=&quot;&quot;]) sum(test!=&quot;&quot;) 4. We are finally to the point where we have code from questions (2) and (3) that we think we may want to use many times. Write a function called count_words which, given a string, description, returns the number of words in description. Test out count_words on the description from the second row of books. How many words are in the description? Relevant topics: functions, unlist, indexing, strsplit Item(s) to submit: R code used to solve the problem. The result of using the function on the description from the second row of books. Solution count_words &lt;- function(description) { split_desc &lt;- unlist(strsplit(description, &quot; &quot;)) return(length(split_desc[split_desc!=&quot;&quot;])) } count_words(books$description[2]) 5. Practice makes perfect! Write a function of your own design that is intended on being used with one of our datasets. Test it out and share the results. Note: You could even pass (as an argument) one of our datasets to your function and calculate a cool statistic or something like that! Maybe your function makes a plot? Who knows? Relevant topics: functions Item(s) to submit: R code used to solve the problem. An example (with output) of using your newly created function. Solution # Could be anything. Project 9 Motivation: A key component to writing efficient code is writing functions. Functions allow us to repeat and reuse coding steps that we used previously, over and over again. If you find you are repeating code over and over, a function may be a good way to reduce lots of lines of code! Context: We've been learning about and using functions all year! Now we are going to learn more about some of the terminology and components of a function, as you will certainly need to be able to write your own functions soon. Scope: r, functions Learning objectives: Gain proficiency using split, merge, and subset. Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc. Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Demonstrate how to use tapply to solve data-driven problems. Comprehend what a function is, and the components of a function in R. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/goodreads/csv Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. We've provided you with a function below. How many arguments does the function have, and what are their names? You can get a book_id from the URL of a goodreads book's webpage. For example, if you search for the book Words of Radianceon goodreads, the book_id contained in the url https://www.goodreads.com/book/show/17332218-words-of-radiance#, is 17332218. Another example is https://www.goodreads.com/book/show/157993.The_Little_Prince?from_search=true&amp;from_srp=true&amp;qid=JJGqUK9Vp9&amp;rank=1, (the little prince) with a book_id of 157993. Find 2 or 3 book_ids and test out the function until you get two successes. Explain in words, what the function is doing, and what options you have. library(imager) books &lt;- read.csv(&quot;/class/datamine/data/goodreads/csv/goodreads_books.csv&quot;) authors &lt;- read.csv(&quot;/class/datamine/data/goodreads/csv/goodreads_book_authors.csv&quot;) get_author_name &lt;- function(my_authors_dataset, my_author_id){ return(my_authors_dataset[my_authors_dataset$author_id==my_author_id,&#39;name&#39;]) } fun_plot &lt;- function(my_authors_dataset, my_books_dataset, my_book_id, display_cover=T) { book_info &lt;- my_books_dataset[my_books_dataset$book_id==my_book_id,] all_books_by_author &lt;- my_books_dataset[my_books_dataset$author_id==book_info$author_id,] author_name &lt;- get_author_name(my_authors_dataset, book_info$author_id) img &lt;- load.image(book_info$image_url) if(display_cover){ par(mfrow=c(1,2)) plot(img, axes=FALSE) } plot(all_books_by_author$num_pages, all_books_by_author$average_rating, ylim=c(0,5.1), pch=21, bg=&#39;grey80&#39;, xlab=&#39;Number of pages&#39;, ylab=&#39;Average rating&#39;, main=paste(&#39;Books by&#39;, author_name)) points(book_info$num_pages, book_info$average_rating,pch=21, bg=&#39;orange&#39;, cex=1.5) } Relevant topics: functions Item(s) to submit: How many arguments does the function have, and what are their names? The result of using the function on 2-3 book_ids. 1-2 sentences explaining what the function does (generally), and what (if any) options the function provides you with. Solution # fun_plot has 4 arguments, my_authors_dataset, my_books_dataset, my_book_id, and display_cover. # Given a `book_id`, the function finds all books by the same author and plots the book cover # next to a plot of ratings where the current book is highlighted. You can choose to show the # cover image or not. fun_plot(authors, books, 17332218) fun_plot(authors, books, 17593132) 2. You may have encountered a situation where the my_book_id was not in our dataset, and hence, didn't get plotted. When writing functions, it is usually best to try and foresee issues like this and have the function fail gracefully, instead of showing some ugly (and sometimes unclear) warning. Add some code at the beginning of our function that checks to see if my_book_id is within our dataset, and if it does not exist, prints &quot;Book ID not found.&quot;, and exits the function. Test it out on book_id=123 and book_id=19063. Hint: Run ?stop to see if that is a function that may be useful. Relevant topics: functions, if/else, stop Item(s) to submit: R code with your new and improved function. The results from fun_plot(123). The results from fun_plot(19063). Solution fun_plot &lt;- function(my_authors_dataset, my_books_dataset, my_book_id, display_cover=T) { if (!(my_book_id %in% my_books_dataset$book_id)) { stop(&quot;Book ID not found.&quot;) } book_info &lt;- my_books_dataset[my_books_dataset$book_id==my_book_id,] all_books_by_author &lt;- my_books_dataset[my_books_dataset$author_id==book_info$author_id,] author_name &lt;- get_author_name(my_authors_dataset, book_info$author_id) img &lt;- load.image(book_info$image_url) if(display_cover){ par(mfrow=c(1,2)) plot(img, axes=FALSE) } plot(all_books_by_author$num_pages, all_books_by_author$average_rating, ylim=c(0,5.1), pch=21, bg=&#39;grey80&#39;, xlab=&#39;Number of pages&#39;, ylab=&#39;Average rating&#39;, main=paste(&#39;Books by&#39;, author_name)) points(book_info$num_pages, book_info$average_rating,pch=21, bg=&#39;orange&#39;, cex=1.5) } fun_plot(authors, books, 19063) fun_plot(authors, books, 123) 3. We have this nice get_author_name function that accepts a dataset (in this case, our authors dataset), and a book_id and returns the name of the author. Write a new function called get_author_id that accepts an authors name and returns the author_id of the author. You can test your function using some of these examples: get_author_id(authors, &quot;Brandon Sanderson&quot;) # 38550 get_author_id(authors, &quot;J.K. Rowling&quot;) # 1077326 Relevant topics: functions Item(s) to submit: R code containing your new function. The results of using your new function on a few authors. Solution get_author_id &lt;- function(my_author_dataset, my_author_name) { return(my_author_dataset[my_author_dataset$name==my_author_name,]$author_id) } get_author_id(authors, &quot;Brandon Sanderson&quot;) # 38550 get_author_id(authors, &quot;J.K. Rowling&quot;) # 1077326 4. See the function below. search_books_for_word &lt;- function(word) { return(books[grepl(word, books$description, fixed=T),]$title) } Given a word, search_books_for_word returns the titles of books where the provided word is inside the book's description. search_books_for_word utilizes the books dataset internally. It requires that the books dataset has been loaded into the environment prior to running (and with the correct name). By including and referencing objects defined outside of our function's scope within our function (in this case the variable books), our search_books_for_word function will be more prone to errors, as any changes to those objects may break our function. For example: our_function &lt;- function(x) { print(paste(&quot;Our argument is:&quot;, x)) print(paste(&quot;Our variable is:&quot;, my_variable)) } # our variable outside the scope of our_function my_variable &lt;- &quot;dog&quot; # run our_function our_function(&quot;first&quot;) # change the variable outside the scope of our function my_variable &lt;- &quot;cat&quot; # run our_function again our_function(&quot;second&quot;) # imagine a scenario where &quot;my_variable&quot; doesn&#39;t exist, our_function would break! rm(my_variable) our_function(&quot;third&quot;) Fix our search_books_for_word function to accept the books dataset as an argument called my_books_dataset and utilize my_books_dataset within the function instead of the global variable books. Relevant topics: functions, read.csv, scoping Item(s) to submit: R code with your new and improved function. An example using the updated function. Solution search_books_for_word &lt;- function(my_books_dataset, word) { return(my_books_dataset[grepl(word, my_books_dataset$description, fixed=T),]$title) } 5. Write your own custom function. Make sure your function includes at least 2 arguments. If you access one of our datasets from within your function (which you definitely should do), use what you learned in (4), to avoid future errors dealing with scoping. Your function could output a cool plot, interesting tidbits of information, or anything else you can think of. Get creative and make a function that is fun to use! Relevant topics: scoping, functions Item(s) to submit: R code used to solve the problem. Examples using your function with included output. Solution # Could be anything. Project 10 Motivation: Functions are powerful. They are building blocks to more complex programs and behavior. In fact, there is an entire programming paradigm based on functions called functional programming. In this project, we will learn to apply functions to entire vectors of data using sapply. Context: We've just taken some time to learn about and create functions. One of the more common &quot;next steps&quot; after creating a function is to use it on a series of data, like a vector. sapply is one of the best ways to do this in R. Scope: r, sapply, functions Learning objectives: Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Utilize apply functions in order to solve a data-driven problem. Gain proficiency using split, merge, and subset. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/okcupid/filtered Questions Important note: Please make sure to double check that your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. Load up the the following datasets into data.frames named users and questions, respectively: /class/datamine/data/okcupid/filtered/users.csv, /class/datamine/data/okcupid/filtered/questions.csv. This is data from users on OkCupid, an online dating app. In your own words, explain what each file contains and how they are related -- its always a good idea to poke around the data to get a better understanding of how things are structured! Hint: Be careful, just because a file ends in .csv, does not mean it is comma-separated. You can change what separator read.csv uses with the sep argument. You can use the readLines function on a file (say, with n=10, for instance), to see the first lines of a file, and determine the character to use with the sep argument. Click here for video Relevant topics: read.csv Item(s) to submit: R code used to solve the problem. 1-2 sentences describing what each file contains and how they are related. Solution # You can show files and subdirectories using list.files list.files(&quot;/class/datamine/data/okcupid&quot;) # You can also take a look at files using file.info file.info(&quot;/class/datamine/data/okcupid/filtered/questions.csv&quot;) # You can peek at a file before reading it all in using readLines readLines(&quot;/class/datamine/data/okcupid/filtered/questions.csv&quot;, n=10) # we can see things are separated by semicolons users &lt;- read.csv(&quot;/class/datamine/data/okcupid/filtered/users.csv&quot;) questions &lt;- read.csv(&quot;/class/datamine/data/okcupid/filtered/questions.csv&quot;, sep=&quot;;&quot;) 2. grep is an incredibly powerful tool available to us in R. We will learn more about grep in the future, but for now, know that a simple application of grep is to find a word in a string. In R, grep is vectorized and can be applied to an entire vector of strings. Use grep to find a question that references &quot;google&quot;. What is the question? Click here for video Click here for video Hint: If at first you don't succeed, run ?grep and check out the ignore.case argument. Hint: To prepare for Question 3, look at the entire row of the questions data frame that has the question about google. The first entry on this row tells you the question that you need, in the users data frame, while working on Question 3. Relevant topics: grep Item(s) to submit: R code used to solve the problem. The text of the question that references Google. Solution questions[grep(&quot;google&quot;, questions$text, ignore.case=T),] 3. In (2) we found a pretty interesting question. What is the percentage of users that Google someone before the first date? Does the proportion change by gender (as defined by gender2)? How about by gender_orientation? Hint: The two videos posted in Question 2 might help. Hint: If you look at the column of users corresponding to the question identified in (2), you will see that this column of users has two possible answers, namely: &quot;No. Why spoil the mystery?&quot; and &quot;Yes. Knowledge is power!&quot;. Hint: Use the tapply function with three inputs: the correct column of users, breaking up the data according to gender2 or according to gender_orientation, and use this as your function in the tapply: function(x) {prop.table(table(x, useNA=&quot;always&quot;))} Relevant topics: functions, tapply, table, prop.table Item(s) to submit: R code used to solve this problem. The results of running the code. Written answers to the questions. Solution prop.table(table(users_filtered$`q170849`)) prop_google &lt;- function(answers_to_q170849){ return(prop.table(table(answers_to_q170849))[2]) } tapply(users_filtered$`q170849`, users_filtered$gender2, prop_google) tapply(users_filtered$`q170849`, users_filtered$gender_orientation, prop_google) # ~35% of people Google people before the first date. # According to this set of data, men Google woman 10% less than woman Google men before the first date. # When broken down by the provided gender orientations, hetero females, bisexual females, and gay males tend to # Google people before the first date more than bisexual males, gay females, and hetero males. 4. In project (8) we created a function called count_words. Use this function and sapply to create a vector which contains the number of words in each row of the column text from the questions dataframe. Call the new vector question_length, and add it as a column to the questions dataframe. count_words &lt;- function(my_text) { my_split_text &lt;- unlist(strsplit(my_text, &quot; &quot;)) return(length(my_split_text[my_split_text!=&quot;&quot;])) } Click here for video Relevant topics: sapply Item(s) to submit: R code used to solve this problem. The result of str(questions) (this shows how your questions data frame looks, after adding the new column called question_length). Solution count_words &lt;- function(description) { split_desc &lt;- unlist(strsplit(description, &quot; &quot;)) return(length(split_desc[split_desc!=&quot;&quot;])) } questions$question_length &lt;- sapply(as.character(questions$text), count_words) 5. Consider this function called number_of_options that accepts a data frame (for instance, questions) number_of_options &lt;- function(myDF) { table(apply(as.matrix(myDF[ ,3:6]), 1, function(x) {sum(!(x==&quot;&quot;))})) } and counts the number of questions that have each possible number of responses. For instance, if we calculate: number_of_options(questions) we get: 0 2 3 4 590 936 519 746 which means that: 590 questions have 0 possible responses; 936 questions have 2 possible responses; 519 questions have 3 possible responses; and 746 questions have 4 possible responses. Now use the split function to break the data frame questions into 7 smaller data frames, according to the value in questions$Keywords. Then use the sapply function to determine, for each possible value of questions$Keywords, the analogous breakdown of questions with different numbers of responses, as we did above. Hint: You can write: mylist &lt;- split(questions, questions$Keywords) sapply(mylist, number_of_options) Click here for video Background/explanation: The way sapply works is the the first argument is by default the first argument to your function, the second argument is the function you want applied, and after that you can specify arguments by name. For example: test1 &lt;- c(1, 2, 3, 4, NA, 5) test2 &lt;- c(9, 8, 6, 5, 4, NA) mylist &lt;- list(first=test1, second=test2) # for a single vector in the list mean(mylist$first, na.rm=T) # what if we want to do this for each vector in the list? # how do we remove na&#39;s? sapply(mylist, mean) # we can specify the arguments that are for the mean function # by naming them after the first two arguments, like this sapply(mylist, mean, na.rm=T) # in the code shown above, na.rm=T is passed to the mean function # just like if you run the following mean(mylist$first, na.rm=T) mean(mylist$second, na.rm=T) # you can include as many arguments to mean as you normally would # and in any order. just make sure to name the arguments sapply(mylist, mean, na.rm=T, trim=0.5) # or sapply(mylist, mean, trim=0.5, na.rm=T) # which is similar to mean(mylist$first, na.rm=T, trim=0.5) mean(mylist$second, na.rm=T, trim=0.5) Relevant topics: sapply, functions, indexing Item(s) to submit: R code used to solve this problem. The results of the running the code. Solution number_of_options &lt;- function(myDF) { table(apply(as.matrix(myDF[ ,3:6]), 1, function(x) {sum(!(x==&quot;&quot;))})) } number_of_options(questions) mylist &lt;- split(questions, questions$Keywords) sapply(mylist, number_of_options) 6. Lots of questions are asked in this okcupid dataset. Explore the dataset, and either calculate an interesting statistic/result using sapply, or generate a graphic (with good x-axis and/or y-axis labels, main labels, legends, etc.), or both! Write 1-2 sentences about your analysis and/or graphic, and explain what you thought you'd find, and what you actually discovered. Click here for video Relevant topics: plotting, functions, sapply Item(s) to submit: R code used to solve this problem. The results from running your code. 1-2 sentences about your analysis and/or graphic, and explain what you thought you'd find, and what you actually discovered. Solution # Could be anything. OPTIONAL QUESTION. Does it appear that there is an association between the length of the question and whether or not users answered the question? Assume NA means &quot;unanswered&quot;. First create a function called percent_answered that, given a vector, returns the percentage of values that are not NA. Use percent_answered and sapply to calculate the percentage of users who answer each question. Plot this result, against the length of the questions. Hint: length_of_questions &lt;- questions$question_length[grep(&quot;^q&quot;, questions$X)] Hint: grep(&quot;^q&quot;, questions$X) returns the column index of every column that starts with &quot;q&quot;. Use the same trick we used in the previous hint, to subset our users data.frame before using sapply to apply percent_answered. Relevant topics: sapply, is.na, length, grep, plot Item(s) to submit: R code used to solve this problem. The plot. Whether or not you think there may or may not be an association between question length and whether or not the question is answered. Solution number_of_options &lt;- function(questions, question_id) { is_empty &lt;- function(value) { if (is.na(value)) return(TRUE) else if (value==&quot;&quot;) return(TRUE) else return(FALSE) } question &lt;- questions[questions$X==question_id,] is_empty(question$option_1) + is_empty(question$option_2) + is_empty(question$option_3) + is_empty(question$option_4) } prop.table(table(sapply(questions$X, number_of_options, questions = questions))) questions$number_options &lt;- sapply(questions$X, number_of_options, questions = questions) prop.table(table(sapply(questions$X, number_of_options, questions = questions))) questions$number_options &lt;- sapply(questions$X, number_of_options, questions = questions) percent_answered &lt;- function(col) { sum(!is.na(col))/length(col) } p_answered &lt;- sapply(users[,grep(&quot;^q&quot;, names(users))], percent_answered) length_of_questions &lt;- questions$question_length[grep(&quot;^q&quot;, questions$X)] plot(length_of_questions, p_answered) Project 11 Motivation: The ability to understand a problem, know what tools are available to you, and select the right tools to get the job done, takes practice. In this project we will use what you've learned so far this semester to solve data-driven problems. In previous projects, we've directed you towards certain tools. In this project, there will be less direction, and you will have the freedom to choose the tools you'd like. Context: You've learned lots this semester about the R environment. You now have experience using a very balanced &quot;portfolio&quot; of R tools. We will practice using these tools on a set of economic data from Zillow. Scope: R Learning objectives: Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Utilize apply functions in order to solve a data-driven problem. Gain proficiency using split, merge, and subset. Comprehend what a function is, and the components of a function in R. Demonstrate the ability to use nested apply functions to solve a data-driven problem. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/zillow Questions 1. Read /class/datamine/data/zillow/Zip_time_series.csv into a data.frame called zipc. Look at the RegionName column. It is supposed to be a 5-digit zip code. Either fix the column by writing a function and applying it to the column, or take the time to read the read.csv documentation by running ?read.csv and use an argument to make sure that column is not read in as an integer (which is why zip codes starting with 0 lose the leading 0 when being read in). Hint: This video demonstrates how to read in data and respect the leading zeroes. Click here for video Relevant topics: read.csv, sapply, functions, strrep, nchar Item(s) to submit: R code used to solve the problem. head of the RegionName column. Solution zipc &lt;- read.csv(&quot;/class/datamine/data/zillow/Zip_time_series.csv&quot;) fix_zip &lt;- function(x) { if (nchar(x) == 5) { return(as.character(x)) } paste0(strrep(&quot;0&quot;, 5-nchar(x)), x) } sapply(zipc$RegionName, fix_zip) # OR zipc &lt;- read.csv(&quot;/class/datamine/data/zillow/Zip_time_series.csv&quot;, colClasses = c(&quot;RegionName&quot;=&quot;character&quot;)) # OR zipc &lt;- data.frame(fread(&quot;/class/datamine/data/zillow/Zip_time_series.csv&quot;)) zipc$Date &lt;- as.Date(zipc$Date) 2. One might assume that the owner of a house tends to value that house more than the buyer. If that was the case, perhaps the median listing price (the price which the seller puts the house on the market, or ask price) would be higher than the ZHVI (Zillow Home Value Index -- essentially an estimate of the home value). For those rows where both MedianListingPrice_AllHomes and ZHVI_AllHomes have non-NA values, on average how much higher or lower is the median listing price? Can you think of any other reasons why this may be? Relevant topics: mean Item(s) to submit: R code used to solve the problem. The result itself and 1-2 sentences talking about whether or not you can think of any other reasons that may explain the result. Solution mean(zipc$MedianListingPrice_AllHomes - zipc$ZHVI_AllHomes, na.rm=T) 3. Convert the Date column to a date using as.Date. How many years of data do we have in this dataset? Create a line plot with lines for the average MedianListingPrice_AllHomes and average ZHVI_AllHomes by year. The result should be a single plot with multiple lines on it. Hint: Here we give two videos to help you with this question. The first video gives some examples about working with dates in R. Click here for video Hint: This second video gives an example about how to plot two line graphs at the same time in R. Click here for video Hint: For a nice addition, add a dotted vertical line on year 2008 near the housing crisis: abline(v=&quot;2008&quot;, lty=&quot;dotted&quot;) Relevant topics: cut, as.Date, tapply, plot, lines, legend Item(s) to submit: R code used to solve the problem. The results of running the code. Solution zipc$Date &lt;- as.Date(zipc$Date, format=&quot;%Y-%m-%d&quot;) zipc$Year &lt;- cut(zipc$Date, breaks=&quot;year&quot;) zipc$Year &lt;- substr(zipc$Year, 1, 4) listings &lt;- tapply(zipc$MedianListingPrice_AllHomes, zipc$Year, mean, na.rm=T) zhvis &lt;- tapply(zipc$ZHVI_AllHomes, zipc$Year, mean, na.rm=T) plot(names(listings), listings, type=&quot;l&quot;, ylim=c(118000, 320000), col=&quot;blue&quot;, lwd=2) lines(names(zhvis), zhvis, col=&quot;tomato&quot;, lwd=2) legend(&quot;topleft&quot;, legend=c(&quot;Listing price&quot;, &quot;ZHVI&quot;), lwd=2, col=c(&quot;blue&quot;, &quot;tomato&quot;)) abline(v=&quot;2008&quot;, lty=&quot;dotted&quot;) 4. Read /class/datamine/data/zillow/State_time_series.csv into a data.frame called states. Calculate the average median listing price by state, and create a map using plot_usmap from the usmap package that shows the average median price by state. Hint: We give a full example about how to plot values, by State, on a map. Click here for video Hint: In order for plot_usmap to work, you must name the column containing states' names to &quot;state&quot;. Hint: To split words like &quot;OhSoCool&quot; into &quot;Oh So Cool&quot;, try this: trimws(gsub('([[:upper:]])', ' \\\\1', &quot;OhSoCool&quot;)). This will be useful as you'll need to correct the RegionName column at some point in time. Notice that this will not completely fix &quot;DistrictofColumbia&quot;. You will need to fix that one manually. Relevant topics: tapply, plot_usmap Item(s) to submit: R code used to solve the problem. The resulting map. Solution states &lt;- read.csv(&quot;/class/datamine/data/zillow/State_time_series.csv&quot;) aux &lt;- tapply(states$MedianListingPrice_AllHomes, states$RegionName, mean, na.rm=T) dat &lt;- data.frame(state=names(aux), val=aux) dat$state &lt;- trimws(gsub(&#39;([[:upper:]])&#39;, &#39; \\\\1&#39;, dat$state)) dat$state[dat$state==&quot;Districtof Columbia&quot;] &lt;- &quot;District of Columbia&quot; plot_usmap(data = dat, values = &quot;val&quot;, color = &quot;black&quot;) + scale_fill_continuous(low = &quot;white&quot;, high = &quot;#C28E0E&quot;, name = &quot;Median price listing&quot;, label = scales::dollar) + theme(legend.position = &quot;right&quot;) 5. Read /class/datamine/data/zillow/County_time_series.csv into a data.frame named counties. Choose a state (or states) that you would like to &quot;dig down&quot; into county-level data for, and create a plot (or plots) like in (4) that show some interesting statistic by county. You can choose average median listing price if you so desire, however, you don't need to! There are other cool data! Hint: Make sure that you remember to aggregate your data by RegionName so the plot renders correctly. Hint: plot_usmap looks for a column named fips. Make sure to rename the RegionName column to fips prior to passing the data.frame to plot_usmap. Hint: If you get Question 4 working correctly, here are the main differences for Question 5. You need the regions to be &quot;counties&quot; instead of &quot;states&quot;, and you need the data.frame to have a column called fips instead of state. These are the main differences between Question 4 and Question 5. Relevant topics: tapply, plot_usmap Item(s) to submit: R code used to solve the problem. The resulting map. Solution counties_avg &lt;- tapply(counties$MedianListingPrice_AllHomes, counties$RegionName, mean, na.rm=T) counties_avg &lt;- data.frame(fips = names(counties_avg), MedianListingPrice = unname(counties_avg)) plot_usmap(data = counties_avg, values = &quot;MedianListingPrice&quot;, include=c(&quot;CA&quot;), color = &quot;black&quot;) + scale_fill_continuous(low = &quot;white&quot;, high = &quot;#C28E0E&quot;, name = &quot;Average median price listing&quot;, label = scales::dollar) + theme(legend.position = &quot;right&quot;) Project 12 Motivation: In the previous project you were forced to do a little bit of date manipulation. Dates can be very difficult to work with, regardless of the language you are using. lubridate is a package within the famous tidyverse, that greatly simplifies some of the most common tasks one needs to perform with date data. Context: We've been reviewing topics learned this semester. In this project we will continue solving data-driven problems, wrangling data, and creating graphics. We will introduce a tidyverse package that adds great stand-alone value when working with dates. Scope: r Learning objectives: Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Utilize apply functions in order to solve a data-driven problem. Gain proficiency using split, merge, and subset. Demostrate the ability to create basic graphs with default settings. Demonstratre the ability to modify axes labels and titles. Incorporate legends using legend(). Demonstrate the ability to customize a plot (color, shape/linetype). Convert strings to dates, and format dates using the lubridate package. Questions 1. Let's continue our exploration of the Zillow time series data. A useful package for dealing with dates is called lubridate. This is part of the famous tidyverse suite of packages. Run the code below to load it. Read the /class/datamine/data/zillow/State_time_series.csv dataset into a data.frame named states. What class and type is the column Date? library(lubridate) Relevant topics: class, typeof Item(s) to submit: R code used to solve the question. class and typeof column Date. Solution library(lubridate) states &lt;- read.csv(&quot;/class/datamine/data/zillow/State_time_series.csv&quot;) typeof(states$Date) class(states$Date) 2. Convert column Date to a corresponding date format using lubridate. Check that you correctly transformed it by checking its class like we did in question (1). Compare and contrast this method of conversion with the solution you came up with for question (3) in the previous project. Which method do you prefer? Hint: Take a look at the following functions from lubridate: ymd, mdy, dym. Hint: Here is a video about ymd, mdy, dym Click here for video Relevant topics: dates, lubridate Item(s) to submit: R code used to solve the question. class of modified column Date. 1-2 sentences stating which method you prefer (if any) and why. Solution states$Date &lt;- ymd(states$Date) class(states$Date) 3. Create 3 new columns in states called year, month, day_of_week (Sun-Sat) using lubridate. Get the frequency table for your newly created columns. Do we have the same amount of data for all years, for all months, and for all days of the week? We did something similar in question (3) in the previous project -- specifically, we broke each date down by year. Which method do you prefer and why? Hint: Take a look at functions month, year, day, wday. Hint: You may find the argument of label in wday useful. Hint: Here is a video about month, year, day, wday Click here for video Relevant topics: dates, lubridate Item(s) to submit: R code used to solve the question. Frequency table for newly created columns. 1-2 sentences answering whether or not we have the same amount of data for all years, months, and days of the week. 1-2 sentences stating which method you prefer (if any) and why. Solution states$year &lt;- year(states$Date) states$month &lt;- month(states$Date) states$day_of_week &lt;- wday(states$Date, label=TRUE) table(states$year) table(states$month) table(states$day_of_week) 4. Is there a better month or set of months to put your house on the market? Use tapply to compare the average DaysOnZillow_AllHomes for all months. Make a barplot showing our results. Make sure your barplot includes &quot;all of the fixings&quot; (title, labeled axes, legend if necessary, etc. Make it look good.). Relevant topics: tapply, barplot Hint: If you want to have the month's abbreviation in your plot, you may find both the month.abb object and the argument names.arg in barplot useful. Hint: This video might help with Question 4. Click here for video Item(s) to submit: R code used to solve the question. The barplot of the average DaysOnZillow_AllHomes for all months. 1-2 sentences answering the question &quot;Is there a better time to put your house on the market?&quot; based on your results. Solution avgDaysPerMonth &lt;- tapply(states$DaysOnZillow_AllHomes, states$month, mean, na.rm=T) barplot(avgDaysPerMonth, names.arg=month.abb, col=&#39;steelblue&#39;, main=&quot;Avg days on Zillow&quot;, ylab=&quot;# Days&quot;) 5. Filter the states data to contain only years from 2010+ and call it states2010plus. Make a lineplot showing the average DaysOnZillow_AllHomes by Date using states2010plus data. Can you spot any trends? Write 1-2 sentences explaining what (if any) trends you see. Relevant topics: subset, tapply, plot Item(s) to submit: R code used to solve the question. The time series lineplot for the average DaysOnZillow_AllHomes per date. 1-2 sentences commenting on the patterns found in the plot, and your impressions of it. Solution states2010plus &lt;- subset(states, year &gt;= 2010) avgdays &lt;- tapply(states2010plus$DaysOnZillow_AllHomes, states2010plus$Date, mean, na.rm=T) plot(ymd(names(avgdays)), avgdays, type=&#39;l&#39;) 6. Do homes sell faster in certain states? For the following states: 'California', 'Indiana', 'NewYork' and 'Florida', make a lineplot for DaysOnZillow_AllHomes by Date with one line per state. Use the states2010plus dataset for this question. Make sure to have each state line colored differently, and to add a legend to your plot. Examine the plot and write 1-2 sentences about any observations you have. Hint: You may want to use the lines function to add the lines for different state. Hint: Make sure to fix the y-axis limits using the ylim argument in plot to properly show all four lines. Hint: You may find the argument col useful to change the color of your line. Hint: To make your legend fit, consider using the states abbreviation, and the arguments ncol and cex of the legend function. Relevant topics: subset, indexing, plot, lines Item(s) to submit: R code used to solve the question. The time series lineplot for DaysOnZillow_AllHomes per date for the 4 states. 1-2 sentences commenting on the patterns found in the plot, and your answer to the question &quot;Do homes sell faster in certain states rather than others?&quot;. Solution indiana &lt;- subset(states2010plus, RegionName==&quot;Indiana&quot;) california &lt;- subset(states2010plus, RegionName==&quot;California&quot;) florida &lt;- subset(states2010plus, RegionName==&quot;Florida&quot;) new_york &lt;- subset(states2010plus, RegionName==&quot;NewYork&quot;) plot(california$Date, california$DaysOnZillow_AllHomes, type=&#39;l&#39;, col=&#39;tomato&#39;, ylim = c(min(states2010plus$DaysOnZillow_AllHomes, na.rm=T), 210), ylab=&#39;Days on zillow&#39;, xlab = &#39;Date&#39;) lines(indiana$Date, indiana$DaysOnZillow_AllHomes, col=&#39;steelblue&#39;) lines(florida$Date, florida$DaysOnZillow_AllHomes, col=&#39;green&#39;) lines(new_york$Date, new_york$DaysOnZillow_AllHomes, col=&#39;black&#39;) legend(&#39;topright&#39;, legend=c(&#39;CA&#39;,&#39;IN&#39;,&#39;FL&#39;, &#39;NY&#39;),lwd=2, col=c(&#39;tomato&#39;,&#39;steelblue&#39;,&#39;green&#39;,&#39;black&#39;), ncol=2, cex=.7) Project 13 Motivation: It's important to be able to lookup and understand the documentation of a new function. You may have looked up the documentation of functions like paste0 or sapply, and noticed that in the &quot;usage&quot; section, one of the arguments is an ellipsis (...). Well, unless you understand what this does, it's hard to really get it. In this project, we will experiment with ellipsis, and write our own function that utilizes one. Context: We've learned about, used, and written functions in many projects this semester. In this project, we will utilize some of the less-known features of functions. Scope: r, functions Learning objectives: Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Utilize apply functions in order to solve a data-driven problem. Gain proficiency using split, merge, and subset. Demostrate the ability to create basic graphs with default settings. Demonstratre the ability to modify axes labels and titles. Incorporate legends using legend(). Demonstrate the ability to customize a plot (color, shape/linetype). Convert strings to dates, and format dates using the lubridate package. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/beer/ Questions 1. Read /class/datamine/data/beer/beers.csv into a data.frame named beers. Read /class/datamine/data/beer/breweries.csv into a data.frame named breweries. Read /class/datamine/data/beer/reviews.csv into a data.frame named reviews. Hint: Notice that reviews.csv is a large file. Luckily, you can use a function from the famous data.table package called fread. The function fread is much faster at reading large file compared to read.csv. It reads the data into a class called data.table. We will learn more about this later on. For now, use fread to read in the reviews.csv data then convert it from the data.table class into a data.frame by wrapping the result of fread in the data.frame function. Hint: Do not forget to load the data.table library before attempeting to use the fread function. Below we show you an example of how fast the fread function is compared toread.csv. microbenchmark(read.csv(&quot;/class/datamine/data/beer/reviews.csv&quot;, nrows=100000), data.frame(fread(&quot;/class/datamine/data/beer/reviews.csv&quot;, nrows=100000)), times=5) Unit: milliseconds expr read.csv(&quot;/class/datamine/data/beer/reviews.csv&quot;, nrows = 1e+05) data.frame(fread(&quot;/class/datamine/data/beer/reviews.csv&quot;, nrows = 1e+05)) min lq mean median uq max neval 5948.6289 6482.3395 6746.8976 7040.5881 7086.6728 7176.2589 5 120.7705 122.3812 127.9842 128.7794 133.7695 134.2205 5 Hint: This video demonstrates how to read the reviews data using fread. Click here for video Relevant topics: fread, data.frame Item(s) to submit: R code used to solve the problem. 2. Take some time to explore the datasets. Like many datasets, our data is broken into 3 &quot;tables&quot;. What columns connect each table? How many breweries in breweries don't have an associated beer in beers? How many beers in beers don't have an associated brewery in breweries? Hint: We compare lists of names using sum or intersect. Similar techniques can be used for Question 2. Click here for video Relevant topics: names, in, logical operators, unique Item(s) to submit: R code used to solve the problem. A description of columns which connect each of the files. How many breweries don't have an associated beer in beers. How many beers don't have an associated brewery in breweries. 3. Run ?sapply and look at the usage section for sapply. If you look at the description for the ... argument, you'll see it is &quot;optional arguments to FUN&quot;. What this means is you can specify additional input for the function you are passing to sapply. One example would be passing T to na.rm in the mean function: sapply(dat, mean, na.rm=T). Use sapply and the strsplit function to separate the types of breweries (types) by commas. Use another sapply to loop through your results and count the number of types for each brewery. Be sure to name your final results n_types. What is the average amount of services (n_types) breweries in IN and MI offer (we are looking for the average of IN and MI combined)? Does that surprise you? Note: When you have one sapply inside of another, or one loop inside of another, or an if/else statement inside of another, this is commonly referred to as nesting. So when Googling, you can type &quot;nested sapply&quot; or &quot;nested if statements&quot;, etc. Hint: We show, in this video, how to find the average number of parts in a midwesterner's name. Perhaps surprisingly, this same technique will be useful in solving Question 3. Click here for video Relevant topics: sapply, ...,strplit, in, mean Item(s) to submit: R code used to solve the question. 1-2 sentences answering the average amount of services breweries in Indiana and Michigan offer, and commenting on this answer. 4. Write a function called compare_beers that accepts a function that you will call FUN, and any number of vectors of beer ids. The function, compare_beers, should cycle through each vector/groups of beer_ids, compute the function, FUN, on the subset of reviews, and print &quot;Group X: some_score&quot; where X is the number 1+, and some_score is the result of applying FUN on the subset of the reviews data. In the example below the function FUN is the median function and we have two vectors/groups of beer_ids passed with c(271781) being group 1 and c(125646, 82352) group 2. Note that even though our example only passes two vectors to our compare_beers function, we want to write the function in a way that we could pass as many vectors as we want to. Example: compare_beers(reviews, median, c(271781), c(125646, 82352)) This example gives the output: Group 1: 4 Group 2: 4.56 For your solution to this question, find the behavior of compare_beers in this example: compare_beers(reviews, median, c(88,92,7971), c(74986,1904), c(34,102,104,355)) Hint: There are different approaches to this question. You can use for loops or sapply. It will probably help to start small and build slowly toward the solution. Hint: This first video shows how to use ... in defining a function. Click here for video Hint: This second video basically walks students through how to build this function. If you use this video to learn how to build this function, please be sure to acknowledge this in your project solutions. Click here for video Relevant topics: in, ..., indexing, paste0, for loops Item(s) to submit: R code used to solve the problem. The result from running the provided example. 5. Beer wars! IN and MI against AZ and CO. Use the function you wrote in question (4) to compare beer_id from each group of states. Make a cool plot of some sort. Be sure to comment on your plot. Hint: Create a vector of beer_ids per group before passing it to your function from (4). Hint: This video demonstrates an example of how to use the compare_beers function. Click here for video Relevant topics: in, ..., indexing, paste0, for loops Item(s) to submit: R code used to solve the problem. The result from running your function. The resulting plot. 1-2 sentence commenting on your plot. Project 14 Motivation: Functions are the building blocks of more complex programming. It's vital that you understand how to read and write functions. In this project we will incrementally build and improve upon a function designed to recommend a beer. Note that you will not be winning any awards for this recommendation system, it is just for fun! Context: One of the main focuses throughout the semester has been on functions, and for good reason. In this project we will continue to exercise our R skills and build up our recommender function. Scope: r, functions Learning objectives: Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Utilize apply functions in order to solve a data-driven problem. Gain proficiency using split, merge, and subset. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/beer/ Questions 1. Read /class/datamine/data/beer/beers.csv into a data.frame named beers. Read /class/datamine/data/beer/breweries.csv into a data.frame named breweries. Read /class/datamine/data/beer/reviews.csv into a data.frame named reviews. As in the previous project, make sure you used the fread function from the data.table package, and convert the data.table to a data.frame. We want to create a very basic beer recommender. We will start simple. Create a function called recommend_a_beer that takes as input my_beer_id (a single value) and returns a vector of beer_ids from the same style. Test your function on 2093. Hint: Make sure you do not include the given my_beer_id in the vector of beer_ids containing the beer_idsof your recommended beers. Hint: You may find the function setdiff useful. Run the example below to get an idea of what it does. Note: You will not win any awards for this recommendation system! x &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;) y &lt;- c(&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;) setdiff(x,y) ## [1] &quot;a&quot; setdiff(y,x) ## [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; Relevant topics: fread, data.frame, function, setdiff Item(s) to submit: R code used to solve the problem. Length of result from recommend_a_beer(2093). The result of 2093 %in% recommend_a_beer(2093). 2. That is a lot of beer recommendations! Let's try to narrow it down. Include an argument in your function called min_score with default value of 4.5. Our recommender will only recommend beer_ids with a review score of at least min_score. Test your improved beer recommender with the same beer_id from question (1). Hint: Note that now we need to look at both beers and reviews datasets. Relevant topics: in, unique, subset/indexing Item(s) to submit: R code used to solve the problem. Length of result from recommend_a_beer(2093). 3. There is still room for improvement (obviously) for our beer recommender. Include a new argument in your function called same_brewery_only with default value FALSE. This argument will determine whether or not our beer recommender will return only beers from the same brewery. Test our newly improved beer recommender with the same beer_id from question (1) with the argument same_brewery_only set to TRUE. Hint: You may find the function intersect useful. Run the example below to get an idea of what it does. x &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;) y &lt;- c(&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;) intersect(x,y) ## [1] &quot;b&quot; &quot;c&quot; intersect(y,x) ## [1] &quot;c&quot; &quot;b&quot; Relevant topics: if/else, subset, intersect, indexing Item(s) to submit: R code used to solve the problem. Length of result from recommend_a_beer(2093, same_brewery_only=TRUE). 4. Oops! Bad idea! Maybe including only beers from the same brewery is not the best idea. Add an argument to our beer recommender named type. If type=style our recommender will recommend beers based on the style as we did in question (3). If type=reviewers, our recommender will recommend beers based on reviewers with &quot;similar taste&quot;. Select reviewers that gave score equal to or greater than min_score for the given beer id (my_beer_id). For those reviewers, find the beer_ids for other beers that these reviewers have given a score of at least min_score. These beer_ids are the ones our recommender will return. Be sure to test our improved recommender on the same beer_id as in (1)-(3). Relevant topics: if, subset, in, setdiff, unique Item(s) to submit: R code used to solve the problem. Length of result from recommend_a_beer(2093, type=&quot;reviewers&quot;). 5. Let's try to narrow down the recommendations. Include an argument called abv_range that indicates the abv range we would like the recommended beers to be at. Set abv_range default value to NULL so that if a user does not specify the abv_range our recommender does not consider it. Test our recommender for beer_id 2093, with abv_range = c(8.9,9.1) and min_score=4.9. Hint: You may find the function is.null useful. Relevant topics: if, &gt;=, &lt;=, intersect Item(s) to submit: R code used to solve the problem. Length of result from recommend_a_beer(2093, abv_range=c(8.9, 9.1), type=&quot;reviewers&quot;, min_score=4.9). 6. Play with our recommend_a_beer function. Include another feature to it. Some ideas are: putting a limit on the number of beer_ids we will return, error catching (what if we don't have reviews for a given beer_id?), including a plot to the output, returning beer names instead of ids or new arguments to decide what beer_ids to recommend. Be creative and have fun! Item(s) to submit: R code used to solve the problem. The result from running the improved recommend_a_beer function showcasing your improvements to it. 1-2 sentecens commenting on what you decided to include and why. Project 15 Motivation: Some people say it takes 20 hours to learn a skill, some say 10,000 hours. What is certain is it definitely takes time. In this project we will explore an interesting dataset and exercise some of the skills learned this semester. Context: This is the final project of the semester. We sincerely hope that you've learned something, and that we've provided you with first hand experience digging through data. Scope: r Learning objectives: Read and write basic (csv) data. Explain and demonstrate: positional, named, and logical indexing. Utilize apply functions in order to solve a data-driven problem. Gain proficiency using split, merge, and subset. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/donerschoose/ Questions 1. Read the data /class/datamine/data/donerschoose/Projects.csv into a data.frame called projects. Make sure you use the function you learned in Project 13 (fread) from the data.table package to read the data. Don't forget to then convert the data.table into a data.frame. Let's do an initial exploration of this data. What types of projects (Project.Type) are there? How many resource categories (Project.Resource.Category) are there? Relevant topics: fread, unique, length Item(s) to submit: R code used to solve the question. 1-2 sentences containing the project's types and how many resource categories are in the dataset. 2. Create two new variables in projects, the number of days a project's lasted and the number of days until the project was fully funded. Name those variables project_duration and time_until_funded, respectively. To calculate them use the project's posted date (Project.Posted.Date), expiration date (Project.Expiration.Date), and fully funded date (Project.Fully.Funded.Date). What are the shortest and longest times until a project is fully funded? For consistency check, see if we have any negative project's duration. If so, how many? Hint: You may find the argument units in the function difftime useful. Hint: Be sure to pay attention to the order of operations of difftime. Hint: Note that if you used the fread function from data.table to read in the data, you will not need to convert the columns as date. Hint: It is not required that you use difftime. Relevant topics: difftime, lubridate Item(s) to submit: R code used to solve the question. Shortest and longest times until a project is fully funded. 1-2 sentences answering whether we have if we have negative project's duration, and if so how many. 3. As you noted in question (2) there may be some project's with negative duration time. As we may have some concerns for the data regarding these projects, filter the projects data to exclude the projects with negative duration, and call this filtered data selected_projects. With that filtered data, make a dotchart for mean time until the project is fully funded (time_until_funded) for the various resource categories (Project.Resource.Category). Make sure to comment on your results. Are they surprising? Could there be another variable influencing this result? If so, name at least one. Hint: You will first need to average time until fully funded for the different categories before making your plot. Hint: To make your dotchart look nicer, you may want to first order the average time until fully funded before passing it to the dotchart function. In addition, consider reducing the y-axis font size using the argument cex. Relevant topics: indexing, subset, tapply, dotchart Item(s) to submit: R code used to solve the question. Resulting dotchart. 1-2 sentences commenting on your plot. Make sure to mention whether you are surprised or not by the results. Don't forget to add if you think there could be more factors influencing your answer, and if so, be sure to give examples. 4. Read /class/datamine/data/donerschoose/Schools.csv into a data.frame called schools. Combine selected_projects and schools by School.ID keeping only School.IDs present in both datasets. Name the combined data.frame selected_projects. Use the newly combined data to determine the percentage of already fully funded projects (Project.Current.Status) for schools in West Lafayette, IN. In addition, determine the state (School.State) with the highest number of projects. Be sure to specify the number of projects this state has. Hint: West Lafayette, IN zip codes are 47906 and 47907. Relevant topics: fread, subset, indexing, merge, table, prop.table, which.max Item(s) to submit: R code used to solve the question. 1-2 sentences answering the percentage of already fully funded projects for schools in West Lafayette, IN, the state with the highest number of projects, and the number of projects this state has. 5. Using the combined selected_projects data, get the school(s) (School.Name), city/cities (School.City) and state(s) (School.State) for the teacher with the highest percentage of fully funded projects (Project.Current.Status). Hint: There are many ways to solve this problem. For example, one option to get the teacher's ID is to create a variable indicating whether or not the project is fully funded and use tapply. Another option is to create prop.table and select the corresponding column/row. Hint: Note that each row in the data corresponds to a unique project ID. Hint: Once you have the teacher's ID, consider filtering projects to contain only rows for which the corresponding teacher's ID is in, and only the columns we are interested in: School.Name, School.City, and School.State. Then, you can get the unique values in this shortened data. Hint: To get only certain columns when subetting, you may find the argument select from subset useful. Relevant topics: indexing, which, max, subset, unique, row.names (if using table), names (if using tapply) Item(s) to submit: R code used to solve the question. Output of your code containing school(s), city(s) and state(s) of the selected teacher. STAT 29000 Project 1 Motivation: In this project we will jump right into an R review. In this project we are going to break one larger data-wrangling problem into discrete parts. There is a slight emphasis on writing functions and dealing with strings. At the end of this project we will have greatly simplified a dataset, making it easy to dig into. Context: We just started the semester and are digging into a large dataset, and in doing so, reviewing R concepts we've previously learned. Scope: data wrangling in R, functions Learning objectives: Comprehend what a function is, and the components of a function in R. Read and write basic (csv) data. Utilize apply functions in order to solve a data-driven problem. Make sure to read about, and use the template found here, and the important information about projects submissions here. You can find useful examples that walk you through relevant material in The Examples Book: https://thedatamine.github.io/the-examples-book It is highly recommended to read through, search, and explore these examples to help solve problems in this project. Important note: It is highly recommended that you use https://rstudio.scholar.rcac.purdue.edu/. Simply click on the link and login using your Purdue account credentials. We decided to move away from ThinLinc and away from the version of RStudio used last year (https://desktop.scholar.rcac.purdue.edu). That version of RStudio is known to have some strange issues when running code chunks. Remember the very useful documentation shortcut ?. To use, simply type ? in the console, followed by the name of the function you are interested in. You can also look for package documentation by using help(package=PACKAGENAME), so for example, to see the documentation for the package ggplot2, we could run: help(package=ggplot2) Sometimes it can be helpful to see the source code of a defined function. A function is any chunk of organized code that is used to perform an operation. Source code is the underlying R or c or c++ code that is used to create the function. To see the source code of a defined function, type the function's name without the (). For example, if we were curious about what the function Reduce does, we could run: Reduce Occasionally this will be less useful as the resulting code will be code that calls c code we can't see. Other times it will allow you to understand the function better. Dataset: /class/datamine/data/airbnb Often times (maybe even the majority of the time) data doesn’t come in one nice file or database. Explore the datasets in /class/datamine/data/airbnb. 1. You may have noted that, for each country, city, and date we can find 3 files: calendar.csv.gz, listings.csv.gz, and reviews.csv.gz (for now, we will ignore all files in the &quot;visualisations&quot; folders). Let's take a look at the data in each of the three types of files. Pick a country, city and date, and read the first 50 rows of each of the 3 datasets (calendar.csv.gz, listings.csv.gz, and reviews.csv.gz). Provide 1-2 sentences explaining the type of information found in each, and what variable(s) could be used to join them. Hint: read.csv has an argument to select the number of rows we want to read. Hint: Depending on the country that you pick, the listings and/or the reviews might not display properly in RMarkdown. So you do not need to display the first 50 rows of the listings and/or reviews, in your RMarkdown document. It is OK to just display the first 50 rows of the calendar entries. Item(s) to submit: Chunk of code used to read the first 50 rows of each dataset. 1-2 sentences briefly describing the information contained in each dataset. Name(s) of variable(s) that could be used to join them. To read a compressed csv, simply use the read.csv function: dat &lt;- read.csv(&quot;/class/datamine/data/airbnb/brazil/rj/rio-de-janeiro/2019-06-19/data/calendar.csv.gz&quot;) head(dat) Solution The calendar.csv.gz file for 2019-07-08 in Hawaii describes the listing_id, date, available (t or f), price, adjusted_price, minimum_nights, and maximum_nights hawaii_calendar &lt;- read.csv(&quot;/class/datamine/data/airbnb/united-states/hi/hawaii/2019-07-08/data/calendar.csv.gz&quot;) head(hawaii_calendar, n=50) The listings.csv.gz file for 2019-07-08 in Hawaii has 106 variables, which describe the very specific attributes of the airbnb listings. hawaii_listings &lt;- read.csv(&quot;/class/datamine/data/airbnb/united-states/hi/hawaii/2019-07-08/data/listings.csv.gz&quot;) dim(hawaii_listings) The reviews.csv.gz file for 2019-07-08 in Hawaii describes the listing_id, id, date, reviewer_id, reviewer_name, and comments hawaii_reviews &lt;- read.csv(&quot;/class/datamine/data/airbnb/united-states/hi/hawaii/2019-07-08/data/reviews.csv.gz&quot;) head(hawaii_reviews, n=50) The variables that might be used to compare the tables are: date, id, listing_id, maximum_nights, minimum_nights, price t &lt;- table(c(names(hawaii_calendar), names(hawaii_listings), names(hawaii_reviews))) t[t &gt; 1] Let's work towards getting this data into an easier format to analyze. From now on, we will focus on the listings.csv.gz datasets. 2. Write a function called get_paths_for_country, that, given a string with the country name, returns a vector with the full paths for all listings.csv.gz files, starting with /class/datamine/data/airbnb/.... For example, the output from get_paths_for_country(&quot;united-states&quot;) should have 28 entries. Here are the first 5 entries in the output: [1] &quot;/class/datamine/data/airbnb/united-states/ca/los-angeles/2019-07-08/data/listings.csv.gz&quot; [2] &quot;/class/datamine/data/airbnb/united-states/ca/oakland/2019-07-13/data/listings.csv.gz&quot; [3] &quot;/class/datamine/data/airbnb/united-states/ca/pacific-grove/2019-07-01/data/listings.csv.gz&quot; [4] &quot;/class/datamine/data/airbnb/united-states/ca/san-diego/2019-07-14/data/listings.csv.gz&quot; [5] &quot;/class/datamine/data/airbnb/united-states/ca/san-francisco/2019-07-08/data/listings.csv.gz&quot; Hint: list.files is useful with the recursive=T option. Hint: Use grep to search for the pattern listings.csv.gz (within the results from the first hint), and use the option value=T to display the values found by the grep function. Item(s) to submit: Chunk of code for your get_paths_for_country function. Solution We extract all 28 of the listings for the United States first: myprefix &lt;- &quot;/class/datamine/data/airbnb/united-states/&quot; paste0(myprefix, grep(&quot;listings.csv.gz&quot;, list.files(myprefix, recursive=T), value=T)) Now we build a function that can do the same thing, for any country get_paths_for_country &lt;- function(mycountry) { myprefix &lt;- paste0(&quot;/class/datamine/data/airbnb/&quot;, mycountry, &quot;/&quot;) paste0(myprefix, grep(&quot;listings.csv.gz&quot;, list.files(myprefix, recursive=T), value=T)) } and we test this for several countries: get_paths_for_country(&quot;united-states&quot;) get_paths_for_country(&quot;brazil&quot;) get_paths_for_country(&quot;south-africa&quot;) get_paths_for_country(&quot;canada&quot;) 3. Write a function called get_data_for_country that, given a string with the country name, returns a data.frame containing the all listings data for that country. Use your previously written function to help you. Hint: Use stringsAsFactors=F in the read.csv function. Hint: Use do.call(rbind, &lt;listofdataframes&gt;) to combine a list of dataframes into a single dataframe. Relevant topics: rbind, lapply, function Item(s) to submit: Chunk of code for your get_data_for_country function. Solution We first get the data from the Canada entries. To do this, we sapply the read.csv function to each of the 6 results from get_paths_for_country(&quot;canada&quot;) In other words, we read in these 6 data frames. myresults &lt;- sapply(get_paths_for_country(&quot;canada&quot;), read.csv, stringsAsFactors=F, simplify=F) We get a list of 6 data frames: length(myresults) class(myresults) class(myresults[[1]]) class(myresults[[6]]) and we can check the dimensions of each of the 6 data frames dim(myresults[[1]]) dim(myresults[[2]]) dim(myresults[[3]]) dim(myresults[[4]]) dim(myresults[[5]]) dim(myresults[[6]]) this is more easily accomplished with another sapply: sapply(myresults, dim) We can rbind all 6 of these data frames into one big data frame as follows: bigDF &lt;- do.call(rbind, myresults) class(bigDF) dim(bigDF) Now we create the desired function called get_data_for_country get_data_for_country &lt;- function(mycountry) { myresults &lt;- sapply(get_paths_for_country(mycountry), read.csv, stringsAsFactors=F, simplify=F) do.call(rbind, myresults) } and we test it on Canada. mynewbigDF &lt;- get_data_for_country(&quot;canada&quot;) The result has the same size as before dim(mynewbigDF) 4. Use your get_data_for_country to get the data for a country of your choice, and make sure to name the data.frame listings. Take a look at the following columns: host_is_superhost, host_has_profile_pic, host_identity_verified, and is_location_exact. What is the data type for each column? (You can use class or typeof or str to see the data type.) These columns would make more sense as logical values (TRUE/FALSE/NA). Write a function called transform_column that, given a column containing lowercase &quot;t&quot;s and &quot;f&quot;s, your function will transform it to logical (TRUE/FALSE/NA) values. Note that NA values for these columns appear as blank (&quot;&quot;), and we need to be careful when transforming the data. Test your function on column host_is_superhost. Relevant topics: class, typeof, str, toupper, as.logical Item(s) to submit: Chunk of code for your transform_column function. Type of transform_column(listings$host_is_superhost). Solution These 4 columns from mynewbigDF (which has the data for Canada) only have values &quot;t&quot;, &quot;f&quot;, &quot;&quot; head(mynewbigDF$host_is_superhost) head(mynewbigDF$host_has_profile_pic) head(mynewbigDF$host_identity_verified) head(mynewbigDF$is_location_exact) Please note the 44 values of &quot;&quot; (which are easy to miss) In the first 3 out of 4 of these columns: table(mynewbigDF$host_is_superhost) table(mynewbigDF$host_has_profile_pic) table(mynewbigDF$host_identity_verified) table(mynewbigDF$is_location_exact) These are all character vectors, which we can check using class, typeof, or str: class(mynewbigDF$host_is_superhost) class(mynewbigDF$host_has_profile_pic) class(mynewbigDF$host_identity_verified) class(mynewbigDF$is_location_exact) typeof(mynewbigDF$host_is_superhost) typeof(mynewbigDF$host_has_profile_pic) typeof(mynewbigDF$host_identity_verified) typeof(mynewbigDF$is_location_exact) str(mynewbigDF$host_is_superhost) str(mynewbigDF$host_has_profile_pic) str(mynewbigDF$host_identity_verified) str(mynewbigDF$is_location_exact) We have several ways to transform a column. For example, we could go element-by-element, and make substitutions, like this: v &lt;- mynewbigDF$host_is_superhost Here is the way that the values look at the start: table(v) v[toupper(v)==&quot;T&quot;] &lt;- TRUE v[toupper(v)==&quot;F&quot;] &lt;- FALSE v[toupper(v)==&quot;&quot;] &lt;- NA and here are the values now: table(v) You might think that the NA values disappeared, but they just do not show up in the table by default. You can force them to appear, and then we see that the counts of the three values are the same as before. table(v, useNA=&quot;always&quot;) Here is the function: transform_column &lt;- function(v) { v[toupper(v)==&quot;T&quot;] &lt;- TRUE v[toupper(v)==&quot;F&quot;] &lt;- FALSE v[toupper(v)==&quot;&quot;] &lt;- NA v } We can try the function on mynewbigDF$host_is_superhost: head(transform_column(mynewbigDF$host_is_superhost)) table(transform_column(mynewbigDF$host_is_superhost)) Another possibility is to make a map, in which we put the old values as the names, and the new values as the values in the vector: mymap &lt;- c(TRUE, FALSE, NA) names(mymap) &lt;- c(&quot;T&quot;, &quot;F&quot;, &quot;&quot;) head(mymap[toupper(mynewbigDF$host_is_superhost)]) and if you do not want the names to appear on the vector, you can remove them, like this: head(unname(mymap[toupper(mynewbigDF$host_is_superhost)])) Finally we can check the table of the results: table(mymap[toupper(mynewbigDF$host_is_superhost)]) This might seem strange, and if you do not like it, you can just use the solution given above. If you do like this, and want to wrap it into a function, we can write: transform_column &lt;- function(v) { mymap &lt;- c(TRUE, FALSE, NA) names(mymap) &lt;- c(&quot;T&quot;, &quot;F&quot;, &quot;&quot;) unname(mymap[toupper(v)]) } and again we can try this new version of the function on mynewbigDF$host_is_superhost: head(transform_column(mynewbigDF$host_is_superhost)) table(transform_column(mynewbigDF$host_is_superhost)) 5. Apply your function transform_column to the columns instant_bookable and is_location_exact in your listings data. Based on your listings data, if you are looking at an instant bookable listing (where instant_bookable is TRUE), would you expect the location to be exact (where is_location_exact is TRUE)? Why or why not? Hint: Make a frequency table, and see how many instant bookable listings have exact location. Relevant topics: apply, table Item(s) to submit: Chunk of code to get a frequency table. 1-2 sentences explaining whether or not we would expect the location to be exact if we were looking at a instant bookable listing. Solution Now we look at the Canada results, for which instant_bookable is TRUE, to see if is_location_exact is TRUE or FALSE: table(transform_column(mynewbigDF$is_location_exact)[transform_column(mynewbigDF$instant_bookable) == TRUE]) In other words, if instant_bookable is TRUE, then we expect the value for is_location_exact to usually be TRUE as well. As a closing note, we could remove the check to see whether the inner values are TRUE because, by default, we will only exact the TRUE values when we do a lookup like this: table(transform_column(mynewbigDF$is_location_exact)[transform_column(mynewbigDF$instant_bookable)]) Project 2 Motivation: The ability to quickly reproduce an analysis is important. It is often necessary that other individuals will need to be able to understand and reproduce an analysis. This concept is so important there are classes solely on reproducible research! In fact, there are papers that investigate and highlight the lack of reproducibility in various fields. If you are interested in reading about this topic, a good place to start is the paper titled &quot;Why Most Published Research Findings Are False&quot;, by John Ioannidis (2005). Context: Making your work reproducible is extremely important. We will focus on the computational part of reproducibility. We will learn RMarkdown to document your analyses so others can easily understand and reproduce the computations that led to your conclusions. Pay close attention as future project templates will be RMarkdown templates. Scope: Understand Markdown, RMarkdown, and how to use it to make your data analysis reproducible. Learning objectives: Use Markdown syntax within an Rmarkdown document to achieve various text transformations. Use RMarkdown code chunks to display and/or run snippets of code. Questions Click here for video 1. Make the following text (including the asterisks) bold: This needs to be **very** bold. Make the following text (including the underscores) italicized: This needs to be _very_ italicized. Important note: Surround your answer in 4 backticks. This will allow you to display the markdown without having the markdown &quot;take effect&quot;. For example: ```` Some *marked* **up** text. ```` Hint: Be sure to check out the Rmarkdown Cheatsheet and our section on Rmarkdown in the book. Note: Rmarkdown is essentially Markdown + the ability to run and display code chunks. In this question, we are actually using Markdown within Rmarkdown! Relevant topics: rmarkdown, escaping characters Item(s) to submit: - 2 lines of markdown text, surrounded by 4 backticks. Note that when compiled, this text will be unmodified, regular text. Solution We can achieve this style of text: This needs to be **very** bold This needs to be _very_ italicized. by using this Markdown text: **This needs to be \\*\\*very\\*\\* bold** _This needs to be \\_very\\_ italicized._ The backslashes specify that we want the asterisks and underscores to appear. 2. Create an unordered list of your top 3 favorite academic interests (some examples could include: machine learning, operating systems, forensic accounting, etc.). Create another ordered list that ranks your academic interests in order of most interested to least interested. Hint: You can learn what ordered and unordered lists are here. Note: Similar to (1), in this question we are dealing with Markdown. If we were to copy and paste the solution to this problem in a Markdown editor, it would be the same result as when we Knit it here. Relevant topics: rmarkdown Item(s) to submit: - Create the lists, this time don't surround your code in backticks. Note that when compiled, this text will appear as nice, formatted lists. Solution An unordered list of my top 3 favorite academic interests is: asymptotic analysis of sequences data science analysis of algorithms An ordered list of my top 3 favorite academic interests is: analysis of algorithms asymptotic analysis of sequences data science 3. Browse https://www.linkedin.com/ and read some profiles. Pay special attention to accounts with an &quot;About&quot; section. Write your own personal &quot;About&quot; section using Markdown. Include the following: A header for this section (your choice of size) that says &quot;About&quot;. The text of your personal &quot;About&quot; section that you would feel comfortable uploading to linkedin, including at least 1 link. Relevant topics: rmarkdown Item(s) to submit: - Create the described profile, don't surround your code in backticks. Solution About I am Professor in Statistics and (by courtesy) of Mathematics and Public Health at Purdue University. My research is in probabilistic, combinatorial, and analytic techniques for the analysis of algorithms and data structures; I am also interested in science of information, game theory, and large-scale computation. I currently serve as Director of The Data Mine Interim Co-Director of the Integrative Data Science Initiative Principal Investigator for the Purdue Statistics Living Learning Community, funded by the National Science Foundation Associate Director for the NSF Center for Science of Information (now a core center in Purdue's Discovery Park) Associate Director of the Actuarial Science Program 4. Your co-worker wrote a report, and has asked you to beautify it. Knowing Rmarkdown, you agreed. Make improvements to this section. At a minimum: Make the title pronounced. Make all links appear as a word or words, rather than the long-form URL. Organize all code into code chunks where code and output are displayed. If the output is really long, just display the code. Make the calls to the library function be evaluated but not displayed. Make sure all warnings and errors that may eventually occur, do not appear in the final document. Feel free to make any other changes that make the report more visually pleasing. ```{r my-load-packages} library(ggplot2) ``` ```{r declare-variable-290, eval=FALSE} my_variable &lt;- c(1,2,3) ``` All About the Iris Dataset This paper goes into detail about the `iris` dataset that is built into r. You can find a list of built-in datasets by visiting https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html or by running the following code: data() The iris dataset has 5 columns. You can get the names of the columns by running the following code: names(iris) Alternatively, you could just run the following code: iris The second option provides more detail about the dataset. According to https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html there is another dataset built-in to r called `iris3`. This dataset is 3 dimensional instead of 2 dimensional. An iris is a really pretty flower. You can see a picture of one here: https://www.gardenia.net/storage/app/public/guides/detail/83847060_mOptimized.jpg In summary. I really like irises, and there is a dataset in r called `iris`. Relevant topics: rmarkdown Item(s) to submit: Make improvements to this section, and place it all under the Question 4 header in your template. Solution my_variable &lt;- c(1,2,3) *** All About the Iris Dataset This paper goes into detail about the iris dataset that is built into r. You can find a list of built-in datasets by visiting the R datasets index or by running the following code: data() The iris dataset has 5 columns. You can get the names of the columns by running the following code: names(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; Alternatively, you could just run the following code: iris The second option provides more detail about the dataset. According to the R iris help page there is another dataset built-in to R called iris3. This dataset is 3 dimensional instead of 2 dimensional. An iris is a really pretty flower. You can see a picture of one here: In summary. I really like irises, and there is a dataset in R called iris. 5. Create a plot using a built-in dataset like iris, mtcars, or Titanic, and display the plot using a code chunk. Make sure the code used to generate the plot is hidden. Include a descriptive caption for the image. Make sure to use an RMarkdown chunk option to create the caption. Relevant topics: rmarkdown, plotting in r Item(s) to submit: Code chunk under that creates and displays a plot using a built-in dataset like iris, mtcars, or Titanic. Solution We create the figure called &quot;Sepal Length for the Iris Data Set&quot;. Figure 1: Sepal Length for the Iris Data Set 6. Insert the following code chunk under the Question 6 header in your template. Try knitting the document. Two things will go wrong. What is the first problem? What is the second problem? ```{r my-load-packages} plot(my_variable) ``` Hint: Take a close look at the name we give our code chunk. Hint: Take a look at the code chunk where my_variable is declared. Relevant topics: rmarkdown Item(s) to submit: The modified version of the inserted code that fixes both problems. A sentence explaining what the first problem was. A sentence explaining what the second problem was. Solution We needed to change the section title from install-packages to a new name, since we already had a section with this same title. In the section called declare-variable, we had eval=F, and as a result, the variable my_variable was never declared. So we removed the option eval=F. plot(my_variable) For Project 2, please submit your .Rmd file and the resulting .pdf file. (For this project, you do not need to submit a .R file.) Project 3 Motivation: The ability to navigate a shell, like bash, and use some of its powerful tools, is very useful. The number of disciplines utilizing data in new ways is ever-growing, and as such, it is very likely that many of you will eventually encounter a scenario where knowing your way around a terminal will be useful. We want to expose you to some of the most useful bash tools, help you navigate a filesystem, and even run bash tools from within an RMarkdown file in RStudio. Context: At this point in time, you will each have varying levels of familiarity with Scholar. In this project we will learn how to use the terminal to navigate a UNIX-like system, experiment with various useful commands, and learn how to execute bash commands from within RStudio in an RMarkdown file. Scope: bash, RStudio Learning objectives: Distinguish differences in /home, /scratch, and /class. Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc. Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc. Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc. Utilize other Scholar resources: rstudio.scholar.rcac.purdue.edu, notebook.scholar.rcac.purdue.edu, desktop.scholar.rcac.purdue.edu, etc. Use man to read and learn about UNIX utilities. Run bash commands from within and RMarkdown file in RStudio. There are a variety of ways to connect to Scholar. In this class, we will primarily connect to RStudio Server by opening a browser and navigating to https://rstudio.scholar.rcac.purdue.edu/, entering credentials, and using the excellent RStudio interface. Here is a video to remind you about some of the basic tools you can use in UNIX/Linux: Click here for video This is the easiest book for learning this stuff; it is short and gets right to the point: https://go.oreilly.com/purdue-university/library/view/-/0596002610 you just log in and you can see it all; we suggest Chapters 1, 3, 4, 5, 7 (you can basically skip chapters 2 and 6 the first time through). It is a very short read (maybe, say, 2 or 3 hours altogether?), just a thin book that gets right to the details. 1. Navigate to https://rstudio.scholar.rcac.purdue.edu/ and login. Take some time to click around and explore this tool. We will be writing and running Python, R, SQL, and bash all from within this interface. Navigate to Tools &gt; Global Options .... Explore this interface and make at least 2 modifications. List what you changed. Here are some changes Kevin likes: Uncheck &quot;Restore .Rdata into workspace at startup&quot;. Change tab width 4. Check &quot;Soft-wrap R source files&quot;. Check &quot;Highlight selected line&quot;. Check &quot;Strip trailing horizontal whitespace when saving&quot;. Uncheck &quot;Show margin&quot;. (Dr Ward does not like to customize his own environment, but he does use the emacs key bindings: Tools &gt; Global Options &gt; Code &gt; Keybindings, but this is only recommended if you already know emacs.) Item(s) to submit: List of modifications you made to your Global Options. Solution Here are some changes Kevin likes: Uncheck &quot;Restore .Rdata into workspace at startup&quot;. Change tab width 4. Check &quot;Soft-wrap R source files&quot;. Check &quot;Highlight selected line&quot;. Check &quot;Strip trailing horizontal whitespace when saving&quot;. Uncheck &quot;Show margin&quot;. 2. There are four primary panes, each with various tabs. In one of the panes there will be a tab labeled &quot;Terminal&quot;. Click on that tab. This terminal by default will run a bash shell right within Scholar, the same as if you connected to Scholar using ThinLinc, and opened a terminal. Very convenient! What is the default directory of your bash shell? Hint: Start by reading the section on man. man stands for manual, and you can find the &quot;official&quot; documentation for the command by typing man &lt;command_of_interest&gt;. For example: # read the manual for the `man` command # use &quot;k&quot; or the up arrow to scroll up, &quot;j&quot; or the down arrow to scroll down man man Relevant topics: man, pwd, ~, .., . Item(s) to submit: The full filepath of default directory (home directory). Ex: Kevin's is: /home/kamstut The bash code used to show your home directory or current directory (also known as the working directory) when the bash shell is first launched. Solution # whatever is stored in the $HOME environment variable # is what ~ represents cd ~ pwd # if we change $HOME, ~ changes too! HOME=/home/kamstut/projects cd ~ pwd # if other users on the linux system share certain files or folders # in their home directory, you can access their home folder similarly ls ~mdw # but they _have_ to give you permissions 3. Learning to navigate away from our home directory to other folders, and back again, is vital. Perform the following actions, in order: Write a single command to navigate to the folder containing our full datasets: /class/datamine/data. Write a command to confirm you are in the correct folder. Write a command to list the files and directories within the data directory. (You do not need to recursively list subdirectories and files contained therein.) What are the names of the files and directories? Write another command to return back to your home directory. Write a command to confirm you are in the correct folder. Note: / is commonly referred to as the root directory in a linux/unix filesystem. Think of it as a folder that contains every other folder in the computer. /home is a folder within the root directory. /home/kamstut is the full filepath of Kevin's home directory. There is a folder home inside the root directory. Inside home is another folder named kamstut which is Kevin's home directory. Relevant topics: man, cd, pwd, ls, ~, .., . Item(s) to submit: Command used to navigate to the data directory. Command used to confirm you are in the data directory. Command used to list files and folders. List of files and folders in the data directory. Command used to navigate back to the home directory. Command used to confirm you are in the home directory. Solution # navigate to the data directory using `cd` (change directory) cd /class/datamine/data # confirm the location using `pwd` (print working directory) pwd # list files ls # cd without any arguments automatically returns to the directory # saved in the $HOME environment variable cd # another trick, if you wanted to _quickly_ return to the data # directory, or most recent directory is the following (uncommented) cd ~- # confirm the location using pwd pwd 4. Let's learn about two more important concepts. . refers to the current working directory, or the directory displayed when you run pwd. Unlike pwd you can use this when navigating the filesystem! So, for example, if you wanted to see the contents of a file called my_file.txt that lives in /home/kamstut (so, a full path of /home/kamstut/my_file.txt), and you are currently in /home/kamstut, you could run: cat ./my_file.txt. .. represents the parent folder or the folder in which your current folder is contained. So let's say I was in /home/kamstut/projects/ and I wanted to get the contents of the file /home/kamstut/my_file.txt. You could do: cat ../my_file.txt. When you navigate a directory tree using . and .. you create paths that are called relative paths because they are relative to your current directory. Alternatively, a full path or (absolute path) is the path starting from the root directory. So /home/kamstut/my_file.txt is the absolute path for my_file.txt and ../my_file.txt is a relative path. Perform the following actions, in order: Write a single command to navigate to the data directory. Write a single command to navigate back to your home directory using a relative path. Do not use ~ or the cd command without a path argument. Relevant topics: man, cd, pwd, ls, ~, .., . Item(s) to submit: Command used to navigate to the data directory. Command used to navigate back to your home directory that uses a relative path. Solution cd /class/datamine/data pwd cd ../../../home/kamstut pwd 5. In Scholar, when you want to deal with really large amounts of data, you want to access scratch (you can read more here). Your scratch directory on Scholar is located here: /scratch/scholar/$USER. $USER is an environment variable containing your username. Test it out: echo /scratch/scholar/$USER. Perform the following actions: Navigate to your scratch directory. Confirm you are in the correct location. Execute myquota. Find the location of the myquota bash script. Output the first 5 and last 5 lines of the bash script. Count the number of lines in the bash script. How many kilobytes is the script? Hint: You could use each of the commands in the relevant topics once. Hint: When you type myquota on Scholar there are sometimes two warnings about xauth but sometimes there are no warnings. If you get a warning that says Warning: untrusted X11 forwarding setup failed: xauth key data not generated it is safe to ignore this error. Hint: Commands often have options. Options are features of the program that you can trigger specifically. You can see the options of a command in the DESCRIPTION section of the man pages. For example: man wc. You can see -m, -l, and -w are all options for wc. To test this out: # using the default wc command. &quot;/class/datamine/data/flights/1987.csv&quot; is the first &quot;argument&quot; given to the command. wc /class/datamine/data/flights/1987.csv # to count the lines, use the -l option wc -l /class/datamine/data/flights/1987.csv # to count the words, use the -w option wc -w /class/datamine/data/flights/1987.csv # you can combine options as well wc -w -l /class/datamine/data/flights/1987.csv # some people like to use a single tack `-` wc -wl /class/datamine/data/flights/1987.csv # order doesn&#39;t matter wc -lw /class/datamine/data/flights/1987.csv Hint: The -h option for the du command is useful. Relevant topics: cd, pwd, type, head, tail, wc, du Item(s) to submit: Command used to navigate to your scratch directory. Command used to confirm your location. Output of myquota. Command used to find the location of the myquota script. Absolute path of the myquota script. Command used to output the first 5 lines of the myquota script. Command used to output the last 5 lines of the myquota script. Command used to find the number of lines in the myquota script. Number of lines in the script. Command used to find out how many kilobytes the script is. Number of kilobytes that the script takes up. Solution # navigate to my scratch folder cd /scratch/scholar/$USER # confirm pwd # what is my quota, execute the myquota script myquota # get the location of the myquota script type myquota # get the first 5 lines of the myquota script head /usr/local/bin/myquota # get the last 5 lines of the myquota script tail /usr/local/bin/myquota # get the number of lines in the myquota script wc -l /usr/local/bin/myquota # get the number of kilobytes of the myquota script du -h --apparent-size /usr/local/bin/myquota ls -la /usr/local/bin/myquota 6. Perform the following operations: Navigate to your scratch directory. Copy and paste the file: /class/datamine/data/flights/1987.csv to your current directory (scratch). Create a new directory called my_test_dir in your scratch folder. Move the file you copied to your scratch directory, into your new folder. Use touch to create an empty file named im_empty.txt in your scratch folder. Remove the directory my_test_dir and the contents of the directory. Remove the im_empty.txt file. Hint: rmdir may not be able to do what you think, instead, check out the options for rm using man rm. Relevant topics: cd, cp, mv, mkdir, touch, rmdir, rm Item(s) to submit: Command used to navigate to your scratch directory. Command used to copy the file, /class/datamine/data/flights/1987.csv to your current directory (scratch). Command used to create a new directory called my_test_dir in your scratch folder. Command used to move the file you copied earlier 1987.csv into your new my_test_dir folder. Command used to create an empty file named im_empty.txt in your scratch folder. Command used to remove the directory and the contents of the directory my_test_dir. Command used to remove the im_empty.txt file. Solution # navigate to the scratch folder cd /scratch/scholar/$USER # copy the 1987.csv file to the current directory (scratch) cp /class/datamine/data/flights/1987.csv . # make a directory in the scratch directory called `my_test_dir` mkdir my_test_dir # move 1987.csv to the new folder mv 1987.csv my_test_dir # create an empty file in the scratch folder touch im_empty.txt # remove the directory and the contents of the directory rm -r my_test_dir # remove the im_empty.txt file rm im_empty.txt 7. Please include a statement in Project 3 that says, &quot;I acknowledge that the STAT 19000/29000/39000 1-credit Data Mine seminar will be recorded and posted on Piazza, for participants in this course.&quot; or if you disagree with this statement, please consult with us at datamine@purdue.edu for an alternative plan. Project 4 Motivation: The need to search files and datasets based on the text held within is common during various parts of the data wrangling process. grep is an extremely powerful UNIX tool that allows you to do so using regular expressions. Regular expressions are a structured method for searching for specified patterns. Regular expressions can be very complicated, even professionals can make critical mistakes. With that being said, learning some of the basics is an incredible tool that will come in handy regardless of the language you are working in. Context: We've just begun to learn the basics of navigating a file system in UNIX using various terminal commands. Now we will go into more depth with one of the most useful command line tools, grep, and experiment with regular expressions using grep, R, and later on, Python. Scope: grep, regular expression basics, utilizing regular expression tools in R and Python Learning objectives: Use grep to search for patterns within a dataset. Use cut to section off and slice up data from the command line. Use wc to count the number of lines of input. You can find useful examples that walk you through relevant material in The Examples Book: https://thedatamine.github.io/the-examples-book It is highly recommended to read through, search, and explore these examples to help solve problems in this project. Important note: I would highly recommend using single quotes ' to surround your regular expressions. Double quotes can have unexpected behavior due to some shell's expansion rules. In addition, pay close attention to escaping certain characters in your regular expressions. Dataset The following questions will use the dataset the_office_dialogue.csv found in Scholar under the data directory /class/datamine/data/. A public sample of the data can be found here: the_office_dialogue.csv Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset. grep stands for (g)lobally search for a (r)egular (e)xpression and (p)rint matching lines. As such, to best demonstrate grep, we will be using it with textual data. You can read about and see examples of grep here. 1. Login to Scholar and use grep to find the dataset we will use this project. The dataset we will use is the only dataset to have the text &quot;Bears. Beets. Battlestar Galactica.&quot;. Where is it located exactly? Relevant topics: grep Item(s) to submit: The grep command used to find the dataset. The name and location in Scholar of the dataset. Solution grep -Ri &quot;bears. beets. battlestar galactica.&quot; /class/datamine /class/datamine/data/movies_and_tv/the_office_dialogue.csv 2. grep prints the line that the text you are searching for appears in. In project 3 we learned a UNIX command to quickly print the first n lines from a file. Use this command to get the headers for the dataset. As you can see, each line in the tv show is a row in the dataset. You can count to see which column the various bits of data live in. Write a line of UNIX commands that searches for &quot;bears. beets. battlestar galactica.&quot; and, rather than printing the entire line, prints only the character who speaks the line, as well as the line itself. Hint: The result if you were to search for &quot;bears. beets. battlestar galactica.&quot; should be: &quot;Jim&quot;,&quot;Fact. Bears eat beets. Bears. Beets. Battlestar Galactica.&quot; Hint: One method to solve this problem would be to pipe the output from grep to cut. Relevant topics: cut, grep Item(s) to submit: The line of UNIX commands used to find the character and original dialogue line that contains &quot;bears. beets. battlestar galactica.&quot;. Solution grep -i &quot;bears. beets. battlestar galactica.&quot; /class/datamine/data/movies_and_tv/the_office_dialogue.csv | cut -d &quot;,&quot; -f 7,8 3. This particular dataset happens to be very small. You could imagine a scenario where the file is many gigabytes and not easy to load completely into R or Python. We are interested in learning what makes Jim and Pam tick as a couple. Use a line of UNIX commands to create a new dataset called jim_and_pam.csv (remember, a good place to store data temporarily is /scratch/scholar/$USER). Include only lines that are spoken by either Jim or Pam, or reference Jim or Pam in any way. How many rows of data are in the new file? How many megabytes is the new file (to the nearest 1/10th of a megabyte)? Hint: Redirection. Hint: It is OK if you get an erroneous line where the word &quot;jim&quot; or &quot;pam&quot; appears as a part of another word. Relevant topics: grep, ls, wc, redirection Item(s) to submit: The line of UNIX commands used to create the new file. The number of rows of data in the new file, and the accompanying UNIX command used to find this out. The number of megabytes (to the nearest 1/10th of a megabyte) that the new file has, and the accompanying UNIX command used to find this out. Solution grep -i &quot;jim\\|pam&quot; /class/datamine/data/movies_and_tv/the_office_dialogue.csv | cut -d &quot;,&quot; -f4,7,8,9,12 &gt; jim_and_pam.csv wc -l jim_and_pam.csv 13779 lines ls -h jim_and_pam.csv 1.4mb 4. Find all lines where either Jim/Pam/Michael/Dwight's name is followed by an exclamation mark. Use only 1 &quot;!&quot; within your regular expression. How many lines are there? Ignore case (whether or not parts of the names are capitalized or not). Relevant topics: grep, basic matches, escaping characters Item(s) to submit: The UNIX command(s) used to solve this problem. The number of lines where either Jim/Pam/Michael/Dwight's name is followed by an exclamation mark. Solution grep -E &#39;(Jim|Pam|Michael|Dwight)!&#39; the_office_dialogue.csv # or grep &#39;\\(Jim\\|Pam\\|Michael\\|Dwight\\)!&#39; the_office_dialogue.csv| wc -l 5. Find all lines that contain the text &quot;that's what&quot; followed by any amount of any text and then &quot;said&quot;. How many lines are there? Relevant topics: grep Item(s) to submit: The UNIX command used to solve this problem. The number of lines that contain the text &quot;that's what&quot; followed by any amount of text and then &quot;said&quot;. Solution grep -i &quot;that&#39;s what .* said&quot; /class/datamine/data/movies_and_tv/the_office_dialogue.csv Regular expressions are really a useful semi language-agnostic tool. What this means is regardless of the programming language your are using, there will be some package that allows you to use regular expressions. In fact, we can use them in both R and Python! This can be particularly useful when dealing with strings. Load up the dataset you discovered in (1) using read.csv. Name the resulting data.frame dat. 6. The text_w_direction column in dat contains the characters' lines with inserted direction that helps characters know what to do as they are reciting the lines. Direction is shown between square brackets &quot;[&quot; &quot;]&quot;. In this two-part question, we are going to use regular expression to detect the directions. (a) Create a new column called has_direction that is set to TRUE if the text_w_direction column has direction, and FALSE otherwise. Use the grepl function in R to accomplish this. Hint: Make sure all opening brackets &quot;[&quot; have a corresponding closing bracket &quot;]&quot;. Hint: Think of the pattern as any line that has a [, followed by any amount of any text, followed by a ], followed by any amount of any text. (b) Modify your regular expression to find lines with 2 or more sets of direction. How many lines have more than 2 directions? Modify your code again and find how many have more than 5. We count the sets of direction in each line by the pairs of square brackets. The following are two simple example sentences. This is a line with [emphasize this] only 1 direction! This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug]. Your solution to part (a) should find both lines a match. However, in part (b) we want the regular expression pattern to find only lines with 2+ directions, so the first line would not be a match. In our actual dataset, for example, dat$text_w_direction[2789] is a line with 2 directions. Relevant topics: grep, grepl, basic matches, escaping characters Item(s) to submit: The R code and regular expression used to solve the first part of this problem. The R code and regular expression used to solve the second part of this problem. How many lines have &gt;= 2 directions? How many lines have &gt;= 5 directions? Solution dat$has_direction &lt;- grepl(&quot;(\\\\[.*\\\\])+&quot;, dat$text_w_direction) # length(grep(&quot;\\\\[.*\\\\].*\\\\[.*\\\\]&quot;, dat$text_w_direction)) length(grep(&quot;\\\\[.*\\\\].*\\\\[.*\\\\].*\\\\[.*\\\\].*\\\\[.*\\\\].*\\\\[.*\\\\]&quot;, dat$text_w_direction)) OPTIONAL QUESTION. Use the str_extract_all function from the stringr package to extract the direction(s) as well as the text between direction(s) from each line. Put the strings in a new column called direction. This is a line with [emphasize this] only 1 direction! This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug]. In this question, your solution may have extracted: [emphasize this] [emphasize this] 2 sets of direction, do you see the difference [shrug] (It is okay to keep the text between neighboring pairs of &quot;[&quot; and &quot;]&quot; for the second line.) Relevant topics: str_extract_all, basic matches, escaping characters Item(s) to submit: The R code used to solve this problem. Solution dat$direction_correct &lt;- str_extract_all(dat$text_w_direction, &quot;(\\\\[[^\\\\[\\\\]]*\\\\])&quot;, simplify=F) # or dat$direction_correct &lt;- str_extract_all(dat$text_w_direction, &quot;(\\\\[.*?\\\\])&quot;, simplify=F) Project 5 Motivation: Becoming comfortable stringing together commands and getting used to navigating files in a terminal is important for every data scientist to do. By learning the basics of a few useful tools, you will have the ability to quickly understand and manipulate files in a way which is just not possible using tools like Microsoft Office, Google Sheets, etc. Context: We've been using UNIX tools in a terminal to solve a variety of problems. In this project we will continue to solve problems by combining a variety of tools using a form of redirection called piping. Scope: grep, regular expression basics, UNIX utilities, redirection, piping Learning objectives: Use cut to section off and slice up data from the command line. Use piping to string UNIX commands together. Use sort and it's options to sort data in different ways. Use head to isolate n lines of output. Use wc to summarize the number of lines in a file or in output. Use uniq to filter out non-unique lines. Use grep to search files effectively. You can find useful examples that walk you through relevant material in The Examples Book: https://thedatamine.github.io/the-examples-book It is highly recommended to read through, search, and explore these examples to help solve problems in this project. Don't forget the very useful documentation shortcut ? for R code. To use, simply type ? in the console, followed by the name of the function you are interested in. In the Terminal, you can use the man command to check the documentation of bash code. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/amazon/amazon_fine_food_reviews.csv A public sample of the data can be found here: amazon_fine_food_reviews.csv Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset. Here are three videos that might also be useful, as you work on Project 5: Click here for video Click here for video Click here for video Questions 1. What is the Id of the most helpful review, according to the highest HelpfulnessNumerator? Important note: You can always pipe output to head in case you want the first few values of a lot of output. Note that if you used sort before head, you may see the following error messages: sort: write failed: standard output: Broken pipe sort: write error This is because head would truncate the output from sort. This is okay. See this discussion for more details. Relevant topics: cut, sort, head, piping Item(s) to submit: Line of UNIX commands used to solve the problem. The Id of the most helpful review. Solution cut -d, -f5 amazon_fine_food_reviews.csv| sort -nr | head -n3 2. Some entries under the Summary column appear more than once. Calculate the proportion of unique summaries over the total number of summaries. Use two lines of UNIX commands to find the numerator and the denominator, and manually calculate the proportion. To further clarify what we mean by unique, if we had the following vector in R, c(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;), its unique values are c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;). Relevant topics: cut, uniq, sort, wc, piping Item(s) to submit: Two lines of UNIX commands used to solve the problem. The ratio of unique Summary's. Solution cut -d, -f9 amazon_fine_food_reviews.csv | sort -u | wc -l cut -d, -f9 amazon_fine_food_reviews.csv | wc -l # 295162/568455 3. Use a chain of UNIX commands, piped in a sequence, to create a frequency table of Score. Relevant topics: cut, uniq, sort, piping Item(s) to submit: The line of UNIX commands used to solve the problem. The frequency table. Solution cut -d, -f7 amazon_fine_food_reviews.csv | sort -n | uniq -c 4. Who is the user with the highest number of reviews? There are two columns you could use to answer this question, but which column do you think would be most appropriate and why? Hint: You may need to pipe the output to sort multiple times. Hint: To create the frequency table, read through the man pages for uniq. Man pages are the &quot;manual&quot; pages for UNIX commands. You can read through the man pages for uniq by running the following: man uniq Relevant topics: cut, uniq, sort, head, piping, man Item(s) to submit: The line of UNIX commands used to solve the problem. The frequency table. Solution cut -d, -f3 amazon_fine_food_reviews.csv | sort | uniq -c | sort -nr | head -n1 # You should use UserId instead of ProfileName as the former is a unique identifier 5. Anecdotally, there seems to be a tendency to leave reviews when we feel strongly (either positive or negative) about a product. For the user with the highest number of reviews (i.e., the user identified in question 4), would you say that they follow this pattern of extremes? Let's consider 5 star reviews to be strongly positive and 1 star reviews to be strongly negative. Let's consider anything in between neither strongly positive nor negative. Hint: You may find the solution to problem (3) useful. Relevant topics: cut, uniq, sort, grep, piping Item(s) to submit: The line of UNIX commands used to solve the problem. Solution grep -i &#39;A3OXHLG6DIBRW8&#39; amazon_fine_food_reviews.csv | cut -d, -f7 | sort | uniq -c 6. Find the most helpful review with a Score of 5. Then (separately) find the most helpful review with a Score of 1. As before, we are considering the most helpful review to be the review with the highest HelpfulnessNumerator. Hint: You can use multiple lines to solve this problem. Relevant topics: sort, head, piping Item(s) to submit: The lines of UNIX commands used to solve the problem. ProductId's of both requested reviews. Solution sort -t, -k7rn,7 -k5rn,5 amazon_fine_food_reviews.csv | head -n2 sort -t, -k7n,7 -k5rn,5 amazon_fine_food_reviews.csv | head -n2 OPTIONAL QUESTION. For only the two ProductIds from the previous question, create a new dataset called scores.csv that contains the ProductIds and Scores from all reviews for these two items. Relevant topics: cut, grep, redirection Item(s) to submit: The line of UNIX commands used to solve the problem. Solution cut -d, -f2,7 amazon_fine_food_reviews.csv| grep -Ei &#39;(B00012182G|B003EMQGVI|B00065LI0A)&#39; &gt; scores.csv Project 6 Motivation: A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. awk is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks. Context: This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and awk. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. Scope: awk, UNIX utilities, bash scripts Learning objectives: Use awk to process and manipulate textual data. Use piping and redirection within the terminal to pass around data between utilities. Dataset The following questions will use the dataset found here or in Scholar: /class/datamine/data/flights/subset/YYYY.csv An example from 1987 data can be found here or in Scholar: /class/datamine/data/flights/subset/1987.csv Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. In previous projects we learned how to get a single column of data from a csv file. Write 1 line of UNIX commands to print the 17th column, the Origin, from 1987.csv. Write another line, this time using awk to do the same thing. Which one do you prefer, and why? Here is an example, from a different data set, to illustrate some differences and similarities between cut and awk: Click here for video Relevant topics: cut, awk Item(s) to submit: One line of UNIX commands to solve the problem without using awk. One line of UNIX commands to solve the problem using awk. 1-2 sentences describing which method you prefer and why. Solution cut -d, -f17 1987.csv awk -F, &#39;{print $17}&#39; 1987.csv 2. Write a bash script that accepts a year (1987, 1988, etc.) and a column n and returns the nth column of the associated year of data. Here are two examples to illustrate how to write a bash script: Click here for video Click here for video Hint: In this example, you only need to turn in the content of your bash script (starting with #!/bin/bash) without evaluation in a code chunk. However, you should test your script before submission to make sure it works. To actually test out your bash script, take the following example. The script is simple and just prints out the first two arguments given to it: #!/bin/bash echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot; If you simply drop that text into a file called my_script.sh, located here: /home/$USER/my_script.sh, and if you run the following: # Setup bash to run; this only needs to be run one time per session. # It makes bash behave a little more naturally in RStudio. exec bash # Navigate to the location of my_script.sh cd /home/$USER # Make sure that the script is runable. # This only needs to be done one time for each new script that you write. chmod 755 my_script.sh # Execute my_script.sh ./my_script.sh okay cool then it will print: First argument: okay Second argument: cool In this example, if we were to turn in the &quot;content of your bash script (starting with #!/bin/bash) in a code chunk, our solution would look like this: #!/bin/bash echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot; And although we aren't running the code chunk above, we know that it works because we tested it in the terminal. Hint: Using awk you could have a script with just two lines: 1 with the &quot;hash-bang&quot; (#!/bin/bash), and 1 with a single awk command. Relevant topics: awk, bash scripts Item(s) to submit: The content of your bash script (starting with #!/bin/bash) in a code chunk. Solution #!/bin/bash awk -F, -v col=$2 &#39;{print $col}&#39; $1.csv 3. How many flights arrived at Indianapolis (IND) in 2008? First solve this problem without using awk, then solve this problem using only awk. Here is a similar example, using the election data set: Click here for video Relevant topics: cut, grep, wc, awk, piping Item(s) to submit: One line of UNIX commands to solve the problem without using awk. One line of UNIX commands to solve the problem using awk. The number of flights that arrived at Indianapolis (IND) in 2008. Solution cut -d, -f18 2008.csv | grep &#39;IND&#39; | wc -l awk -F, &#39;{if ($18 == &quot;IND&quot;) count++}END{print count}&#39; 2008.csv 4. Do you expect the number of unique origins and destinations to be the same based on flight data in the year 2008? Find out, using any command line tool you'd like. Are they indeed the same? How many unique values do we have per category (Origin, Dest)? Here is an example to help you with the last part of the question, about Origin-to-Destination pairs. We analyze the city-state pairs from the election data: Click here for video Relevant topics: cut, sort, uniq, wc, awk Item(s) to submit: 1-2 sentences explaining whether or not you expect the number of unique origins and destinations to be the same. The UNIX command(s) used to figure out if the number of unique origins and destinations are the same. The number of unique values per category (Origin, Dest). Solution cut -d, -f17 2008.csv | sort | uniq | wc -l cut -d, -f18 2008.csv | sort | uniq | wc -l 5. In (4) we found that there are not the same number of unique Origin's as Dest's. Find the IATA airport code for all Origin's that don't appear in a Dest and all Dest's that don't appear in an Origin in the 2008 data. Hint: The examples on this page should help. Note that these examples are based on Process Substitution, which basically allows you to specify commands whose output would be used as the input of comm. There should be no space between &lt; and (, otherwise your bash will not work as intended. Relevant topics: comm, cut, sort, uniq, redirection Item(s) to submit: The line(s) of UNIX command(s) used to answer the question. The list of Origins that don't appear in Dest. The list of Dests that don't appear in Origin. Solution comm -23 &lt;(cut -d, -f17 2008.csv | sort | uniq) &lt;(cut -d, -f18 2008.csv | sort | uniq) comm -13 &lt;(cut -d, -f17 2008.csv | sort | uniq) &lt;(cut -d, -f18 2008.csv | sort | uniq) 6. What was the percentage of flights in 2008 per unique Origin with the Dest of &quot;IND&quot;? What percentage of flights had &quot;PHX&quot; as Origin (among all flights with Dest of &quot;IND&quot;? Here is an example using the percentages of donations contributed from CEOs from various States: Click here for video Hint: You can do the mean calculation in awk by dividing the result from (3) by the number of unique Origin's that have a Dest of &quot;IND&quot;. Relevant topics: awk, sort, grep, wc Item(s) to submit: The percentage of flights in 2008 per unique Origin with the Dest of &quot;IND&quot;. 1-2 sentences explaining how &quot;PHX&quot; compares (as a unique ORIGIN) to the other Origins (all with the Dest of &quot;IND&quot;)? Solution awk -F, &#39;{if($18==&quot;IND&quot;) print $17}&#39; 2008.csv | sort -u | wc -l awk -F, &#39;{if($18==&quot;IND&quot;) print $17}&#39; 2008.csv | grep -i PHX | wc -l OPTIONAL QUESTION. Write a bash script that takes a year and IATA airport code and returns the year, and the total number of flights to and from the given airport. Example rows may look like: 1987, 12345 1988, 44 Run the script with inputs: 1991 and ORD. Include the output in your submission. Relevant topics: bash scripts, cut, piping, grep, wc Item(s) to submit: The content of your bash script (starting with &quot;#!/bin/bash&quot;) in a code chunk. The output of the script given 1991 and ORD as inputs. Solution #!/bin/bash FLIGHTS_OUT=&quot;$(cut -d, -f17 $1.csv | grep -i $2 | wc -l)&quot; FLIGHTS_IN=&quot;$(cut -d, -f18 $1.csv | grep -i $2 | wc -l)&quot; echo &quot;$((FLIGHTS_OUT + FLIGHTS_IN)), $1&quot; Project 7 Motivation: A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. awk is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks. Context: This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and awk. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. Scope: awk, UNIX utilities, bash scripts Learning objectives: Use awk to process and manipulate textual data. Use piping and redirection within the terminal to pass around data between utilities. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/flights/subset/YYYY.csv An example of the data for the year 1987 can be found here. Sometimes if you are about to dig into a dataset, it is good to quickly do some sanity checks early on to make sure the data is what you expect it to be. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. Write a line of code that prints a list of the unique values in the DayOfWeek column. Write a line of code that prints a list of the unique values in the DayOfMonth column. Write a line of code that prints a list of the unique values in the Month column. Use the 1987.csv dataset. Are the results what you expected? Relevant topics: cut, sort Item(s) to submit: 3 lines of code used to get a list of unique values for the chosen columns. 1-2 sentences explaining whether or not the results are what you expected. Solution cut -d, -f3 1987.csv | sort -nu cut -d, -f4 1987.csv | sort -nu cut -d, -f2 1987.csv | sort -nu # We can print the unique values in the DayOfWeek, DayOfMonth, and Month, # using these three respectively pipelines in bash shell: cat /class/datamine/data/flights/subset/1987.csv | cut -d, -f4 | sort | uniq cat /class/datamine/data/flights/subset/1987.csv | cut -d, -f3 | sort | uniq cat /class/datamine/data/flights/subset/1987.csv | cut -d, -f2 | sort | uniq # or we can do this equivalently with awk: cat /class/datamine/data/flights/subset/1987.csv | awk -F, &#39;{print $4}&#39; | sort | uniq cat /class/datamine/data/flights/subset/1987.csv | awk -F, &#39;{print $3}&#39; | sort | uniq cat /class/datamine/data/flights/subset/1987.csv | awk -F, &#39;{print $2}&#39; | sort | uniq 2. Our files should have 29 columns. For a given file, write a line of code that prints any lines that do not have 29 columns. Test it on 1987.csv, were there any rows without 29 columns? Hint: See here. NF looks like it may be useful! Relevant topics: awk Item(s) to submit: Line of code used to solve the problem. 1-2 sentences explaining whether or not there were any rows without 29 columns. Solution awk -F, &#39;{if (NF != 29) print $0}&#39; 1987.csv # We can print any lines that do not have 29 columns, as follows: cat /class/datamine/data/flights/subset/1987.csv | awk -F, &#39;{if (NF != 29) {print $0}}&#39; # Note that there are no such lines. # In other words, every line has exactly 29 columns. 3. Write a bash script that, given a &quot;begin&quot; year and &quot;end&quot; year, cycles through the associated files and prints any lines that do not have 29 columns. Relevant topics: awk, bash scripts Item(s) to submit: The content of your bash script (starting with &quot;#!/bin/bash&quot;) in a code chunk. The results of running your bash scripts from year 1987 to 2008. Solution #!/bin/bash for ((f=$1; f&lt;=$2; f++)); do awk -F, &#39;{if (NF != 29) print $0}&#39; $f.csv done # Here is a bash script for which the variable i cycles through the # desired years, and the script prints any lines that do not have 29 fields. #!/bin/bash for (( i=$1; i&lt;=$2; i++ )) do cat /class/datamine/data/flights/subset/${i}.csv | awk -F, &#39;{if (NF != 29) {print $0}}&#39; done # change permissions, to make the script executable chmod 755 myscript.sh ./myscript.sh 1987 2008 # run the bash script # As in question 2, note that there are no such lines. # In other words, every line has exactly 29 columns. 4. awk is a really good tool to quickly get some data and manipulate it a little bit. The column Distance contains the distances of the flights in miles. Use awk to calculate the total distance traveled by the flights in 1990, and show the results in both miles and kilometers. To convert from miles to kilometers, simply multiply by 1.609344. Example output: Miles: 12345 Kilometers: 19867.35168 Relevant topics: awk, piping Item(s) to submit: The code used to solve the problem. The results of running the code. Solution awk -F, &#39;{miles=miles+$19}END{print &quot;Miles: &quot; miles, &quot;\\nKilometers:&quot; miles*1.609344}&#39; 1990.csv # The total distance traveled in miles is stored in mytotaldistance # and at the end, we print mytotaldistance (which is given in miles) # and also mytotaldistance*1.609344 (which is the total distance in # kilometers). cat /class/datamine/data/flights/subset/1990.csv | awk -F, &#39;{mytotaldistance = mytotaldistance + $19} END{print &quot;The total distance is &quot;, mytotaldistance, &quot; miles.&quot;; print &quot;Equivalently, this is &quot;, mytotaldistance*1.609344, &quot; kilometers.&quot;}&#39; 5. Use awk to calculate the sum of the number of DepDelay minutes, grouped according to DayOfWeek. Use 2007.csv. Example output: DayOfWeek: 0 1: 1234567 2: 1234567 3: 1234567 4: 1234567 5: 1234567 6: 1234567 7: 1234567 Note: 1 is Monday. Relevant topics: awk, sort, piping Item(s) to submit: The code used to solve the problem. The output from running the code. Solution awk -F, &#39;{delay[$4]=delay[$4]+$16}END{for (d in delay) print d&quot;: &quot;, delay[d]}&#39; 2007.csv | sort -n # We store the DepDelay in an array called mytotaldelays: cat /class/datamine/data/flights/subset/2007.csv | awk -F, &#39;{mytotaldelays[$4] = mytotaldelays[$4] + $16} END{for (i=1; i&lt;=7; i++) {print &quot;The total DepDelay in minutes is &quot;, mytotaldelays[i], &quot; on day &quot;, i, &quot; of the week.&quot;}}&#39; 6. It wouldn't be fair to compare the total DepDelay minutes by DayOfWeek as the number of flights may vary. One way to take this into account is to instead calculate an average. Modify (5) to calculate the average number of DepDelay minutes by the number of flights per DayOfWeek. Use 2007.csv. Example output: DayOfWeek: 0 1: 1.234567 2: 1.234567 3: 1.234567 4: 1.234567 5: 1.234567 6: 1.234567 7: 1.234567 Relevant topics: awk, sort, piping Item(s) to submit: The code used to solve the problem. The output from running the code. Solution awk -F, &#39;{delay[$4]=delay[$4]+$16; flights[$4]++}END{for (d in delay) print d&quot;: &quot;, delay[d]/flights[d]}&#39; 2007.csv | sort -n # Now we also keep track of mynumberofflights on each day of the week. cat /class/datamine/data/flights/subset/2007.csv | awk -F, &#39;{mytotaldelays[$4] = mytotaldelays[$4] + $16; mynumberofflights[$4] = mynumberofflights[$4] + 1} END{for (i=1; i&lt;=7; i++) {print &quot;The average DepDelay in minutes is &quot;, mytotaldelays[i]/mynumberofflights[i], &quot; on day &quot;, i, &quot; of the week.&quot;}}&#39; Project 8 Motivation: A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. awk is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks. Context: This is the last part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and awk. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. Scope: awk, UNIX utilities, bash scripts Learning objectives: Use awk to process and manipulate textual data. Use piping and redirection within the terminal to pass around data between utilities. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/flights/subset/YYYY.csv An example of the data for the year 1987 can be found here. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. Let's say we have a theory that there are more flights on the weekend days (Friday, Saturday, Sunday) than the rest of the days, on average. We can use awk to quickly check it out and see if maybe this looks like something that is true! Write a line of awk code that, prints the total number of flights that occur on weekend days, followed by the total number of flights that occur on the weekdays. Complete this calculation for 2008 using the 2008.csv file. Note: Under the column DayOfWeek, Monday through Sunday are represented by 1-7, respectively. Relevant topics: awk Item(s) to submit: Line of awk code that solves the problem. The result: the number of flights on the weekend days, followed by the number of flights on the weekdays for the flights during 2008. Solution awk -F, &#39;{if ($4==1 || $4==2 || $4==3 || $4==4) weekday++; else (weekend++)}END{print weekend,weekday}&#39; 2008.csv awk -F, &#39;{if ($4 ~ /1|2|3|4/) weekday++; else (weekend++)}END{print weekend,weekday}&#39; 2008.csv 2. Note that in (1), we are comparing 3 days to 4! Write a line of awk code that, prints the average number of flights on a weekend day, followed by the average number of flights on the weekdays. Continue to use data for 2008. Hint: You don't need a large if statement to do this, you can use the ~ comparison operator. Relevant topics: awk Item(s) to submit: Line of awk code that solves the problem. The result: the average number of flights on the weekend days, followed by the average number of flights on the weekdays for the flights during 2008. Solution awk -F, &#39;{if ($4==1 || $4==2 || $4==3 || $4==4) weekday++; else (weekend++)}END{print weekend/3,weekday/4}&#39; 2008.csv awk -F, &#39;{if ($4 ~ /1|2|3|4/) weekday++; else (weekend++)}END{print weekend/3,weekday/4}&#39; 2008.csv 3. We want to look to see if there may be some truth to the whole &quot;snow bird&quot; concept where people will travel to warmer states like Florida and Arizona during the Winter. Let's use the tools we've learned to explore this a little bit. Take a look at airports.csv. In particular run the following: head airports.csv Notice how all of the non-numeric text is surrounded by quotes. The surrounding quotes would need to be escaped for any comparison within awk. This is messy and we would prefer to create a new file called new_airports.csv without any quotes. Write a line of code to do this. Note: You may be wondering why we are asking you to do this. This sort of situation (where you need to deal with quotes) happens a lot! It's important to practice and learn ways to fix these things. Hint: You could use gsub within awk to replace '&quot;' with ''. You can find how to use gsub here. Hint: If you leave out the column number argument to gsub it will apply the substitution to every field in every column. Hint: cat new_airports.csv | wc -l # should be 159 without header Relevant topics: awk, redirection Item(s) to submit: Line of awk code used to create the new dataset. Solution awk -F, &#39;{gsub(/&quot;/, &quot;&quot;); print $0}&#39; airports.csv &gt; new_airports.csv 4. Write a line of commands that creates a new dataset called az_fl_airports.txt. az_fl_airports.txt should only contain a list of airport codes for all airports from both Arizona (AZ) and Florida (FL). Use the file we created in (3),new_airports.csv as a starting point. How many airports are there? Did you expect this? Use a line of bash code to count this. Create a new dataset (called az_fl_flights.txt) that contains all of the data for flights into or out of Florida and Arizona (using the 2008.csv file). Use the newly created dataset, az_fl_airports.txt to accomplish this. Hint: https://unix.stackexchange.com/questions/293684/basic-grep-awk-help-extracting-all-lines-containing-a-list-of-terms-from-one-f Hint: cat az_fl_flights.txt | wc -l # should be 484705 Relevant topics: awk, wc, piping Item(s) to submit: All UNIX commands used to answer the questions. The number of airports. 1-2 sentences explaining whether you expected this number of airports. Solution awk -F, &#39;{if ($4 == &quot;AZ&quot; || $4 == &quot;FL&quot;) print $1}&#39; new_airports.csv &gt; az_fl_airports.txt wc -l az_fl_airports.txt grep -w -F -f az_fl_airports.txt 2008.csv 5. Write a bash script that accepts the year as an argument and performs the same operations as in question 4, returning the number of flights into and out of both AZ and FL for any given year. Relevant topics: bash scripts, grep, for loop, redirection Item(s) to submit: The content of your bash script (starting with &quot;#!/bin/bash&quot;) in a code chunk. The line of UNIX code you used to execute the script and create the new dataset. Solution #!/bin/bash for ((f=$1; f&lt;=$2; f++)); do grep -w -F -f $3 $f.csv done ./my_script.sh 1987 2008 az_fl_airports.txt &gt; az_fl_flights.csv Project 9 Motivation: Structured Query Language (SQL) is a language used for querying and manipulating data in a database. SQL can handle much larger amounts of data than R and Python can alone. SQL is incredibly powerful. In fact, cloudflare, a billion dollar company, had much of its starting infrastructure built on top of a Postgresql database (per this thread on hackernews). Learning SQL is well worth your time! Context: There are a multitude of RDBMSs (relational database management systems). Among the most popular are: MySQL, MariaDB, Postgresql, and SQLite. As we've spent much of this semester in the terminal, we will start in the terminal using SQLite. Scope: SQL, sqlite Learning objectives: Explain the advantages and disadvantages of using a database over a tool like a spreadsheet. Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause. Basic clauses: select, order by, limit, desc, asc, count, where, from, etc. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/lahman/lahman.db This is the Lahman Baseball Database. You can find its documentation here, including the definitions of the tables and columns. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. Important note: For this project all solutions should be done using SQL code chunks. To connect to the database, copy and paste the following before your solutions in your .Rmd: ```{r, include=F} library(RSQLite) lahman &lt;- dbConnect(RSQLite::SQLite(), &quot;/class/datamine/data/lahman/lahman.db&quot;) ``` Each solution should then be placed in a code chunk like this: ```{sql, connection=lahman} SELECT * FROM batting LIMIT 1; ``` If you want to use a SQLite-specific function like .tables (or prefer to test things in the Terminal), you will need to use the Terminal to connect to the database and run queries. To do so, you can connect to RStudio Server at https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal. In the terminal execute the command: sqlite3 /class/datamine/data/lahman/lahman.db From there, the SQLite-specific commands will function properly. They will not function properly in an SQL code chunk. To display the SQLite-specific commands in a code chunk without running the code, use a code chunk with the option eval=F like this: ```{sql, connection=lahman, eval=F} SELECT * FROM batting LIMIT 1; ``` This will allow the code to be displayed without throwing an error. 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal and access the Lahman database. How many tables are available? Hint: To connect to the database, do the following: sqlite3 /class/datamine/data/lahman/lahman.db Hint: This is a good resource. Relevant topics: sqlite3 Item(s) to submit: How many tables are available in the Lahman database? The sqlite3 commands used to figure out how many tables are available. Solution .tables 2. Some people like to try to visit all 30 MLB ballparks in their lifetime. Use SQL commands to get a list of parks and the cities they're located in. For your final answer, limit the output to 10 records/rows. Note: There may be more than 30 parks in your result, this is ok. For long results, you can limit the number of printed results using the LIMIT clause. Hint: Make sure you take a look at the column names and get familiar with the data tables. If working from the Terminal, to see the header row as a part of each query result, run the following: .headers on Relevant topics: SELECT, FROM, LIMIT Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT parkname, city FROM parks LIMIT 10; 3. There is nothing more exciting to witness than a home run hit by a batter. It's impressive if a player hits more than 40 in a season. Find the hitters who have hit 60 or more home runs (HR) in a season. List their playerID, yearID, home run total, and the teamID they played for. Hint: There are 8 occurrences of home runs greater than or equal to 60. Hint: The batting table is where you should look for this question. Relevant topics: SELECT, FROM, WHERE, LIMIT Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT teamID, yearID, teamID, HR FROM batting WHERE HR&gt;=60; 4. Make a list of players born on your birth day (don't worry about the year). Display their first names, last names, and birth year. Order the list descending by their birth year. Hint: The people table is where you should look for this question. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT nameFirst, nameLast, birthYear FROM people WHERE birthMonth==5 AND birthDay==29 ORDER BY birthYear DESC LIMIT 10; 5. Get the Cleveland (CLE) Pitching Roster from the 2016 season (playerID, W, L, SO). Order the pitchers by number of Strikeouts (SO) in descending order. Hint: The pitching table is where you should look for this question. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT playerID, W, L, SO FROM pitching WHERE teamID==&#39;CLE&#39; AND yearID==2016 ORDER BY SO DESC LIMIT 10; 6. Find the 10 team and year pairs that have the most number of Errors (E) between 1960 and 1970. Display their Win and Loss counts too. What is the name of the team that appears in 3rd place in the ranking of the team and year pairs? Hint: The teams table is where you should look for this question. Hint: The BETWEEN clause is useful here. Hint: It is OK to use multiple queries to answer the question. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT teamID, franchID, yearID, W, L, E FROM teams WHERE yearID BETWEEN 1960 AND 1970 ORDER BY E DESC LIMIT 10; SELECT franchName FROM teamsfranchises WHERE franchID==&#39;LAA&#39;; SELECT teamID, franchID, yearID, W, L, E FROM teams WHERE yearID &gt;= 1960 AND yearID &lt;= 1970 ORDER BY E DESC LIMIT 10; SELECT franchName FROM teamsfranchises WHERE franchID==&#39;LAA&#39;; 7. Find the playerID for Bob Lemon. What year and team was he on when he got the most wins as a pitcher (use table pitching)? What year and team did he win the most games as a manager (use table managers)? Hint: It is OK to use multiple queries to answer the question. Note: There was a tie among the two years in which Bob Lemon had the most wins as a pitcher. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT playerID FROM people WHERE nameFirst==&#39;Bob&#39; AND nameLast==&#39;Lemon&#39;; SELECT teamID, yearID, W FROM pitching WHERE playerID==&#39;lemonbo01&#39; ORDER BY W DESC; SELECT teamID, yearID, W FROM managers WHERE playerID==&#39;lemonbo01&#39; ORDER BY W DESC; Project 10 Motivation: Although SQL syntax may still feel unnatural and foreign, with more practice it will start to make more sense. The ability to read and write SQL queries is a bread-and-butter skill for anyone working with data. Context: We are in the second of a series of projects that focus on learning the basics of SQL. In this project, we will continue to harden our understanding of SQL syntax, and introduce common SQL functions like AVG, MIN, and MAX. Scope: SQL, sqlite Learning objectives: Explain the advantages and disadvantages of using a database over a tool like a spreadsheet. Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause. Basic clauses: select, order by, limit, desc, asc, count, where, from, etc. Utilize SQL functions like min, max, avg, sum, and count to solve data-driven problems. Dataset The following questions will use the dataset similar to the one from Project 9, but this time we will use a MariaDB version of the database, which is also hosted on Scholar, at scholar-db.rcac.purdue.edu. As in Project 9, this is the Lahman Baseball Database. You can find its documentation here, including the definitions of the tables and columns. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. Important note: For this project all solutions should be done using R code chunks, and the RMariaDB package. Run the following code to load the library: library(RMariaDB) 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and, rather than navigating to the terminal like we did in the previous project, instead, create a connection to our MariaDB lahman database using the RMariaDB package in R, and the credentials below. Confirm the connection by running the following code chunk: con &lt;- dbConnect(RMariaDB::MariaDB(), host=&quot;scholar-db.rcac.purdue.edu&quot;, db=&quot;lahmandb&quot;, user=&quot;lahman_user&quot;, password=&quot;HitAH0merun&quot;) head(dbGetQuery(con, &quot;SHOW tables;&quot;)) Hint: In the example provided, the variable con from the dbConnect function is the connection. Each query that you make, using the dbGetQuery, needs to use this connection con. You can change the name con if you want to (it is user defined), but if you change the name con, you need to change it on all of your connections. If your connection to the database dies while you are working on the project, you can always re-run the dbConnect line again, to reset your connection to the database. Relevant topics: RMariaDB, dbConnect, dbGetQuery Item(s) to submit: R code used to solve the problem. Output from running your (potentially modified) head(dbGetQuery(con, &quot;SHOW tables;&quot;)). Solution library(RMariaDB) host &lt;- &quot;scholar-db.rcac.purdue.edu&quot; dbname &lt;- &quot;lahmandb&quot; user &lt;- &quot;lahman_user&quot; password &lt;- &quot;HitAH0merun&quot; con &lt;- dbConnect(MariaDB(),host=host, dbname=dbname, user=user, password=password) head(dbGetQuery(con, &quot;SHOW tables;&quot;)) 2. How many players are members of the 40/40 club? These are players that have stolen at least 40 bases (SB) and hit at least 40 home runs (HR) in one year. Hint: Use the batting table. Important note: You only need to run library(RMariaDB) and the dbConnect portion of the code a single time towards the top of your project. After that, you can simply reuse your connection con to run queries. Important note: In our project template, for this project, make all of the SQL queries using the dbGetQuery function, which returns the results directly in R. Therefore, your RMarkdown blocks for this project should all be {r} blocks (as opposed to the {sql} blocks used in Project 9). Hint: You can use dbGetQuery to run your queries from within R. Example: dbGetQuery(con, &quot;SELECT * FROM battings LIMIT 5;&quot;) Note: We already demonstrated the correct SQL query to use for the 40/40 club in this video, but now we want you to use RMariaDB to solve this query: Click here for video Relevant topics: dbGetQuery, AND/OR, DISTINCT, COUNT Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con,&quot;SELECT DISTINCT COUNT(*) FROM batting WHERE HR&gt;=40 AND SB&gt;=40;&quot;) 3. Find Corey Kluber's lifetime across his career (i.e., use SUM from SQL to summarize his achievements) in two categories: strikeouts (SO) and walks (BB). Also display his Strikeouts to Walks ratio. A Strikeout to Walks ratio is calculated by this equation: \\(\\frac{Strikeouts}{Walks}\\). Click here for video Important note: Questions in this project need to be solved using SQL when possible. You will not receive credit for a question if you use sum in R rather than SUM in SQL. Hint: Use the people table to find the playerID and use the pitching table to find the statistics. Relevant topics: dbGetQuery, SUM, SELECT, FROM, WHERE Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con, &quot;SELECT playerID FROM people WHERE nameFirst =&#39;Corey&#39; AND nameLast = &#39;Kluber&#39;;&quot;) dbGetQuery(con, &quot;SELECT SUM(SO), SUM(BB), SUM(SO)/SUM(BB) FROM pitching WHERE playerID =&#39;klubeco01&#39;;&quot;) 4. How many times in total has Giancarlo Stanton struck out in years in which he played for &quot;MIA&quot; or &quot;FLO&quot;? Hint: Use the people table to find the playerID and use the batting table to find the statistics. Relevant topics: dbGetQuery, AND/OR, SUM Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con, &quot;SELECT playerID FROM people WHERE nameFirst =&#39;Giancarlo&#39; AND nameLast = &#39;Stanton&#39;;&quot;) dbGetQuery(con, &quot;SELECT COUNT(*), SUM(SO) FROM batting WHERE playerID = &#39;stantmi03&#39; AND (teamID = &#39;MIA&#39; OR teamID = &#39;FLO&#39;);&quot;) 5. The Batting Average is a metric for a batter's performance. The Batting Average in a year is calculated by \\(\\frac{H}{AB}\\) (the number of hits divided by at-bats). Considering (only) the years between 2000 and 2010, calculate the (seasonal) Batting Average for each batter who had more than 300 at-bats in a season. List the top 5 batting averages next to playerID, teamID, and yearID. Hint: Use the batting table. Relevant topics: dbGetQuery, ORDER BY, BETWEEN Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con,&quot;SELECT playerID, teamID, yearID, H, AB, H/AB FROM batting WHERE yearID BETWEEN 2000 AND 2010 AND AB&gt;300 ORDER BY H/AB DESC LIMIT 5;&quot;) 6. How many unique players have hit &gt; 50 home runs (HR) in a season? Hint: Use the batting table. Hint: If you view DISTINCT as being paired with SELECT, instead, think of it as being paired with one of the fields you are selecting. Relevant topics: dbGetQuery, DISTINCT, COUNT Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con, &quot;SELECT COUNT(DISTINCT playerID) FROM batting WHERE HR&gt;50;&quot;) # As you can see, DISTINCT is with the field playerID, not with SELECT. # The following query should help distinguish this. dbGetQuery(con, &quot;SELECT COUNT(DISTINCT playerID), COUNT(playerID) FROM batting WHERE HR&gt;50;&quot;) 7. Find the number of unique players that attended Purdue University. Start by finding the schoolID for Purdue and then find the number of players who played there. Do the same for IU. Who had more? Purdue or IU? Use the information you have in the database, and the power of R to create a misleading graphic that makes Purdue look better than IU, even if just at first glance. Make sure you label the graphic. Hint: Use the schools table to get the schoolIDs, and the collegeplaying table to get the statistics. Hint: You can mess with the scale of the y-axis. You could (potentially) filter the data to start from a certain year or be between two dates. Hint: To find IU's id, try the following query: SELECT schoolID FROM schools WHERE name_full LIKE '%indiana%';. You can find more about the LIKE clause and % here. Relevant topics: dbGetQuery, plotting in R, DISTINCT, COUNT Item(s) to submit: R code used to solve the problem. The result of running the R code. dbGetQuery(con, &quot;SELECT schoolID FROM schools WHERE name_full=&#39;Purdue University&#39;;&quot;) dbGetQuery(con, &quot;SELECT schoolID FROM schools WHERE LIKE &#39;%indiana%&#39;;&quot;) dbGetQuery(con,&quot;SELECT COUNT(DISTINCT playerID) FROM collegeplaying WHERE schoolID =&#39;purdue&#39;;&quot;) dbGetQuery(con,&quot;SELECT DISTINCT playerID FROM collegeplaying WHERE schoolID =&#39;indiana&#39;;&quot;) purdue &lt;- length(unique(dbGetQuery(con,&quot;SELECT playerID FROM collegeplaying WHERE schoolID =&#39;purdue&#39;;&quot;))$playerID) iu &lt;- length(unique(dbGetQuery(con,&quot;SELECT playerID FROM collegeplaying WHERE schoolID =&#39;indiana&#39;;&quot;))$playerID) barplot(purdue, iu) barplot(log(c(purdue, iu)), main=&quot;Pro baseball players: Indiana vs. Purdue&quot;, col=c(&quot;#990000&quot;,&quot;#CFB53B&quot;), names.arg=c(&quot;Purdue&quot;, &quot;IU&quot;), cex.names=.5, ylab=&quot;Log count&quot;) Project 11 Motivation: Being able to use results of queries as tables in new queries (also known as writing sub-queries), and calculating values like MIN, MAX, and AVG in aggregate are key skills to have in order to write more complex queries. In this project we will learn about aliasing, writing sub-queries, and calculating aggregate values. Context: We are in the middle of a series of projects focused on working with databases and SQL. In this project we introduce aliasing, sub-queries, and calculating aggregate values using a much larger dataset! Scope: SQL, SQL in R Learning objectives: Demonstrate the ability to interact with popular database management systems within R. Solve data-driven problems using a combination of SQL and R. Basic clauses: SELECT, ORDER BY, LIMIT, DESC, ASC, COUNT, WHERE, FROM, etc. Showcase the ability to filter, alias, and write subqueries. Perform grouping and aggregate data using group by and the following functions: COUNT, MAX, SUM, AVG, LIKE, HAVING. Explain when to use having, and when to use where. Dataset The following questions will use the elections database. Similar to Project 10, this database is hosted on Scholar. Moreover, Question 1 also involves the following data files found in Scholar: /class/datamine/data/election/itcontYYYY.txt (for example, data for year 1980 would be /class/datamine/data/election/itcont1980.txt) A public sample of the data can be found here: https://www.datadepot.rcac.purdue.edu/datamine/data/election/itcontYYYY.txt (for example, data for year 1980 would be https://www.datadepot.rcac.purdue.edu/datamine/data/election/itcont1980.txt) Questions Important note: For this project you will need to connect to the database elections using the RMariaDB package in R. Include the following code chunk in the beginning of your RMarkdown file: ```{r setup-database-connection} library(RMariaDB) con &lt;- dbConnect(RMariaDB::MariaDB(), host=&quot;scholar-db.rcac.purdue.edu&quot;, db=&quot;elections&quot;, user=&quot;elections_user&quot;, password=&quot;Dataelect!98&quot;) ``` When a question involves SQL queries in this project, you may use a SQL code chunk (with {sql}), or an R code chunk (with {r}) and functions like dbGetQuery as you did in Project 10. Please refer to Question 5 in the project template for examples. 1. Approximately how large was the lahman database (use the sqlite database in Scholar: /class/datamine/data/lahman/lahman.db)? Use UNIX utilities you've learned about this semester to write a line of code to return the size of that .db file (in MB). The data we consider in this project are much larger. Use UNIX utilities (bash and awk) to write another line of code that calculates the total amount of data in the elections folder /class/datamine/data/election/. How much data (in MB) is there? The data in that folder has been added to the elections database, all aggregated in the elections table. Write a SQL query that returns the number of rows of data are in the database. How many rows of data are in the table elections? Note: These are some examples of how to get the sizes of collections of files in UNIX. Click here for video Hint: The SQL query will take some time! Be patient. Note: You may use more than one code chunk in your RMarkdown file for the different tasks. Note: We will accept values that represent either apparent or allocated size, as well as estimated disk usage. To get the size from ls and du to match, use the --apparent-size option with du. Note: A Megabyte (MB) is actually 1000^2 bytes, not 1024^2. A Mebibyte (MiB) is 1024^2 bytes. See here for more information. For this question, either solution will be given full credit. This is a potentially useful example. Relevant topics: SQL, SQL in R, awk, ls, du Item(s) to submit: Line of code (bash/awk) to show the size (in MB) of the lahman database file. Approximate size of the lahman database in MB. Line of code (bash/awk) to calculate the size (in MB) of the entire elections dataset in /class/datamine/data/election. The size of the elections data in MB. SQL query used to find the number of rows of data in the elections table in the elections database. The number of rows in the elections table in the elections database. Solution ls -la *.txt | awk &#39;{ total += $4; }END{print total/1000000}&#39; SELECT COUNT(*) FROM elections; 2. Write a SQL query using the LIKE command to find a unique list of zip_codes that start with &quot;479&quot;. Write another SQL query and answer: How many unique zip_codes are there that begin with &quot;479&quot;? Note: Here are some examples about SQL that might be relevant for Questions 2 and 3 in this project. Click here for video Hint: The first query returns a list of zip codes, and the second returns a count. Hint: Make sure you only select zip_codes. Relevant topics: SQL, LIKE Item(s) to submit: SQL queries used to answer the question. The first 5 results from running the query. Solution SELECT DISTINCT zip_code FROM elections WHERE zip_code LIKE &#39;479%&#39; LIMIT 5; SELECT COUNT(DISTINCT zip_code) FROM elections WHERE zip_code LIKE &#39;479%&#39;; 3. Write a SQL query that counts the number of donations (rows) that are from Indiana. How many donations are from Indiana? Rewrite the query and create an alias for our field so it doesn't read COUNT(*) but rather Indiana Donations. Hint: You may enclose an alias's name in quotation marks (single or double) when the name contains space. Relevant topics: SQL, WHERE, aliasing Item(s) to submit: SQL query used to answer the question. The result of the SQL query. Solution SELECT COUNT(*) FROM elections WHERE state=&#39;IN&#39;; SELECT COUNT(*) AS &#39;Indiana Donations&#39; FROM elections WHERE state=&#39;IN&#39;; 4. Rewrite the query in (3) so the result is displayed like: IN: 1234567. Note, if instead of &quot;IN&quot; we wanted &quot;OH&quot;, only the WHERE clause should be modified, and the display should automatically change to OH: 1234567. In other words, the state abbreviation should be dynamic, not static. Note: This video demonstrates how to use CONCAT in a MySQL query. Click here for video Hint: Use CONCAT and aliasing to accomplish this. Hint: Remember, state contains the state abbreviation. Relevant topics: SQL, aliasing, CONCAT Item(s) to submit: SQL query used to answer the question. Solution SELECT CONCAT(state, &quot;: &quot;, COUNT(*)) AS &#39;Donations&#39; FROM elections WHERE state=&#39;IN&#39;; 5. In (2) we wrote a query that returns a unique list of zip_codes that start with &quot;479&quot;. In (3) we wrote a query that counts the number of donations that are from Indiana. Use our query from (2) as a sub-query to find how many donations come from areas with zip_codes starting with &quot;479&quot;. What percent of donations in Indiana come from said zip_codes? Note: This video gives two examples of sub-queries. Click here for video Hint: You can simply manually calculate the percent using the count in (2) and (5). Relevant topics: SQL, aliasing, subqueries, IN Item(s) to submit: SQL queries used to answer the question. The percentage of donations from Indiana from zip_codes starting with &quot;479&quot;. Solution SELECT COUNT(*) FROM elections WHERE zip_code IN (SELECT DISTINCT zip_code FROM elections WHERE zip_code LIKE &#39;479%&#39;); 6. In (3) we wrote a query that counts the number of donations that are from Indiana. When running queries like this, a natural &quot;next question&quot; is to ask the same question about another state. SQL gives us the ability to calculate functions in aggregate when grouping by a certain column. Write a SQL query that returns the state, number of donations from each state, the sum of the donations (transaction_amt). Which 5 states gave the most donations (highest count)? Order you result from most to least. Note: In this video we demonstrate GROUP BY, ORDER BY, DESC, and other aspects of MySQL that might help with this question. Click here for video Hint: You may want to create an alias in order to sort. Relevant topics: SQL, GROUP BY Item(s) to submit: SQL query used to answer the question. Which 5 states gave the most donations? Solution SELECT state, COUNT(*) AS &#39;cnt&#39;, SUM(transaction_amt) AS &#39;total&#39; FROM elections GROUP BY state ORDER BY cnt DESC; Project 12 Motivation: Databases are comprised of many tables. It is imperative that we learn how to combine data from multiple tables using queries. To do so we perform joins! In this project we will explore learn about and practice using joins on a database containing bike trip information from the Bay Area Bike Share. Context: We've introduced a variety of SQL commands that let you filter and extract information from a database in an systematic way. In this project we will introduce joins, a powerful method to combine data from different tables. Scope: SQL, sqlite, joins Learning objectives: Briefly explain the differences between left and inner join and demonstrate the ability to use the join statements to solve a data-driven problem. Perform grouping and aggregate data using group by and the following functions: COUNT, MAX, SUM, AVG, LIKE, HAVING. Showcase the ability to filter, alias, and write subqueries. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/bay_area_bike_share/bay_area_bike_share.db A public sample of the data can be found here. Important note: For this project all solutions should be done using SQL code chunks. To connect to the database, copy and paste the following before your solutions in your .Rmd: ```{r, include=F} library(RSQLite) bikeshare &lt;- dbConnect(RSQLite::SQLite(), &quot;/class/datamine/data/bay_area_bike_share/bay_area_bike_share.db&quot;) ``` Each solution should then be placed in a code chunk like this: ```{sql, connection=bikeshare} SELECT * FROM station LIMIT 5; ``` If you want to use a SQLite-specific function like .tables (or prefer to test things in the Terminal), you will need to use the Terminal to connect to the database and run queries. To do so, you can connect to RStudio Server at https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal. In the terminal execute the command: sqlite3 /class/datamine/data/bay_area_bike_share/bay_area_bike_share.db From there, the SQLite-specific commands will function properly. They will not function properly in an SQL code chunk. To display the SQLite-specific commands in a code chunk without running the code, use a code chunk with the option eval=F like this: ```{sql, connection=bikeshare, eval=F} SELECT * FROM station LIMIT 5; ``` This will allow the code to be displayed without throwing an error. There are a variety of ways to join data using SQL. With that being said, if you are able to understand and use a LEFT JOIN and INNER JOIN, you can perform all of the other types of joins (RIGHT JOIN, FULL OUTER JOIN). You can see the documentation here: SQL, INNER JOIN, LEFT JOIN Questions 1. Aliases can be created for tables, fields, and even results of aggregate functions (like MIN, MAX, COUNT, AVG, etc.). In addition, you can combine fields using the sqlite concatenate operator || (see here). Write a query that returns the first 5 records of information from the station table formatted in the following way: (id) name @ (lat, long) For example: (84) Ryland Park @ (37.342725, -121.895617) Hint: Here is a video about how to concatenate strings in SQLite. Click here for vidoe Relevant topics: aliasing, concat Item(s) to submit: SQL query used to solve this problem. The first 5 records of information from the station table. Solution SELECT &#39;(&#39;||id||&#39;) &#39;||name||&#39; @ (&#39;||lat||&#39;,&#39;||long||&#39;)&#39; FROM station LIMIT 5; 2. There is a variety of interesting weather information in the weather table. Write a query that finds the average mean_temperature_f by zip_code. Which is on average the warmest zip_code? Use aliases to format the result in the following way: Zip Code|Avg Temperature 94041|61.3808219178082 Note that this is the output if you use sqlite in the terminal. While the output in your knitted pdf file may look different, you should name the columns accordingly. Hint: Here is a video about GROUP BY, ORDER BY, DISTINCT, and COUNT Click here for video Relevant topics: aliasing, GROUP BY, AVG Item(s) to submit: SQL query used to solve this problem. The results of the query copy and pasted. Solution SELECT zip_code AS &#39;Zip Code&#39;, AVG(mean_temperature_f) AS &#39;Avg Temperature&#39; FROM weather GROUP BY zip_code; 3. From (2) we can see that there are only 5 zip_codes with weather information. How many unique zip_codes do we have in the trip table? Write a query that finds the number of unique zip_codes in the trip table. Write another query that lists the zip_code and count of the number of times the zip_code appears. If we had originally assumed that the zip_code was related to the location of the trip itself, we were wrong. Can you think of a likely explanation for the unexpected zip_code values in the trip table? Hint: There could be missing values in zip_code. We want to avoid them in SQL queries, for now. You can learn more about the missing values (or NULL) in SQL here. Relevant topics: GROUP BY, COUNT Item(s) to submit: SQL queries used to solve this problem. 1-2 sentences explainging what a possible explanation for the zip_codes could be. Solution SELECT COUNT(DISTINCT zip_code) FROM trip; SELECT zip_code, COUNT(zip_code) FROM trip GROUP BY zip_code; 4. In (2) we wrote a query that finds the average mean_temperature_f by zip_code. What if we want to tack on our results in (2) to information from each row in the station table based on the zip_codes? To do this, use an INNER JOIN. INNER JOIN combines tables based on specified fields, and returns only rows where there is a match in both the &quot;left&quot; and &quot;right&quot; tables. Hint: Use the query from (2) as a sub query within your solution. Hint: Here is a video about JOIN and LEFT JOIN. Click here for video Relevant topics: INNER JOIN, subqueries, aliasing Item(s) to submit: SQL query used to solve this problem. Solution SELECT * FROM station as s INNER JOIN (SELECT zip_code AS &#39;Zip Code&#39;, AVG(mean_temperature_f) AS &#39;Avg Temperature&#39; FROM weather GROUP BY zip_code) AS sub ON s.zip_code=sub.&#39;Zip Code&#39;; 5. In (3) we alluded to the fact that the zip_codes in the trip table aren't very consistent. Users can enter a zip code when using the app. This means that zip_code can be from anywhere in the world! With that being said, if the zip_code is one of the 5 zip_codes for which we have weather data (from question 2), we can add that weather information to matching rows of the trip table. In (4) we used an INNER JOIN to append some weather information to each row in the station table. For this question, write a query that performs an INNER JOIN and appends weather data from the weather table to the trip data from the trip table. Limit your output to 5 lines. Important note: Notice that the weather data has about 1 row of weather information for each date and each zip code. This means you may have to join your data based on multiple constraints instead of just 1 like in (4). In the trip table, you can use start_date for for the date information. Hint: You will want to wrap your dates and datetimes in sqlite's date function prior to comparison. Relevant topics: INNER JOIN, aliasing Item(s) to submit: SQL query used to solve this problem. First 5 lines of output. Solution SELECT * FROM trip AS t INNER JOIN weather AS w ON t.zip_code=w.zip_code AND date(t.start_date)=date(w.date) LIMIT 5; Project 13 Motivation: Databases you will work with won't necessarily come organized in the way that you like. Getting really comfortable writing longer queries where you have to perform many joins, alias fields and tables, and aggregate results, is important. In addition, gaining some familiarity with terms like primary key, and foreign key will prove useful when you need to search for help online. In this project we will write some more complicated queries with a fun database. Proper preparation prevents poor performance, and that means practice! Context: We are towards the end of a series of projects that give you an opportunity to practice using SQL. In this project, we will reinforce topics you've already learned, with a focus on subqueries and joins. Scope: SQL, sqlite Learning objectives: Write and run SQL queries in sqlite on real-world data. Identify primary and foreign keys in a SQL table. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/movies_and_tv/imdb.db Important note: For this project you will use SQLite to access the data. To connect to the database, copy and paste the following before your solutions in your .Rmd: ```{r, include=F} library(RSQLite) imdb &lt;- dbConnect(RSQLite::SQLite(), &quot;/class/datamine/data/movies_and_tv/imdb.db&quot;) ``` If you want to use a SQLite-specific function like .tables (or prefer to test things in the Terminal), you will need to use the Terminal to connect to the database and run queries. To do so, you can connect to RStudio Server at https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal. In the terminal execute the command: sqlite3 /class/datamine/data/movies_and_tv/imdb.db From there, the SQLite-specific commands will function properly. They will not function properly in an SQL code chunk. To display the SQLite-specific commands in a code chunk without running the code, use a code chunk with the option eval=F like this: ```{sql, connection=imdb, eval=F} SELECT * FROM titles LIMIT 5; ``` This will allow the code to be displayed without throwing an error. Questions 1. A primary key is a field in a table which uniquely identifies a row in the table. Primary keys must be unique values, and this is enforced at the database level. A foreign key is a field whose value matches a primary key in a different table. A table can have 0-1 primary key, but it can have 0+ foreign keys. Examine the titles table. Do you think there are any primary keys? How about foreign keys? Now examine the episodes table. Based on observation and the column names, do you think there are any primary keys? How about foreign keys? Hint: A primary key can also be a foreign key. Hint: Here are two videos. The first video will remind you how to find the names of all of the tables in the imdb database. The second video will introduce you to the titles and episodes tables in the imdb database. Click here for video Click here for video Relevant topics: primary key, foreign key Item(s) to submit: List any primary or foreign keys in the titles table. List any primary or foreign keys in the episodes table. 2. If you paste a title_id to the end of the following url, it will pull up the page for the title. For example, https://www.imdb.com/title/tt0413573 leads to the page for the TV series Grey's Anatomy. Write a SQL query to confirm that the title_id tt0413573 does indeed belong to Grey's Anatomy. Then browse imdb.com and find your favorite TV show. Get the title_id from the url of your favorite TV show and run the following query, to confirm that the TV show is in our database: SELECT * FROM titles WHERE title_id=&#39;&lt;title id here&gt;&#39;; Make sure to replace &quot;&lt;title id here&gt;&quot; with the title_id of your favorite show. If your show does not appear, or has only a single season, pick another show until you find one we have in our database with multiple seasons. Relevant topics: SELECT, WHERE Item(s) to submit: SQL query used to confirm that title_id tt0413573 does indeed belong to Grey's Anatomy. The output of the query. The title_id of your favorite TV show. SQL query used to confirm the title_id for your favorite TV show. The output of the query. 3. The episode_title_id column in the episodes table references titles of individual episodes of a TV series. The show_title_id references the titles of the show itself. With that in mind, write a query that gets a list of all of the episodes_title_ids (found in the episodes table), with the associated primary_title (found in the titles table) for each episode of Grey's Anatomy. Relevant topics: INNER JOIN Hint: This video shows how to extract titles of episodes in the imdb database. Click here for video Item(s) to submit: SQL query used to solve the problem in a code chunk. 4. We want to write a query that returns the title and rating of the highest rated episode of your favorite TV show, which you chose in (2). In order to do so, we will break the task into two parts in (4) and (5). First, write a query that returns a list of episode_title_ids (found in the episodes table), with the associated primary_title (found in the titles table) for each episode. Hint: This part is just like question (3) but this time with your favorite TV show, which you chose in (2). Hint: This video shows how to use a subquery, to JOIN a total of three tables in the imdb database. Click here for video Relevant topics: INNER JOIN, aliasing Item(s) to submit: SQL query used to solve the problem in a code chunk. The first 5 results from your query. 5. Write a query that adds the rating to the end of each episode. To do so, use the query you wrote in (4) as a subquery. Which episode has the highest rating? Is it also your favorite episode? Relevant topics: INNER JOIN, aliasing, subqueries, ORDER BY, DESC, LIMIT Note: Various helpful examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL query used to solve the problem in a code chunk. The episode_title_id, primary_title, and rating of the top rated episode from your favorite TV series, in question (2). A statement saying whether the highest rated episode is also your favorite episode. Project 14 Motivation: As we learned earlier in the semester, bash scripts are a powerful tool when you need to perform repeated tasks in a UNIX-like system. In addition, sometimes preprocessing data using UNIX tools prior to analysis in R or Python is useful. Ample practice is integral to becoming proficient with these tools. As such, we will be reviewing topics learned earlier in the semester. Context: We've just ended a series of projects focused on SQL. In this project we will begin to review topics learned throughout the semester, starting writing bash scripts using the various UNIX tools we learned about in Projects 3 through 8. Scope: awk, UNIX utilities, bash scripts, fread Learning objectives: Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc. Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc. Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc. Use grep to search files effectively. Use cut to section off data from the command line. Use piping to string UNIX commands together. Use awk for data extraction, and preprocessing. Create bash scripts to automate a process or processes. Dataset The following questions will use PLOTSNAP.csv from the data folder found in Scholar: /class/datamine/data/forest To read more about PLOTSNAP.csv that you will be working with: https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/plot-level-data-gathered-through-forest/metadata#fields Questions 1. Take a look at at PLOTSNAP.csv. Write a line of awk code that displays the STATECD followed by the number of rows with that STATECD. Relevant topics: awk Item(s) to submit: Code used to solve the problem. Count of the following STATECDs: 1, 2, 4, 5, 6 2. Unfortunately, there isn't a very accessible list available that shows which state each STATECD represents. This is no problem for us though, the dataset has LAT and LON! Write some bash that prints just the STATECD, LAT, and LON. Note: There are 92 columns in our dataset: awk -F, 'NR==1{print NF}' PLOTSNAP.csv. To create a list of STATECD to state, we only really need STATECD, LAT, and LON. Keeping the other 89 variables will keep our data at 2.1gb. Relevant topics: cut, awk Item(s) to submit: Code used to solve the problem. The output of your code piped to head. 3. fread is a &quot;Fast and Friendly File Finagler&quot;. It is part of the very popular data.table package in R. We will learn more about this package next semester. For now, read the documentation here and use the cmd argument in conjunction with your bash code from (2) to read the data of STATECD, LAT, and LON into a data.table in your R environment. Relevant topics: fread Item(s) to submit: Code used to solve the problem. The head of the resulting data.table. 4. We are going to further understand the data from question (3) by finding the actual locations based on the LAT and LON columns. We can use the library revgeo to get a location given a pair of longitude and latitude values. revgeo uses a free API hosted by photon in order to do so. For example: library(revgeo) revgeo(longitude=-86.926153, latitude=40.427055, output=&#39;frame&#39;) The code above will give you the address information in six columns, from the most-granular housenumber to the least-granular country. Depending on the coordinates, revgeo may or may not give you results for each column. For this question, we are going to keep only the state column. There are over 4 million rows in our dataset -- we do not want to hit photon's API that many times. Instead, we are going to do the following: First: Unless you feel comfortable using data.table, convert your data.table to a data.frame: my_dataframe &lt;- data.frame(my_datatable) Second: Calculate the average LAT and LON for each STATECD, and call the new data.frame, dat. This should result in 57 rows of lat/long pairs. Third: For each row in dat, run a reverse geocode and append the state to a new column called STATE. Hint: To calculate the average LAT and LON for each STATECD, you could use the sqldf package to run SQL queries on your data.frame. Hint: mapply is a useful apply function to use to solve this problem. Hint: Here is some extra help: library(revgeo) points &lt;- data.frame(latitude=c(40.433663, 40.432104, 40.428486), longitude=c(-86.916584, -86.919610, -86.920866)) # Note that the &quot;output&quot; argument gets passed to the &quot;revgeo&quot; function. mapply(revgeo, points$longitude, points$latitude, output=&quot;frame&quot;) # The output isn&#39;t in a great format, and we&#39;d prefer to just get the &quot;state&quot; data. # Let&#39;s wrap &quot;revgeo&quot; into another function that just gets &quot;state&quot; and try again. get_state &lt;- function(lon, lat) { return(revgeo(lon, lat, output=&quot;frame&quot;)[&quot;state&quot;]) } mapply(get_state, points$longitude, points$latitude) Important note: It is okay to get &quot;Not Found&quot; for some of the addresses. Relevant topics: apply functions, sqldf Item(s) to submit: Code used to solve the problem. The head of the resulting data.frame. Project 15 Motivation: We've done a lot of work with SQL this semester. Let's review concepts in this project and mix and match R and SQL to solve data-driven problems. Context: In this project, we will reinforce topics you've already learned, with a focus on SQL. Scope: SQL, sqlite, R Learning objectives: Write and run SQL queries in sqlite on real-world data. Use SQL from within R. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/movies_and_tv/imdb.db Questions 1. What is the first year where our database has &gt; 1000 titles? Use the premiered column in the titles table as our year. What year has the most titles? Hint: There could be missing values in premiered. We want to avoid them in SQL queries, for now. You can learn more about the missing values (or NULL) in SQL here. Relevant topics: COUNT, GROUP BY, ORDER BY, DESC, HAVING, NULL Item(s) to submit: SQL queries used to answer the questions. What year is the first year to have &gt; 1000 titles? What year has the most titles? 2. How many, and what are the unique types from the titles table? Fpr the year found in question (1) with the most titles, how many titles of each type are there? Item(s) to submit: SQL queries used to answer the questions. How many and what are the unique types from the titles table? A list of type and and count for the year (premiered) that had the most titles. F.R.I.E.N.D.S is a popular tv show. They have an interesting naming convention for the names of their episodes. They all begin with the text &quot;The One ...&quot;. There are 6 primary characters in the show: Chandler, Joey, Monica, Phoebe, Rachel, and Ross. Let's use SQL and R to take a look at how many times each characters' names appear in the title of the episodes. 3. Write a query that gets the episode_title_id, primary_title, rating, and votes, of all of the episodes of Friends (title_id is tt0108778). Hint: You can slightly modify the solution to question (5) in project 13. Relevant topics: INNER JOIN, subqueries, aliasing Item(s) to submit: SQL query used to answer the question. First 5 results of the query. 4. Now that you have a working query, connect to the database and run the query to get the data into an R data frame. In previous projects, we learned how to used regular expressions to search for text. For each character, how many episodes primary_titles contained their name? Relevant topics: SQL in R, grep Item(s) to submit: R code in a code chunk that was used to find the solution. The solution pasted below the code chunk. 5. Create a graphic showing our results in (4) using your favorite package. Make sure the plot has a good title, x-label, y-label, and try to incorporate some of the following colors: #273c8b, #bd253a, #016f7c, #f56934, #016c5a, #9055b1, #eaab37. Relevant topics: plotting Item(s) to submit: The R code used to generate the graphic. The graphic in a png or jpg/jpeg format. 6. Use a combination of SQL and R to find which of the following 3 genres has the highest average rating for movies (see type column from titles table): Romance, Comedy, Animation. In the titles table, you can find the genres in the genres column. There may be some overlap (i.e. a movie may have more than one genre), this is ok. To query rows which have the genre Action as one of its genres: SELECT * FROM titles WHERE genres LIKE &#39;%action%&#39;; Relevant topics: LIKE, INNER JOIN Item(s) to submit: Any code you used to solve the problem in a code chunk. The average rating of each of the genres listed for movies. STAT 39000 Project 1 Motivation: In this project we will jump right into an R review. In this project we are going to break one larger data-wrangling problem into discrete parts. There is a slight emphasis on writing functions and dealing with strings. At the end of this project we will have greatly simplified a dataset, making it easy to dig into. Context: We just started the semester and are digging into a large dataset, and in doing so, reviewing R concepts we've previously learned. Scope: data wrangling in R, functions Learning objectives: Comprehend what a function is, and the components of a function in R. Read and write basic (csv) data. Utilize apply functions in order to solve a data-driven problem. Make sure to read about, and use the template found here, and the important information about projects submissions here. You can find useful examples that walk you through relevant material in The Examples Book: https://thedatamine.github.io/the-examples-book It is highly recommended to read through, search, and explore these examples to help solve problems in this project. Important note: It is highly recommended that you use https://rstudio.scholar.rcac.purdue.edu/. Simply click on the link and login using your Purdue account credentials. We decided to move away from ThinLinc and away from the version of RStudio used last year (https://desktop.scholar.rcac.purdue.edu). The version of RStudio is known to have some strange issues when running code chunks. Remember the very useful documentation shortcut ?. To use, simply type ? in the console, followed by the name of the function you are interested in. You can also look for package documentation by using help(package=PACKAGENAME), so for example, to see the documentation for the package ggplot2, we could run: help(package=ggplot2) Sometimes it can be helpful to see the source code of a defined function. A function is any chunk of organized code that is used to perform an operation. Source code is the underlying R or c or c++ code that is used to create the function. To see the source code of a defined function, type the function's name without the (). For example, if we were curious about what the function Reduce does, we could run: Reduce Occasionally this will be less useful as the resulting code will be code that calls c code we can't see. Other times it will allow you to understand the function better. Dataset: /class/datamine/data/airbnb Often times (maybe even the majority of the time) data doesn’t come in one nice file or database. Explore the datasets in /class/datamine/data/airbnb. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. You may have noted that, for each country, city, and date we can find 3 files: calendar.csv.gz, listings.csv.gz, and reviews.csv.gz (for now, we will ignore all files in the &quot;visualisations&quot; folders). Let's take a look at the data in each of the three types of files. Pick a country, city and date, and read the first 50 rows of each of the 3 datasets (calendar.csv.gz, listings.csv.gz, and reviews.csv.gz). Provide 1-2 sentences explaining the type of information found in each, and what variable(s) could be used to join them. Hint: read.csv has an argument to select the number of rows we want to read. Hint: Depending on the country that you pick, the listings and/or the reviews might not display properly in RMarkdown. So you do not need to display the first 50 rows of the listings and/or reviews, in your RMarkdown document. It is OK to just display the first 50 rows of the calendar entries. Item(s) to submit: Chunk of code used to read the first 50 rows of each dataset. 1-2 sentences briefly describing the information contained in each dataset. Name(s) of variable(s) that could be used to join them. To read a compressed csv, simply use the read.csv function: dat &lt;- read.csv(&quot;/class/datamine/data/airbnb/brazil/rj/rio-de-janeiro/2019-06-19/data/calendar.csv.gz&quot;) head(dat) Let's work towards getting this data into an easier format to analyze. From now on, we will focus on the listings.csv.gz datasets. Solution The calendar.csv.gz file for 2019-07-08 in Hawaii describes the listing_id, date, available (t or f), price, adjusted_price, minimum_nights, and maximum_nights hawaii_calendar &lt;- read.csv(&quot;/class/datamine/data/airbnb/united-states/hi/hawaii/2019-07-08/data/calendar.csv.gz&quot;) head(hawaii_calendar, n=50) The listings.csv.gz file for 2019-07-08 in Hawaii has 106 variables, which describe the very specific attributes of the airbnb listings. hawaii_calendar &lt;- read.csv(&quot;/class/datamine/data/airbnb/united-states/hi/hawaii/2019-07-08/data/listings.csv.gz&quot;) dim(hawaii_listings) The reviews.csv.gz file for 2019-07-08 in Hawaii describes the listing_id, id, date, reviewer_id, reviewer_name, and comments hawaii_calendar &lt;- read.csv(&quot;/class/datamine/data/airbnb/united-states/hi/hawaii/2019-07-08/data/reviews.csv.gz&quot;) head(hawaii_reviews, n=50) The variables that might be used to compare the tables are: date, id, listing_id, maximum_nights, minimum_nights, price t &lt;- table(c(names(hawaii_calendar), names(hawaii_listings), names(hawaii_reviews))) t[t &gt; 1] 2. Write a function called get_paths_for_country, that, given a string with the country name, returns a vector with the full paths for all listings.csv.gz files, starting with /class/datamine/data/airbnb/.... For example, the output from get_paths_for_country(&quot;united-states&quot;) should have 28 entries. Here are the first 5 entries in the output: [1] &quot;/class/datamine/data/airbnb/united-states/ca/los-angeles/2019-07-08/data/listings.csv.gz&quot; [2] &quot;/class/datamine/data/airbnb/united-states/ca/oakland/2019-07-13/data/listings.csv.gz&quot; [3] &quot;/class/datamine/data/airbnb/united-states/ca/pacific-grove/2019-07-01/data/listings.csv.gz&quot; [4] &quot;/class/datamine/data/airbnb/united-states/ca/san-diego/2019-07-14/data/listings.csv.gz&quot; [5] &quot;/class/datamine/data/airbnb/united-states/ca/san-francisco/2019-07-08/data/listings.csv.gz&quot; Hint: list.files is useful with the recursive=T option. Hint: Use grep to search for the pattern listings.csv.gz (within the results from the first hint), and use the option value=T to display the values found by the grep function. Item(s) to submit: Chunk of code for your get_paths_for_country function. Solution We extract all 28 of the listings for the United States first: myprefix &lt;- &quot;/class/datamine/data/airbnb/united-states/&quot; paste0(myprefix, grep(&quot;listings.csv.gz&quot;, list.files(myprefix, recursive=T), value=T)) Now we build a function that can do the same thing, for any country get_paths_for_country &lt;- function(mycountry) { myprefix &lt;- paste0(&quot;/class/datamine/data/airbnb/&quot;, mycountry, &quot;/&quot;) paste0(myprefix, grep(&quot;listings.csv.gz&quot;, list.files(myprefix, recursive=T), value=T)) } and we test this for several countries: get_paths_for_country(&quot;united-states&quot;) get_paths_for_country(&quot;brazil&quot;) get_paths_for_country(&quot;south-africa&quot;) get_paths_for_country(&quot;canada&quot;) 3. Write a function called get_data_for_country that, given a string with the country name, returns a data.frame containing the all listings data for that country. Use your previously written function to help you. Hint: Use stringsAsFactors=F in the read.csv function. Hint: Use do.call(rbind, &lt;listofdataframes&gt;) to combine a list of dataframes into a single dataframe. Relevant topics: rbind, lapply, function Item(s) to submit: Chunk of code for your get_data_for_country function. Solution We first get the data from the Canada entries. To do this, we sapply the read.csv function to each of the 6 results from get_paths_for_country(&quot;canada&quot;) In other words, we read in these 6 data frames. myresults &lt;- sapply(get_paths_for_country(&quot;canada&quot;), read.csv, stringsAsFactors=F, simplify=F) We get a list of 6 data frames: length(myresults) class(myresults) class(myresults[[1]]) class(myresults[[6]]) and we can check the dimensions of each of the 6 data frames dim(myresults[[1]]) dim(myresults[[2]]) dim(myresults[[3]]) dim(myresults[[4]]) dim(myresults[[5]]) dim(myresults[[6]]) this is more easily accomplished with another sapply: sapply(myresults, dim) We can rbind all 6 of these data frames into one big data frame as follows: bigDF &lt;- do.call(rbind, myresults) class(bigDF) dim(bigDF) Now we create the desired function called get_data_for_country get_data_for_country &lt;- function(mycountry) { myresults &lt;- sapply(get_paths_for_country(mycountry), read.csv, stringsAsFactors=F, simplify=F) do.call(rbind, myresults) } and we test it on Canada. mynewbigDF &lt;- get_data_for_country(&quot;canada&quot;) The result has the same size as before dim(mynewbigDF) 4. Use your get_data_for_country to get the data for a country of your choice, and make sure to name the data.frame listings. Take a look at the following columns: host_is_superhost, host_has_profile_pic, host_identity_verified, and is_location_exact. What is the data type for each column? (You can use class or typeof or str to see the data type.) These columns would make more sense as logical values (TRUE/FALSE/NA). Write a function called transform_column that, given a column containing lowercase &quot;t&quot;s and &quot;f&quot;s, your function will transform it to logical (TRUE/FALSE/NA) values. Note that NA values for these columns appear as blank (&quot;&quot;), and we need to be careful when transforming the data. Test your function on column host_is_superhost. Relevant topics: class, typeof, str, toupper, as.logical Item(s) to submit: Chunk of code for your transform_column function. Type of transform_column(listings$host_is_superhost). Solution These 4 columns from mynewbigDF (which has the data for Canada) only have values &quot;t&quot;, &quot;f&quot;, &quot;&quot; head(mynewbigDF$host_is_superhost) head(mynewbigDF$host_has_profile_pic) head(mynewbigDF$host_identity_verified) head(mynewbigDF$is_location_exact) Please note the 44 values of &quot;&quot; (which are easy to miss) In the first 3 out of 4 of these columns: table(mynewbigDF$host_is_superhost) table(mynewbigDF$host_has_profile_pic) table(mynewbigDF$host_identity_verified) table(mynewbigDF$is_location_exact) These are all character vectors, which we can check using class, typeof, or str: class(mynewbigDF$host_is_superhost) class(mynewbigDF$host_has_profile_pic) class(mynewbigDF$host_identity_verified) class(mynewbigDF$is_location_exact) typeof(mynewbigDF$host_is_superhost) typeof(mynewbigDF$host_has_profile_pic) typeof(mynewbigDF$host_identity_verified) typeof(mynewbigDF$is_location_exact) str(mynewbigDF$host_is_superhost) str(mynewbigDF$host_has_profile_pic) str(mynewbigDF$host_identity_verified) str(mynewbigDF$is_location_exact) We have several ways to transform a column. For example, we could go element-by-element, and make substitutions, like this: v &lt;- mynewbigDF$host_is_superhost Here is the way that the values look at the start: table(v) v[toupper(v)==&quot;T&quot;] &lt;- TRUE v[toupper(v)==&quot;F&quot;] &lt;- FALSE v[toupper(v)==&quot;&quot;] &lt;- NA and here are the values now: table(v) You might think that the NA values disappeared, but they just do not show up in the table by default. You can force them to appear, and then we see that the counts of the three values are the same as before. table(v, useNA=&quot;always&quot;) Here is the function: transform_column &lt;- function(v) { v[toupper(v)==&quot;T&quot;] &lt;- TRUE v[toupper(v)==&quot;F&quot;] &lt;- FALSE v[toupper(v)==&quot;&quot;] &lt;- NA v } We can try the function on mynewbigDF$host_is_superhost: head(transform_column(mynewbigDF$host_is_superhost)) table(transform_column(mynewbigDF$host_is_superhost)) Another possibility is to make a map, in which we put the old values as the names, and the new values as the values in the vector: mymap &lt;- c(TRUE, FALSE, NA) names(mymap) &lt;- c(&quot;T&quot;, &quot;F&quot;, &quot;&quot;) head(mymap[toupper(mynewbigDF$host_is_superhost)]) and if you do not want the names to appear on the vector, you can remove them, like this: head(unname(mymap[toupper(mynewbigDF$host_is_superhost)])) Finally we can check the table of the results: table(mymap[toupper(mynewbigDF$host_is_superhost)]) This might seem strange, and if you do not like it, you can just use the solution given above. If you do like this, and want to wrap it into a function, we can write: transform_column &lt;- function(v) { mymap &lt;- c(TRUE, FALSE, NA) names(mymap) &lt;- c(&quot;T&quot;, &quot;F&quot;, &quot;&quot;) unname(mymap[toupper(v)]) } and again we can try this new version of the function on mynewbigDF$host_is_superhost: head(transform_column(mynewbigDF$host_is_superhost)) table(transform_column(mynewbigDF$host_is_superhost)) 5. Create a histogram for response rates (host_response_rate) for super hosts (where host_is_superhost is TRUE). If your listings do not contain any super hosts, load data from a different country. Note that we first need to convert host_response_rate from a character containing &quot;%&quot; signs to a numeric variable. Relevant topics: gsub, as.numeric Item(s) to submit: Chunk of code used to answer the question. Histogram of response rates for super hosts. Solution Now we look at the Canada results, for which host_is_superhost is TRUE. We make a histogram of the host_response_rate for those values. To do this, we first get the host_response_rate values (for which host_is_superhost is TRUE) myvalues &lt;- mynewbigDF$host_response_rate[transform_column(mynewbigDF$host_is_superhost) == TRUE] and then we remove the percentage symbols from myvalues and convert the character vector to numbers, and then finally make the histogram. hist(as.numeric(gsub(&quot;%&quot;, &quot;&quot;, myvalues))) As a closing note, we could remove the check to see whether the inner values are TRUE because, by default, we will only exact the TRUE values when we do a lookup like this: myvalues &lt;- mynewbigDF$host_response_rate[transform_column(mynewbigDF$host_is_superhost)] hist(as.numeric(gsub(&quot;%&quot;, &quot;&quot;, myvalues))) Project 2 Motivation: The ability to quickly reproduce an analysis is important. It is often necessary that other individuals will need to be able to understand and reproduce an analysis. This concept is so important there are classes solely on reproducible research! In fact, there are papers that investigate and highlight the lack of reproducibility in various fields. If you are interested in reading about this topic, a good place to start is the paper titled &quot;Why Most Published Research Findings Are False&quot;, by John Ioannidis (2005). Context: Making your work reproducible is extremely important. We will focus on the computational part of reproducibility. We will learn RMarkdown to document your analyses so others can easily understand and reproduce the computations that led to your conclusions. Pay close attention as future project templates will be RMarkdown templates. Scope: Understand Markdown, RMarkdown, and how to use it to make your data analysis reproducible. Learning objectives: Use Markdown syntax within an Rmarkdown document to achieve various text transformations. Use RMarkdown code chunks to display and/or run snippets of code. Questions Click here for video 1. Make the following text (including the asterisks) bold: This needs to be **very** bold. Make the following text (including the underscores) italicized: This needs to be _very_ italicized. Important note: Surround your answer in 4 backticks. This will allow you to display the markdown without having the markdown &quot;take effect&quot;. For example: ```` Some *marked* **up** text. ```` Hint: Be sure to check out the Rmarkdown Cheatsheet and our section on Rmarkdown in the book. Note: Rmarkdown is essentially Markdown + the ability to run and display code chunks. In this question, we are actually using Markdown within Rmarkdown! Relevant topics: rmarkdown, escaping characters Item(s) to submit: - 2 lines of markdown text, surrounded by 4 backticks. Note that when compiled, this text will be unmodified, regular text. Solution We can achieve this style of text: This needs to be **very** bold This needs to be _very_ italicized. by using this Markdown text: **This needs to be \\*\\*very\\*\\* bold** _This needs to be \\_very\\_ italicized._ The backslashes specify that we want the asterisks and underscores to appear. 2. Create an unordered list of your top 3 favorite academic interests (some examples could include: machine learning, operating systems, forensic accounting, etc.). Create another ordered list that ranks your academic interests in order of most interested to least interested. Hint: You can learn what ordered and unordered lists are here. Note: Similar to (1), in this question we are dealing with Markdown. If we were to copy and paste the solution to this problem in a Markdown editor, it would be the same result as when we Knit it here. Relevant topics: rmarkdown Item(s) to submit: - Create the lists, this time don't surround your code in backticks. Note that when compiled, this text will appear as nice, formatted lists. Solution An unordered list of my top 3 favorite academic interests is: asymptotic analysis of sequences data science analysis of algorithms An ordered list of my top 3 favorite academic interests is: analysis of algorithms asymptotic analysis of sequences data science 3. Browse https://www.linkedin.com/ and read some profiles. Pay special attention to accounts with an &quot;About&quot; section. Write your own personal &quot;About&quot; section using Markdown. Include the following: A header for this section (your choice of size) that says &quot;About&quot;. The text of your personal &quot;About&quot; section that you would feel comfortable uploading to linkedin, including at least 1 link. Relevant topics: rmarkdown Item(s) to submit: - Create the described profile, don't surround your code in backticks. Solution About I am Professor in Statistics and (by courtesy) of Mathematics and Public Health at Purdue University. My research is in probabilistic, combinatorial, and analytic techniques for the analysis of algorithms and data structures; I am also interested in science of information, game theory, and large-scale computation. I currently serve as Director of The Data Mine Interim Co-Director of the Integrative Data Science Initiative Principal Investigator for the Purdue Statistics Living Learning Community, funded by the National Science Foundation Associate Director for the NSF Center for Science of Information (now a core center in Purdue's Discovery Park) Associate Director of the Actuarial Science Program ##### Solution The  equation is \\[P(A\\ |\\ B) = \\frac{P(B\\ |\\ A)P(A)}{P(B)}\\] 4. LaTeX is a powerful editing tool where you can create beautifully formatted equations and formulas. Replicate the equation found here as closely as possible. Hint: Lookup &quot;latex mid&quot; and &quot;latex frac&quot;. Item(s) to submit: Replicate the equation using LaTeX under the Question 4 header in your template. Solution my_variable &lt;- c(1,2,3) *** All About the Iris Dataset This paper goes into detail about the iris dataset that is built into r. You can find a list of built-in datasets by visiting the R datasets index or by running the following code: data() The iris dataset has 5 columns. You can get the names of the columns by running the following code: names(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; Alternatively, you could just run the following code: iris The second option provides more detail about the dataset. According to the R iris help page there is another dataset built-in to R called iris3. This dataset is 3 dimensional instead of 2 dimensional. An iris is a really pretty flower. You can see a picture of one here: In summary. I really like irises, and there is a dataset in R called iris. 5. Your co-worker wrote a report, and has asked you to beautify it. Knowing Rmarkdown, you agreed. Make improvements to this section. At a minimum: Make the title pronounced. Make all links appear as a word or words, rather than the long-form URL. Organize all code into code chunks where code and output are displayed. If the output is really long, just display the code. Make the calls to the library function be evaluated but not displayed. Make sure all warnings and errors that may eventually occur, do not appear in the final document. Feel free to make any other changes that make the report more visually pleasing. ```{r my-load-packages} library(ggplot2) ``` ```{r declare-variable-390, eval=FALSE} my_variable &lt;- c(1,2,3) ``` All About the Iris Dataset This paper goes into detail about the `iris` dataset that is built into r. You can find a list of built-in datasets by visiting https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html or by running the following code: data() The iris dataset has 5 columns. You can get the names of the columns by running the following code: names(iris) Alternatively, you could just run the following code: iris The second option provides more detail about the dataset. According to https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html there is another dataset built-in to r called `iris3`. This dataset is 3 dimensional instead of 2 dimensional. An iris is a really pretty flower. You can see a picture of one here: https://www.gardenia.net/storage/app/public/guides/detail/83847060_mOptimized.jpg In summary. I really like irises, and there is a dataset in r called `iris`. Relevant topics: rmarkdown Item(s) to submit: Make improvements to this section, and place it all under the Question 5 header in your template. Solution Here are the Sepal.Length values from the iris data set. plot(iris$Sepal.Length, main=&quot;Iris Sepal Lengths&quot;) 6. Create a plot using a built-in dataset like iris, mtcars, or Titanic, and display the plot using a code chunk. Make sure the code used to generate the plot is hidden. Include a descriptive caption for the image. Make sure to use an RMarkdown chunk option to create the caption. Relevant topics: rmarkdown, plotting in r Item(s) to submit: Code chunk under that creates and displays a plot using a built-in dataset like iris, mtcars, or Titanic. 7. Insert the following code chunk under the Question 7 header in your template. Try knitting the document. Two things will go wrong. What is the first problem? What is the second problem? ```{r my-load-packages} plot(my_variable) ``` Hint: Take a close look at the name we give our code chunk. Hint: Take a look at the code chunk where my_variable is declared. Relevant topics: rmarkdown Item(s) to submit: The modified version of the inserted code that fixes both problems. A sentence explaining what the first problem was. A sentence explaining what the second problem was. Solution We needed to change the section title from install-packages to a new name, since we already had a section with this same title. In the section called declare-variable, we had eval=F, and as a result, the variable my_variable was never declared. So we removed the option eval=F. plot(my_variable) For Project 2, please submit your .Rmd file and the resulting .pdf file. (For this project, you do not need to submit a .R file.) OPTIONAL QUESTION. RMarkdown is also an excellent tool to create a slide deck. Use the information here or here to convert your solutions into a slide deck rather than the regular PDF. You may experiment with slidy, ioslides or beamer, however, make your final set of solutions use beamer as the output is a PDF. Make any needed modifications to make the solutions knit into a well-organized slide deck (For example, include slide breaks and make sure the contents are shown completely.). Modify (2) so the bullets are incrementally presented as the slides progress. Important note: You do not need to submit the original PDF for this project, just the beamer slide version of the PDF. Relevant topics: rmarkdown Item(s) to submit: The modified version of the solutions in beamer slide form. Project 3 Motivation: The ability to navigate a shell, like bash, and use some of its powerful tools, is very useful. The number of disciplines utilizing data in new ways is ever-growing, and as such, it is very likely that many of you will eventually encounter a scenario where knowing your way around a terminal will be useful. We want to expose you to some of the most useful bash tools, help you navigate a filesystem, and even run bash tools from within an RMarkdown file in RStudio. Context: At this point in time, you will each have varying levels of familiarity with Scholar. In this project we will learn how to use the terminal to navigate a UNIX-like system, experiment with various useful commands, and learn how to execute bash commands from within RStudio in an RMarkdown file. Scope: bash, RStudio Learning objectives: Distinguish differences in /home, /scratch, and /class. Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc. Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc. Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc. Utilize other Scholar resources: rstudio.scholar.rcac.purdue.edu, notebook.scholar.rcac.purdue.edu, desktop.scholar.rcac.purdue.edu, etc. Use man to read and learn about UNIX utilities. Run bash commands from within and RMarkdown file in RStudio. There are a variety of ways to connect to Scholar. In this class, we will primarily connect to RStudio Server by opening a browser and navigating to https://rstudio.scholar.rcac.purdue.edu/, entering credentials, and using the excellent RStudio interface. Here is a video to remind you about some of the basic tools you can use in UNIX/Linux: Click here for video This is the easiest book for learning this stuff; it is short and gets right to the point: https://go.oreilly.com/purdue-university/library/view/-/0596002610 you just log in and you can see it all; we suggest Chapters 1, 3, 4, 5, 7 (you can basically skip chapters 2 and 6 the first time through). It is a very short read (maybe, say, 2 or 3 hours altogether?), just a thin book that gets right to the details. 1. Navigate to https://rstudio.scholar.rcac.purdue.edu/ and login. Take some time to click around and explore this tool. We will be writing and running Python, R, SQL, and bash all from within this interface. Navigate to Tools &gt; Global Options .... Explore this interface and make at least 2 modifications. List what you changed. Here are some changes Kevin likes: Uncheck &quot;Restore .Rdata into workspace at startup&quot;. Change tab width 4. Check &quot;Soft-wrap R source files&quot;. Check &quot;Highlight selected line&quot;. Check &quot;Strip trailing horizontal whitespace when saving&quot;. Uncheck &quot;Show margin&quot;. (Dr Ward does not like to customize his own environment, but he does use the emacs key bindings: Tools &gt; Global Options &gt; Code &gt; Keybindings, but this is only recommended if you already know emacs.) Item(s) to submit: List of modifications you made to your Global Options. Solution Here are some changes Kevin likes: Uncheck &quot;Restore .Rdata into workspace at startup&quot;. Change tab width 4. Check &quot;Soft-wrap R source files&quot;. Check &quot;Highlight selected line&quot;. Check &quot;Strip trailing horizontal whitespace when saving&quot;. Uncheck &quot;Show margin&quot;. 2. There are four primary panes, each with various tabs. In one of the panes there will be a tab labeled &quot;Terminal&quot;. Click on that tab. This terminal by default will run a bash shell right within Scholar, the same as if you connected to Scholar using ThinLinc, and opened a terminal. Very convenient! What is the default directory of your bash shell? Hint: Start by reading the section on man. man stands for manual, and you can find the &quot;official&quot; documentation for the command by typing man &lt;command_of_interest&gt;. For example: # read the manual for the `man` command # use &quot;k&quot; or the up arrow to scroll up, &quot;j&quot; or the down arrow to scroll down man man Relevant topics: man, pwd, ~, .., . Item(s) to submit: The full filepath of default directory (home directory). Ex: Kevin's is: /home/kamstut The bash code used to show your home directory or current directory (also known as the working directory) when the bash shell is first launched. Solution # whatever is stored in the $HOME environment variable # is what ~ represents cd ~ pwd # if we change $HOME, ~ changes too! HOME=/home/kamstut/projects cd ~ pwd # if other users on the linux system share certain files or folders # in their home directory, you can access their home folder similarly ls ~mdw # but they _have_ to give you permissions 3. Learning to navigate away from our home directory to other folders, and back again, is vital. Perform the following actions, in order: Write a single command to navigate to the folder containing our full datasets: /class/datamine/data. Write a command to confirm you are in the correct folder. Write a command to list the files and directories within the data directory. (You do not need to recursively list subdirectories and files contained therein.) What are the names of the files and directories? Write another command to return back to your home directory. Write a command to confirm you are in the correct folder. Note: / is commonly referred to as the root directory in a linux/unix filesystem. Think of it as a folder that contains every other folder in the computer. /home is a folder within the root directory. /home/kamstut is the full filepath of Kevin's home directory. There is a folder home inside the root directory. Inside home is another folder named kamstut which is Kevin's home directory. Relevant topics: man, cd, pwd, ls, ~, .., . Item(s) to submit: Command used to navigate to the data directory. Command used to confirm you are in the data directory. Command used to list files and folders. List of files and folders in the data directory. Command used to navigate back to the home directory. Commnad used to confirm you are in the home directory. Solution # navigate to the data directory using `cd` (change directory) cd /class/datamine/data # confirm the location using `pwd` (print working directory) pwd # list files ls # cd without any arguments automatically returns to the directory # saved in the $HOME environment variable cd # another trick, if you wanted to _quickly_ return to the data # directory, or most recent directory is the following (uncommented) cd ~- # confirm the location using pwd pwd 4. Let's learn about two more important concepts. . refers to the current working directory, or the directory displayed when you run pwd. Unlike pwd you can use this when navigating the filesystem! So, for example, if you wanted to see the contents of a file called my_file.txt that lives in /home/kamstut (so, a full path of /home/kamstut/my_file.txt), and you are currently in /home/kamstut, you could run: cat ./my_file.txt. .. represents the parent folder or the folder in which your current folder is contained. So let's say I was in /home/kamstut/projects/ and I wanted to get the contents of the file /home/kamstut/my_file.txt. You could do: cat ../my_file.txt. When you navigate a directory tree using . and .. you create paths that are called relative paths because they are relative to your current directory. Alternatively, a full path or (absolute path) is the path starting from the root directory. So /home/kamstut/my_file.txt is the absolute path for my_file.txt and ../my_file.txt is a relative path. Perform the following actions, in order: Write a single command to navigate to the data directory. Write a single command to navigate back to your home directory using a relative path. Do not use ~ or the cd command without a path argument. Relevant topics: man, cd, pwd, ls, ~, .., . Item(s) to submit: Command used to navigate to the data directory. Command used to navigate back to your home directory that uses a relative path. Solution cd /class/datamine/data pwd cd ../../../home/kamstut pwd 5. In Scholar, when you want to deal with really large amounts of data, you want to access scratch (you can read more here). Your scratch directory on Scholar is located here: /scratch/scholar/$USER. $USER is an environment variable containing your username. Test it out: echo /scratch/scholar/$USER. Perform the following actions: Navigate to your scratch directory. Confirm you are in the correct location. Execute myquota. Find the location of the myquota bash script. Output the first 5 and last 5 lines of the bash script. Count the number of lines in the bash script. How many kilobytes is the script? Hint: You could use each of the commands in the relevant topics once. Hint: When you type myquota on Scholar there are sometimes two warnings about xauth but sometimes there are no warnings. If you get a warning that says Warning: untrusted X11 forwarding setup failed: xauth key data not generated it is safe to ignore this error. Hint: Commands often have options. Options are features of the program that you can trigger specifically. You can see the options of a command in the DESCRIPTION section of the man pages. For example: man wc. You can see -m, -l, and -w are all options for wc. To test this out: # using the default wc command. &quot;/class/datamine/data/flights/1987.csv&quot; is the first &quot;argument&quot; given to the command. wc /class/datamine/data/flights/1987.csv # to count the lines, use the -l option wc -l /class/datamine/data/flights/1987.csv # to count the words, use the -w option wc -w /class/datamine/data/flights/1987.csv # you can combine options as well wc -w -l /class/datamine/data/flights/1987.csv # some people like to use a single tack `-` wc -wl /class/datamine/data/flights/1987.csv # order doesn&#39;t matter wc -lw /class/datamine/data/flights/1987.csv Hint: The -h option for the du command is useful. Relevant topics: cd, pwd, type, head, tail, wc, du Item(s) to submit: Command used to navigate to your scratch directory. Command used to confirm your location. Output of myquota. Command used to find the location of the myquota script. Absolute path of the myquota script. Command used to output the first 5 lines of the myquota script. Command used to output the last 5 lines of the myquota script. Command used to find the number of lines in the myquota script. Number of lines in the script. Command used to find out how many kilobytes the script is. Number of kilobytes that the script takes up. Solution # navigate to my scratch folder cd /scratch/scholar/$USER # confirm pwd # what is my quota, execute the myquota script myquota # get the location of the myquota script type myquota # get the first 5 lines of the myquota script head /usr/local/bin/myquota # get the last 5 lines of the myquota script tail /usr/local/bin/myquota # get the number of lines in the myquota script wc -l /usr/local/bin/myquota # get the number of kilobytes of teh myquota script du -h --apparent-size /usr/local/bin/myquota ls -la /usr/local/bin/myquota 6. Perform the following operations: Navigate to your scratch directory. Copy and paste the file: /class/datamine/data/flights/1987.csv to your current directory (scratch). Create a new directory called my_test_dir in your scratch folder. Move the file you copied to your scratch directory, into your new folder. Use touch to create an empty file named im_empty.txt in your scratch folder. Remove the directory my_test_dir and the contents of the directory. Remove the im_empty.txt file. Hint: rmdir may not be able to do what you think, instead, check out the options for rm using man rm. Relevant topics: cd, cp, mv, mkdir, touch, rmdir, rm Item(s) to submit: Command used to navigate to your scratch directory. Command used to copy the file, /class/datamine/data/flights/1987.csv to your current directory (scratch). Command used to create a new directory called my_test_dir in your scratch folder. Command used to move the file you copied earlier 1987.csv into your new my_test_dir folder. Command used to create an empty file named im_empty.txt in your scratch folder. Command used to remove the directory and the contents of the directory my_test_dir. Command used to remove the im_empty.txt file. Solution # navigate to the scratch folder cd /scratch/scholar/$USER # copy the 1987.csv file to the current directory (scratch) cp /class/datamine/data/flights/1987.csv . # make a directory in the scratch directory called `my_test_dir` mkdir my_test_dir # move 1987.csv to the new folder mv 1987.csv my_test_dir # create an empty file in the scratch folder touch im_empty.txt # remove the directory and the contents of the directory rm -r my_test_dir # remove the im_empty.txt file rm im_empty.txt 7. Please include a statement in Project 3 that says, &quot;I acknowledge that the STAT 19000/29000/39000 1-credit Data Mine seminar will be recorded and posted on Piazza, for participants in this course.&quot; or if you disagree with this statement, please consult with us at datamine@purdue.edu for an alternative plan. Project 4 Motivation: The need to search files and datasets based on the text held within is common during various parts of the data wrangling process. grep is an extremely powerful UNIX tool that allows you to do so using regular expressions. Regular expressions are a structured method for searching for specified patterns. Regular expressions can be very complicated, even professionals can make critical mistakes. With that being said, learning some of the basics is an incredible tool that will come in handy regardless of the language you are working in. Context: We've just begun to learn the basics of navigating a file system in UNIX using various terminal commands. Now we will go into more depth with one of the most useful command line tools, grep, and experiment with regular expressions using grep, R, and later on, Python. Scope: grep, regular expression basics, utilizing regular expression tools in R and Python Learning objectives: Use grep to search for patterns within a dataset. Use cut to section off and slice up data from the command line. Use wc to count the number of lines of input. You can find useful examples that walk you through relevant material in The Examples Book: https://thedatamine.github.io/the-examples-book It is highly recommended to read through, search, and explore these examples to help solve problems in this project. Important note: I would highly recommend using single quotes ' to surround your regular expressions. Double quotes can have unexpected behavior due to some shell's expansion rules. In addition, pay close attention to escaping certain characters in your regular expressions. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/movies_and_tv/the_office_dialogue.csv A public sample of the data can be found here: the_office_dialogue.csv Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset. grep stands for (g)lobally search for a (r)egular (e)xpression and (p)rint matching lines. As such, to best demonstrate grep, we will be using it with textual data. You can read about and see examples of grep here. 1. Login to Scholar and use grep to find the dataset we will use this project. The dataset we will use is the only dataset to have the text &quot;Bears. Beets. Battlestar Galactica.&quot;. What is the name of the dataset and where is it located? Relevant topics: grep Item(s) to submit: The grep command used to find the dataset. The name and location in Scholar of the dataset. Use grep and grepl within R to solve a data-driven problem. Solution grep -Ri &quot;bears. beets. battlestar galactica.&quot; /class/datamine /class/datamine/data/the_office/the_office_dialogue.csv 2. grep prints the line that the text you are searching for appears in. In project 3 we learned a UNIX command to quickly print the first n lines from a file. Use this command to get the headers for the dataset. As you can see, each line in the tv show is a row in the dataset. You can count to see which column the various bits of data live in. Write a line of UNIX commands that searches for &quot;bears. beets. battlestar galactica.&quot; and, rather than printing the entire line, prints only the character who speaks the line, as well as the line itself. Hint: The result if you were to search for &quot;bears. beets. battlestar galactica.&quot; should be: &quot;Jim&quot;,&quot;Fact. Bears eat beets. Bears. Beets. Battlestar Galactica.&quot; Hint: One method to solve this problem would be to pipe the output from grep to cut. Relevant topics: cut, grep Item(s) to submit: The line of UNIX commands used to find the character and original dialogue line that contains &quot;bears. beets. battlestar galactica.&quot;. Solution grep -i &quot;bears. beets. battlestar galactica.&quot; /class/datamine/data/movies_and_tv/the_office_dialogue.csv | cut -d &quot;,&quot; -f 7,8 3. Find all of the lines where Pam is called &quot;Beesley&quot; instead of &quot;Pam&quot; or &quot;Pam Beesley&quot;. Hint: A negative lookbehind would be one way to solve this, in order to use a negative lookbehind with grep make sure to add the -P option. In addition, make sure to use single quotes to make sure your regular expression is taken literally. If you use double quotes, variables are expanded. Relevant topics: grep Item(s) to submit: The UNIX command used to solve this problem. Solution grep -Pi &#39;(?&lt;!Pam )Beesley&#39; Regular expressions are really a useful semi language-agnostic tool. What this means is regardless of the programming language your are using, there will be some package that allows you to use regular expressions. In fact, we can use them in both R and Python! This can be particularly useful when dealing with strings. Load up the dataset you discovered in (1) using read.csv. Name the resulting data.frame dat. 4. The text_w_direction column in dat contains the characters' lines with inserted direction that helps characters know what to do as they are reciting the lines. Direction is shown between square brackets &quot;[&quot; &quot;]&quot;. In this two-part question, we are going to use regular expression to detect the directions. (a) Create a new column called has_direction that is set to TRUE if the text_w_direction column has direction, and FALSE otherwise. Use the grepl function in R to accomplish this. Hint: Make sure all opening brackets &quot;[&quot; have a corresponding closing bracket &quot;]&quot;. Hint: Think of the pattern as any line that has a [, followed by any amount of any text, followed by a ], followed by any amount of any text. (b) Modify your regular expression to find lines with 2 or more sets of direction. How many lines have more than 2 directions? Modify your code again and find how many have more than 5. We count the sets of direction in each line by the pairs of square brackets. The following are two simple example sentences. This is a line with [emphasize this] only 1 direction! This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug]. Your solution to part (a) should find both lines a match. However, in part (b) we want the regular expression pattern to find only lines with 2+ directions, so the first line would not be a match. In our actual dataset, for example, dat$text_w_direction[2789] is a line with 2 directions. Relevant topics: grep, grepl, basic matches, escaping characters Item(s) to submit: The R code and regular expression used to solve the first part of this problem. The R code and regular expression used to solve the second part of this problem. How many lines have &gt;= 2 directions? How many lines have &gt;= 5 directions? Solution dat$has_direction &lt;- grepl(&quot;(\\\\[.*\\\\])+&quot;, dat$text_w_direction) # length(grep(&quot;\\\\[.*\\\\].*\\\\[.*\\\\]&quot;, dat$text_w_direction)) length(grep(&quot;\\\\[.*\\\\].*\\\\[.*\\\\].*\\\\[.*\\\\].*\\\\[.*\\\\].*\\\\[.*\\\\]&quot;, dat$text_w_direction)) 5. Use the str_extract_all function from the stringr package to extract the direction(s) as well as the text between direction(s) from each line. Put the strings in a new column called direction. This is a line with [emphasize this] only 1 direction! This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug]. In this question, your solution may have extracted: [emphasize this] [emphasize this] 2 sets of direction, do you see the difference [shrug] (It is okay to keep the text between neighboring pairs of &quot;[&quot; and &quot;]&quot; for the second line.) Relevant topics: str_extract_all, basic matches, escaping characters Item(s) to submit: The R code used to solve this problem. Solution dat$direction_correct &lt;- str_extract_all(dat$text_w_direction, &quot;(\\\\[[^\\\\[\\\\]]*\\\\])&quot;, simplify=F) # or dat$direction_correct &lt;- str_extract_all(dat$text_w_direction, &quot;(\\\\[.*?\\\\])&quot;, simplify=F) OPTIONAL QUESTION. Repeat (5) but this time make sure you only capture the brackets and text within the brackets. Save the results in a new column called direction_correct. You can test to see if it is working by running the following code: dat$direction_correct[747] This is a line with [emphasize this] only 1 direction! This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug]. In (5), your solution may have extracted: [emphasize this] [emphasize this] 2 sets of direction, do you see the difference [shrug] This is ok for (5). In this question, however, we want to fix this to only extract: [emphasize this] [emphasize this] [shrug] Hint: This regular expression will be hard to read. Hint: The pattern we want is: literal opening bracket, followed by 0+ of any character other than the literal [ or literal ], followed by a literal closing bracket. Relevant topics: str_extract_all Item(s) to submit: The R code used to solve this problem. Solution dat$direction_correct &lt;- str_extract_all(dat$text_w_direction, &quot;(\\\\[[^\\\\[\\\\]]*\\\\])&quot;, simplify=F) # or dat$direction_correct &lt;- str_extract_all(dat$text_w_direction, &quot;(\\\\[.*?\\\\])&quot;, simplify=F) Project 5 Motivation: Becoming comfortable stringing together commands and getting used to navigating files in a terminal is important for every data scientist to do. By learning the basics of a few useful tools, you will have the ability to quickly understand and manipulate files in a way which is just not possible using tools like Microsoft Office, Google Sheets, etc. Context: We've been using UNIX tools in a terminal to solve a variety of problems. In this project we will continue to solve problems by combining a variety of tools using a form of redirection called piping. Scope: grep, regular expression basics, UNIX utilities, redirection, piping Learning objectives: Use cut to section off and slice up data from the command line. Use piping to string UNIX commands together. Use sort and it's options to sort data in different ways. Use head to isolate n lines of output. Use wc to summarize the number of lines in a file or in output. Use uniq to filter out non-unique lines. Use grep to search files effectively. You can find useful examples that walk you through relevant material in The Examples Book: https://thedatamine.github.io/the-examples-book It is highly recommended to read through, search, and explore these examples to help solve problems in this project. Don't forget the very useful documentation shortcut ? for R code. To use, simply type ? in the console, followed by the name of the function you are interested in. In the Terminal, you can use the man command to check the documentation of bash code. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/amazon/amazon_fine_food_reviews.csv A public sample of the data can be found here: amazon_fine_food_reviews.csv Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset. Here are three videos that might also be useful, as you work on Project 5: Click here for video Click here for video Click here for video Questions 1. What is the Id of the most helpful review, according to the highest HelpfulnessNumerator? Important note: You can always pipe output to head in case you want the first few values of a lot of output. Note that if you used sort before head, you may see the following error messages: sort: write failed: standard output: Broken pipe sort: write error This is because head would truncate the output from sort. This is okay. See this discussion for more details. Relevant topics: cut, sort, head, piping Item(s) to submit: Line of UNIX commands used to solve the problem. The Id of the most helpful review. Solution cut -d, -f5 amazon_fine_food_reviews.csv| sort -nr | head -n3 2. Some entries under the Summary column appear more than once. Calculate the proportion of unique summaries over the total number of summaries. Use two lines of UNIX commands to find the numerator and the denominator, and manually calculate the proportion. To further clarify what we mean by unique, if we had the following vector in R, c(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;), its unique values are c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;). Relevant topics: cut, uniq, sort, wc, piping Item(s) to submit: Two lines of UNIX commands used to solve the problem. The ratio of unique Summary's. Solution cut -d, -f9 amazon_fine_food_reviews.csv | sort -u | wc -l cut -d, -f9 amazon_fine_food_reviews.csv | wc -l 3. Use a chain of UNIX commands, piped in a sequence, to create a frequency table of Score. Relevant topics: cut, uniq, sort, piping Item(s) to submit: The line of UNIX commands used to solve the problem. The frequency table. Solution cut -d, -f7 amazon_fine_food_reviews.csv | sort -n | uniq -c 4. Who is the user with the highest number of reviews? There are two columns you could use to answer this question, but which column do you think would be most appropriate and why? Hint: You may need to pipe the output to sort multiple times. Hint: To create the frequency table, read through the man pages for uniq. Man pages are the &quot;manual&quot; pages for UNIX commands. You can read through the man pages for uniq by running the following: man uniq Relevant topics: cut, uniq, sort, head, piping, man Item(s) to submit: The line of UNIX commands used to solve the problem. The frequency table. Solution cut -d, -f3 amazon_fine_food_reviews.csv | sort | uniq -c | sort -nr | head -n1 5. Anecdotally, there seems to be a tendency to leave reviews when we feel strongly (either positive or negative) about a product. For the user with the highest number of reviews (i.e., the user identified in question 4), would you say that they follow this pattern of extremes? Let's consider 5 star reviews to be strongly positive and 1 star reviews to be strongly negative. Let's consider anything in between neither strongly positive nor negative. Hint: You may find the solution to problem (3) useful. Relevant topics: cut, uniq, sort, grep, piping Item(s) to submit: The line of UNIX commands used to solve the problem. Solution grep -i &#39;A3OXHLG6DIBRW8&#39; amazon_fine_food_reviews.csv | cut -d, -f7 | sort | uniq -c 6. Find the most helpful review with a Score of 5. Then (separately) find the most helpful review with a Score of 1. As before, we are considering the most helpful review to be the review with the highest HelpfulnessNumerator. Hint: You can use multiple lines to solve this problem. Relevant topics: sort, head, piping Item(s) to submit: The lines of UNIX commands used to solve the problem. ProductId's of both requested reviews. Solution sort -t, -k7rn,7 -k5rn,5 amazon_fine_food_reviews.csv | head -n2 sort -t, -k7n,7 -k5rn,5 amazon_fine_food_reviews.csv | head -n2 7. For only the two ProductIds from the previous question, create a new dataset called scores.csv that contains the ProductIds and Scores from all reviews for these two items. Relevant topics: cut, grep, redirection Item(s) to submit: The line of UNIX commands used to solve the problem. Solution cut -d, -f2,7 amazon_fine_food_reviews.csv| grep -Ei &#39;(B00012182G|B003EMQGVI|B00065LI0A)&#39; &gt; scores.csv OPTIONAL QUESTION. Use R to load up scores.csv into a new data.frame called dat. Create a histogram for each products' Score. Compare the most helpful review Score with the Score's given in the histogram. Based on this comparison, point out some curiosities about the product that may be worth exploring. For example, if a product receives many high scores, but has a super helpful review that gives the product 1 star, I may tend to wonder if the product is not as great as it seems to be. Relevant topics: read.csv, hist Item(s) to submit: R code used to create the histograms. 3 histograms, 1 for each ProductId. 1-2 sentences describing the curious pattern that you would like to further explore. Solution dat &lt;- read.csv(&quot;scores.csv&quot;, header=F) par(mfrow=c(1,3)) tapply(dat$V2, dat$V1, hist) Project 6 Motivation: A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. awk is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks. Context: This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and awk. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. Scope: awk, UNIX utilities, bash scripts Learning objectives: Use awk to process and manipulate textual data. Use piping and redirection within the terminal to pass around data between utilities. Use output created from the terminal to create a plot using R. Dataset The following questions will use the dataset found here or in Scholar: /class/datamine/data/flights/subset/YYYY.csv An example from 1987 data can be found here or in Scholar: /class/datamine/data/flights/subset/1987.csv Questions 1. In previous projects we learned how to get a single column of data from a csv file. Write 1 line of UNIX commands to print the 17th column, the Origin, from 1987.csv. Write another line, this time using awk to do the same thing. Which one do you prefer, and why? Here is an example, from a different data set, to illustrate some differences and similarities between cut and awk: Click here for video Relevant topics: cut, awk Item(s) to submit: One line of UNIX commands to solve the problem without using awk. One line of UNIX commands to solve the problem using awk. 1-2 sentences describing which method you prefer and why. Solution cut -d, -f17 1987.csv awk -F, &#39;{print $17}&#39; 1987.csv 2. Write a bash script that accepts a year (1987, 1988, etc.) and a column n and returns the nth column of the associated year of data. Here are two examples to illustrate how to write a bash script: Click here for video Click here for video Hint: In this example, you only need to turn in the content of your bash script (starting with #!/bin/bash) without evaluation in a code chunk. However, you should test your script before submission to make sure it works. To actually test out your bash script, take the following example. The script is simple and just prints out the first two arguments given to it: #!/bin/bash echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot; If you simply drop that text into a file called my_script.sh, located here: /home/$USER/my_script.sh, and if you run the following: # Setup bash to run; this only needs to be run one time per session. # It makes bash behave a little more naturally in RStudio. exec bash # Navigate to the location of my_script.sh cd /home/$USER # Make sure that the script is runable. # This only needs to be done one time for each new script that you write. chmod 755 my_script.sh # Execute my_script.sh ./my_script.sh okay cool then it will print: First argument: okay Second argument: cool In this example, if we were to turn in the &quot;content of your bash script (starting with #!/bin/bash) in a code chunk, our solution would look like this: #!/bin/bash echo &quot;First argument: $1&quot; echo &quot;Second argument: $2&quot; And although we aren't running the code chunk above, we know that it works because we tested it in the terminal. Hint: Using awk you could have a script with just two lines: 1 with the &quot;hash-bang&quot; (#!/bin/bash), and 1 with a single awk command. Relevant topics: awk, bash scripts Item(s) to submit: The content of your bash script (starting with #!/bin/bash) in a code chunk. Solution #!/bin/bash awk -F, -v col=$2 &#39;{print $col}&#39; $1.csv 3. How many flights arrived at Indianapolis (IND) in 2008? First solve this problem without using awk, then solve this problem using only awk. Here is a similar example, using the election data set: Click here for video Relevant topics: cut, grep, wc, awk, piping Item(s) to submit: One line of UNIX commands to solve the problem without using awk. One line of UNIX commands to solve the problem using awk. The number of flights that arrived at Indianapolis (IND) in 2008. Solution cut -d, -f18 2008.csv | grep &#39;IND&#39; | wc -l awk -F, &#39;{if ($18 == &quot;IND&quot;) count++}END{print count}&#39; 2008.csv 4. Do you expect the number of unique origins and destinations to be the same based on flight data in the year 2008? Find out, using any command line tool you'd like. Are they indeed the same? How many unique values do we have per category (Origin, Dest)? Here is an example to help you with the last part of the question, about Origin-to-Destination pairs. We analyze the city-state pairs from the election data: Click here for video Relevant topics: cut, sort, uniq, wc, awk Item(s) to submit: 1-2 sentences explaining whether or not you expect the number of unique origins and destinations to be the same. The UNIX command(s) used to figure out if the number of unique origins and destinations are the same. The number of unique values per category (Origin, Dest). Solution cut -d, -f17 2008.csv | sort | uniq | wc -l cut -d, -f18 2008.csv | sort | uniq | wc -l 5. In (4) we found that there are not the same number of unique Origin's as Dest's. Find the IATA airport code for all Origin's that don't appear in a Dest and all Dest's that don't appear in an Origin in the 2008 data. Hint: The examples on this page should help. Note that these examples are based on Process Substitution, which basically allows you to specify commands whose output would be used as the input of comm. There should be no space between &lt; and (, otherwise your bash will not work as intended. Relevant topics: comm, cut, sort, uniq, redirection Item(s) to submit: The line(s) of UNIX command(s) used to answer the question. The list of Origins that don't appear in Dest. The list of Dests that don't appear in Origin. Solution comm -23 &lt;(cut -d, -f17 2008.csv | sort | uniq) &lt;(cut -d, -f18 2008.csv | sort | uniq) comm -13 &lt;(cut -d, -f17 2008.csv | sort | uniq) &lt;(cut -d, -f18 2008.csv | sort | uniq) 6. What was the percentage of flights in 2008 per unique Origin with the Dest of &quot;IND&quot;? What percentage of flights had &quot;PHX&quot; as Origin (among all flights with Dest of &quot;IND&quot;? Here is an example using the percentages of donations contributed from CEOs from various States: Click here for video Hint: You can do the mean calculation in awk by dividing the result from (3) by the number of unique Origin's that have a Dest of &quot;IND&quot;. Relevant topics: awk, sort, grep, wc Item(s) to submit: The percentage of flights in 2008 per unique Origin with the Dest of &quot;IND&quot;. 1-2 sentences explaining how &quot;PHX&quot; compares (as a unique ORIGIN) to the other Origins (all with the Dest of &quot;IND&quot;)? Solution awk -F, &#39;{if($18==&quot;IND&quot;) print $17}&#39; 2008.csv | sort -u | wc -l awk -F, &#39;{if($18==&quot;IND&quot;) print $17}&#39; 2008.csv | grep -i PHX | wc -l 7. Write a bash script that takes a year and IATA airport code and returns the year, and the total number of flights to and from the given airport. Example rows may look like: 1987, 12345 1988, 44 Run the script with inputs: 1991 and ORD. Include the output in your submission. Relevant topics: bash scripts, cut, piping, grep, wc Item(s) to submit: The content of your bash script (starting with &quot;#!/bin/bash&quot;) in a code chunk. The output of the script given 1991 and ORD as inputs. Solution #!/bin/bash FLIGHTS_OUT=&quot;$(cut -d, -f17 $1.csv | grep -i $2 | wc -l)&quot; FLIGHTS_IN=&quot;$(cut -d, -f18 $1.csv | grep -i $2 | wc -l)&quot; echo &quot;$((FLIGHTS_OUT + FLIGHTS_IN)), $1&quot; OPTIONAL QUESTION. Pick your favorite airport and get its IATA airport code. Write a bash script that, given the first year, last year, and airport code, runs the bash script from (7) for all years in the provided range for your given airport, or loops through all of the files for the given airport, appending all of the data to a new file called my_airport.csv. Relevant topics: bash scripts, cut, grep, wc, for loops, echo, redirection Item(s) to submit: The content of your bash script (starting with &quot;#!/bin/bash&quot;) in a code chunk. Solution #!/bin/bash for ((f=$1; f&lt;=$2; f++)); do FLIGHTS_OUT=&quot;$(cut -d, -f17 $f.csv | grep -i $3 | wc -l)&quot; FLIGHTS_IN=&quot;$(cut -d, -f18 $f.csv | grep -i $3 | wc -l)&quot; echo &quot;$((FLIGHTS_OUT + FLIGHTS_IN)), $f&quot; done OPTIONAL QUESTION. In R, load my_airport.csv and create a line plot showing the year-by-year change. Label your x-axis &quot;Year&quot;, your y-axis &quot;Num Flights&quot;, and your title the name of the IATA airport code. Write 1-2 sentences with your observations. Relevant topics: read.csv, lines Item(s) to submit: Line chart showing year-by-year change in flights into and out of the chosen airport. R code used to create the chart. 1-2 sentences with your observations. Solution dat &lt;- read.csv(&quot;my_airport.csv&quot;, header=F) plot(dat[,2], dat[,1],xlab=&quot;Year&quot;, ylab=&quot;Num Flights&quot;, main=&quot;JFK&quot;, type=&quot;l&quot;) Project 7 Motivation: A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. awk is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks. Context: This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and awk. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. Scope: awk, UNIX utilities, bash scripts Learning objectives: Use awk to process and manipulate textual data. Use piping and redirection within the terminal to pass around data between utilities. Dataset: The following questions will use the dataset found in Scholar: /class/datamine/data/flights/subset/YYYY.csv An example of the data for the year 1987 can be found here. Sometimes if you are about to dig into a dataset, it is good to quickly do some sanity checks early on to make sure the data is what you expect it to be. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. Write a line of code that prints a list of the unique values in the DayOfWeek column. Write a line of code that prints a list of the unique values in the DayOfMonth column. Write a line of code that prints a list of the unique values in the Month column. Use the 1987.csv dataset. Are the results what you expected? Relevant topics: cut, sort Item(s) to submit: 3 lines of code used to get a list of unique values for the chosen columns. 1-2 sentences explaining whether or not the results are what you expected. Solution cut -d, -f3 1987.csv | sort -nu cut -d, -f4 1987.csv | sort -nu cut -d, -f2 1987.csv | sort -nu 2. Our files should have 29 columns. For a given file, write a line of code that prints any lines that do not have 29 columns. Test it on 1987.csv, were there any rows without 29 columns? Hint: See here. NF looks like it may be useful! Relevant topics: awk Item(s) to submit: Line of code used to solve the problem. 1-2 sentences explaining whether or not there were any rows without 29 columns. Solution awk -F, &#39;{if (NF != 29) print $0}&#39; 1987.csv 3. Write a bash script that, given a &quot;begin&quot; year and &quot;end&quot; year, cycles through the associated files and prints any lines that do not have 29 columns. Relevant topics: awk, bash scripts Item(s) to submit: The content of your bash script (starting with &quot;#!/bin/bash&quot;) in a code chunk. The results of running your bash scripts from year 1987 to 2008. Solution #!/bin/bash for ((f=$1; f&lt;=$2; f++)); do awk -F, &#39;{if (NF != 29) print $0}&#39; $f.csv done 4.awk is a really good tool to quickly get some data and manipulate it a little bit. The column Distance contains the distances of the flights in miles. Use awk to calculate the total distance traveled by the flights in 1990, and show the results in both miles and kilometers. To convert from miles to kilometers, simply multiply by 1.609344. Example output: Miles: 12345 Kilometers: 19867.35168 Relevant topics: awk, piping Item(s) to submit: The code used to solve the problem. The results of running the code. Solution awk -F, &#39;{miles=miles+$19}END{print &quot;Miles: &quot; miles, &quot;\\nKilometers:&quot; miles*1.609344}&#39; 1990.csv 5. Use awk to calculate the sum of the number of DepDelay minutes, grouped according to DayOfWeek. Use 2007.csv. Example output: DayOfWeek: 0 1: 1234567 2: 1234567 3: 1234567 4: 1234567 5: 1234567 6: 1234567 7: 1234567 Note: 1 is Monday. Relevant topics: awk, sort, piping Item(s) to submit: The code used to solve the problem. The output from running the code. Solution awk -F, &#39;{delay[$4]=delay[$4]+$16}END{for (d in delay) print d&quot;: &quot;, delay[d]}&#39; 2007.csv | sort -n 6. It wouldn't be fair to compare the total DepDelay minutes by DayOfWeek as the number of flights may vary. One way to take this into account is to instead calculate an average. Modify (5) to calculate the average number of DepDelay minutes by the number of flights per DayOfWeek. Use 2007.csv. Example output: DayOfWeek: 0 1: 1.234567 2: 1.234567 3: 1.234567 4: 1.234567 5: 1.234567 6: 1.234567 7: 1.234567 Relevant topics: awk, sort, piping Item(s) to submit: The code used to solve the problem. The output from running the code. Solution awk -F, &#39;{delay[$4]=delay[$4]+$16; flights[$4]++}END{for (d in delay) print d&quot;: &quot;, delay[d]/flights[d]}&#39; 2007.csv | sort -n 7. Anyone who has flown knows how frustrating it can be waiting for takeoff, or deboarding the aircraft. These roughly translate to TaxiOut and TaxiIn respectively. If you were to fly into or out of IND what is your expected total taxi time? Use 2007.csv. Note: Taxi times are in minutes. Relevant topics: awk, grep Item(s) to submit: The code used to solve the problem. The output from running the code. Solution grep -i IND 2007.csv | awk -F, &#39;{taxi=taxi+$20+$21; count++}END{print taxi/count}&#39; Project 8 Motivation: A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. awk is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks. Context: This is the last part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and awk. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. Scope: awk, UNIX utilities, bash scripts Learning objectives: Use awk to process and manipulate textual data. Use piping and redirection within the terminal to pass around data between utilities. Dataset: The following questions will use the dataset found in Scholar: /class/datamine/data/flights/subset/YYYY.csv An example of the data for the year 1987 can be found here. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. 1. Let's say we have a theory that there are more flights on the weekend days (Friday, Saturday, Sunday) than the rest of the days, on average. We can use awk to quickly check it out and see if maybe this looks like something that is true! Write a line of awk code that, prints the total number of flights that occur on weekend days, followed by the total number of flights that occur on the weekdays. Complete this calculation for 2008 using the 2008.csv file. Modify your code to instead print the average number of flights that occur on weekend days, followed by the average number of flights that occur on the weekdays. Hint: You don't need a large if statement to do this, you can use the ~ comparison operator. Relevant topics: awk Item(s) to submit: Lines of awk code that solves the problem. The result: the number of flights on the weekend days, followed by the number of flights on the weekdays for the flights during 2008. The result: the average number of flights on the weekend days, followed by the average number of flights on the weekdays for the flights during 2008. Solution awk -F, &#39;{if ($4 ~ /1|2|3|4/) weekday++; else (weekend++)}END{print weekend,weekday}&#39; 2008.csv awk -F, &#39;{if ($4 ~ /1|2|3|4/) weekday++; else (weekend++)}END{print weekend/3,weekday/4}&#39; 2008.csv 2. We want to look to see if there may be some truth to the whole &quot;snow bird&quot; concept where people will travel to warmer states like Florida and Arizona during the Winter. Let's use the tools we've learned to explore this a little bit. Take a look at airports.csv. In particular run the following: head airports.csv Notice how all of the non-numeric text is surrounded by quotes. The surrounding quotes would need to be escaped for any comparison within awk. This is messy and we would prefer to create a new file called new_airports.csv without any quotes. Write a line of code to do this. Note: You may be wondering why we are asking you to do this. This sort of situation (where you need to deal with quotes) happens a lot! It's important to practice and learn ways to fix these things. Hint: You could use gsub within awk to replace '&quot;' with ''. You can find how to use gsub here. Hint: If you leave out the column number argument to gsub it will apply the substitution to every field in every column. Hint: cat new_airports.csv | wc -l # should be 159 without header Relevant topics: awk, redirection Item(s) to submit: Line of awk code used to create the new dataset. Solution awk -F, &#39;{gsub(/&quot;/, &quot;&quot;); print $0}&#39; airports.csv &gt; new_airports.csv 3. Write a line of commands that creates a new dataset called az_fl_airports.txt. az_fl_airports.txt should only contain a list of airport codes for all airports from both Arizona (AZ) and Florida (FL). Use the file we created in (3),new_airports.csv as a starting point. How many airports are there? Did you expect this? Use a line of bash code to count this. Create a new dataset (called az_fl_flights.txt) that contains all of the data for flights into or out of Florida and Arizona (using the 2008.csv file). Use the newly created dataset, az_fl_airports.txt to accomplish this. Hint: https://unix.stackexchange.com/questions/293684/basic-grep-awk-help-extracting-all-lines-containing-a-list-of-terms-from-one-f Hint: cat az_fl_flights.txt | wc -l # should be 484705 Relevant topics: awk, wc, piping Item(s) to submit: All UNIX commands used to answer the questions. The number of airports. 1-2 sentences explaining whether you expected this number of airports. Solution awk -F, &#39;{if ($4 == &quot;AZ&quot; || $4 == &quot;FL&quot;) print $1}&#39; new_airports.csv &gt; az_fl_airports.txt wc -l az_fl_airports.txt 4. Write a bash script that accepts the start year, end year, and filename containing airport codes (az_fl_airports.txt), and outputs the data for flights into or out of any of the airports listed in the provided filename (az_fl_airports.txt). The script should output data for flights using all of the years of data in the provided range. Run the bash script to create a new file called az_fl_flights_total.csv. Relevant topics: bash scripts, grep, for loop, redirection Item(s) to submit: The content of your bash script (starting with &quot;#!/bin/bash&quot;) in a code chunk. The line of UNIX code you used to execute the script and create the new dataset. Solution #!/bin/bash for ((f=$1; f&lt;=$2; f++)); do grep -w -F -f $3 $f.csv done ./my_script.sh 1987 2008 az_fl_airports.txt &gt; az_fl_flights_total.csv 5. Use the newly created dataset, az_fl_flights_total.csv, from question 4 to calculate the total number of flights into and out of both states by month, and by year, for a total of 3 columns (year, month, flights). Export this information to a new file called snowbirds.csv. Load up your newly created dataset and use either R or Python (or some other tool) to create a graphic that illustrates whether or not we believe the &quot;snowbird effect&quot; effects flights. Include a description of your graph, as well as your (anecdotal) conclusion. Hint: You can use 1 dimensional arrays to accomplish this if the key is the combination of, for example, the year and month. Relevant topics: awk, redirection Item(s) to submit: The line of awk code used to create the new dataset, snowbirds.csv. Code used to create the visualization in a code chunk. The generated plot as either a png or jpg/jpeg. 1-2 sentences describing your plot and your conclusion. Solution awk -F, &#39;{M[$1&quot;,&quot;$2]++}END{for (monthyear in M) print monthyear&quot;,&quot;M[monthyear]}&#39; az_fl_flights_total.csv &gt; snowbirds.csv library(ggplot2) dat &lt;- read.csv(&quot;snowbirds.csv&quot;, header=F) names(dat) &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;n_flights&quot;) ggplot(dat) + geom_line(aes(x=month, y=n_flights, group=year, col = as.factor(year))) + geom_point(aes(x=month, y=n_flights)) + theme_classic(base_size=14) + scale_x_continuous(breaks=1:12, labels = month.abb) + labs(x= &#39;Month&#39;, y=&#39;&#39;, title = &#39;Total flights per month in/out AZ and FL (1987-2008)&#39;, col=&#39;Year&#39;) Project 9 Motivation: Structured Query Language (SQL) is a language used for querying and manipulating data in a database. SQL can handle much larger amounts of data than R and Python can alone. SQL is incredibly powerful. In fact, cloudflare, a billion dollar company, had much of its starting infrastructure built on top of a Postgresql database (per this thread on hackernews). Learning SQL is well worth your time! Context: There are a multitude of RDBMSs (relational database management systems). Among the most popular are: MySQL, MariaDB, Postgresql, and SQLite. As we've spent much of this semester in the terminal, we will start in the terminal using SQLite. Scope: SQL, sqlite Learning objectives: Explain the advantages and disadvantages of using a database over a tool like a spreadsheet. Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause. Basic clauses: select, order by, limit, desc, asc, count, where, from, etc. Dataset: The following questions will use the dataset found in Scholar: /class/datamine/data/lahman/lahman.db This is the Lahman Baseball Database. You can find its documentation here, including the definitions of the tables and columns. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. Important note: For this project all solutions should be done using SQL code chunks. To connect to the database, copy and paste the following before your solutions in your .Rmd: ```{r, include=F} library(RSQLite) lahman &lt;- dbConnect(RSQLite::SQLite(), &quot;/class/datamine/data/lahman/lahman.db&quot;) ``` Each solution should then be placed in a code chunk like this: ```{sql, connection=lahman} SELECT * FROM batting LIMIT 1; ``` If you want to use a SQLite-specific function like .tables (or prefer to test things in the Terminal), you will need to use the Terminal to connect to the database and run queries. To do so, you can connect to RStudio Server at https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal. In the terminal execute the command: sqlite3 /class/datamine/data/lahman/lahman.db From there, the SQLite-specific commands will function properly. They will not function properly in an SQL code chunk. To display the SQLite-specific commands in a code chunk without running the code, use a code chunk with the option eval=F like this: ```{sql, connection=lahman, eval=F} SELECT * FROM batting LIMIT 1; ``` This will allow the code to be displayed without throwing an error. 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal and access the Lahman database. How many tables are available? Hint: To connect to the database, do the following: sqlite3 /class/datamine/data/lahman/lahman.db Hint: This is a good resource. Relevant topics: sqlite3 Item(s) to submit: How many tables are available in the Lahman database? The sqlite3 commands used to figure out how many tables are available. Solution .tables 2. Some people like to try to visit all 30 MLB ballparks in their lifetime. Use SQL commands to get a list of parks and the cities they're located in. For your final answer, limit the output to 10 records/rows. Note: There may be more than 30 parks in your result, this is ok. For long results, you can limit the number of printed results using the LIMIT clause. Hint: Make sure you take a look at the column names and get familiar with the data tables. If working from the Terminal, to see the header row as a part of each query result, run the following: .headers on Relevant topics: SELECT, FROM, LIMIT Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT parkname, city FROM parks LIMIT 10; 3. There is nothing more exciting to witness than a home run hit by a batter. It's impressive if a player hits more than 40 in a season. Find the hitters who have hit 60 or more home runs (HR) in a season. List their playerID, yearID, home run total, and the teamID they played for. Hint: There are 8 occurrences of home runs greater than or equal to 60. Hint: The batting table is where you should look for this question. Relevant topics: SELECT, FROM, WHERE, LIMIT Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT teamID, yearID, teamID, HR FROM batting WHERE HR&gt;=60; 4. Make a list of players born on your birth day (don't worry about the year). Display their first names, last names, and birth year. Order the list descending by their birth year. Hint: The people table is where you should look for this question. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT nameFirst, nameLast, birthYear FROM people WHERE birthMonth==5 AND birthDay==29 ORDER BY birthYear DESC LIMIT 10; 5. Get the Cleveland (CLE) Pitching Roster from the 2016 season (playerID, W, L, SO). Order the pitchers by number of Strikeouts (SO) in descending order. Hint: The pitching table is where you should look for this question. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT playerID, W, L, SO FROM pitching WHERE teamID==&#39;CLE&#39; AND yearID==2016 ORDER BY SO DESC LIMIT 10; 6. Find the 10 team and year pairs that have the most number of Errors (E) between 1960 and 1970. Display their Win and Loss counts too. What is the name of the team that appears in 3rd place in the ranking of the team and year pairs? Hint: The teams table is where you should look for this question. Hint: The BETWEEN clause is useful here. Hint: It is OK to use multiple queries to answer the question. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT teamID, franchID, yearID, W, L, E FROM teams WHERE yearID BETWEEN 1960 AND 1970 ORDER BY E DESC LIMIT 10; SELECT franchName FROM teamsfranchises WHERE franchID==&#39;LAA&#39;; SELECT teamID, franchID, yearID, W, L, E FROM teams WHERE yearID &gt;= 1960 AND yearID &lt;= 1970 ORDER BY E DESC LIMIT 10; SELECT franchName FROM teamsfranchises WHERE franchID==&#39;LAA&#39;; 7. Find the playerID for Bob Lemon. What year and team was he on when he got the most wins as a pitcher (use table pitching)? What year and team did he win the most games as a manager (use table managers)? Hint: It is OK to use multiple queries to answer the question. Note: There was a tie among the two years in which Bob Lemon had the most wins as a pitcher. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT playerID FROM people WHERE nameFirst==&#39;Bob&#39; AND nameLast==&#39;Lemon&#39;; SELECT teamID, yearID, W FROM pitching WHERE playerID==&#39;lemonbo01&#39; ORDER BY W DESC; SELECT teamID, yearID, W FROM managers WHERE playerID==&#39;lemonbo01&#39; ORDER BY W DESC; 8. Find the AL West (use lgID and divID to specify AL West) home run (HR), walk (BB), and stolen base (SB) totals by team between 2000 and 2010. Which team and year combo led in each category in the decade? Hint: The teams table is where you should look for this question. Hint: It is OK to use multiple queries to answer the question. Hint: Use divID == 'W' as one of the conditions. Please note using double quotes: divID == &quot;W&quot; will not work. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The team-year combination that ranked top in each category. Solution SELECT teamID, yearID, HR, BB, SB FROM teams WHERE lgID==&#39;AL&#39; AND divID==&#39;W&#39; AND yearID BETWEEN 2000 AND 2010 ORDER BY HR DESC LIMIT 1; SELECT teamID, yearID, HR, BB, SB FROM teams WHERE lgID==&#39;AL&#39; AND divID==&#39;W&#39; AND yearID BETWEEN 2000 AND 2010 ORDER BY BB DESC LIMIT 1; SELECT teamID, yearID, HR, BB, SB FROM teams WHERE lgID==&#39;AL&#39; AND divID==&#39;W&#39; AND yearID BETWEEN 2000 AND 2010 ORDER BY SB DESC LIMIT 1; 9. Get a list of the following by year: wins (W), losses (L), Home Runs Hit (HR), homeruns allowed (HRA), and total home game attendance (attendance) for the Detroit Tigers when winning a World Series (WSWin is Y) or when winning league champion (LgWin is Y). Hint: The teams table is where you should look for this question. Hint: Be careful with the order of operations for AND and OR. Remember you can force order of operations using parentheses. Relevant topics: SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN Note: Examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL code used to solve the problem. The first 10 results of the query. Solution SELECT teamID, yearID, W, L, HR, HRA, attendance FROM teams WHERE teamID==&#39;DET&#39; AND (WSWin==&#39;Y&#39; OR LgWin==&#39;Y&#39;); Project 10 Motivation: Although SQL syntax may still feel unnatural and foreign, with more practice it will start to make more sense. The ability to read and write SQL queries is a bread-and-butter skill for anyone working with data. Context: We are in the second of a series of projects that focus on learning the basics of SQL. In this project, we will continue to harden our understanding of SQL syntax, and introduce common SQL functions like AVG, MIN, and MAX. Scope: SQL, sqlite Learning objectives: Explain the advantages and disadvantages of using a database over a tool like a spreadsheet. Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause. Basic clauses: select, order by, limit, desc, asc, count, where, from, etc. Utilize SQL functions like min, max, avg, sum, and count to solve data-driven problems. Dataset The following questions will use the dataset similar to the one from Project 9, but this time we will use a MariaDB version of the database, which is also hosted on Scholar, at scholar-db.rcac.purdue.edu. As in Project 9, this is the Lahman Baseball Database. You can find its documentation here, including the definitions of the tables and columns. Questions Important note: Please make sure to double check that the your submission does indeed contain the files you think it does. You can do this by downloading your submission from Gradescope after uploading. If you can see all of your files and they open up properly on your computer, you should be good to go. Important note: Please make sure to look at your knit PDF before submitting. PDFs should be relatively short and not contain huge amounts of printed data. Remember you can use functions like head to print a sample of the data or output. Extremely large PDFs will be subject to lose points. Important note: For this project all solutions should be done using R code chunks, and the RMariaDB package. Run the following code to load the library: library(RMariaDB) 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and, rather than navigating to the terminal like we did in the previous project, instead, create a connection to our MariaDB lahman database using the RMariaDB package in R, and the credentials below. Confirm the connection by running the following code chunk: con &lt;- dbConnect(RMariaDB::MariaDB(), host=&quot;scholar-db.rcac.purdue.edu&quot;, db=&quot;lahmandb&quot;, user=&quot;lahman_user&quot;, password=&quot;HitAH0merun&quot;) head(dbGetQuery(con, &quot;SHOW tables;&quot;)) Hint: In the example provided, the variable con from the dbConnect function is the connection. Each query that you make, using the dbGetQuery, needs to use this connection con. You can change the name con if you want to (it is user defined), but if you change the name con, you need to change it on all of your connections. If your connection to the database dies while you are working on the project, you can always re-run the dbConnect line again, to reset your connection to the database. Relevant topics: RMariaDB, dbConnect, dbGetQuery Item(s) to submit: R code used to solve the problem. Output from running your (potentially modified) head(dbGetQuery(con, &quot;SHOW tables;&quot;)). Solution library(RMariaDB) host &lt;- &quot;scholar-db.rcac.purdue.edu&quot; dbname &lt;- &quot;lahmandb&quot; user &lt;- &quot;lahman_user&quot; password &lt;- &quot;HitAH0merun&quot; con &lt;- dbConnect(MariaDB(),host=host, dbname=dbname, user=user, password=password) head(dbGetQuery(con, &quot;SHOW tables;&quot;)) 2. How many players are members of the 40/40 club? These are players that have stolen at least 40 bases (SB) and hit at least 40 home runs (HR) in one year. Hint: Use the batting table. Important note: You only need to run library(RMariaDB) and the dbConnect portion of the code a single time towards the top of your project. After that, you can simply reuse your connection con to run queries. Important note: In our project template, for this project, make all of the SQL queries using the dbGetQuery function, which returns the results directly in R. Therefore, your RMarkdown blocks for this project should all be {r} blocks (as opposed to the {sql} blocks used in Project 9). Hint: You can use dbGetQuery to run your queries from within R. Example: dbGetQuery(con, &quot;SELECT * FROM battings LIMIT 5;&quot;) Note: We already demonstrated the correct SQL query to use for the 40/40 club in this video, but now we want you to use RMariaDB to solve this query: Click here for video Relevant topics: dbGetQuery, AND/OR, DISTINCT, COUNT Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con,&quot;SELECT DISTINCT COUNT(*) FROM batting WHERE HR&gt;=40 AND SB&gt;=40;&quot;) 3. How many times in total has Giancarlo Stanton struck out in years in which he played for &quot;MIA&quot; or &quot;FLO&quot;? Click here for video Important note: Questions in this project need to be solved using SQL when possible. You will not receive credit for a question if you use sum in R rather than SUM in SQL. Hint: Use the people table to find the playerID and use the batting table to find the statistics. Relevant topics: dbGetQuery, AND/OR, SUM Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con, &quot;SELECT playerID FROM people WHERE nameFirst =&#39;Giancarlo&#39; AND nameLast = &#39;Stanton&#39;;&quot;) dbGetQuery(con, &quot;SELECT COUNT(*), SUM(SO) FROM batting WHERE playerID = &#39;stantmi03&#39; AND (teamID = &#39;MIA&#39; OR teamID = &#39;FLO&#39;);&quot;) 4. The Batting Average is a metric for a batter's performance. The Batting Average in a year is calculated by \\(\\frac{H}{AB}\\) (the number of hits divided by at-bats). Considering (only) the years between 2000 and 2010, calculate the (seasonal) Batting Average for each batter who had more than 300 at-bats in a season. List the top 5 batting averages next to playerID, teamID, and yearID. Hint: Use the batting table. Relevant topics: dbGetQuery, ORDER BY, BETWEEN Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con,&quot;SELECT playerID, teamID, yearID, H, AB, H/AB FROM batting WHERE yearID BETWEEN 2000 AND 2010 AND AB&gt;300 ORDER BY H/AB DESC LIMIT 5;&quot;) 5. How many unique players have hit &gt; 50 home runs (HR) in a season? Hint: If you view DISTINCT as being paired with SELECT, instead, think of it as being paired with one of the fields you are selecting. Relevant topics: dbGetQuery, DISTINCT, COUNT Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con, &quot;SELECT COUNT(DISTINCT playerID) FROM batting WHERE HR&gt;50;&quot;) # As you can see, DISTINCT is with the field playerID, not with SELECT. # The following query should help distinguish this. dbGetQuery(con, &quot;SELECT COUNT(DISTINCT playerID), COUNT(playerID) FROM batting WHERE HR&gt;50;&quot;) 6. Find the number of unique players that attended Purdue University. Start by finding the schoolID for Purdue and then find the number of players who played there. Do the same for IU. Who had more? Purdue or IU? Use the information you have in the database, and the power of R to create a misleading graphic that makes Purdue look better than IU, even if just at first glance. Make sure you label the graphic. Hint: Use the schools table to get the schoolIDs, and the collegeplaying table to get the statistics. Hint: You can mess with the scale of the y-axis. You could (potentially) filter the data to start from a certain year or be between two dates. Hint: To find IU's id, try the following query: SELECT schoolID FROM schools WHERE name_full LIKE '%indiana%';. You can find more about the LIKE clause and % here. Relevant topics: dbGetQuery, plotting in R, DISTINCT, COUNT Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution dbGetQuery(con, &quot;SELECT schoolID FROM schools WHERE name_full=&#39;Purdue University&#39;;&quot;) dbGetQuery(con, &quot;SELECT schoolID FROM schools WHERE LIKE &#39;%indiana%&#39;;&quot;) dbGetQuery(con,&quot;SELECT COUNT(DISTINCT playerID) FROM collegeplaying WHERE schoolID =&#39;purdue&#39;;&quot;) dbGetQuery(con,&quot;SELECT DISTINCT playerID FROM collegeplaying WHERE schoolID =&#39;indiana&#39;;&quot;) purdue &lt;- length(unique(dbGetQuery(con,&quot;SELECT playerID FROM collegeplaying WHERE schoolID =&#39;purdue&#39;;&quot;))$playerID) iu &lt;- length(unique(dbGetQuery(con,&quot;SELECT playerID FROM collegeplaying WHERE schoolID =&#39;indiana&#39;;&quot;))$playerID) barplot(purdue, iu) barplot(log(c(purdue, iu)), main=&quot;Pro baseball players: Indiana vs. Purdue&quot;, col=c(&quot;#990000&quot;,&quot;#CFB53B&quot;), names.arg=c(&quot;Purdue&quot;, &quot;IU&quot;), cex.names=.5, ylab=&quot;Log count&quot;) 7. Use R, SQL and the lahman database to create an interesting infographic. For those of you who are not baseball fans, try doing a Google image search for &quot;baseball plots&quot; for inspiration. Make sure the plot is polished, has appropriate labels, color, etc. Relevant topics: SQL, plotting in R Item(s) to submit: R code used to solve the problem. The result of running the R code. Solution # Could be anything. Project 11 Motivation: Being able to use results of queries as tables in new queries (also known as writing sub-queries), and calculating values like MIN, MAX, and AVG in aggregate are key skills to have in order to write more complex queries. In this project we will learn about aliasing, writing sub-queries, and calculating aggregate values. Context: We are in the middle of a series of projects focused on working with databases and SQL. In this project we introduce aliasing, sub-queries, and calculating aggregate values using a much larger dataset! Scope: SQL, SQL in R Learning objectives: Demonstrate the ability to interact with popular database management systems within R. Solve data-driven problems using a combination of SQL and R. Basic clauses: SELECT, ORDER BY, LIMIT, DESC, ASC, COUNT, WHERE, FROM, etc. Showcase the ability to filter, alias, and write subqueries. Perform grouping and aggregate data using group by and the following functions: COUNT, MAX, SUM, AVG, LIKE, HAVING. Explain when to use having, and when to use where. Dataset The following questions will use the elections database. Similar to Project 10, this database is hosted on Scholar. Moreover, Question 1 also involves the following data files found in Scholar: /class/datamine/data/election/itcontYYYY.txt (for example, data for year 1980 would be /class/datamine/data/election/itcont1980.txt) A public sample of the data can be found here: https://www.datadepot.rcac.purdue.edu/datamine/data/election/itcontYYYY.txt (for example, data for year 1980 would be https://www.datadepot.rcac.purdue.edu/datamine/data/election/itcont1980.txt) Questions Important note: For this project you will need to connect to the database elections using the RMariaDB package in R. Include the following code chunk in the beginning of your RMarkdown file: ```{r setup-database-connection} library(RMariaDB) con &lt;- dbConnect(RMariaDB::MariaDB(), host=&quot;scholar-db.rcac.purdue.edu&quot;, db=&quot;elections&quot;, user=&quot;elections_user&quot;, password=&quot;Dataelect!98&quot;) ``` When a question involves SQL queries in this project, you may use a SQL code chunk (with {sql}), or an R code chunk (with {r}) and functions like dbGetQuery as you did in Project 10. Please refer to Question 5 in the project template for examples. 1. Approximately how large was the lahman database (use the sqlite database in Scholar: /class/datamine/data/lahman/lahman.db)? Use UNIX utilities you've learned about this semester to write a line of code to return the size of that .db file (in MB). The data we consider in this project are much larger. Use UNIX utilities (bash and awk) to write another line of code that calculates the total amount of data in the elections folder /class/datamine/data/election/. How much data (in MB) is there? The data in that folder has been added to the elections database, all aggregated in the elections table. Write a SQL query that returns the number of rows of data are in the database. How many rows of data are in the table elections? Note: These are some examples of how to get the sizes of collections of files in UNIX. Click here for video Hint: The SQL query will take some time! Be patient. Note: You may use more than one code chunk in your RMarkdown file for the different tasks. Note: We will accept values that represent either apparent or allocated size, as well as estimated disk usage. To get the size from ls and du to match, use the --apparent-size option with du. Note: A Megabyte (MB) is actually 1000^2 bytes, not 1024^2. A Mebibyte (MiB) is 1024^2 bytes. See here for more information. For this question, either solution will be given full credit. This is a potentially useful example. Relevant topics: SQL, SQL in R, awk, ls, du Item(s) to submit: Line of code (bash/awk) to show the size (in MB) of the lahman database file. Approximate size of the lahman database in MB. Line of code (bash/awk) to calculate the size (in MB) of the entire elections dataset in /class/datamine/data/election. The size of the elections data in MB. SQL query used to find the number of rows of data in the elections table in the elections database. The number of rows in the elections table in the elections database. Solution ls -la *.txt | awk &#39;{ total += $4; }END{print total/1000000}&#39; SELECT COUNT(*) FROM elections; 2. Write a SQL query using the LIKE command to find a unique list of zip_codes that start with &quot;479&quot;. Write another SQL query and answer: How many unique zip_codes are there that begin with &quot;479&quot;? Note: Here are some examples about SQL that might be relevant for Questions 2 and 3 in this project. Click here for video Hint: The first query returns a list of zip codes, and the second returns a count. Hint: Make sure you only select zip_codes. Relevant topics: SQL, LIKE Item(s) to submit: SQL queries used to answer the question. The first 5 results from running the query. Solution SELECT DISTINCT zip_code FROM elections WHERE zip_code LIKE &#39;479%&#39; LIMIT 5; SELECT COUNT(DISTINCT zip_code) FROM elections WHERE zip_code LIKE &#39;479%&#39;; 3. Write a SQL query that counts the number of donations (rows) that are from Indiana. How many donations are from Indiana? Rewrite the query and create an alias for our field so it doesn't read COUNT(*) but rather Indiana Donations. Hint: You may enclose an alias's name in quotation marks (single or double) when the name contains space. Relevant topics: SQL, WHERE, aliasing Item(s) to submit: SQL query used to answer the question. The result of the SQL query. Solution SELECT COUNT(*) FROM elections WHERE state=&#39;IN&#39;; SELECT COUNT(*) AS &#39;Indiana Donations&#39; FROM elections WHERE state=&#39;IN&#39;; 4. Rewrite the query in (3) so the result is displayed like: IN: 1234567. Note, if instead of &quot;IN&quot; we wanted &quot;OH&quot;, only the WHERE clause should be modified, and the display should automatically change to OH: 1234567. In other words, the state abbreviation should be dynamic, not static. Note: This video demonstrates how to use CONCAT in a MySQL query. Click here for video Hint: Use CONCAT and aliasing to accomplish this. Hint: Remember, state contains the state abbreviation. Relevant topics: SQL, aliasing, CONCAT Item(s) to submit: SQL query used to answer the question. Solution SELECT CONCAT(state, &quot;: &quot;, COUNT(*)) AS &#39;Donations&#39; FROM elections WHERE state=&#39;IN&#39;; 5. In (2) we wrote a query that returns a unique list of zip_codes that start with &quot;479&quot;. In (3) we wrote a query that counts the number of donations that are from Indiana. Use our query from (2) as a sub-query to find how many donations come from areas with zip_codes starting with &quot;479&quot;. What percent of donations in Indiana come from said zip_codes? Note: This video gives two examples of sub-queries. Click here for video Hint: You can simply manually calculate the percent using the count in (2) and (5). Relevant topics: SQL, aliasing, subqueries, IN Item(s) to submit: SQL queries used to answer the question. The percentage of donations from Indiana from zip_codes starting with &quot;479&quot;. Solution SELECT COUNT(*) FROM elections WHERE zip_code IN (SELECT DISTINCT zip_code FROM elections WHERE zip_code LIKE &#39;479%&#39;); 6. In (3) we wrote a query that counts the number of donations that are from Indiana. When running queries like this, a natural &quot;next question&quot; is to ask the same question about another state. SQL gives us the ability to calculate functions in aggregate when grouping by a certain column. Write a SQL query that returns the state, number of donations from each state, the sum of the donations (transaction_amt). Which 5 states gave the most donations (highest count)? Order you result from most to least. Note: In this video we demonstrate GROUP BY, ORDER BY, DESC, and other aspects of MySQL that might help with this question. Click here for video Hint: You may want to create an alias in order to sort. Relevant topics: SQL, GROUP BY Item(s) to submit: SQL query used to answer the question. Which 5 states gave the most donations? Solution SELECT state, COUNT(*) AS &#39;cnt&#39;, SUM(transaction_amt) AS &#39;total&#39; FROM elections GROUP BY state ORDER BY cnt DESC; 7. Write a query that gets the number of donations, and sum of donations, by year, for Indiana. Create one or more graphics that highlights the year-by-year changes. Write a short 1-2 sentences explaining your graphic(s). Relevant topics: SQL in R, GROUP, date functions in SQL Item(s) to submit: SQL query used to answer the question. R code used to create your graphic(s). 1 or more graphics in png/jpeg format. 1-2 sentences summarizing your graphic(s). Solution library(RMariaDB) library(ggplot2) library(tidyverse) db &lt;- dbConnect(RMariaDB::MariaDB(), host=&quot;scholar-db.rcac.purdue.edu&quot;, db=&quot;elections&quot;, user=&quot;elections_user&quot;, password=&quot;Dataelect!98&quot;) dat &lt;- dbGetQuery(db, &quot;SELECT COUNT(*) as cnt, SUM(transaction_amt) as amt, YEAR(transaction_dt) as yr FROM elections WHERE state=&#39;IN&#39; GROUP BY yr ORDER BY yr;&quot;) filtered_dat = filter(dat, between(yr, 1978, 2020)) ggplot(filtered_dat, aes(x=yr,y=amt/1e6)) + geom_line() + geom_point() + theme_bw() + labs(x=&#39;Year&#39;, y=&#39;Amount in millions&#39;) ggplot(filtered_dat, aes(x=yr,y=amt/(1e3*cnt))) + geom_line() + geom_point() + theme_bw() + labs(x=&#39;Year&#39;, y=&#39;Average amount in thousands&#39;) ggplot(filtered_dat, aes(x=yr,y=as.numeric(cnt)/1e3)) + geom_line() + geom_point() + theme_bw() + labs(x=&#39;Year&#39;, y=&#39;Number of donations in thousands&#39;) Project 12 Motivation: Databases are comprised of many tables. It is imperative that we learn how to combine data from multiple tables using queries. To do so we perform joins! In this project we will explore learn about and practice using joins on a database containing bike trip information from the Bay Area Bike Share. Context: We've introduced a variety of SQL commands that let you filter and extract information from a database in an systematic way. In this project we will introduce joins, a powerful method to combine data from different tables. Scope: SQL, sqlite, joins Learning objectives: Briefly explain the differences between left and inner join and demonstrate the ability to use the join statements to solve a data-driven problem. Perform grouping and aggregate data using group by and the following functions: COUNT, MAX, SUM, AVG, LIKE, HAVING. Showcase the ability to filter, alias, and write subqueries. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/bay_area_bike_share/bay_area_bike_share.db A public sample of the data can be found here. Important note: For this project all solutions should be done using SQL code chunks. To connect to the database, copy and paste the following before your solutions in your .Rmd: ```{r, include=F} library(RSQLite) bikeshare &lt;- dbConnect(RSQLite::SQLite(), &quot;/class/datamine/data/bay_area_bike_share/bay_area_bike_share.db&quot;) ``` Each solution should then be placed in a code chunk like this: ```{sql, connection=bikeshare} SELECT * FROM station LIMIT 5; ``` If you want to use a SQLite-specific function like .tables (or prefer to test things in the Terminal), you will need to use the Terminal to connect to the database and run queries. To do so, you can connect to RStudio Server at https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal. In the terminal execute the command: sqlite3 /class/datamine/data/bay_area_bike_share/bay_area_bike_share.db From there, the SQLite-specific commands will function properly. They will not function properly in an SQL code chunk. To display the SQLite-specific commands in a code chunk without running the code, use a code chunk with the option eval=F like this: ```{sql, connection=bikeshare, eval=F} SELECT * FROM station LIMIT 5; ``` This will allow the code to be displayed without throwing an error. There are a variety of ways to join data using SQL. With that being said, if you are able to understand and use a LEFT JOIN and INNER JOIN, you can perform all of the other types of joins (RIGHT JOIN, FULL OUTER JOIN). You can see the documentation here: SQL, INNER JOIN, LEFT JOIN Questions 1. Aliases can be created for tables, fields, and even results of aggregate functions (like MIN, MAX, COUNT, AVG, etc.). In addition, you can combine fields using the sqlite concatenate operator || (see here). Write a query that returns the first 5 records of information from the station table formatted in the following way: (id) name @ (lat, long) For example: (84) Ryland Park @ (37.342725, -121.895617) Hint: Here is a video about how to concatenate strings in SQLite. Click here for video Relevant topics: aliasing, concat Item(s) to submit: SQL query used to solve this problem. The first 5 records of information from the station table. Solution SELECT &#39;(&#39;||id||&#39;) &#39;||name||&#39; @ (&#39;||lat||&#39;,&#39;||long||&#39;)&#39; FROM station LIMIT 5; 2. There is a variety of interesting weather information in the weather table. Write a query that finds the average mean_temperature_f by zip_code. Which is on average the warmest zip_code? Use aliases to format the result in the following way: Zip Code|Avg Temperature 94041|61.3808219178082 Note that this is the output if you use sqlite in the terminal. While the output in your knitted pdf file may look different, you should name the columns accordingly. Hint: Here is a video about GROUP BY, ORDER BY, DISTINCT, and COUNT Click here for video Relevant topics: aliasing, GROUP BY, AVG Item(s) to submit: SQL query used to solve this problem. The results of the query copy and pasted. Solution SELECT zip_code AS &#39;Zip Code&#39;, AVG(mean_temperature_f) AS &#39;Avg Temperature&#39; FROM weather GROUP BY zip_code; 3. From (2) we can see that there are only 5 zip_codes with weather information. How many unique zip_codes do we have in the trip table? Write a query that finds the number of unique zip_codes in the trip table. Write another query that lists the zip_code and count of the number of times the zip_code appears. If we had originally assumed that the zip_code was related to the location of the trip itself, we were wrong. Can you think of a likely explanation for the unexpected zip_code values in the trip table? Hint: There could be missing values in zip_code. We want to avoid them in SQL queries, for now. You can learn more about the missing values (or NULL) in SQL here. Relevant topics: GROUP BY, COUNT Item(s) to submit: SQL queries used to solve this problem. 1-2 sentences explainging what a possible explanation for the zip_codes could be. Solution SELECT COUNT(DISTINCT zip_code) FROM trip; SELECT zip_code, COUNT(zip_code) FROM trip GROUP BY zip_code; 4. In (2) we wrote a query that finds the average mean_temperature_f by zip_code. What if we want to tack on our results in (2) to information from each row in the station table based on the zip_codes? To do this, use an INNER JOIN. INNER JOIN combines tables based on specified fields, and returns only rows where there is a match in both the &quot;left&quot; and &quot;right&quot; tables. Hint: Use the query from (2) as a sub query within your solution. Hint: Here is a video about JOIN and LEFT JOIN. Click here for video Relevant topics: INNER JOIN, subqueries, aliasing Item(s) to submit: SQL query used to solve this problem. Solution SELECT * FROM station as s INNER JOIN (SELECT zip_code AS &#39;Zip Code&#39;, AVG(mean_temperature_f) AS &#39;Avg Temperature&#39; FROM weather GROUP BY zip_code) AS sub ON s.zip_code=sub.&#39;Zip Code&#39;; 5. In (3) we alluded to the fact that the zip_codes in the trip table aren't very consistent. Users can enter a zip code when using the app. This means that zip_code can be from anywhere in the world! With that being said, if the zip_code is one of the 5 zip_codes for which we have weather data (from question 2), we can add that weather information to matching rows of the trip table. In (4) we used an INNER JOIN to append some weather information to each row in the station table. For this question, write a query that performs an INNER JOIN and appends weather data from the weather table to the trip data from the trip table. Limit your output to 5 lines. Important note: Notice that the weather data has about 1 row of weather information for each date and each zip code. This means you may have to join your data based on multiple constraints instead of just 1 like in (4). In the trip table, you can use start_date for for the date information. Hint: You will want to wrap your dates and datetimes in sqlite's date function prior to comparison. Relevant topics: INNER JOIN, aliasing Item(s) to submit: SQL query used to solve this problem. First 5 lines of output. Solution SELECT * FROM trip AS t INNER JOIN weather AS w ON t.zip_code=w.zip_code AND date(t.start_date)=date(w.date) LIMIT 5; 6. How many rows are in the result from (5) (when not limiting to 5 lines)? How many rows are in the trip table? As you can see, a large proportion of the data from the trip table did not match the data from the weather table, and therefore was removed from the result. What if we want to keep all of the data from the trip table and add on data from the weather table if we have a match? Write a query to accomplish this. How many rows are in the result? Item(s) to submit: SQL query used to find how many rows from the result in (5). The number of rows in the result of (5). SQL query to find how many rows are in the trip table. The number of rows in the trip table. SQL query to keep all of the data from the trip table and add on matching data from the weather table when available. The number of rows in the result. Solution SELECT COUNT(*) FROM trip AS t INNER JOIN weather AS w ON t.zip_code=w.zip_code AND date(t.start_date)=date(w.date); SELECT COUNT(*) FROM trip; SELECT * FROM trip AS t LEFT JOIN weather AS w ON t.zip_code=w.zip_code AND date(t.start_date)=date(w.date) LIMIT 5; SELECT COUNT(*) FROM trip AS t LEFT JOIN weather AS w ON t.zip_code=w.zip_code AND date(t.start_date)=date(w.date); Project 13 Motivation: Databases you will work with won't necessarily come organized in the way that you like. Getting really comfortable writing longer queries where you have to perform many joins, alias fields and tables, and aggregate results, is important. In addition, gaining some familiarity with terms like primary key, and foreign key will prove useful when you need to search for help online. In this project we will write some more complicated queries with a fun database. Proper preparation prevents poor performance, and that means practice! Context: We are towards the end of a series of projects that give you an opportunity to practice using SQL. In this project, we will reinforce topics you've already learned, with a focus on subqueries and joins. Scope: SQL, sqlite Learning objectives: Write and run SQL queries in sqlite on real-world data. Identify primary and foreign keys in a SQL table. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/movies_and_tv/imdb.db Important note: For this project you will use SQLite to access the data. To connect to the database, copy and paste the following before your solutions in your .Rmd: ```{r, include=F} library(RSQLite) imdb &lt;- dbConnect(RSQLite::SQLite(), &quot;/class/datamine/data/movies_and_tv/imdb.db&quot;) ``` If you want to use a SQLite-specific function like .tables (or prefer to test things in the Terminal), you will need to use the Terminal to connect to the database and run queries. To do so, you can connect to RStudio Server at https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal. In the terminal execute the command: sqlite3 /class/datamine/data/movies_and_tv/imdb.db From there, the SQLite-specific commands will function properly. They will not function properly in an SQL code chunk. To display the SQLite-specific commands in a code chunk without running the code, use a code chunk with the option eval=F like this: ```{sql, connection=imdb, eval=F} SELECT * FROM titles LIMIT 5; ``` This will allow the code to be displayed without throwing an error. Questions 1. A primary key is a field in a table which uniquely identifies a row in the table. Primary keys must be unique values, and this is enforced at the database level. A foreign key is a field whose value matches a primary key in a different table. A table can have 0-1 primary key, but it can have 0+ foreign keys. Examine the titles table. Do you think there are any primary keys? How about foreign keys? Now examine the episodes table. Based on observation and the column names, do you think there are any primary keys? How about foreign keys? Hint: A primary key can also be a foreign key. Hint: Here are two videos. The first video will remind you how to find the names of all of the tables in the imdb database. The second video will introduce you to the titles and episodes tables in the imdb database. Click here for video Click here for video Relevant topics: primary key, foreign key Item(s) to submit: List any primary or foreign keys in the titles table. List any primary or foreign keys in the episodes table. 2. If you paste a title_id to the end of the following url, it will pull up the page for the title. For example, https://www.imdb.com/title/tt0413573 leads to the page for the TV series Grey's Anatomy. Write a SQL query to confirm that the title_id tt0413573 does indeed belong to Grey's Anatomy. Then browse imdb.com and find your favorite TV show. Get the title_id from the url of your favorite TV show and run the following query, to confirm that the TV show is in our database: SELECT * FROM titles WHERE title_id=&#39;&lt;title id here&gt;&#39;; Make sure to replace &quot;&lt;title id here&gt;&quot; with the title_id of your favorite show. If your show does not appear, or has only a single season, pick another show until you find one we have in our database with multiple seasons. Relevant topics: SELECT, WHERE Item(s) to submit: SQL query used to confirm that title_id tt0413573 does indeed belong to Grey's Anatomy. The output of the query. The title_id of your favorite TV show. SQL query used to confirm the title_id for your favorite TV show. The output of the query. 3. The episode_title_id column in the episodes table references titles of individual episodes of a TV series. The show_title_id references the titles of the show itself. With that in mind, write a query that gets a list of all of the episodes_title_ids (found in the episodes table), with the associated primary_title (found in the titles table) for each episode of Grey's Anatomy. Relevant topics: INNER JOIN Hint: This video shows how to extract titles of episodes in the imdb database. Click here for video Item(s) to submit: SQL query used to solve the problem in a code chunk. 4. We want to write a query that returns the title and rating of the highest rated episode of your favorite TV show, which you chose in (2). In order to do so, we will break the task into two parts in (4) and (5). First, write a query that returns a list of episode_title_ids (found in the episodes table), with the associated primary_title (found in the titles table) for each episode. Hint: This part is just like question (3) but this time with your favorite TV show, which you chose in (2). Hint: This video shows how to use a subquery, to JOIN a total of three tables in the imdb database. Click here for video Relevant topics: INNER JOIN, aliasing Item(s) to submit: SQL query used to solve the problem in a code chunk. The first 5 results from your query. 5. Write a query that adds the rating to the end of each episode. To do so, use the query you wrote in (4) as a subquery. Which episode has the highest rating? Is it also your favorite episode? Relevant topics: INNER JOIN, aliasing, subqueries, ORDER BY, DESC, LIMIT Note: Various helpful examples that utilize the relevant topics in this problem can be found here. Item(s) to submit: SQL query used to solve the problem in a code chunk. The episode_title_id, primary_title, and rating of the top rated episode from your favorite TV series, in question (2). A statement saying whether the highest rated episode is also your favorite episode. 6. Write a query that returns the season_number (from the episodes table), and average rating (from the ratings table) for each season of your favorite TV show from (2). Write another query that only returns the season number and rating for the highest rated season. Consider the highest rated season the season with the highest average. Relevant topics: INNER JOIN, aliasing, GROUP BY, AVG Item(s) to submit: The 2 SQL queries used to solve the problems in two code chunks. 7. Write a query that returns the primary_title and rating of the highest rated episode per season for your favorite TV show from question (2). Note: You can show one highest rated episode for each season, without the need to worry about ties. Relevant topics: MAX, subqueries, GROUP BY, INNER JOIN, aliasing Item(s) to submit: The SQL query used to solve the problem. The output from your query. Project 14 Motivation: As we learned earlier in the semester, bash scripts are a powerful tool when you need to perform repeated tasks in a UNIX-like system. In addition, sometimes preprocessing data using UNIX tools prior to analysis in R or Python is useful. Ample practice is integral to becoming proficient with these tools. As such, we will be reviewing topics learned earlier in the semester. Context: We've just ended a series of projects focused on SQL. In this project we will begin to review topics learned throughout the semester, starting writing bash scripts using the various UNIX tools we learned about in Projects 3 through 8. Scope: awk, UNIX utilities, bash scripts, fread Learning objectives: Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc. Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc. Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc. Use grep to search files effectively. Use cut to section off data from the command line. Use piping to string UNIX commands together. Use awk for data extraction, and preprocessing. Create bash scripts to automate a process or processes. Dataset: The following questions will use PLOTSNAP.csv and TREE.csv from the data folder found in Scholar: /class/datamine/data/forest To read more about the two files from this dataset that you will be working with: PLOTSNAP.csv: https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/plot-level-data-gathered-through-forest/metadata#fields TREE.csv: https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/tree-level-data-gathered-through-forest/metadata AND https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/tree-level-data-gathered-through-forest/data Questions 1. Take a look at at PLOTSNAP.csv. Write a line of awk code that displays the STATECD followed by the number of rows with that STATECD. Relevant topics: awk Item(s) to submit: Code used to solve the problem. Count of the following STATECDs: 1, 2, 4, 5, 6 2. Unfortunately, there isn't a very accessible list available that shows which state each STATECD represents. This is no problem for us though, the dataset has LAT and LON! Write some bash that prints just the STATECD, LAT, and LON. Note: There are 92 columns in our dataset: awk -F, 'NR==1{print NF}' PLOTSNAP.csv. To create a list of STATECD to state, we only really need STATECD, LAT, and LON. Keeping the other 89 variables will keep our data at 2.1gb. Relevant topics: cut, awk Item(s) to submit: Code used to solve the problem. The output of your code piped to head. 3. fread is a &quot;Fast and Friendly File Finagler&quot;. It is part of the very popular data.table package in R. We will learn more about this package next semester. For now, read the documentation here and use the cmd argument in conjunction with your bash code from (2) to read the data of STATECD, LAT, and LON into a data.table in your R environment. Relevant topics: fread Item(s) to submit: Code used to solve the problem. The head of the resulting data.table. 4. We are going to further understand the data from question (3) by finding the actual locations based on the LAT and LON columns. We can use the library revgeo to get a location given a pair of longitude and latitude values. revgeo uses a free API hosted by photon in order to do so. For example: library(revgeo) revgeo(longitude=-86.926153, latitude=40.427055, output=&#39;frame&#39;) The code above will give you the address information in six columns, from the most-granular housenumber to the least-granular country. Depending on the coordinates, revgeo may or may not give you results for each column. For this question, we are going to keep only the state column. There are over 4 million rows in our dataset -- we do not want to hit photon's API that many times. Instead, we are going to do the following: First: Unless you feel comfortable using data.table, convert your data.table to a data.frame: my_dataframe &lt;- data.frame(my_datatable) Second: Calculate the average LAT and LON for each STATECD, and call the new data.frame, dat. This should result in 57 rows of lat/long pairs. Third: For each row in dat, run a reverse geocode and append the state to a new column called STATE. Hint: To calculate the average LAT and LON for each STATECD, you could use the sqldf package to run SQL queries on your data.frame. Hint: mapply is a useful apply function to use to solve this problem. Hint: Here is some extra help: library(revgeo) points &lt;- data.frame(latitude=c(40.433663, 40.432104, 40.428486), longitude=c(-86.916584, -86.919610, -86.920866)) # Note that the &quot;output&quot; argument gets passed to the &quot;revgeo&quot; function. mapply(revgeo, points$longitude, points$latitude, output=&quot;frame&quot;) # The output isn&#39;t in a great format, and we&#39;d prefer to just get the &quot;state&quot; data. # Let&#39;s wrap &quot;revgeo&quot; into another function that just gets &quot;state&quot; and try again. get_state &lt;- function(lon, lat) { return(revgeo(lon, lat, output=&quot;frame&quot;)[&quot;state&quot;]) } mapply(get_state, points$longitude, points$latitude) Important note: It is okay to get &quot;Not Found&quot; for some of the addresses. Relevant topics: apply functions, sqldf Item(s) to submit: Code used to solve the problem. The head of the resulting data.frame. 5. Use the leaflet, addTiles, and addCircles functions from the leaflet package to map our average latitude and longitude data from question (4) to a map (should be a total of 57 lat/long pairs). Hint: See here for an example of adding points to a map. Relevant topics: leaflet Item(s) to submit: Code used to create the map. The map itself as output from running the code chunk. 6. Write a bash script that accepts at least 1 argument, and performs a useful task using at least 1 dataset from the forest folder in /class/datamine/data/forest. An example of a useful task could be printing a report of summary statistics for the data. Feel free to get creative. Note that tasks must be non-trivial -- a bash script that counts the number of lines in a file is not appropriate. Make sure to properly document (via comments) what your bash script does. Also ensure that your script returns columnar data with appropriate separating characters (for example a csv). Relevant topics: bash scripts, awk, unix utilities Item(s) to submit: The content of your bash script starting from #!/bin/bash. Example output from running your script as intended. A description of what your script does. 7. You used fread in question (2). Now use the cmd argument in conjunction with your script from (6) to read the script output into a data.table in your R environment. Relevant topics: fread Item(s) to submit: The R code used to read in and preprocess your data using your bash script from (6). The head of the resulting data.table. Project 15 Motivation: We've done a lot of work with SQL this semester. Let's review concepts in this project and mix and match R and SQL to solve data-driven problems. Context: In this project, we will reinforce topics you've already learned, with a focus on SQL. Scope: SQL, sqlite, R Learning objectives: Write and run SQL queries in sqlite on real-world data. Use SQL from within R. Dataset The following questions will use the dataset found in Scholar: /class/datamine/data/movies_and_tv/imdb.db F.R.I.E.N.D.S is a popular tv show. They have an interesting naming convention for the names of their episodes. They all begin with the text &quot;The One ...&quot;. There are 6 primary characters in the show: Chandler, Joey, Monica, Phoebe, Rachel, and Ross. Let's use SQL and R to take a look at how many times each characters' names appear in the title of the episodes. Questions 1. Write a query that gets the episode_title_id, primary_title, rating, and votes, of all of the episodes of Friends (title_id is tt0108778). Hint: You can slightly modify the solution to question (5) in project 13. Relevant topics: INNER JOIN, subqueries, aliasing Item(s) to submit: SQL query used to answer the question. First 5 results of the query. 2. Now that you have a working query, connect to the database and run the query to get the data into an R data frame. In previous projects, we learned how to used regular expressions to search for text. For each character, how many episodes primary_titles contained their name? Relevant topics: SQL in R, grep Item(s) to submit: R code in a code chunk that was used to find the solution. The solution pasted below the code chunk. 3. Create a graphic showing our results in (2) using your favorite package. Make sure the plot has a good title, x-label, y-label, and try to incorporate some of the following colors: #273c8b, #bd253a, #016f7c, #f56934, #016c5a, #9055b1, #eaab37. Relevant topics: plotting Item(s) to submit: The R code used to generate the graphic. The graphic in a png or jpg/jpeg format. 4. Now we will turn our focus to other information in the database. Use a combination of SQL and R to find which of the following 3 genres has the highest average rating for movies (see type column from titles table): Romance, Comedy, Animation. In the titles table, you can find the genres in the genres column. There may be some overlap (i.e. a movie may have more than one genre), this is ok. To query rows which have the genre Action as one of its genres: SELECT * FROM titles WHERE genres LIKE &#39;%action%&#39;; Relevant topics: LIKE, INNER JOIN Item(s) to submit: Any code you used to solve the problem in a code chunk. The average rating of each of the genres listed for movies. 5. Write a function called top_episode in R which accepts the path to the imdb.db database, as well as the title_id of a tv series (for example, &quot;tt0108778&quot; or &quot;tt1266020&quot;), and returns the season_number, episode_number, primary_title, and rating of the highest rated episode in the series. Test it out on some of your favorite series, and share the results. Relevant topics: functions, INNER JOIN, ORDER BY Item(s) to submit: Any code you used to solve the problem in a code chunk. The results for at least 3 of your favorite tv series. "],
["corporate-partners.html", "Corporate Partners", " Corporate Partners "],
["contributors.html", "Contributors", " Contributors We are extremely thankful for all of our contributors! Get your name added to the list by making a contribution. "]
]
