{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT29000 Project 6 Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are dealing with two primary libraries: `requests` and `beautifulsoup4`. `requests` is an HTTP library that has utilities for downloading data from the web. `beautifulsoup4` is more or less a package that helps us extract the data we actually want, from the files (html, xml, json) we download from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to those libraries, we will be using `pandas` and by extension, `numpy` (`pandas` is built on `numpy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the links to the official documentation for `requests` and `beautifulsoup4`:\n",
    "- [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#)\n",
    "- [requests](https://requests.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some links where you can read more about how to use these two libraries:\n",
    "- https://realpython.com/python-web-scraping-practical-introduction/\n",
    "- https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last quick note is that it will be immensely useful to use your browser to peek at the structure of a website's HTML in order to figure out patterns. Typically, you can do so by right clicking on a page and clicking on: \"view page source\", or \"inspect element\". The former shows the entire web page, and the latter tries to show you the HTML responsible for where you right-clicked on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" xmlns=\"http://www.w3.org/1999/xhtml\" prefix=\"fb: http://ww\n",
      "<!DOCTYPE html>\n",
      "<html dir=\"ltr\" lang=\"en\" prefix=\"fb: http://www.facebook.com/2008/fbml og: http://o\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at rotten tomatoes at https://www.rottentomatoes.com/\n",
    "# Let's write a function that returns a list of tuples where the first \n",
    "# element in each tuple is a movie name, and the second is the box office $$.\n",
    "# You can see that that info is freely available and updated on the home page.\n",
    "\n",
    "# Before we write the function lets walk through some steps.\n",
    "\n",
    "# Download the html.\n",
    "html = requests.get('https://www.rottentomatoes.com/')\n",
    "print(html) # 200 means success!\n",
    "\n",
    "# Show it as html (the first 100 characters of the text).\n",
    "print(html.text[:100])\n",
    "\n",
    "# Okay, now that we have the html\n",
    "# let's feed it to beautiful soup.\n",
    "soup = bsoup(html.text)\n",
    "\n",
    "# Show it as html (the first 100 characters of the text).\n",
    "# The method prettify() displays the text in a nicer way \n",
    "# (e.g., showing each tag in one line).\n",
    "print(soup.prettify()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this snippet of HTML brings up a good. Jupyter notebooks provide you with a way to properly display and run HTML without getting an error. We will demonstrate below using the `%%html` magic command. You can read more about this command, and others [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-da97b316b31c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-da97b316b31c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    <html>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# In a normal code cell, including html, like the html below, will cause an error.\n",
    "<html>\n",
    "    <body>\n",
    "        <div class=\"content\">\n",
    "            <h1>My HTML page!</h1>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will use the magic `%%html` command, and everything will work just as we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "    <body>\n",
       "        <div style=\"text-align: center;\">\n",
       "            <h1>My HTML page!</h1>\n",
       "        </div>\n",
       "    </body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<html>\n",
    "    <body>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h1>My HTML page!</h1>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<section class=\"media-lists__category-section media-lists__top-box-office\" id=\"top-box-office\">\n",
      "<div class=\"media-lists__main-section-header-group\">\n",
      "<h2 class=\"h3 h--neusa\">Top Box Office</h2>\n",
      "<a href=\"/showtimes/\">Get Tickets</a>\n",
      "</div>\n",
      "<table class=\"media-lists__table table\">\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/sonic_the_hedgehog_2020\">\n",
      "<span class=\"icon icon--tiny icon__fresh\"></span>\n",
      "<span>63%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/sonic_the_hedgehog_2020\">Sonic the Hedgehog</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/sonic_the_hedgehog_2020\">$58M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/birds_of_prey_2020\">\n",
      "<span class=\"icon icon--tiny icon__certified_fresh\"></span>\n",
      "<span>78%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/birds_of_prey_2020\">Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn)</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/birds_of_prey_2020\">$17.3M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/fantasy_island_2020\">\n",
      "<span class=\"icon icon--tiny icon__rotten\"></span>\n",
      "<span>9%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/fantasy_island_2020\">Fantasy Island</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/fantasy_island_2020\">$12.3M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/the_photograph_2020\">\n",
      "<span class=\"icon icon--tiny icon__fresh\"></span>\n",
      "<span>74%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/the_photograph_2020\">The Photograph</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/the_photograph_2020\">$12.3M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/bad_boys_for_life\">\n",
      "<span class=\"icon icon--tiny icon__certified_fresh\"></span>\n",
      "<span>77%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/bad_boys_for_life\">Bad Boys for Life</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/bad_boys_for_life\">$11.6M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/1917_2019\">\n",
      "<span class=\"icon icon--tiny icon__certified_fresh\"></span>\n",
      "<span>89%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/1917_2019\">1917</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/1917_2019\">$8.2M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/parasite_2019\">\n",
      "<span class=\"icon icon--tiny icon__certified_fresh\"></span>\n",
      "<span>99%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/parasite_2019\">Parasite (Gisaengchung)</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/parasite_2019\">$5.8M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/jumanji_the_next_level\">\n",
      "<span class=\"icon icon--tiny icon__fresh\"></span>\n",
      "<span>71%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/jumanji_the_next_level\">Jumanji: The Next Level</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/jumanji_the_next_level\">$5.7M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/dolittle\">\n",
      "<span class=\"icon icon--tiny icon__rotten\"></span>\n",
      "<span>14%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/dolittle\">Dolittle</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/dolittle\">$5M</a>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td class=\"media-lists__td-rating\">\n",
      "<a href=\"/m/downhill_2020\">\n",
      "<span class=\"icon icon--tiny icon__rotten\"></span>\n",
      "<span>39%</span>\n",
      "</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-title\">\n",
      "<a href=\"/m/downhill_2020\">Downhill</a>\n",
      "</td>\n",
      "<td class=\"media-lists__td-date\">\n",
      "<a href=\"/m/downhill_2020\">$4.7M</a>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "<div class=\"home__view-all-group\">\n",
      "<a href=\"/browse/in-theaters/\">View All</a>\n",
      "</div>\n",
      "</section>]\n",
      "1\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# At this point, I have everything downloaded and ready to parse through.\n",
    "# My next step is to look at the website and figure out where the info I want \n",
    "# is at in the HTML.\n",
    "\n",
    "# Start by right clicking on the \"Top Box Office\" title and inspecting the element.\n",
    "# If you look closely, you can see that the entirety of that \"box\" of information\n",
    "# is within: <section id=\"top-box-office\" class=\"media-lists__category-section media-lists__top-box-office\"> </div>\n",
    "\n",
    "# Let's break this down. \"section\" is an HTML element. Every element has a start and end \"tag\".\n",
    "# In this case, <section id=\"top-box-office\" class=\"...\"> is the start tag, and </div> \n",
    "# is the end tag. \"id\" and \"class\" are called \"attributes\". Often times attributes are\n",
    "# given logical meaning and patterns by the individual who made the webpage. In web scraping, attributes \n",
    "# are useful because we can use them to isolate parts of the html.\n",
    "\n",
    "# In this case, it looks like the \"id\" attribute may be unique! Let's see what we can do.\n",
    "box_office_html = soup.find_all(id=\"top-box-office\")\n",
    "print(box_office_html) # The output html contents (in texts) is relatively long!\n",
    "\n",
    "# Excellent! It worked! We captured everything in between the \"div\" tags, and you can see \n",
    "# that we were correct in thinking that the \"id\" attribute was unique!\n",
    "print(len(box_office_html))\n",
    "\n",
    "# If it were not unique, box_office_html would have a len > 1. For example:\n",
    "print(len(soup.find_all(class_=\"icon\")))\n",
    "\n",
    "# Note that because \"class\" is a special python keyword, in order to search for \n",
    "# an HTML attribute called \"class\", we specify \"class_\" in the find_all method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "6\n",
      "[<span class=\"icon icon--tiny icon__certified_fresh\"></span>, <span class=\"icon icon--tiny icon__certified_fresh\"></span>, <span class=\"icon icon--tiny icon__certified_fresh\"></span>, <span class=\"icon icon--tiny icon__certified_fresh\"></span>, <span class=\"icon icon--tiny icon__certified_fresh\"></span>, <span class=\"icon icon--tiny icon__certified_fresh\"></span>]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "<section class=\"media-lists__category-section media-lists__top-box-office\" id=\"t\n"
     ]
    }
   ],
   "source": [
    "# Another important note: An attribute may have more than one value.\n",
    "# For example: <div class=\"icon icon--tiny icon__certified_fresh\"></div>\n",
    "# In this instance, the \"class\" attribute has 3 values: icon, icon--tiny, & icon__certified_fresh.\n",
    "\n",
    "# These would all find our example span tag given above:\n",
    "# (the output is omitted)\n",
    "soup.find_all(class_=\"icon\")\n",
    "soup.find_all(class_=\"icon--tiny\")\n",
    "soup.find_all(class_=\"icon__certified_fresh\")\n",
    "\n",
    "# To see how many tags we find:\n",
    "print(len(soup.find_all(class_=\"icon\")))\n",
    "print(len(soup.find_all(class_=\"icon--tiny\")))\n",
    "print(len(soup.find_all(class_=\"icon__certified_fresh\")))\n",
    "\n",
    "# But if you want to search on multiple values, order matters:\n",
    "print(soup.find_all(class_=\"icon icon--tiny icon__certified_fresh\")) # works\n",
    "print(soup.find_all(class_=\"icon icon--tiny\")) # doesn't work, not complete\n",
    "print(soup.find_all(class_=\"icon--tiny icon icon__certified_fresh\")) # doesn't work\n",
    "\n",
    "# If you'd like, you can also search on multiple attributes:\n",
    "our_attributes_dict = {\"class\": \"media-lists__top-box-office\", \"id\": \"top-box-office\"}\n",
    "another_box_office_html = soup.find_all(attrs=our_attributes_dict) \n",
    "print(len(another_box_office_html))\n",
    "print(another_box_office_html[0].prettify()[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$58M', '$17.3M', '$12.3M', '$12.3M', '$11.6M', '$8.2M', '$5.8M', '$5.7M', '$5M', '$4.7M']\n",
      "['Sonic the Hedgehog', 'Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn)', 'Fantasy Island', 'The Photograph', 'Bad Boys for Life', '1917', 'Parasite (Gisaengchung)', 'Jumanji: The Next Level', 'Dolittle', 'Downhill']\n",
      "['/m/sonic_the_hedgehog_2020', '/m/birds_of_prey_2020', '/m/fantasy_island_2020', '/m/the_photograph_2020', '/m/bad_boys_for_life', '/m/1917_2019', '/m/parasite_2019', '/m/jumanji_the_next_level', '/m/dolittle', '/m/downhill_2020']\n"
     ]
    }
   ],
   "source": [
    "# Back to the topic. We have box_office_html, and we are looking to extract a list of movie names and \n",
    "# a list of $$ values.\n",
    "\n",
    "# Now we need to look for patterns in the structure that may let us separate the data.\n",
    "# For instance, it looks like the $$ values are in \"a\" tags that are in \"td\" tags with \n",
    "# the class attribute is \"media-lists__td-date\".\n",
    "\n",
    "# We will first obtain the \"td\" tags in a list\n",
    "money = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-date\")\n",
    "\n",
    "# Now lets get the \"a\" tags from each of the \"td\" tags\n",
    "money_list = [td.find(\"a\").text for td in money]\n",
    "\n",
    "# or\n",
    "money_list = [td.a.text for td in money]\n",
    "print(money_list)\n",
    "\n",
    "# Excellent.\n",
    "\n",
    "# Now for the movie names (and let's also add the id's)\n",
    "names = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-title\")\n",
    "names = [name.a.text for name in names]\n",
    "print(names)\n",
    "\n",
    "ids = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-title\")\n",
    "\n",
    "# note that to get the attribute value itself, you can access it like a \n",
    "# dict\n",
    "ids = [i.a['href'] for i in ids] \n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sonic the Hedgehog', 'Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn)', 'Fantasy Island', 'The Photograph', 'Bad Boys for Life', '1917', 'Parasite (Gisaengchung)', 'Jumanji: The Next Level', 'Dolittle', 'Downhill'] ['$58M', '$17.3M', '$12.3M', '$12.3M', '$11.6M', '$8.2M', '$5.8M', '$5.7M', '$5M', '$4.7M'] ['/m/sonic_the_hedgehog_2020', '/m/birds_of_prey_2020', '/m/fantasy_island_2020', '/m/the_photograph_2020', '/m/bad_boys_for_life', '/m/1917_2019', '/m/parasite_2019', '/m/jumanji_the_next_level', '/m/dolittle', '/m/downhill_2020']\n"
     ]
    }
   ],
   "source": [
    "# Great. Let's put this together in a function\n",
    "from typing import Tuple\n",
    "\n",
    "# The -> Tuple[str, str] part is called type hinting.\n",
    "# It is completely optional, and tells the user\n",
    "# exactly what types are returned. This makes it \n",
    "# easier for users who have to read the code, as\n",
    "# well as better for documentation and IDE's. You \n",
    "# can read more: https://stackoverflow.com/questions/32557920/what-are-type-hints-in-python-3-5\n",
    "def get_box_office() -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Scrape the Top Box Office: movie names, $ values,\n",
    "    and the end of the links to the movie pages. Return\n",
    "    them as 3 lists.\n",
    "    \"\"\"\n",
    "    # GET the rottentomatoes home page\n",
    "    html = requests.get('https://www.rottentomatoes.com/')\n",
    "\n",
    "    # Create the bs4 parser\n",
    "    soup = bsoup(html.text)\n",
    "\n",
    "    # Get the enclosing box office \"div\" tag from rottentomatoes.com\n",
    "    box_office_html = soup.find_all(id=\"top-box-office\")\n",
    "\n",
    "    # Isolate and parse the $\n",
    "    money = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-date\")\n",
    "    money = [td.a.text for td in money]\n",
    "\n",
    "    # Isolate and parse the movie names\n",
    "    names = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-title\")\n",
    "    names = [name.a.text for name in names]\n",
    "    \n",
    "    # Isolate and parse the ids\n",
    "    ids = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-title\")\n",
    "    ids = [i.a['href'] for i in ids] \n",
    "\n",
    "    return names, money, ids\n",
    "\n",
    "# Test things out\n",
    "money, names, ids = get_box_office()\n",
    "print(money, names, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               names box_office  \\\n",
      "0                                 Sonic the Hedgehog       $58M   \n",
      "1  Birds of Prey (And the Fantabulous Emancipatio...     $17.3M   \n",
      "2                                     Fantasy Island     $12.3M   \n",
      "3                                     The Photograph     $12.3M   \n",
      "4                                  Bad Boys for Life     $11.6M   \n",
      "\n",
      "                        rt_id  \n",
      "0  /m/sonic_the_hedgehog_2020  \n",
      "1       /m/birds_of_prey_2020  \n",
      "2      /m/fantasy_island_2020  \n",
      "3      /m/the_photograph_2020  \n",
      "4        /m/bad_boys_for_life  \n"
     ]
    }
   ],
   "source": [
    "# Let's make it return a pandas dataframe instead\n",
    "def get_box_office() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape the Top Box Office: movie names, $ values,\n",
    "    and the end of the links to the movie pages. Return\n",
    "    them as 3 lists.\n",
    "    \"\"\"\n",
    "    # GET the rottentomatoes home page\n",
    "    html = requests.get('https://www.rottentomatoes.com/')\n",
    "\n",
    "    # Create the bs4 parser\n",
    "    soup = bsoup(html.text)\n",
    "\n",
    "    # Get the enclosing box office \"div\" tag from rottentomatoes.com\n",
    "    box_office_html = soup.find_all(id=\"top-box-office\")\n",
    "\n",
    "    # Isolate and parse the $\n",
    "    money = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-date\")\n",
    "    money = [td.a.text for td in money]\n",
    "\n",
    "    # Isolate and parse the movie names\n",
    "    names = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-title\")\n",
    "    names = [name.a.text for name in names]\n",
    "    \n",
    "    # Isolate and parse the ids\n",
    "    ids = box_office_html[0].find_all(\"td\", class_=\"media-lists__td-title\")\n",
    "    ids = [i.a['href'] for i in ids] \n",
    "\n",
    "    return pd.DataFrame(data = {\"names\": names, \"box_office\": money, \"rt_id\": ids})\n",
    "\n",
    "print(get_box_office().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "    \n",
    "**!!!**\n",
    "\n",
    "**You do not need the following examples to solve the project, however, they are included in case you are interested, or would like to see more.**\n",
    "\n",
    "**!!!**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's pretty neat. You could imagine putting that into a script\n",
    "# and making the script run every day to update a database, or \n",
    "# append to an excel file.\n",
    "\n",
    "# A lot of times you will need to use scraping in order to collect data to do\n",
    "# an analysis or make a comparison. What if we wanted to do the latter?\n",
    "\n",
    "# Take a look at these two links:\n",
    "# https://www.rottentomatoes.com/browse/dvd-streaming-all?minTomato=95&maxTomato=100&services=netflix_iw\n",
    "# https://www.rottentomatoes.com/browse/dvd-streaming-all?minTomato=95&maxTomato=100&services=amazon_prime\n",
    "\n",
    "# By clicking around a website and carefully observing the URL, you can figure out patterns\n",
    "# that the API utilizes and use those patterns to get the information onto the web page that you want. \n",
    "# Let's write a function that accepts 3 arguments: minTomato, maxTomato, and service, and returns\n",
    "# a list of link id's for every qualifying movie.\n",
    "def get_movie_links(minTomato: int, maxTomato: int, service: str) -> Tuple[str]:\n",
    "    url = None\n",
    "    if service.lower() == \"netflix\":\n",
    "        url = f'https://www.rottentomatoes.com/browse/dvd-streaming-all?minTomato={minTomato}&maxTomato={maxTomato}&services=netflix_iw'\n",
    "    elif service.lower() == \"amazon\":\n",
    "        url = f'https://www.rottentomatoes.com/browse/dvd-streaming-all?minTomato={minTomato}&maxTomato={maxTomato}&services=amazon_prime'\n",
    "    else:\n",
    "        sys.exit(f'Service {service} not found.')\n",
    "\n",
    "    \n",
    "# Ok, good start. But how do we get all the movies to show, not just the first page?\n",
    "# If you inspect the \"Show More\" button, you can see that rottentomatoes is being \n",
    "# clever and not showing us how to modify the url to show more. Luckily,\n",
    "# if you click to inspect the element, and change to the \"console\", you can see\n",
    "# that a request was made to a different url, rottentomatoes.com/api/private/v2.0/...\n",
    "# This is what we are looking for. They have an API that returns already\n",
    "# organized data. Lets figure this out.\n",
    "\n",
    "# https://www.rottentomatoes.com/api/private/v2.0/browse?minTomato=95&maxTomato=100&services=amazon_prime&type=dvd-streaming-all&page=1\n",
    "# If you go to that url, you can see that they are returning structured JSON data, already organized! Looks like only 32 results at a time.\n",
    "# We can handle this type of structured data too!\n",
    "html = requests.get('https://www.rottentomatoes.com/api/private/v2.0/browse?minTomato=95&maxTomato=100&services=amazon_prime&type=dvd-streaming-all&page=1')\n",
    "\n",
    "# Instead of using html.text, lets use html.json()\n",
    "print(type(html.json()))\n",
    "\n",
    "# A dict is easy to navigate, great.\n",
    "json = html.json()\n",
    "\n",
    "# 3 keys: counts, results, and debugUrl\n",
    "print(json.keys())\n",
    "\n",
    "# Let's look at counts.\n",
    "print(json['counts'])\n",
    "\n",
    "# That is useful. It tells us the total # of results for our query. We could use this \n",
    "# to calculate the page #'s.\n",
    "print(type(json['results'])) # hmm, its a list\n",
    "\n",
    "print(len(json['results'])) # oh okay, the length is the number of movies, 32\n",
    "\n",
    "# Let's see if each movie is a dict\n",
    "print(type(json['results'][0]))\n",
    "\n",
    "# Great. Let's see what information we have for each movie.\n",
    "print(json['results'][0].keys())\n",
    "\n",
    "# Excellent. Since we have all of the movie information already here, we don't need to messily scrape each\n",
    "# movies webpage for more information. Let's use this API instead. I want to know how netflix and amazon \n",
    "# compare when looking at movies with 95+ tomatoScore. Specifically, how many reviews per movie on avg?\n",
    "json['results'][0]['tomatoScore']\n",
    "\n",
    "# Wait, I'm not seeing how many reviews each movie has. Time to step back and see if we can get that info.\n",
    "# Let's see how we could find this number for WALL-E: https://www.rottentomatoes.com/m/wall_e\n",
    "# Open up the browser console (usually ctrl+shift+j), and navigate to that page and investigate the \n",
    "# requests to see if we can find an API again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far: https://www.rottentomatoes.com/m/wall_e/reviews, so it looks like they just tack /reviews \n",
    "# to the end of the regular movie link. Looks like 20 reviews per page, and 1-20 on the last page.\n",
    "# Let's write a function to count the number of reviews on a page.\n",
    "\n",
    "# First we need to investigate.\n",
    "html = requests.get(\"https://www.rottentomatoes.com/m/wall_e/reviews?type=&sort=&page=1\")\n",
    "soup = bsoup(html.text)\n",
    "\n",
    "# Lets right click and inspect one of those rows with a review.\n",
    "# The whole table of reviews is inside: <div class=\"review_table\"></div>\n",
    "# Each review lies inside: <div class=\"row review_table_row\"></div>\n",
    "# That's convenient. We can just pull the entire table and count the \n",
    "# rows.\n",
    "def count_reviews(url:str) -> int:\n",
    "    html = requests.get(url)\n",
    "    soup = bsoup(html.text)\n",
    "\n",
    "    table = soup.find_all(class_=\"review_table\")\n",
    "\n",
    "    # Let's ensure that we got exactly 1\n",
    "    if len(table) > 1:\n",
    "        sys.exit(\"Retrieved more than one div tag with class review_table.\")\n",
    "    \n",
    "    if len(table) < 1:\n",
    "        return 0\n",
    "\n",
    "    return len(table[0].find_all(class_=\"row review_table_row\"))\n",
    "\n",
    "# Ok, let's test it\n",
    "print(count_reviews(\"https://www.rottentomatoes.com/m/wall_e/reviews?type=&sort=&page=1\"))\n",
    "print(count_reviews(\"https://www.rottentomatoes.com/m/wall_e/reviews?type=&sort=&page=13\"))\n",
    "\n",
    "# Perfect. Now let's write a function that counts the reviews for a movie, \n",
    "# given the rt_id thing: /m/something\n",
    "def count_movie_reviews(rt_id: str) -> int:\n",
    "    url = f'https://www.rottentomatoes.com{rt_id}/reviews'\n",
    "    html = requests.get(url)\n",
    "    soup = bsoup(html.text)\n",
    "\n",
    "    # Get page count, wait we didnt cover this...\n",
    "\n",
    "# Navigate to: https://www.rottentomatoes.com/m/wall_e/reviews\n",
    "# right click and inspect the part that says page 1 of X.\n",
    "# The class attribute is pageInfo\n",
    "html = requests.get(\"https://www.rottentomatoes.com/m/wall_e/reviews\")\n",
    "soup = bsoup(html.text)\n",
    "\n",
    "# There are 2 which makes sense. They are identical though.\n",
    "print(soup.find_all(class_=\"pageInfo\"))\n",
    "\n",
    "# Extract the values of the tag\n",
    "print(soup.find(class_=\"pageInfo\").string)\n",
    "\n",
    "# The number we want is always after \"of\", unless of course\n",
    "# there is only a single page. If that is the case, just\n",
    "# set the last_page to 1\n",
    "soup.find(class_=\"pageInfo\").string.split(\"of\")[1].strip()\n",
    "\n",
    "# Ok, back to the function:\n",
    "def count_movie_reviews(rt_id: str) -> int:\n",
    "    url = f'https://www.rottentomatoes.com{rt_id}/reviews'\n",
    "    html = requests.get(url)\n",
    "    soup = bsoup(html.text)\n",
    "\n",
    "    # Get page count. Note that this \"try\" \"except\" stuff isn't important\n",
    "    # for now. Just know that python \"tries\" to do what is in the \"try\"\n",
    "    # block. If it encounters an exception, it runs the code in the \"except\"\n",
    "    # block.\n",
    "    try:\n",
    "        last_page = soup.find(class_=\"pageInfo\").string.split(\"of\")[1].strip()\n",
    "    except:\n",
    "        last_page = 1\n",
    "\n",
    "    url = f'https://www.rottentomatoes.com{rt_id}/reviews?page={last_page}'\n",
    "\n",
    "    # Count the reviews\n",
    "    last_page_count = count_reviews(url)\n",
    "\n",
    "    return last_page_count + 20*(int(last_page)-1)\n",
    "\n",
    "# Let's test it out\n",
    "print(count_movie_reviews(\"/m/wall_e\"))\n",
    "print(count_movie_reviews(\"/m/frozen_2013\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our new function to compare the number of reviews for Netflix and Amazon \n",
    "# movies that have 95+ rating.\n",
    "amazonHTML = requests.get('https://www.rottentomatoes.com/api/private/v2.0/browse?minTomato=95&maxTomato=100&services=amazon_prime&type=dvd-streaming-all&page=1')\n",
    "netflixHTML = requests.get('https://www.rottentomatoes.com/api/private/v2.0/browse?minTomato=95&maxTomato=100&services=netflix_iw&type=dvd-streaming-all&page=1')\n",
    "\n",
    "amazon = amazonHTML.json()\n",
    "netflix = netflixHTML.json()\n",
    "\n",
    "# Remember to get the rt_id we want:\n",
    "print(amazon['results'][0]['url'])\n",
    "\n",
    "# and to get page #'s\n",
    "from math import ceil\n",
    "print(ceil(amazon['counts']['total']/amazon['counts']['count']))\n",
    "\n",
    "# Loop through all of the pages of amazons 95+ movies\n",
    "amazonLinks = []\n",
    "for page in range(1, ceil(amazon['counts']['total']/amazon['counts']['count'])+1):\n",
    "    html = requests.get(f'https://www.rottentomatoes.com/api/private/v2.0/browse?minTomato=95&maxTomato=100&services=amazon_prime&type=dvd-streaming-all&page={page}')\n",
    "    json = html.json()\n",
    "\n",
    "    # Loop through all of the 32 or less movies\n",
    "    for result in json['results']:\n",
    "        amazonLinks.append(result['url'])\n",
    "\n",
    "# Do the same for netflix\n",
    "netflixLinks = []\n",
    "for page in range(1, ceil(amazon['counts']['total']/amazon['counts']['count'])+1):\n",
    "    html = requests.get(f'https://www.rottentomatoes.com/api/private/v2.0/browse?minTomato=95&maxTomato=100&services=netflix_iw&type=dvd-streaming-all&page={page}')\n",
    "    json = html.json()\n",
    "\n",
    "    # Loop through all of the 32 or less movies\n",
    "    for result in json['results']:\n",
    "        netflixLinks.append(result['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to cyle through and use our count_movie_reviews function\n",
    "\n",
    "amazonReviewCount = 0\n",
    "netflixReviewCount = 0\n",
    "for link in amazonLinks:\n",
    "    amazonReviewCount += count_movie_reviews(link)\n",
    "\n",
    "for link in netflixLinks:\n",
    "    netflixReviewCount += count_movie_reviews(link)\n",
    "\n",
    "print(amazonReviewCount/len(amazonLinks))\n",
    "print(netflixReviewCount/len(netflixLinks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
