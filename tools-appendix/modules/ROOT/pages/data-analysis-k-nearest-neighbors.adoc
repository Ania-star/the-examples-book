= k-nearest neighbors (KNN)
:page-mathjax: true

$k$-nearest neighbors is a relatively simple xref:data-modeling/choosing-model/parameterization.adoc[non-parametric] supervised learning algorithm. It can be used for both regression and classification. It works by finding the $k$ closest observations in a dataset to develop the training set.

== A Brief History

$k$-Nearest Neighbors was first https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf[presented in the early 1950's].

== Code Examples

NOTE: All of the code examples are written in Python, unless otherwise noted.

=== Containers

TIP: These are code examples in the form of Jupyter notebooks running in a container that come with all the data, libraries, and code you'll need to run it. https://the-examples-book.com/starter-guides/data-engineering/containers/using-data-mine-containers[Click here to learn why you should be using containers, along with how to do so.]

TIP: Quickstart: https://docs.docker.com/get-docker/[Download Docker], then run the commands below in a terminal. 

[source,bash]
----
#pull container, only needs to be run once
docker pull ghcr.io/thedatamine/starter-guides:k-nearest-neighbors

#run container
docker run -p 8888:8888 -it ghcr.io/thedatamine/starter-guides:k-nearest-neighbors
----

Need help implementing any of this code? Feel free to reach out to mailto:datamine-help@purdue.edu[datamine-help@purdue.edu] and we can help!

== Resources

All resources are chosen by Data Mine staff to be of decent quality, and most if not all content is free. 

=== Websites

https://www.ibm.com/topics/knn[What is the k-nearest neighbors Algorithm? (IBM)]

https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761[Machine Learning Basics with the K-Nearest Neighbors Algorithm (Towards Data Science)]

https://www.w3schools.com/python/python_ml_knn.asp[Machine Learning - K-nearest neighbors (KNN) (W3 Schools)]

https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html[Lecture 2: k-nearest neighbors (Cornell)]

=== Videos

https://www.youtube.com/watch?v=0p0o5cmgLdE[K Nearest Neighbors | Intuitive explained | Machine Learning Basics (Intuitive Machine Learning, ~2 minutes)]

https://www.youtube.com/watch?v=HVXime0nQeI[StatQuest: K-nearest neighbors, Clearly Explained (StatQuest With Josh Starmer, ~5 minutes)]

https://www.youtube.com/watch?v=09mb78oiPkA[10. Introduction to Learning, Nearest Neighbors (MIT, ~50 minutes)]

=== Books

https://www.statlearning.com[Introduction to Statistical Learning, Chapter 4.5]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/uc5e95/alma99169806259101081[New Developments in Unsupervised Outlier Detection: Algorithms and Applications]

=== Articles

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4916348/[Introduction to machine learning: k-nearest neighbors]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/5imsd2/cdi_pubmedcentral_primary_oai_pubmedcentral_nih_gov_4978658[The Distance Function Effect on k-nearest neighbor Classification for Medical Datasets]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/5imsd2/cdi_proquest_journals_2121518739[Multi-class K-support Vector Nearest Neighbor for Mango Leaf Classification]