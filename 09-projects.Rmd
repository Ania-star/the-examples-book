# Projects {#projects}

## Templates {#templates}

Our course project template can be found [here](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd), or on Scholar: 

`/class/datamine/apps/templates/project_template.Rmd`

**Important note:** We've updated the template to allow a code chunk option that prevents content from running off the page. Simply add `linewidth=80` to any code chunk that creates output that runs off the page.

This video demonstrates:

* opening a browser (emphasizing Firefox as the best choice),
* opening RStudio Server Pro (https://rstudio.scholar.rcac.purdue.edu),
* introducing (basics) about what RStudio looks like,
* checking to see that the students are using R 4.0,
* running the initial (one-time) setup script,
* opening the project template,
* knitting the template into a PDF file, and
* finally handling the popup blocker, which can potentially block the PDF.

**[Click here for video](https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_444kq84l&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_5wx961lv)**

Students in STAT 19000, 29000, and 39000 are to use this as a template for all project submissions. The template includes a code chunk that "activates" our Python environment, and adjusts some default settings. In addition, it provides examples on how to include solutions for Python, R, Bash, and SQL. Every question should be clearly marked with a third-level header (using 3 `#`s) followed by `Question 1`, `Question 2`, etc. Sections for solutions should be added or removed, based on the number of questions in the given project. All code chunks are to be run and solutions displayed for the compiled PDF submission.

Any format or template related questions should be asked in Piazza.

## Submissions {#submissions}

Unless otherwise specified, all projects will need 2-4 submitted files: 

1. A compiled PDF file (built using the template), with all code and output.
2. The .Rmd file (based off of [the template](#templates)), used to Knit the final PDF.
3. If it is a project containing R code, a .R file containing all of the R code with comments explaining what the code does. _Note: This is **not** an .Rmd file._
4. If it is a project containing Python code, a .py file containing all of the Python code.

## STAT 19000

### Topics

The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration.

|Language|Project #|Name|Topics|
|--------|---------|----|------|
|Python|1|Intro to Python: part I|declaring variables, printing, running cells, exporting to different formats, etc.|
|Python|2|Intro to Python: part II|lists, tuples, if statements, opening files, pandas, matplotlib, etc.|
|Python|3|Intro to Python: part III|sets, dicts, pandas, matplotlib, lists, tuples, etc.|
|Python|4|Control flow in Python|if statements, for loops, dicts, lists, matplotlib, etc.|
|Python|5|Scientific computing/Data wrangling: part I|timing, I/O, indexing in pandas, pandas functions, matplotlib, etc.|
|Python|6|Functions: part I|writing functions, docstrings, pandas, etc.|
|Python|7|Functions: part II|writing functions, docstrings, pandas, etc.|
|Python|8|Scientific computing/Data wrangling: part II|building a recommendation system|
|Python|9|Scientific computing/Data wrangling: part III|building a recommendation system, continued...|
|Python|10|Packages|Learn more about Python packaging, importing, etc.|
|Python|11|Python Classes: part I|writing classes in Python to build a game, dunder methods, attributes, methods, etc.|
|Python|12|Python Classes: part II|writing classes in Python to build a game, dunder methods, attributes, methods, etc., continued...|
|Python|13|Data wrangling & matplotlib: part I|more pandas, more matplotlib, wrangling with increased difficulty, etc.|
|Python|14|Data wrangling & matplotlib: part II|more pandas, more matplotlib, wrangling with increased difficulty, etc.|

### Project 1 {#p01-190}

---

**Motivation:** In this course we require the majority of project submissions to include a compiled PDF, a .Rmd file based off of [our template](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd), and a code file (a .R file if the project is in R, a .py file if the project is in Python). Although RStudio makes it easy to work with both Python and R, there are occasions where working out a Python problem in a Jupyter Notebook could be convenient. For that reason, we will introduce Jupyter Notebook in this project.

**Context:** This is the first in a series of projects that will introduce Python and its tooling to students.

**Scope:** jupyter notebooks, rstudio, python

**Learning objectives:** 

```{block, type="bbox"}
- Use Jupyter Notebook to run Python code and create Markdown text.
- Use RStudio to run Python code and compile your final PDF.
- Gain exposure to Python control flow and reading external data.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/open_food_facts/openfoodfacts.tsv`

#### Questions

##### 1. Navigate to https://notebook.scholar.rcac.purdue.edu/ and sign in with your Purdue credentials (_without_ BoilerKey). This is an instance of Jupyter Notebook. The main screen will show a series of files and folders that are in your `$HOME` directory. Create a new notebook by clicking on `New > f2020-s2021`. 

##### Change the name of your notebook to "LASTNAME_FIRSTNAME_project01" where "LASTNAME" is your family name, and "FIRSTNAME" is your given name. Try to export your notebook (using the `File` dropdown menu, choosing the option `Download as`), what format options (for example, `.pdf`) are available to you?

**Important note:** `f2020-s2021` is the name of our course notebook kernel. A notebook kernel is an engine that runs code in a notebook. ipython kernels run Python code. `f2020-s2021` is an ipython kernel that we've created for our course Python environment, which contains a variety of compatible, pre-installed packages for you to use. When you select `f2020-s2021` as your kernel, all of the packages in our course environment are automatically made available to you.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_4g2lwx5g)

If the kernel `f2020-s2021` does not appear in Jupyter Notebooks, you can make it appear as follows:

Login to [https://rstudio.scholar.rcac.purdue.edu](https://rstudio.scholar.rcac.purdue.edu)  
Click on `Tools > Shell...` (in the menu)  
In the shell (terminal looking thing that should say something like: `bash-4.2$`), type the following followed by Enter/Return: `/class/datamine/apps/runme`  
Then click on `Session > Restart R` (in the menu)  
You should now have access to the course kernel named `f2020-s2021` in [https://notebook.scholar.rcac.purdue.edu](https://notebook.scholar.rcac.purdue.edu)  

```{block, type="bbox"}
**Item(s) to submit:**

- A list of export format options.
```

##### Solution

```{block, type="solution"}
- .ipynb
- .py
- .html
- .md
- .rst
- .tex
- .pdf
```

##### 2. Each "box" in a Jupyter Notebook is called a _cell_. There are two primary types of cells: code, and markdown. By default, a cell will be a code cell. Place the following Python code inside the first cell, and run the cell. What is the output? 

```{python, eval=F}
from thedatamine import hello_datamine
hello_datamine()
```

**Hint:** You can run the code in the currently selected cell by using the GUI (the buttons), as well as by pressing `Ctrl+Return/Enter`. 

```{block, type="bbox"}
**Item(s) to submit:**

- Output from running the provided code.
```

##### Solution

```{block, type="solution"}
"Hello student! Welcome to The Data Mine!"
```

##### 3. Jupyter Notebooks allow you to easily pull up documentation, similar to `?function` in R. To do so, use the `help` function, like this: `help(my_function)`. What is the output from running the help function on `hello_datamine`? Can you modify the code from question (2) to print a customized message? Create a new _markdown_ cell and explain what you did to the code from question (2) to make the message customized.

**Important note:** Some Jupyter-only methods to do this are:

- Click on the function of interest and type `Shift+Tab` or `Shift+Tab+Tab`.
- Run `function?`, for example, `print?`.

**Important note:** You can also see the source code of a function in a Jupyter Notebook by typing `function??`, for example, `print??`.

```{block, type="bbox"}
**Item(s) to submit:**

- Output from running the `help` function on `hello_datamine`.
- Modified code from question (2) that prints a customized message.
```

##### Solution

```{python, eval=F, class.source="solution"}
help(hello_datamine)
```

```{block, type="solution"}
Help on function hello_datamine in module thedatamine.core:

hello_datamine(name: str = 'student') -> None
    Prints a hello message to a Data Mine student.
    
    Args:
        str (name, optional): The name of a student. Defaults to 'student'.
```

```{python, eval=F, class.source="solution"}
hello_datamine("Kevin")
```

##### 4. At this point in time, you've now got the basics of running Python code in Jupyter Notebooks. There is really not a whole lot more to it. For this class, however, we will continue to create RMarkdown documents in addition to the compiled PDFs. You are welcome to use Jupyter Notebooks for personal projects or for testing things out, however, we will still require an RMarkdown file (.Rmd), PDF (generated from the RMarkdown file), and .py file (containing your python code). For example, please move your solutions from Questions 1, 2, 3 from Jupyter Notebooks over to RMarkdown (we discuss RMarkdown below). Let's learn how to run Python code chunks in RMarkdown.

##### Sign in to https://rstudio.scholar.rcac.purdue.edu (_with_ BoilerKey). Projects in The Data Mine should all be submitted using our template found [here](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd) or on Scholar (`/class/datamine/apps/templates/project_template.Rmd`).

##### Open the project template and save it into your home directory, in a new RMarkdown file named `project01.Rmd`. Prior to running any Python code, run `datamine_py()` in the R console, just like you did at the beginning of every project from the first semester.

##### Code chunks are parts of the RMarkdown file that contains code. You can identify what type of code a code chunk contains by looking at the _engine_ in the curly braces "{" and "}". As you can see, it is possible to mix and match different languages just by changing the engine. Move the solutions for questions 1-3 to your `project01.Rmd`. Make sure to place all Python code in `python` code chunks. Run the `python` code chunks to ensure you get the same results as you got when running the Python code in a Jupyter Notebook. 

**Important note:** Make sure to run `datamine_py()` in the R console prior to attempting to run any Python code.

**Hint:** The end result of the `project01.Rmd` should look _similar_ to [this](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/example02.Rmd).

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_nhkygxg9)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_tdz3wmim)

```{block, type="bbox"}
**Item(s) to submit:**

- `project01.Rmd` with the solutions from questions 1-3 (including any Python code in `python` code chunks). 
```

##### Solution

```{block, type="solution"}
Done.
```

##### 5. It is not a Data Mine project without data! [Here](#p-csv-pkg) are some examples of reading in data line by line using the `csv` package. How many columns are in the following dataset: `/class/datamine/data/open_food_facts/openfoodfacts.tsv`? Print the first row, the number of columns, and then exit the loop after the first iteration using the `break` keyword.

**Hint:** You can get the number of elements in a list by using the `len` method. For example: `len(my_list)`.

**Hint:** You can use the `break` keyword to exit a loop. As soon as `break` is executed, the loop is exited and the code immediately following the loop is run.

```{python, eval=F}
for my_row in my_csv_reader:
    print(my_row)
    break

print("Exited loop as soon as 'break' was run.")
```

**Hint:** `'\t'` represents a tab in Python.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ck74xlzq)

**Important note:** If you get a Dtype warning, feel free to just ignore it. 

**Relevant topics:** [for loops](#p-for-loops), [break](#p-break), [print](#p-print)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve this problem.
- The first row printed, and the number of columns printed.
```

##### Solution

```{python, eval=F, class.source="solution"}
import csv
with open('/class/datamine/data/open_food_facts/openfoodfacts.tsv') as my_file:
    my_reader = csv.reader(my_file, delimiter='\t')
    for row in my_reader:
        print(row)
        print(len(row))
        break # prematurely leave the loop
```

##### 6 (optional). Unlike in R, where many of the tools you need are built-in (`read.csv`, data.frames, etc.), in Python, you will need to rely on packages like `numpy` and `pandas` to do the bulk of your data science work. 

##### In R it would be really easy to find the mean of the 151st column, `caffeine_100g`:

```{r, eval=F}
myDF <- read.csv("/class/datamine/data/open_food_facts/openfoodfacts.tsv", sep="\t", quote="")
mean(myDF$caffeine_100g, na.rm=T) # 2.075503
```

##### If you were to try to modify our loop from question (5) to do the same thing, you will run into a myriad of issues, just to try and get the mean of a column. Luckily, it is easy to do using `pandas`:

```{python, eval=F}
import pandas as pd
myDF = pd.read_csv("/class/datamine/data/open_food_facts/openfoodfacts.tsv", sep="\t")
myDF["caffeine_100g"].mean() # 2.0755028571428573
```

##### Take a look at some of the methods you can perform using pandas [here](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats). Perform an interesting calculation in R, and replicate your work using `pandas`. Which did you prefer, Python or R?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ybx1iukd)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Python code used to solve the problem.
```

##### Solution

```{python, eval=F, class.source="solution"}
# could be anything.
```


---

### Project 2 {#p02-190}

---

**Motivation:** In Python it is very important to understand some of the data types in a little bit more depth than you would in R. Many of the data types in Python will seem very familiar. A `character` in R is similar to a `str` in Python. An `integer` in R is an `int` in Python. A `numeric` in R is similar to a `float` in Python. A `logical` in R is similar to a `bool` in Python. In addition to all of that, there are some very popular classes that packages like `numpy` and `pandas` introduces. On the other hand, there are some data types in Python like `tuple`s, `list`s, `set`s, and `dict`s that diverge from R a little bit more. It is integral to understand some basic concepts before jumping too far into everything. 

**Context:** This is the second project introducing some basic data types, and demonstrating some familiar control flow concepts, all while digging right into a dataset.

**Scope:** tuples, lists, if statements, opening files

**Learning objectives:** 

```{block, type="bbox"}
- List the differences between lists & tuples and when to use each.
- Gain familiarity with string methods, list methods, and tuple methods.
- Demonstrate the ability to read and write data of various formats using various packages.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/craigslist/vehicles.csv`

#### Questions

##### 1. Read in the dataset `/class/datamine/data/craigslist/vehicles.csv` into a `pandas` DataFrame called `myDF`. `pandas` is an integral tool for various data science tasks in Python. You can read a quick intro [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html). We will be slowly introducing bits and pieces of this package throughout the semester. Similarly, we will try to introduce byte-sized (ha!) portions of plotting packages to slowly build up your skills.

##### How big is the dataset (in Mb or Gb)?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1bhwhkt2)

**Hint:** Remember to check out a question's _relevant topics_. We try very hard to link you to content and examples that will get you up and running as _quickly_ as possible.

**Relevant topics:** [pandas read_csv](#p-pandas-read_csv), [get filesize in Python](#p-pathlib-path)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
```

##### 2. In question (1) we read in our data into a `pandas` DataFrame. Use one of the `pandas` DataFrame [attributes](https://pandas.pydata.org/docs/reference/frame.html#attributes-and-underlying-data) to get the number of columns and rows of our dataset. How many columns and rows are there? Use f-strings to print a message, for example:

````
There are 123 columns in the DataFrame!
There are 321 rows in the DataFrame!
````

##### In project 1, we learned how to read a csv file in, line-by-line, and print values. Use the `csv` package to print _just_ the first row, which should contain the names of the columns, OR instead of using the `csv` package, use one of the `pandas` attributes from `myDF` (to print the column names).

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_cifzobbk)

**Relevant topics:** [csv read csv](#p-csv-pkg), [pandas DataFrame](#p-pandas-dataframe), [f-strings](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python), [break](#p-break)

```{block, type="bbox"}
**Item(s) to submit:**

- The output from printing the f-strings.
- Python code used to solve the problem.
```

##### 3. Use the `csv` or `pandas` package to get a [list](#p-lists-and-tuples) called `our_columns` that contains the column names. Add a string, "extra", to the end of `our_columns`. Print the second value in the list. Without using a loop, print the 1st, 3rd, 5th, etc. elements of the list. Print the last four elements of the list ( "state", "lat", "long", and "extra") by accessing their negative index.

##### "extra" doesn't belong in our list, you can easily remove this value from our list by doing the following:

```{python, eval=F}
our_columns.pop(25)

# or even this, as pop removes the last value by default
our_columns.pop()
```

##### BUT the problem with this solution is that you must know the index of the value you'd like to remove, and sometimes you do not know the index of the value. Instead, please show how to use a [list method](#p-list-methods) to remove "extra" by _value_ rather than by _index_.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1z6kxfn1)

**Relevant topics:** [csv read csv](#p-csv-pkg), [break](#p-break), [append](#p-list-methods), [indexing](#p-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running your code.
```

##### 4. `matplotlib` is one of the primary plotting packages in Python. You are provided with the following code:

```{python, eval=F}
my_values = tuple(myDF.loc[:, 'odometer'].dropna().to_list())
```

##### The result is a _tuple_ containing the odometer readings from all of the vehicles in our dataset. Create a lineplot of the odometer readings.

##### Well, that plot doesn't seem too informative. Let's first sort the values in our tuple:

```{python, eval=F}
my_values.sort()
```

##### What happened? A tuple is immutable. What this means is that once the contents of a tuple are declared they cannot be modified. For example:

```{python, eval=F}
# This will fail because tuples are immutable
my_values[0] = 100
```

##### You can read a good article about this [here](http://www.compciv.org/guides/python/fundamentals/tuples-immutable/). In addition, [here](https://stackoverflow.com/questions/1708510/list-vs-tuple-when-to-use-each) is a great post that gives you an idea when using a tuple might be a good idea. Okay, so let's go back to our problem. We know that lists _are_ mutable (and therefore sortable), so convert `my_values` to a list and then sort, and re-plot.

##### It looks like there are some (potential) outliers that are making our plot look a little wonky. For the sake of seeing how the plot would look, use negative indexing to plot the sorted values _minus_ the last 50 values (the 50 highest values). New new plot may not look _that_ different, that is okay.

**Hint:** To prevent plotting values on the same plot, close your plot with the `close` method, for example:

```{python, eval=F}
import matplotlib.pyplot as plt
my_values = [1,2,3,4,5]
plt.plot(my_values)
plt.show()
plt.close()
```

**Relevant topics:** [list methods](#p-list-methods), [indexing](#p-indexing), [matplotlib lineplot](#p-matplotlib-lineplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running your code.
```

##### 5. We've covered a lot in this project! Use what you've learned so far to do one (or more) of the following tasks:

##### - Create a cool graphic using `matplotlib`, that summarizes some data from our dataset.

##### - Use `pandas` and your investigative skills to sift through the dataset and glean an interesting factoid.

##### - Create some commented coding examples that highlight the differences between lists and tuples. Include at least 3 examples.

**Relevant topics:** [pandas](#p-pandas), [indexing](#p-indexing), [matplotlib](#p-matplotlib)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running your code.
```

---

### Project 3 {#p03-190}

---

**Motivation:** A dictionary (referred to as a `dict`) is one of the most useful data structures in Python. You can think about them as a data structure containing _key_: _value_ pairs. Under the hood, a `dict` is essentially a data structure called a _hash table_. [Hash tables](https://en.wikipedia.org/wiki/Hash_table) are a data structure with a useful set of properties. The time needed for searching, inserting, or removing a piece of data has a constant average lookup time, meaning that no matter how big your hash table grows to be, inserting, searching, or deleting a piece of data will _usually_ take about the same amount of time.  (The worst case time increases linearly.) Dictionaries (`dict`) are used a lot, so it is worthwhile to understand them. Although not used quite as often, another important data type called a `set`, is also worthwhile learning about.

**Context:** In our third project, we introduce some basic data types, and we demonstrate some familiar control flow concepts, all while digging right into a dataset. Throughout the course, we will slowly introduce concepts from `pandas`, and popular plotting packages.

**Scope:** dicts, sets, if/else statements, opening files, tuples, lists

**Learning objectives:** 

```{block, type="bbox"}
- Explain what is a `dict` is and why it is useful.
- Understand how a `set` works and when it could be useful.
- List the differences between lists & tuples and when to use each.
- Gain familiarity with string methods, list methods, and tuple methods.
- Gain familiarity with dict methods.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/craigslist/vehicles.csv`

#### Questions

##### 1. In project 2 we learned how to read in data using `pandas`. Read in the (`/class/datamine/data/craigslist/vehicles.csv`) dataset into a DataFrame called `myDF` using `pandas`. In R we can get a sneak peek at the data by doing something like:

```{r, eval=F}
head(myDF) # where myDF is a data.frame
```

##### There is a very similar (and aptly named method) in `pandas` that allows us to do the exact same thing with a `pandas` DataFrame. Get the `head` of `myDF`, and take a moment to consider how much time it would take to get this information if we didn't have this nice `head` method.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1rqwiges)

**Relevant topics:** [pandas read_csv](#p-pandas-read_csv), [head](#p-pandas-dataframe)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The `head` of the dataset.
```

##### 2. Dictionaries, often referred to as dicts, are really powerful. There are two primary ways to "get" information from a dict. One is to use the `get` method, the other is to use square brackets and strings. Test out the following to understand the differences between the two:

```{python, eval=F}
my_dict = {"fruits": ["apple", "orange", "pear"], "person": "John", "vegetables": ["carrots", "peas"]}

# If "person" is indeed a key, they will function the same way
my_dict["person"]
my_dict.get("person")

# If the key does not exist, like below, they will not 
# function the same way.
my_dict.get("height") # Returns None when key doesn't exist
print(my_dict.get("height")) # By printing, we can see None in this case
my_dict["height"] # Throws a KeyError exception because the key, "height" doesn't exist
```

##### Look at the dataset. Let's say that we want to prepare a subset of this data to create a graphic. Specifically, let's say we want to create a barplot that shows the number of vehicles on craigslist by year of make. Create a dict called `my_dict` that contains key:value pairs where the keys are years, and the values are a single int representing the number of vehicles from that year on craigslist. Use the `year` column, a loop, and a dict to accomplish this. Print the dictionary. You can use the following code to extract the `year` column as a list. In the next project we will learn how to loop over `pandas` DataFrames.

**Hint:** If you get a `KeyError`, remember, you must declare each key value pair just like any other variable. Use the following code to initialize each `year` key to the value 0.

```{python, eval=F}
myyears = myDF['year'].dropna().to_list()

# get a list containing each unique year
unique_years = list(set(myyears))

# for each year (key), initialize the value (value) to 0
my_dict = {}
for year in unique_years:
    my_dict[year] = 0
```

**Hint:** Here are some of the results you should get:

```{python, eval=F}
print(my_dict[1912]) # 5
print(my_dict[1982]) # 185
print(my_dict[2014]) # 31703
```

**Note:** There is a special kind of `dict` called a `defaultdict`, that allows you to give default values to a `dict`, giving you the ability to "skip" initialization. We will show you this when we release the solutions to this project!  It is not required, but it is interesting to know about!

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_cl3b1xyy)

**Relevant topics:** [dicts](#p-dicts)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- `my_dict` printed.
```

##### 3. After completing question (2) you can easily access the number of vehicles from a given year. For example, to get the number of vehicles on craigslist from 1912, just run:

```{python, eval=F}
my_dict[1912]

# or

my_dict.get(1912)
```

##### A `dict` stores its data in key:value pairs. Identify a "key" from `my_dict`, as well as the associated "value". As you can imagine, having data in this format can be very beneficial. One benefit is the ability to easily create a graphic using `matplotlib`. Use `matplotlib` to create a bar graph with the year on the x-axis, and the number of vehicles from that year on the y-axis. 

**Hint:** To use `matplotlib`, first import it:

```{python, eval=F}
import matplotlib.pyplot as plt

# now you can use it, for example
plt.plot([1,2,3,1])
plt.show()
plt.close()
```

**Hint:** The `keys` method and `values` method from `dict` could be useful here.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_mn3a3kss)

**Relevant topics:** [dicts](#p-dicts), [matplotlib](#p-matplotlib), [barplot]{#p-matplotlib-barplot}

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The resulting plot.
- A sentence giving an example of a "key" and associated "value" from `my_dict` (e.g., a sentence explaining the 1912 example above).
```

##### 4. In the hint in question (3), we used a `set` to quickly get a list of unique years in a list. Some other common uses of a `set` are when you want to get a list of values that are in one list but not another, or get a list of values that are present in both lists. Examine the following code. You'll notice that we are looping over many values. Replace the code for each of the three examples below with code that uses *no* loops whatsoever.
##### 4. In the hint in question (3), we used a `set` to quickly get a list of unique years in a list. Some other common uses of a `set` are when you want to get a list of values that are in one list but not another, or get a list of values that are present in both lists. Examine the following code. You'll notice that we are looping over many values. Replace the code with code that uses *no* loops whatsoever.

```{python, eval=F}
listA = [1, 2, 3, 4, 5, 6, 12, 12]
listB = [2, 1, 7, 7, 7, 2, 8, 9, 10, 11, 12, 13]

# 1. values in list A but not list B
# values in list A but not list B
onlyA = []
for valA in listA:
    if valA not in listB and valA not in onlyA:
        onlyA.append(valA)

print(onlyA) # [3, 4, 5, 6]

# 2. values in listB but not list A
onlyB = []
for valB in listB:
    if valB not in listA and valB not in onlyB:
        onlyB.append(valB)

print(onlyB) # [7, 8, 9, 10, 11, 13]

# 3. values in both lists
# values in both lists
in_both_lists = []
for valA in listA:
    if valA in listB and valA not in in_both_lists:
        in_both_lists.append(valA)

print(in_both_lists) # [1,2,12]
```

**Hint:** You should use a `set`.

**Note:** In addition to being easier to read, using a `set` is _much_ faster than loops!

**Relevant topics:** [sets](#p-sets)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running the code.
```

##### 5. The value of a dictionary does not have to be a single value (like we've shown so far). It can be _anything_. Observe that there is latitude and longitude data for each row in our DataFrame (`lat` and `long`, respectively). Wouldn't it be useful to be able to quickly "get" pairs of latitude and longitude data for a given state? 

##### First, run the following code to get a list of tuples where the first value is the `state`, the second value is the `lat`, and the third value is the `long`. 

```{python, eval=F}
states_list = list(myDF.loc[:, ["state", "lat", "long"]].dropna().to_records(index=False))
states_list[0:3] # [('az', 34.4554, -114.269), ('or', 46.1837, -123.824), ('sc', 34.9352, -81.9654)]

# to get the first tuple
states_list[0] # ('az', 34.4554, -114.269)

# to get the first value in the first tuple
states_list[0][0] # az

# to get the second tuple
states_list[1] # ('or', 46.1837, -123.824)

# to get the first value in the second tuple
states_list[1][0] # or
```

**Hint:** If you have an issue where you cannot append values to a specific key, make sure to first initialize the specific key to an empty list so the append method is available to use.

##### Now, organize the latitude and longitude data in a dictionary called `geoDict` such that each state from the `state` column is a key, and the respective value is a list of tuples, where the first value in each tuple is the latitude (`lat`) and the second value is the longitude (`long`). For example, the first 2 (lat,long) pairs in Indiana (`"in"`) are:

```{python, eval=F}
geoDict.get("in")[0:2] # [(39.0295, -86.8675), (38.8585, -86.4806)]
len(geoDict.get("in")) # 5687
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_zrd28fgo)

##### Now that you can easily access latitude and longitude pairs for a given state, run the following code to plot the points for Texas (the `state` value is `"tx"`). Include the the graphic produced below in your solution, but feel free to experiment with other states. 

##### NOTE:  You do NOT need to include this portion of Question 5 in your Markdown `.Rmd` file.  We cannot get this portion to build in Markdown, but please do include it in your Python `.py` file.

```{python, eval=F}
from shapely.geometry import Point
import geopandas as gpd
from geopandas import GeoDataFrame

usa = gpd.read_file('/class/datamine/data/craigslist/cb_2018_us_state_20m.shp')
usa.crs = {'init': 'epsg:4269'}

pts = [Point(y,x) for x, y in geoDict.get("tx")]
gdf = gpd.GeoDataFrame(geometry=pts, crs = 4269)
fig, gax = plt.subplots(1, figsize=(10,10))
base = usa[usa['NAME'].isin(['Hawaii', 'Alaska', 'Puerto Rico']) == False].plot(ax=gax, color='white', edgecolor='black')
gdf.plot(ax=base, color='darkred', marker="*", markersize=10)
plt.show()
plt.close()

# to save to jpg:
plt.savefig('q5.jpg')
```

**Relevant topics:** [dicts](#p-dicts), [lists and tuples](#p-lists-and-tuples)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Graphic file (`q5.jpg`) produced for the given state.
```

##### 6. Use your new skills to extract some sort of information from our dataset and create a graphic. This can be as simple or complicated as you are comfortable with!

**Relevant topics:** [dicts](#p-dicts), [lists and tuples](#p-lists-and-tuples)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The graphic produced using the code.
```

---

### Project 4 {#p04-190}

---

**Motivation:** We've now been introduced to a variety of core Python data structures. Along the way we've touched on a bit of `pandas`, `matplotlib`, and have utilized some control flow features like for loops, if statements. We will continue to touch on `pandas` and `matplotlib`, but we will take a deeper dive in this project and learn more about control flow, all while digging into the data!

**Context:** We just finished a project where we were able to see the power of dictionaries and sets. In this project we will take a step back and make sure we are able to really grasp control flow (if/else statements, loops, etc.) in Python. 

**Scope:** python, dicts, lists, if/else statements, for loops

**Learning objectives:** 

```{block, type="bbox"}
- List the differences between lists & tuples and when to use each.
- Explain what is a dict and why it is useful.
- Demonstrate a working knowledge of control flow in python: if/else statements, while loops, for loops, etc.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/craigslist/vehicles.csv`

#### Questions

##### 1. Unlike in R, where traditional loops are rare and typically accomplished via one of the apply functions, in Python, loops are extremely common and important to understand. In Python, any iterator can be looped over. Some common iterators are: tuples, lists, dicts, sets, `pandas` Series, and `pandas` DataFrames. In the previous project we had some examples of looping over lists, let's learn how to loop over `pandas` Series and Dataframes!

##### Load up our dataset `/class/datamine/data/craigslist/vehicles.csv` into a DataFrame called `myDF`. In project (3), we organized the latitude and longitude data in a dictionary called `geoDict` such that each state from the `state` column is a key, and the respective value is a list of tuples, where the first value in each tuple is the latitude (`lat`) and the second value is the longitude (`long`). Repeat this question, but **do not** use lists, instead use `pandas` to accomplish this.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```


##### 2. Wow! The solution to question (1) was _slow_. In general, you'll want to avoid looping over large DataFrames. [Here](https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas/55557758#55557758) is a pretty good explanation of why, as well as a good system on what to try when computing something. In this case, we could have used indexing to get latitude and longitude values for each state, and would have no need to build this dict.

##### Regardless, we now have an opportunity to practice iterating over a dictionary, list, _and_ tuple, all at once! Loop through `geoDict` and use f-strings to print the state abbreviation. Print the first latitude and longitude pair, as well as every 5000th latitude and longitude pair for each state. Round values to the hundreths place. For example, if the state was "pu", and it had 12000 latitude and longitude pairs, we would print the following:

````
pu:
Lat: 41.41, Long: 41.41
Lat: 22.21, Long: 21.21
Lat: 11.11, Long: 10.22
````

##### In the above example, `Lat: 41.41, Long: 41.41` would be the first pair, `Lat: 22.21, Long: 21.21` would be the 5000th pair, and `Lat: 11.11, Long: 10.22` would be the 10000th pair. Make sure to use f-strings to round the latitude and longitude values to two decimal places.

**Hint:** [Enumerate]() is a useful function that adds an index to our loop.

**Hint:** Using and if statement and the [modulo operator]() could be useful.

**Note:** Whenever we have a loop _within_ another loop, the "inner" loop is called a "nested" loop, as it is "nested" inside of the other.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. We are curious about how the year of the car (`year`) effects the price (`price`). In R, we could get the median price by year easily, using `tapply`:

```{r, eval=F}
tapply(myDF$price, myDF$year, median, na.rm=T)
```

##### Using `pandas`, we would do this:

```{python, eval=F}
res = myDF.groupby(['year'], dropna=True).median()
```

##### These are very convenient functions that do a lot of work for you. If we were to take a look at the median price of cars by year, it would look like:

```{python, eval=F}
import matplotlib.pyplot as plt

res = myDF.groupby(['year'], dropna=True).median()["price"]
plt.bar(res.index, res.values)
```

##### Using _just_ the `pandas` code provided below, and no other `pandas` code, calculate the median `price` by `year`. Replicate the plot generated by running the code above, using your calculated median values.

```{python, eval=F}
my_list = list(myDF.loc[:, ["year", "price",]].dropna().to_records(index=False))
```

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The barplot.
```

##### 4. Now, calculate the mean `price` by `year`, and create a barplot with the `price` on the y-axis and `year` on the x-axis. Whoa! Something is odd here. Explain what is happening. Modify your code to use an if statement to "weed out" the likely erroneous value. Re-plot your values.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The barplot.
```

##### 5. List comprehensions are a neat feature of Python that allows for a more concise syntax for smaller loops. While at first they may seem difficult and more confusing, eventually they grow on you. For example, say you wanted to capitalize every `state` in a list full of states:

```{python, eval=F}
my_states = myDF['state'].to_list()
my_states = [state.upper() for state in my_states]
```

##### Or, maybe you wanted to find the average price of cars in "excellent" condition (without `pandas`):

```{python, eval=F}
my_list = list(myDF.loc[:, ["condition", "price",]].dropna().to_records(index=False))
my_list = [price for (condition, price) in my_list if condition == "excellent"]
sum(my_list)/len(my_list)
```

##### Do the following using list comprehensions, and the provided code:

```{python, eval=F}
my_list = list(myDF.loc[:, ["state", "price",]].dropna().to_records(index=False))
```

- Calculate the average price of vehicles from Indiana (`in`).
- Calculate the average price of vehicles from Indiana (`in`), Michigan (`mi`), and Illinois (`il`) combined.

```{python, eval=F}
my_list = list(myDF.loc[:, ["manufacturer", "year", "price",]].dropna().to_records(index=False))
```

- Calculate the average price of a "honda" (`manufacturer`) that is 2010 or newer (`year`).

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 6. Let's use a package called `spacy` to try and parse phone numbers out of the `description` column. First, simply loop through and print the text and the label. What is the label of the majority of the phone numbers you can see?

```{python, eval=F}
import spacy

# get list of descriptions
my_list = list(myDF.loc[:, ["description",]].dropna().to_records(index=False))
my_list = [m[0] for m in my_list]

# load the pre-built spacy model
nlp = spacy.load("en_core_web_lg")

# apply the model to a description
doc = nlp(my_list[0])

# print the text and label of each "entity"
for entity in doc.ents:
    print(entity.text, entity.label_)
```

##### Use an if statement to filter out all entities that are not the label you see. Loop through again and see what our printed data looks like. There is still a lot of data there that we _don't_ want to capture, right? Phone numbers in the US are _usually_ 7 (5555555), 8 (555-5555), 10 (5555555555), 11 (15555555555), 12 (555-555-5555), or 14 (1-555-555-5555) digits. In addition to your first "filter", add another "filter" that keeps only text where the text is one of those lengths.

##### That is starting to look better, but there are still some erroneous values. Come up with another "filter", and loop through our data again. Explain what your filter does and make sure that it does a better job on the first 10 documents than when we don't use your filter.

**Note:** If you get an error when trying to knit that talks about "unicode" characters, this is caused by trying to print special characters (non-ascii). An easy fix is just to remove all non-ascii text. You can do this with the `encode` string method. For example:

Instead of: 

```{python, eval=F}
for entity in doc.ents:
    print(entity.text, entity.label_)
```

Do:

```{python, eval=F}
for entity in doc.ents:
    print(entity.text.encode('ascii', errors='ignore'), entity.label_)
```

**Note:** It can be fun to utilize machine learning and natural language processing, but that doesn't mean it is always the best solution! We could get rid of all of our filters and use regular expressions with much better results! We will demonstrate this in our solution.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- 1-2 sentences explaining what your filter does.
```


---

### Project 5 {#p05-190}

---

**Motivation:** Up until this point we've utilized bits and pieces of the `pandas` library to perform various tasks. In this project we will formally introduce `pandas` and `numpy`, and utilize their capabilities to solve data-driven problems.

**Context:** By now you'll have had some limited exposure to `pandas`. This is the first in a three project series that covers some of the main components of both the `numpy` and `pandas` libraries. We will take a two project intermission to learn about functions, and then continue.

**Scope:** python, pandas, numpy, DataFrames, Series, ndarrays, indexing

**Learning objectives:** 

```{block, type="bbox"}
- Distinguish the differences between numpy, pandas, DataFrames, Series, and ndarrays.
- Use numpy, scipy, and pandas to solve a variety of data-driven problems.
- Demonstrate the ability to read and write data of various formats using various packages.
- View and access data inside DataFrames, Series, and ndarrays.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/stackoverflow/unprocessed/2018.csv`

#### Questions

##### 1. Take a look at the [`pandas` docs](https://pandas.pydata.org/docs/reference/io.html). There are a _lot_ of formats that `pandas` has the ability to read. The most popular formats in this course are: csv (with commas or some other separator), excel, json, or some database. CSV is very prevalent, but it was not designed to work well with large amounts of data. Newer formats like parquet and feather are designed from the ground up to be efficient, and take advantage of special processor instruction set called SIMD. The benefits of using these formats can be significant. Let's do some experiments!

##### How much space do each of the following files take up on Scholar: `2018.csv`, `2018.parquet`, and `2018.feather`? How much smaller (as a percentage) is the parquet file than the csv? How much smaller (as a percentage) is the feather file than the csv? Use f-strings to format the percentages.

##### Time reading in the following files: `2018.csv`, `2018.parquet`, and `2018.feather`. How much faster (as a percentage) is reading the parquet file than the csv? How much faster (as a percentage) is reading the feather file than the csv? Use f-strings to format the percentages.

##### To time a piece of code, you can use the `block-timer` package:

```{python, eval=F}
from block_timer.timer import Timer

with Timer(title="Using dict to declare a dict") as t1:
    my_dict = dict()

with Timer(title="Using {} to declare a dict") as t2:
    my_dict = {}

# or if you need more fine-tuned values
print(t1.elapsed)
print(t2.elapsed)
```

##### Read the `2018.csv` file into a `pandas` DataFrame called `my2018`. Time writing the contents of `my2018` to the following files: `2018.csv`, `2018.parquet`, and `2018.feather`. Write the files to your scratch directory: `/scratch/scholar/<username>`, where `<username>` is your username. How much faster (as a percentage) is reading the parquet file than the csv? How much faster (as a percentage) is reading the feather file than the csv? Use f-strings to format the percentages.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. A _method_ is just a function associated with an object or class. For example, `mean` is just a method of the `pandas` DataFrame:

```{python, eval=F}
# myDF is an object of class DataFrame
# mean is a method of the DataFrame class
myDF.mean()
```

##### In `pandas` there are two main methods used for indexing: [`loc` and `iloc`](https://pandas.pydata.org/docs/user_guide/indexing.html#different-choices-for-indexing). Use the column `Student` and indexing in `pandas` to calculate what percentage of respondents are students and not students. Consider the respondent to be a student if the `Student` column is anything but "No". Create a new DataFrame called `not_students` that is a subset of the original dataset _without_ students. 

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. In `pandas`, if you were to isolate a single column using indexing, like this:

```{python, eval=F}
myDF.loc[:, "Student"]
```

##### The result would be a `pandas` Series. A Series is the 1-dimensional equivalent of a DataFrame. 

```{python, eval=F}
type(myDF.loc[:, "Student"]) # pandas.core.series.Series
```

##### `pandas` and `numpy` make it very easy to convert between a Series, ndarray, and list. [Here](https://miro.medium.com/max/1400/1*rv1JADavAhDKN4-3iM7phQ.png) is a very useful graphic to highlight how to do this. Look at the `DevType` column in `not_students`. As you can see, a single value may contain a list of semi-colon-separated professions. Create a list with a unique group of all the possible professions. Consider each semi-colon-separated value a profession. How many professions are there? 

##### It looks like somehow the profession "Student" got in there even though we filtered by the `Student` column. Use `myDF` to get a subset of our data for which the respondents replied "No" to `Student`, yet put "Student" as a `DevType`. How many respondents are in that subset?

**Hint:** See [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing).

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The number of professions there are.
- The number of respondents that replied "No" to `Student`, yet put "Student" as the `DevType`.
```

##### 4. As you can see, while perhaps a bit more strict, indexing in `pandas` is not that much more difficult than indexing in R. While not always necessary, remembering to put ":" to indicate "all columns" or "all rows" makes life easier. In addition, remembering to put parentheses around logical groupings is also a good thing. Practice makes perfect! Randomly select 100 males and 100 females. How many of each sample is in each `Age` category? (_Do not_ use the `sample` method yet, but instead use numeric indexing and `random`)

```{python}
import random

print(f"A random integer between 1 and 100 is {random.randint(1, 101)}")
```

##### It would be nice to visualize these results. `pandas` Series have some built in methods to create plots. Use [this] method to generate a bar plot for both males and females. How do they compare?

**Hint:** You may need to import `matplotlib` in order to display the graphic:

```{python, eval=F}
import matplotlib.pyplot as plt 

# female barplot code here
plt.show()

# male barplot code here
plt.show()
```

**Hint:** Once you have your male and female DataFrames, the `value_counts` method found [here](https://pandas.pydata.org/docs/reference/series.html#computations-descriptive-stats) may be particularly useful.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. `pandas` really helps out when it comes to working with data in Python. This is really cool dataset, use your newfound skills to do a mini-analysis. Your mini-analysis should include 1 or more graphics, along with some interesting observation you made while exploring the data.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- A graphic.
- 1-2 sentences explaining your interesting observation and graphic.
```

---

## STAT 29000

### Topics

The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration.

|Language|Project #|Name|Topics|
|--------|---------|----|------|
|Python|1|Web scraping: part I|xml, lxml, pandas,  etc.|
|Python|2|Web scraping: part II|requests, functions, xml, loops, if statements, etc.|
|Python|3|Web scraping: part III|selenium, lxml, lists, pandas, etc.|
|Python|4|Web scraping: part IV|requests, beautifulsoup4, lxml, selenium, xml, cronjobs, loops, etc.|
|Python|5|Web scraping: part V|web scraping + related topics|
|Python|6|Plotting in Python: part I|ways to plot in Python, more work with pandas, etc.|
|Python|7|Plotting in Python: part II|ways to plot in Python, more work with pandas, etc.|
|Python|8|Writing scripts: part I|how to write scripts in Python, more work with pandas, matplotlib, etc.|
|Python|9|Writing scripts: part II|how to write scripts in Python, argparse, more work with pandas, matplotlib, etc.|
|R|10|ggplot: part I|ggplot basics|
|R|11|ggplot: part II|more ggplot|
|R|12|tidyverse & data.table: part I|data wrangling and computation using tidyverse packages and data.table|
|R|13|tidyverse & data.table: part II|data wrangling and computation using tidyverse packages and data.table|
|R|14|tidyverse & data.table: part III|data wrangling and computation using tidyverse packages and data.table|

### Project 1 {#p01-290}

---

**Motivation:** Extensible Markup Language or XML is a very important file format for storing structured data. Even though formats like JSON, and csv tend to be more prevalent, many, many legacy systems still use XML, and it remains an appropriate format for storing complex data. In fact, JSON and csv are quickly becoming less relevant as new formats and serialization methods like [parquet](https://arrow.apache.org/faq/) and [protobufs](https://developers.google.com/protocol-buffers) are becoming more common.

**Context:** In previous semesters we've explored XML. In this project we will refresh our skills and, rather than exploring XML in R, we will use the `lxml` package in Python. This is the first project in a series of 5 projects focused on web scraping in R and Python.

**Scope:** python, XML

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Match XML terms to sections of XML demonstrating working knowledge.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/apple/health/watch_dump.xml`

#### Resources

We realize that for many of you this is a big "jump" right into Python. Don't worry! Python is a very intuitive language with a clean syntax. It is easy to read and write. We will do our very best to keep things as straightforward as possible, especially in the early learning stages of the class.

We will be actively updating the examples book with videos and more examples throughout the semester. Ask a question in Piazza and perhaps we will add an example straight to the book to help out.

Some potentially useful resources for the semester include:

- The STAT 19000 projects. We are easing 19000 students into Python and will post solutions each week. It would be well worth 10 minutes to look over the questions and solutions each week.
- [Here](https://towardsdatascience.com/cheat-sheet-for-python-dataframe-r-dataframe-syntax-conversions-450f656b44ca) is a decent cheat sheet that helps you quickly get an idea of how to do something you know how to do in R, in Python.
- [The Examples Book](https://thedatamine.github.io/the-examples-book/) -- updating daily with more examples and videos. Be sure to click on the "relevant topics" links as we try to point you to topics with examples that should be particularly useful to solve the problems we assign.

#### Questions

**Important note:** It would be well worth your time to read through the [xml section](#xml) of the book, as well as take the time to work through [pandas 10 minute intro](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html).

##### 1. A good first step when working with XML is to get an idea how your document is structured. Normally, there should be good documentation that spells this out for you, but it is good to know what to do when you _don't_ have the documentation. Start by finding the "root" node. What is the name of the root node of the provided dataset?

**Hint:** Make sure to import the `lxml` package first:

```{python, eval=F}
from lxml import etree
```

Here are two videos about running Python in RStudio:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_nhkygxg9)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_tdz3wmim)

and here is a video about XML scraping in Python:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1ywsbxyp)

**Relevant topics:** [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from lxml import etree

tree = etree.parse("/class/datamine/data/apple/health/watch_dump.xml")

tree.xpath("/*")[0].tag
```

##### 2. Remember, XML can be nested. In question (1) we figured out what the root node was called. What are the names of the next "tier" of elements?

**Hint:** Now that we know the root node, you could use the root node name as a part of your xpath expression.

**Hint:** As you may have noticed in question (1) the `xpath` method returns a list. Sometimes this list can contain many repeated tag names. Since our goal is to see the names of the second "tier" elements, you could convert the resulting `list` to a `set` to quickly see the unique list as `set`'s only contain unique values.

**Relevant topics:** [for loops](#p-for-loops), [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
set([x.tag for x in tree.xpath("/HealthData/*")])
```

##### 3. Continue to explore each "tier" of data until there isn't any left. Name the "full paths" of all of the "last tier" tags.

**Hint:** Let's say a "last tier" tag is just a path where there are no more nested elements. For example, `/HealthData/Workout/WorkoutRoute/FileReference` is a "last tier" tag. If you try and get the nested elements for it, they don't exist:

```{python, eval=F}
tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")
```

**Hint:** Here are 3 of the 7 "full paths":

````
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Record/MetadataEntry
/HealthData/ActivitySummary
````

**Relevant topics:** [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
print(set([x.tag for x in tree.xpath("/HealthData/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/ActivitySummary/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/MetadataEntry/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEvent/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEntry/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/MetadataEntry/*")]))
```

```{block, type="solution"}
/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Workout/WorkoutRoute/MetadataEntry
/HealthData/Record/MetadataEntry
/HealthData/Workout/WorkoutEvent
/HealthData/Workout/WorkoutEntry
/HealthData/ActivitySummary
```

##### 4. At this point in time you may be asking yourself "but where is the data"? Depending on the structure of the XML file, the data could either be between tags like:

```{html, eval=F}
<some_tag>mydata</some_tag>
```

##### Or, it could be in an attribute:

```{html, eval=F}
<question answer="tac">What is cat spelled backwards?</question>
```

##### Collect the "ActivitySummary" data, and convert the list of dicts to a `pandas` DataFrame. The following is an example of converting a list of dicts to a `pandas` DataFrame called `myDF`:

```{python, eval=F}
import pandas as pd

list_of_dicts = []
list_of_dicts.append({'columnA': 1, 'columnB': 2})
list_of_dicts.append({'columnB': 4, 'columnA': 1}) 

myDF = pd.DataFrame(list_of_dicts)
```

**Hint:** It is important to note that an element's "attrib" attribute looks and feels like a `dict`, but it is actually a `lxml.etree._Attrib`. If you try to convert a list of `lxml.etree._Attrib` to a `pandas` DataFrame, it will not work out as you planned. Make sure to first convert each `lxml.etree._Attrib` to a `dict` before converting to a DataFrame. You can do so like:

```{python, eval=F}
# this will convert a single `lxml.etree._Attrib` to a dict
my_dict = dict(my_lxml_etree_attrib)
```

**Relevant topics:** [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
dat = tree.xpath("/HealthData/ActivitySummary")
list_of_dicts = []

for e in dat:
    list_of_dicts.append(dict(e.attrib))
    
myDF = pd.DataFrame(data=list_of_dicts)
myDF.sort_values(['activeEnergyBurned'], ascending=False).head()
```

##### 5. `pandas` is a Python package that provides the DataFrame and Series classes. A DataFrame is very similar to a data.frame in R and can be used to manipulate the data within very easily. A Series is the class that handles a single column of a DataFrame. Go through the [pandas in 10 minutes](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) page from the official documentation. Sort, find, and print the top 5 rows of data based on the "activeEnergyBurned" column. 

**Relevant topics:** [pandas](#p-pandas), [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
# could be anything
```

---

### Project 2 {#p02-290}

---

**Motivation:** Web scraping is is the process of taking content off of the internet. Typically this goes hand-in-hand with parsing or processing the data. Depending on the task at hand, web scraping can be incredibly simple. With that being said, it can quickly become difficult. Typically, students find web scraping fun and empowering.   

**Context:** In the previous project we gently introduced XML and xpath expressions. In this project, we will learn about web scraping, scrape data from The New York Times, and parse through our newly scraped data using xpath expressions.  

**Scope:** python, web scraping, xml

**Learning objectives:** [html](#html)

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

You will be extracting your own data from online in this project. There is no base dataset.

#### Questions

##### 1. The New York Times is one of the most popular newspapers in the United States. Open a modern browser (preferably Firefox or Chrome), and navigate to https://nytimes.com. 

##### By the end of this project you will be able to scrape some data from this website! The first step is to explore the structure of the website. You can either right click and click on "view page source", which will pull up a page full of HTML used to render the page. Alternatively, if you want to focus on a single element, an article title, for example, right click on the article title and click on "inspect element". This will pull up an inspector that allows you to see portions of the HTML.

##### Click around the website and explore the HTML however you see fit. Open a few front page articles and notice how most articles start with a bunch of really important information, namely: an article title, summary, picture, picture caption, picture source, author portraits, authors, and article datetime.

##### For example:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

![](./images/nytimes_image.jpg)

##### Copy and paste the **h1** element (in its entirety) containing the article title (for the article provided) in an HTML code chunk. Do the same for the same article's summary.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_eew0i16y)

**Relevant topics:** [html](#html)

```{block, type="bbox"}
**Item(s) to submit:**

- 2 code chunks containing the HTML requested.
```

##### 2. In question (1) we copied two elements of an article. When scraping data from a website, it is important to continually consider the patterns in the structure. Specifically, it is important to consider whether or not the defining characteristics you use to parse the scraped data will continue to be in the same format for _new_ data. What do I mean by defining characterstic? I mean some combination of tag, attribute, and content from which you can isolate the data of interest. 

##### For example, given a link to a new nytimes article, do you think you could isolate the article title by using the `id="link-4686dc8b"` attribute of the **h1** tag? Maybe, or maybe not, but it sure seems like "link-4686dc8b" might be unique to the article and not able to be used given a new article.

##### Write an xpath expression to isolate the article title, and another xpath expression to isolate the article summary. 

**Important note:** You do _not_ need to test your xpath expression yet, we will be doing that shortly.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath)

```{block, type="bbox"}
**Item(s) to submit:**

- Two xpath expressions in an HTML code chunk. 
```

##### 3. Use the `requests` package to scrape the webpage containing our article from questions (1) and (2). Use the `lxml.html` package and the `xpath` method to test out your xpath expressions from question (2). Did they work? Print the content of the elements to confirm.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_qw2xb058)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. Here are a list of article links from https://nytimes.com:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html

https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html

##### Write a function called `get_article_and_summary` that accepts a string called `link` as an argument, and returns both the article title and summary. Test `get_article_and_summary` out on each of the provided links:

```{python, eval=F}
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title: {title}, Summary: {summary}')
```

**Hint:** The first line of your function should look like this:

`def get_article_and_summary(myURL: str) -> (str, str):`

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_jrtrt5fo)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_7yhabkeg)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. In question (1) we mentioned a myriad of other important information given at the top of most New York Times articles. Choose **one** other listed pieces of information and copy, paste, and update your solution to question (4) to scrape and return those chosen pieces of information.

**Important note:** If you choose to scrape non-textual data, be sure to return data of an appropriate type. For example, if you choose to scrape one of the images, either print the image or return a PIL object.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

---


### Project 3 {#p03-290}

---

**Motivation:** Web scraping takes practice, and it is important to work through a variety of common tasks in order to know how to handle those tasks when you next run into them. In this project, we will use a variety of scraping tools in order to scrape data from https://trulia.com. 

**Context:** In the previous project, we got our first taste at actually scraping data from a website, and using a parser to extract the information we were interested in. In this project, we will introduce some tasks that will require you to use a tool that let's you interact with a browser, selenium. 

**Scope:** python, web scraping, selenium

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use the beautifulsoup4 package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Visit https://trulia.com. Many websites have a similar interface, i.e. a bold and centered search bar for a user to interact with. Using `selenium` write Python code that that first finds the `input` element, and then types "West Lafayette, IN" followed by an emulated "Enter/Return". Confirm you code works by printing the url after that process completes.

**Hint:** You will want to use `time.sleep` to pause a bit after the search so the updated url is returned.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml)  

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Use your code from question (1) to test out the following queries:

- West Lafayette, IN (City, State)
- 47906 (Zip)
- 4505 Kahala Ave, Honolulu, HI 96816 (Full address)

##### If you look closely you will see that there are patterns in the url. For example, the following link would probably bring up homes in Crawfordsville, IN: https://trulia.com/IN/Crawfordsville. With that being said, if you only had a zip code, like 47933, it wouldn't be easy to guess https://www.trulia.com/IN/Crawfordsville/47933/, hence, one reason why the search bar is useful.

##### If you used xpath expressions to complete question (1), instead use a [different method](https://selenium-python.readthedocs.io/locating-elements.html#locating-elements) to find the `input` element. If you used a different method, use xpath expressions to complete question (1).

**Relevant topics:** [selenium](#p-selenium), [xml](#xml) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. Let's call the page after a city/state or zipcode search a "sales page". For example:

![](./images/trulia.png)

##### Use `requests` to scrape the entire page: https://www.trulia.com/IN/West_Lafayette/47906/. Use `lxml.html` to parse the page and get all of the `image` elements that make up the house pictures on the left side of the website.

**Important note:** Make sure you are actually scraping what you think you are scraping! Try printing your html to confirm it has the content you think it should have:

```{python, eval=F}
response = requests.get(...)
print(response.text)
```

**Hint:** Are you human? Depends. Sometimes if you add a header to your request, it won't ask you if you are human. Let's pretend we are Firefox:

```{python, eval=F}
my_headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get(..., headers=my_headers)
```

##### Okay, after all of that work you may have discovered that only a few images have actually been scraped. If you cycle through all of the `img` elements and try to print the value of the `src` attribute, this will be clear:

```{python, eval=F}
for element in elements:
    print(element.attrib.get("src"))
```

##### This is because the webpage is not immediately, _completely_ loaded. This is a common website behavior to make things appear faster. If you pay close to when you load https://www.trulia.com/IN/Crawfordsville/47933/, and you quickly scroll down, you will see images being to render all of the way, slowly. What we need to do to fix this, is use `selenium` to behave like a human and scroll prior to scraping the page! Try using the following code to slowly scroll down the page before finding the elements:

```{python, eval=F}
# driver setup and get the url

# Needed to get the window size set right and scroll in headless mode
height = driver.execute_script('return document.body.scrollHeight')
driver.set_window_size(1080,height+100)

def scroll(driver, scroll_point):  
    driver.execute_script(f'window.scrollTo(0, {scroll_point});')
    time.sleep(5) 
    
scroll(driver, scroll_height/4)
scroll(driver, scroll_height/4*2)
scroll(driver, scroll_height/4*3)
scroll(driver, scroll_height)

# find_elements_by_*
```

**Hint:** At the time of writing there should be about 86 links to images of homes.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. Write a function called `avg_house_cost` that accepts a zip code as an argument, and returns the average cost of the first page of homes. Now, to make this a more meaningful statistic, filter for "3+" beds and _then_ find the average. Test `avg_house_cost` out on the following zip codes and print the average costs. 

**Important note:** Use `selenium` to "click" on the "3+ beds" filter.

**Hint:** If you get an error that tells you `button` is not clickable because it is covered by an `li` element, try clicking on the `li` element instead. 

**Hint:** You will want to wait a solid 10-15 seconds for the sales page to load before trying to select or click on anything.

**Hint:** Your results may end up including prices for "Homes Near \<ZIPCODE\>". This is okay. Even better if you manage to remove those results. If you _do_ choose to remove those results, take a look at the `data-testid` attribute with value `search-result-list-container`. Perhaps only selecting the children of the first element will get the desired outcome.

**Hint:** You can use the following code to remove the non-numeric text from a string, and then convert to an integer:

```{python, eval=F}
import re

int(re.sub("[^0-9]", "", "removenon45454_numbers$"))
```

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops), [functions](#p-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. Get creative. Either add an interesting feature to your function from (4), or use `matplotlib` to generate some sort of accompanying graphic with your output. Make sure to explain what your additions do.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

---

### Project 4 {#p04-290}

---

**Motivation:** In this project we will continue to hone your web scraping skills, introduce you to some "gotchas", and give you a little bit of exposure to a powerful tool called cron.

**Context:** We are in the second to last project focused on web scraping. This project will introduce some supplementary tools that work well with web scraping: cron, sending emails from Python, etc. 

**Scope:** python, web scraping, selenium, cron

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use the beautifulsoup4 package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Check out the following website: https://project4.tdm.wiki

##### Use `selenium` to scrape and print the 6 colors of pants offered.

**Hint:** You _may_ have to interact with the webpage for certain elements to render.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Websites are updated frequently. You can imagine a scenario where a change in a website is a sign that there is more data available, or that something of note has happened. This is a fake website designed to help students emulate real changes to a website. Specifically, there is one part of the website that has two possible states (let's say, state `A` and state `B`). Upon refreshing the website, or scraping the website again, there is an $x\%$ chance that the website will be in state `A` and a $1-x\%$ chance the website will be in state `B`.

##### Describe the two states (the thing that changes), and scrape the website enough to estimate $x$.

**Hint:** You _will_ need to interact with the website to "see" the change.

**Hint:** Since we are just asking about a state, and not any specific element, you could use the `page_source` attribute of the `selenium` driver to scrape the entire page instead of trying to use xpath expressions to find a specific element.

**Hint:** Your estimate of $x$ does not need to be perfect.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- What state `A` and `B` represent.
- An estimate for `x`.
```

##### 3. Dig into the changing "thing" from question (2). What specifically is changing? Use selenium and xpath expressions to scrape and print the content. What are the two possible values for the content? 

**Hint:** `parent::` and `following-sibling::` may be useful [xpath axes](https://www.w3schools.com/xml/xpath_axes.asp) to use.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example using `following-sibling::`](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. The following code allows you to send an email using Python from your Purdue email account. Replace the username and password with your own information and send a test email to yourself to ensure that it works.

**Important note:** Do **NOT** include your password in your homework submission. Any time you need to type your password in you final submission just put something like "SUPERSECRETPASSWORD" or "MYPASSWORD".

```{python, eval=F}
def send_purdue_email(my_purdue_email, my_password, to):
    import smtplib, ssl
    
    port = 587  # For starttls
    smtp_server = "smtp.purdue.edu"
    sender_email = my_purdue_email
    receiver_email = to
    password = my_password
    message = """\
    Subject: Test subject
    
    This is the email body."""
    
    context = ssl.create_default_context()
    with smtplib.SMTP(smtp_server, port) as server:
        server.ehlo()  # Can be omitted
        server.starttls(context=context)
        server.ehlo()  # Can be omitted
        server.login(sender_email, password)
        server.sendmail(sender_email, receiver_email, message)
        
# this sends an email from kamstut@purdue.edu to mdw@purdue.edu
# replace supersecretpassword with your own password
# do NOT include your password in your homework submission.
send_purdue_email("kamstut@purdue.edu", "supersecretpassword", "mdw@purdue.edu")
```

**Relevant topics:** [functions](#p-writing-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- Screenshot showing your received the email.
```

##### 5. The following is the content of a new Python script called `is_in_stock.py`:

```{python, eval=F}
#!/usr/bin/env python3

def send_purdue_email(my_purdue_email, my_password, to):
    import smtplib, ssl
    
    port = 587  # For starttls
    smtp_server = "smtp.purdue.edu"
    sender_email = my_purdue_email
    receiver_email = to
    password = my_password
    message = """\
    Subject: Test subject
    
    This is the email body."""
    
    context = ssl.create_default_context()
    with smtplib.SMTP(smtp_server, port) as server:
        server.ehlo()  # Can be omitted
        server.starttls(context=context)
        server.ehlo()  # Can be omitted
        server.login(sender_email, password)
        server.sendmail(sender_email, receiver_email, message)
        
def main():
    # scrape element from question 3
    
    # does the text indicate it is in stock?
    
    # if yes, send email to yourself telling you it is in stock.
    
    # otherwise, gracefully end script using the "pass" Python keyword

if __name__ == "__main__":
    main()
```

##### First, make a copy of the script in your `$HOME` directory:

```{bash, eval=F}
cp /class/datamine/data/scraping/is_in_stock.py $HOME/is_in_stock.py
```

##### Next, change the permissions of the `$HOME/is_in_stock.py` script so only YOU can read, write, and execute it:

```{bash, eval=F}
chmod +rwx $HOME/is_in_stock.py
```

##### The script should now appear in RStudio, in your home directory. Open the script and fill in the `main` function as indicated by the comments. We want the script to scrape to see whether the pants from question 3 are in stock or not.

##### A cron job is a task that runs at a certain interval. Create a cron job that runs your script, `$HOME/is_in_stock.py` every 5 minutes. Wait 10-15 minutes and verify that it is working properly.

**Relevant topics:** [cron](#cron), [crontab guru](https://crontab.guru)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The content of your cron job in a bash code chunk.
- The content of your `is_in_stock.py` script.
```

---


## STAT 39000

### Topics

The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration.

|Language|Project #|Name|Topics|
|--------|---------|----|------|
|Python|1|Web scraping: part I|xml, lxml, pandas,  etc.|
|Python|2|Web scraping: part II|requests, functions, xml, loops, if statements, etc.|
|Python|3|Web scraping: part III|selenium, lxml, lists, pandas, etc.|
|Python|4|Web scraping: part IV|requests, beautifulsoup4, lxml, selenium, xml, cronjobs, loops, etc.|
|Python|5|Web scraping: part V|web scraping + related topics|
|Python|6|Plotting in Python: part I|ways to plot in Python, more work with pandas, etc.|
|Python|7|Plotting in Python: part II|ways to plot in Python, more work with pandas, etc.|
|Python|8|Writing scripts: part I|how to write scripts in Python, more work with pandas, matplotlib, etc.|
|Python|9|Writing scripts: part II|how to write scripts in Python, argparse, more work with pandas, matplotlib, etc.|
|R|10|ggplot: part I|ggplot basics|
|R|11|ggplot: part II|more ggplot|
|R|12|tidyverse & data.table: part I|data wrangling and computation using tidyverse packages and data.table, maybe some slurm?|
|R|13|tidyverse & data.table: part II|data wrangling and computation using tidyverse packages and data.table, maybe some slurm?|
|R|14|tidyverse & data.table: part III|data wrangling and computation using tidyverse packages and data.table, maybe some slurm?|

### Project 1 {#p01-390}

---

**Motivation:** Extensible Markup Language or XML is a very important file format for storing structured data. Even though formats like JSON, and csv tend to be more prevalent, many, many legacy systems still use XML, and it remains an appropriate format for storing complex data. In fact, JSON and csv are quickly becoming less relevant as new formats and serialization methods like [parquet](https://arrow.apache.org/faq/) and [protobufs](https://developers.google.com/protocol-buffers) are becoming more common.

**Context:** In previous semesters we've explored XML. In this project we will refresh our skills and, rather than exploring XML in R, we will use the `lxml` package in Python. This is the first project in a series of 5 projects focused on web scraping in R and Python.

**Scope:** python, XML

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Match XML terms to sections of XML demonstrating working knowledge.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/apple/health/watch_dump.xml`

#### Resources

We realize that it may be a while since you've used Python. That's okay! We are going to be taking things at a much more reasonable pace than Spring 2020.

Some potentially useful resources for the semester include:

- The STAT 19000 projects. We are easing 19000 students into Python and will post solutions each week. It would be well worth 10 minutes to look over the questions and solutions each week.
- [Here](https://towardsdatascience.com/cheat-sheet-for-python-dataframe-r-dataframe-syntax-conversions-450f656b44ca) is a decent cheat sheet that helps you quickly get an idea of how to do something you know how to do in R, in Python.
- [The Examples Book](https://thedatamine.github.io/the-examples-book/) -- updating daily with more examples and videos. Be sure to click on the "relevant topics" links as we try to point you to topics with examples that should be particularly useful to solve the problems we assign.

#### Questions

**Important note:** It would be well worth your time to read through the [xml section](#xml) of the book, as well as take the time to work through [pandas 10 minute intro](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html).

##### 1. A good first step when working with XML is to get an idea how your document is structured. Normally, there should be good documentation that spells this out for you, but it is good to know what to do when you _don't_ have the documentation. Start by finding the "root" node. What is the name of the root node of the provided dataset?

**Hint:** Make sure to import the `lxml` package first:

```{python, eval=F}
from lxml import etree
```

Here are two videos about running Python in RStudio:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_nhkygxg9)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_tdz3wmim)

and here is a video about XML scraping in Python:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1ywsbxyp)

**Relevant topics:** [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from lxml import etree

tree = etree.parse("/class/datamine/data/apple/health/watch_dump.xml")

tree.xpath("/*")[0].tag
```

##### 2. Remember, XML can be nested. In question (1) we figured out what the root node was called. What are the names of the next "tier" of elements?

**Hint:** Now that we know the root node, you could use the root node name as a part of your xpath expression.

**Hint:** As you may have noticed in question (1) the `xpath` method returns a list. Sometimes this list can contain many repeated tag names. Since our goal is to see the names of the second "tier" elements, you could convert the resulting `list` to a `set` to quickly see the unique list as `set`'s only contain unique values.

**Relevant topics:** [for loops](#p-for-loops), [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
set([x.tag for x in tree.xpath("/HealthData/*")])
```

##### 3. Continue to explore each "tier" of data until there isn't any left. Name the "full paths" of all of the "last tier" tags.

**Hint:** Let's say a "last tier" tag is just a path where there are no more nested elements. For example, `/HealthData/Workout/WorkoutRoute/FileReference` is a "last tier" tag. If you try and get the nested elements for it, they don't exist:

```{python, eval=F}
tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")
```

**Hint:** Here are 3 of the 7 "full paths":

````
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Record/MetadataEntry
/HealthData/ActivitySummary
````

**Relevant topics:** [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
print(set([x.tag for x in tree.xpath("/HealthData/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/ActivitySummary/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/MetadataEntry/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEvent/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEntry/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/MetadataEntry/*")]))
```

```{block, type="solution"}
/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Workout/WorkoutRoute/MetadataEntry
/HealthData/Record/MetadataEntry
/HealthData/Workout/WorkoutEvent
/HealthData/Workout/WorkoutEntry
/HealthData/ActivitySummary
```

##### 4. At this point in time you may be asking yourself "but where is the data"? Depending on the structure of the XML file, the data could either be between tags like:

```{html, eval=F}
<some_tag>mydata</some_tag>
```

##### Or, it could be in an attribute:

```{html, eval=F}
<question answer="tac">What is cat spelled backwards?</question>
```

##### Collect the "ActivitySummary" data, and convert the list of dicts to a `pandas` DataFrame. The following is an example of converting a list of dicts to a `pandas` DataFrame called `myDF`:

```{python, eval=F}
import pandas as pd

list_of_dicts = []
list_of_dicts.append({'columnA': 1, 'columnB': 2})
list_of_dicts.append({'columnB': 4, 'columnA': 1}) 

myDF = pd.DataFrame(list_of_dicts)
```

**Hint:** It is important to note that an element's "attrib" attribute looks and feels like a `dict`, but it is actually a `lxml.etree._Attrib`. If you try to convert a list of `lxml.etree._Attrib` to a `pandas` DataFrame, it will not work out as you planned. Make sure to first convert each `lxml.etree._Attrib` to a `dict` before converting to a DataFrame. You can do so like:

```{python, eval=F}
# this will convert a single `lxml.etree._Attrib` to a dict
my_dict = dict(my_lxml_etree_attrib)
```

**Relevant topics:** [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
dat = tree.xpath("/HealthData/ActivitySummary")
list_of_dicts = []

for e in dat:
    list_of_dicts.append(dict(e.attrib))
    
myDF = pd.DataFrame(data=list_of_dicts)
myDF.sort_values(['activeEnergyBurned'], ascending=False).head()
```

##### 5. `pandas` is a Python package that provides the DataFrame and Series classes. A DataFrame is very similar to a data.frame in R and can be used to manipulate the data within very easily. A Series is the class that handles a single column of a DataFrame. Go through the [pandas in 10 minutes](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) page from the official documentation. Sort, find, and print the top 5 rows of data based on the "activeEnergyBurned" column. 

**Relevant topics:** [pandas](#p-pandas), [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
# could be anything
```

---

### Project 2 {#p02-390}

---

**Motivation:** Web scraping is is the process of taking content off of the internet. Typically this goes hand-in-hand with parsing or processing the data. Depending on the task at hand, web scraping can be incredibly simple. With that being said, it can quickly become difficult. Typically, students find web scraping fun and empowering.   

**Context:** In the previous project we gently introduced XML and xpath expressions. In this project, we will learn about web scraping, scrape data from The New York Times, and parse through our newly scraped data using xpath expressions.  

**Scope:** python, web scraping, xml

**Learning objectives:** [html](#html)

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

You will be extracting your own data from online in this project. There is no base dataset.

#### Questions

##### 1. The New York Times is one of the most popular newspapers in the United States. Open a modern browser (preferably Firefox or Chrome), and navigate to https://nytimes.com. 

##### By the end of this project you will be able to scrape some data from this website! The first step is to explore the structure of the website. You can either right click and click on "view page source", which will pull up a page full of HTML used to render the page. Alternatively, if you want to focus on a single element, an article title, for example, right click on the article title and click on "inspect element". This will pull up an inspector that allows you to see portions of the HTML.

##### Click around the website and explore the HTML however you see fit. Open a few front page articles and notice how most articles start with a bunch of really important information, namely: an article title, summary, picture, picture caption, picture source, author portraits, authors, and article datetime.

##### For example:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

![](./images/nytimes_image.jpg)

##### Copy and paste the **h1** element (in its entirety) containing the article title (for the article provided) in an HTML code chunk. Do the same for the same article's summary.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_eew0i16y)

**Relevant topics:** [html](#html)

```{block, type="bbox"}
**Item(s) to submit:**

- 2 code chunks containing the HTML requested.
```

##### 2. In question (1) we copied two elements of an article. When scraping data from a website, it is important to continually consider the patterns in the structure. Specifically, it is important to consider whether or not the defining characteristics you use to parse the scraped data will continue to be in the same format for _new_ data. What do I mean by defining characterstic? I mean some combination of tag, attribute, and content from which you can isolate the data of interest. 

##### For example, given a link to a new nytimes article, do you think you could isolate the article title by using the `id="link-4686dc8b"` attribute of the **h1** tag? Maybe, or maybe not, but it sure seems like "link-4686dc8b" might be unique to the article and not able to be used given a new article.

##### Write an xpath expression to isolate the article title, and another xpath expression to isolate the article summary. 

**Important note:** You do _not_ need to test your xpath expression yet, we will be doing that shortly.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath)

```{block, type="bbox"}
**Item(s) to submit:**

- Two xpath expressions in an HTML code chunk. 
```

##### 3. Use the `requests` package to scrape the webpage containing our article from questions (1) and (2). Use the `lxml.html` package and the `xpath` method to test out your xpath expressions from question (2). Did they work? Print the content of the elements to confirm.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_qw2xb058)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. Here are a list of article links from https://nytimes.com:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html

https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html

##### Write a function called `get_article_and_summary` that accepts a string called `link` as an argument, and returns both the article title and summary. Test `get_article_and_summary` out on each of the provided links:

```{python, eval=F}
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title: {title}, Summary: {summary}')
```

**Hint:** The first line of your function should look like this:

`def get_article_and_summary(myURL: str) -> (str, str):`

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_jrtrt5fo)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_7yhabkeg)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. In question (1) we mentioned a myriad of other important information given at the top of most New York Times articles. Choose **two** other listed pieces of information and copy, paste, and update your solution to question (4) to scrape and return those chosen pieces of information.

**Important note:** If you choose to scrape non-textual data, be sure to return data of an appropriate type. For example, if you choose to scrape one of the images, either print the image or return a PIL object.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

---

### Project 3 {#p03-390}

---

**Motivation:** Web scraping takes practice, and it is important to work through a variety of common tasks in order to know how to handle those tasks when you next run into them. In this project, we will use a variety of scraping tools in order to scrape data from https://trulia.com. 

**Context:** In the previous project, we got our first taste at actually scraping data from a website, and using a parser to extract the information we were interested in. In this project, we will introduce some tasks that will require you to use a tool that let's you interact with a browser, selenium. 

**Scope:** python, web scraping, selenium

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use the beautifulsoup4 package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Visit https://trulia.com. Many websites have a similar interface, i.e. a bold and centered search bar for a user to interact with. Using `selenium` write Python code that that first finds the `input` element, and then types "West Lafayette, IN" followed by an emulated "Enter/Return". Confirm you code works by printing the url after that process completes.

**Hint:** You will want to use `time.sleep` to pause a bit after the search so the updated url is returned.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml)  

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Use your code from question (1) to test out the following queries:

- West Lafayette, IN (City, State)
- 47906 (Zip)
- 4505 Kahala Ave, Honolulu, HI 96816 (Full address)

##### If you look closely you will see that there are patterns in the url. For example, the following link would probably bring up homes in Crawfordsville, IN: https://trulia.com/IN/Crawfordsville. With that being said, if you only had a zip code, like 47933, it wouldn't be easy to guess https://www.trulia.com/IN/Crawfordsville/47933/, hence, one reason why the search bar is useful.

##### If you used xpath expressions to complete question (1), instead use a [different method](https://selenium-python.readthedocs.io/locating-elements.html#locating-elements) to find the `input` element. If you used a different method, use xpath expressions to complete question (1).

**Relevant topics:** [selenium](#p-selenium), [xml](#xml) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. Let's call the page after a city/state or zipcode search a "sales page". For example:

![](./images/trulia.png)

##### Use `requests` to scrape the entire page: https://www.trulia.com/IN/West_Lafayette/47906/. Use `lxml.html` to parse the page and get all of the `image` elements that make up the house pictures on the left side of the website.

**Important note:** Make sure you are actually scraping what you think you are scraping! Try printing your html to confirm it has the content you think it should have:

```{python, eval=F}
response = requests.get(...)
print(response.text)
```

**Hint:** Are you human? Depends. Sometimes if you add a header to your request, it won't ask you if you are human. Let's pretend we are Firefox:

```{python, eval=F}
my_headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get(..., headers=my_headers)
```

##### Okay, after all of that work you may have discovered that only a few images have actually been scraped. If you cycle through all of the `img` elements and try to print the value of the `src` attribute, this will be clear:

```{python, eval=F}
for element in elements:
    print(element.attrib.get("src"))
```

##### This is because the webpage is not immediately, _completely_ loaded. This is a common website behavior to make things appear faster. If you pay close to when you load https://www.trulia.com/IN/Crawfordsville/47933/, and you quickly scroll down, you will see images being to render all of the way, slowly. What we need to do to fix this, is use `selenium` to behave like a human and scroll prior to scraping the page! Try using the following code to slowly scroll down the page before finding the elements:

```{python, eval=F}
# driver setup and get the url

# Needed to get the window size set right and scroll in headless mode
height = driver.execute_script('return document.body.scrollHeight')
driver.set_window_size(1080,height+100)

def scroll(driver, scroll_point):  
    driver.execute_script(f'window.scrollTo(0, {scroll_point});')
    time.sleep(5) 
    
scroll(driver, scroll_height/4)
scroll(driver, scroll_height/4*2)
scroll(driver, scroll_height/4*3)
scroll(driver, scroll_height)

# find_elements_by_*
```

**Hint:** At the time of writing there should be about 86 links to images of homes.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. Write a function called `avg_house_cost` that accepts a zip code as an argument, and returns the average cost of the first page of homes. Now, to make this a more meaningful statistic, filter for "3+" beds and _then_ find the average. Test `avg_house_cost` out on the following zip codes and print the average costs. 

**Important note:** Use `selenium` to "click" on the "3+ beds" filter.

**Hint:** If you get an error that tells you `button` is not clickable because it is covered by an `li` element, try clicking on the `li` element instead. 

**Hint:** You will want to wait a solid 10-15 seconds for the sales page to load before trying to select or click on anything.

**Hint:** Your results may end up including prices for "Homes Near \<ZIPCODE\>". This is okay. Even better if you manage to remove those results. If you _do_ choose to remove those results, take a look at the `data-testid` attribute with value `search-result-list-container`. Perhaps only selecting the children of the first element will get the desired outcome.

**Hint:** You can use the following code to remove the non-numeric text from a string, and then convert to an integer:

```{python, eval=F}
import re

int(re.sub("[^0-9]", "", "removenon45454_numbers$"))
```

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops), [functions](#p-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. Get creative. Either add an interesting feature to your function from (4), or use `matplotlib` to generate some sort of accompanying graphic with your output. Make sure to explain what your additions do.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

---

### Project 4 {#p04-290}

---

**Motivation:** In this project we will continue to hone your web scraping skills, introduce you to some "gotchas", and give you a little bit of exposure to a powerful tool called cron.

**Context:** We are in the second to last project focused on web scraping. This project will introduce some supplementary tools that work well with web scraping: cron, sending emails from Python, etc. 

**Scope:** python, web scraping, selenium, cron

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use the beautifulsoup4 package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Check out the following website: https://project4.tdm.wiki

##### Use `selenium` to scrape and print the 6 colors of pants offered.

**Hint:** You _may_ have to interact with the webpage for certain elements to render.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Websites are updated frequently. You can imagine a scenario where a change in a website is a sign that there is more data available, or that something of note has happened. This is a fake website designed to help students emulate real changes to a website. Specifically, there is one part of the website that has two possible states (let's say, state `A` and state `B`). Upon refreshing the website, or scraping the website again, there is an $x\%$ chance that the website will be in state `A` and a $1-x\%$ chance the website will be in state `B`.

##### Describe the two states (the thing that changes), and scrape the website enough to estimate $x$.

**Hint:** You _will_ need to interact with the website to "see" the change.

**Hint:** Since we are just asking about a state, and not any specific element, you could use the `page_source` attribute of the `selenium` driver to scrape the entire page instead of trying to use xpath expressions to find a specific element.

**Hint:** Your estimate of $x$ does not need to be perfect.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- What state `A` and `B` represent.
- An estimate for `x`.
```

##### 3. Dig into the changing "thing" from question (2). What specifically is changing? Use selenium and xpath expressions to scrape and print the content. What are the two possible values for the content? 

**Hint:** `parent::` and `following-sibling::` may be useful [xpath axes](https://www.w3schools.com/xml/xpath_axes.asp) to use.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example using `following-sibling::`](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. The following code allows you to send an email using Python from your Purdue email account. Replace the username and password with your own information and send a test email to yourself to ensure that it works.

**Important note:** Do **NOT** include your password in your homework submission. Any time you need to type your password in you final submission just put something like "SUPERSECRETPASSWORD" or "MYPASSWORD".

```{python, eval=F}
def send_purdue_email(my_purdue_email, my_password, to):
    import smtplib, ssl
    
    port = 587  # For starttls
    smtp_server = "smtp.purdue.edu"
    sender_email = my_purdue_email
    receiver_email = to
    password = my_password
    message = """\
    Subject: Test subject
    
    This is the email body."""
    
    context = ssl.create_default_context()
    with smtplib.SMTP(smtp_server, port) as server:
        server.ehlo()  # Can be omitted
        server.starttls(context=context)
        server.ehlo()  # Can be omitted
        server.login(sender_email, password)
        server.sendmail(sender_email, receiver_email, message)
        
# this sends an email from kamstut@purdue.edu to mdw@purdue.edu
# replace supersecretpassword with your own password
# do NOT include your password in your homework submission.
send_purdue_email("kamstut@purdue.edu", "supersecretpassword", "mdw@purdue.edu")
```

**Relevant topics:** [functions](#p-writing-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- Screenshot showing your received the email.
```

##### 5. The following is the content of a new Python script called `is_in_stock.py`:

```{python, eval=F}
#!/usr/bin/env python3

def send_purdue_email(my_purdue_email, my_password, to):
    import smtplib, ssl
    
    port = 587  # For starttls
    smtp_server = "smtp.purdue.edu"
    sender_email = my_purdue_email
    receiver_email = to
    password = my_password
    message = """\
    Subject: Test subject
    
    This is the email body."""
    
    context = ssl.create_default_context()
    with smtplib.SMTP(smtp_server, port) as server:
        server.ehlo()  # Can be omitted
        server.starttls(context=context)
        server.ehlo()  # Can be omitted
        server.login(sender_email, password)
        server.sendmail(sender_email, receiver_email, message)
        
def main():
    # scrape element from question 3
    
    # does the text indicate it is in stock?
    
    # if yes, send email to yourself telling you it is in stock.
    
    # otherwise, gracefully end script using the "pass" Python keyword

if __name__ == "__main__":
    main()
```

##### First, make a copy of the script in your `$HOME` directory:

```{bash, eval=F}
cp /class/datamine/data/scraping/is_in_stock.py $HOME/is_in_stock.py
```

##### Next, change the permissions of the `$HOME/is_in_stock.py` script so only YOU can read, write, and execute it:

```{bash, eval=F}
chmod +rwx $HOME/is_in_stock.py
```

##### The script should now appear in RStudio, in your home directory. Open the script and fill in the `main` function as indicated by the comments. We want the script to scrape to see whether the pants from question 3 are in stock or not.

##### A cron job is a task that runs at a certain interval. Create a cron job that runs your script, `$HOME/is_in_stock.py` every 5 minutes. Wait 10-15 minutes and verify that it is working properly.

**Relevant topics:** [cron](#cron), [crontab guru](https://crontab.guru)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The content of your cron job in a bash code chunk.
- The content of your `is_in_stock.py` script.
```

##### 6. Take a look at the byline of each pair of pants (the sentences starting with "Perfect for..."). Inspect the HTML. Try and scrape the text using xpath expressions like you normally would. What happens? Are you able to scrape it? Google around and come up with your best explanation of what is happening.

**Relevant topics:** pseudo elements

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- An explanation of what is happening.
```


---
