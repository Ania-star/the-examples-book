= Naive Bayes

Naive Bayes is a machine learning method for classification that makes simple ("naive") classifications for all the data points. While other, more complex methods might outshine for very complex data, Naive Bayes performs remarkably well despite its simplicity. There are many different algorithms one can use for Naive Bayes, yet they all operate on one simple assumption: that all the features are unrelated to each other. It finds the probability that any given data point is associated with any one of the classes, and then uses the class with the highest probability as the answer.

While Naive Bayes uses some notions from Bayes Theorem, you don't need to know much about Bayesianism to use Naive Bayes for machine learning.

== Common Applications

=== Common Problem Types

- Classification
- Tasks where "good enough" accuracy is ok
- Tasks where Curse of Dimensionality is a problem (Naive Bayes doesn't suffer from the Curse as much)
- Analysis where quick and computationally efficient predictions matter (such as real time speech processing)

== A Brief History

While Bayes Theorem itself has been around for a few hundred years, the origins of Naive Bayes are a bit obscure. The assumption that variables are independent was thought to be absurd and more complex alternatives were preferred, especially in the past century. However, as classification tasks became more common with the rise of artificial intelligence tasks in general, Naive Bayes (sometimes called Independence Bayes, Idiot's Bayes, etc) was given a second look and while there are certainly superior methods for complex data, the surprisingly simple and fast computation for classification that Naive Bayes offered was rediscovered, especially with the rise of machine learning in the past 30 years. For further reading, we suggest https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1751-5823.2001.tb00465.x[this paper].

== Code Examples

NOTE: All of the code examples are written in Python, unless otherwise noted.

=== Containers

TIP: These are code examples in the form of Jupyter notebooks running in a container that come with all the data, libraries, and code you'll need to run it. https://the-examples-book.com/starter-guides/data-engineering/containers/using-data-mine-containers[Click here to learn why you should be using containers, along with how to do so.]

TIP: Quickstart: https://docs.docker.com/get-docker/[Download Docker], then run the commands below in a terminal. 

[source,bash]
----
#pull container, only needs to be run once
docker pull ghcr.io/thedatamine/starter-guides:naive-bayes

#run container
docker run -p 8888:8888 -it ghcr.io/thedatamine/starter-guides:naive-bayes
----

Need help implementing any of this code? Feel free to reach out to mailto:datamine-help@purdue.edu[datamine-help@purdue.edu] and we can help!

== Resources

All resources are chosen by Data Mine staff to be of decent quality, and most if not all content is free. 

=== Websites

https://www.ibm.com/topics/naive-bayes[What Are Naive Bayes Classifiers? (IBM)]

https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c[Naive Bayes Classifier (Towards Data Science)]

https://www.turing.com/kb/an-introduction-to-naive-bayes-algorithm-for-beginners[An Introduction to Naive Bayes Algorithm for Beginners (Turing)]

=== Videos

https://www.youtube.com/watch?v=l3dZ6ZNFjo0[Naive Bayes Classifier | Naive Bayes Algorithm | Naive Bayes Classifier With Example | Simplilearn (Simplilearn, ~45 minutes)]

https://www.youtube.com/watch?v=O2L2Uv9pdDA[Naive Bayes, Clearly Explained!!! (StatQuest With Josh Starmer, ~15 minutes)]

https://www.youtube.com/watch?v=lFJbZ6LVxN8[The Math Behind Bayesian Classifiers Clearly Explained! (Normalized Nerd, ~12 minutes)]

https://www.youtube.com/watch?v=Q8l0Vip5YUw[Naive Bayes classifier: A friendly approach (Serrano.academy, ~20 minutes)]

=== Books

https://www.statlearning.com[Introduction to Statistical Learning (see Chapter 4.4.4)]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/uc5e95/alma99170206697801081[Data Algorithms (see Chapter 14)]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/uc5e95/alma99169167196901081[The Na√Øve Bayes Model for Unsupervised Word Sense Disambiguation Aspects Concerning Feature Selection]

=== Articles 

https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf[The Optimality of Naive Bayes]