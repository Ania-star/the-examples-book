= Transformers

If you interacted with ChatGPT in any way at all, you've interacted with AI that uses transformers (the "GPT" in ChatGPT stands for "Generative Pretrained **Transformer**"). But what is a transformer? Transformers are neural networks that utilize attention mechanisms (https://arxiv.org/abs/1706.03762[inspired by the famous paper published in 2017, "Attention is All You Need"]) and the encoder decoder framework. They build off the previous modeling leader (LSTM, a type of RNN) and outperform by a wide margin by being combined with transfer learning (the "Pretrained" portion in GPT). To summarize, the novelty of transformers lie in 3 key areas:

* Attention Mechanisms
* Transfer Learning
* The Encoder-Decoder Framework

If you will be working with LLM's (and by extension, transformers) you should learn about those 3 areas in particular.

== Large Language Models (LLM's)

https://openai.com/research/better-language-models[Large Language Models] today are almost all trained with using a transformer architecture, at least to some degree if not entirely. Large Language Models are designed and trained on large amounts of corpora with the intent of creating a general purpose model that can both understand and generate language. They utilize billions of parameters, massive computational resources, but can perform remarkably well.

LLM's (at least the most successful ones including OpenAI's ChatGPT, Google's Bard, Meta's LLaMa, etc) require so much development and computational resources so as to make their design more or less impossible for a small team to produce (hence why large corporate investment has been the promulgator of these models, and not a few clever data scientists at a startup). That said, if you wish to *build* transformers/LLM's, you ought to join one of the organizations already making them (or focus on making small niche models- at that point it might not be a *Large* Language Model however). 

If you seek to incorporate a LLM into your project, start with the API for whichever LLM you think is best. Most of them are both open source and free to use for any purpose (research or commercial in particular). Here are the links to some of the most well known ones:

* https://platform.openai.com/docs/guides/gpt
* https://ai.meta.com/llama/
* https://github.com/dsdanielpark/Bard-API
* https://huggingface.co/docs/transformers/model_doc/bloom

== How Can I Utilize Transformers In My Project?

Transformer architecture has seen use primarily in NLP, but also in https://towardsdatascience.com/transformer-in-cv-bbdb58bf335e[computer vision] and https://arxiv.org/abs/2212.04356[audio]. If you plan to utilize LLM's in any way, then you are almost certainly going to be interacting with a transformer today, even if indirectly. Since transformers are still relatively new, their application space is somewhat unclear and there is much research being done to understand their strengths, weaknesses, and applications.

== Resources

The content here is hand selected by Data Mine staff, and all of it is free for Purdue students (including the book links); most of it should be free for National Data Mine students as well (check your school's digital library resources for the books). 

=== Videos

https://www.youtube.com/watch?v=fjJOgb-E41w[Attention mechanism: overview (Google, ~5 minutes)]

https://www.youtube.com/watch?v=PSs6nxngL6k[Attention for Neural Networks, Clearly Explained!!! (StatQuest With Josh Starmer, ~15 minutes)]

https://www.youtube.com/watch?v=iDulhoQ2pro[Attention Is All You Need (Attention Is All You Need Authors, ~27 minutes)]

https://www.youtube.com/watch?v=PfuSVI7bKMs[Transfer Learning for Natural Language Processing (NLP) (Cloudera, ~2 minutes)]

https://www.youtube.com/watch?v=acxqoltilME[NLP Demystified 15: Transformers From Scratch + Pre-training and Transfer Learning With BERT/GPT (Future Mojo, ~2 hours)]

https://www.youtube.com/watch?v=LE3NfEULV6k[Transfer learning and Transformer models (ML Tech Talks) (Google, ~45 minutes)]

https://www.youtube.com/watch?v=L8HKweZIOmg[Sequence-to-Sequence (seq2seq) Encoder-Decoder Neural Networks, Clearly Explained!!! (StatQuest With Josh Starmer, ~17 minutes)]

https://www.youtube.com/watch?v=0_4KEb08xrE[Transformer models: Encoder-Decoders (Hugging Face, ~7 minutes)]

https://www.youtube.com/watch?v=jCrgzJlxTKg[Sequence To Sequence Learning With Neural Networks| Encoder And Decoder In-depth Intuition (Krish Naik, ~13 minutes)]

=== Websites

https://machinelearningmastery.com/the-attention-mechanism-from-scratch/[The Attention Mechanism from Scratch (Machine Learning Mastery)]

https://blog.floydhub.com/attention-mechanism/[Attention Mechanism (FloydHub Blog)]

https://www.cloudskillsboost.google/course_templates/537[Google Cloud]

https://www.geeksforgeeks.org/ml-attention-mechanism/[Geeks For Geeks]

=== Books

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/uc5e95/alma99170255082801081[Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf (Oâ€™Reilly, 2022)]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/uc5e95/alma99170320349901081[Transformers for machine learning: a deep dive (2022)]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/5imsd2/cdi_skillsoft_books24x7_bks000163945[Introduction to Transformers for NLP: With the Hugging Face Library and Models to Solve Problems (2022)]

=== Articles

https://arxiv.org/abs/1706.03762[Attention Is All You Need (2017)]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/5imsd2/cdi_doaj_primary_oai_doaj_org_article_698d17e12f3643bca29a5ee13007c75b[Spam Detection Using Bidirectional Transformers and Machine Learning Classifier Algorithms (2023)]

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/5imsd2/cdi_doaj_primary_oai_doaj_org_article_48022b59bf0d4fa88b70e7eeffa6891d[Categorizing Vaccine Confidence With a Transformer-Based Machine Learning Model: Analysis of Nuances of Vaccine Sentiment in Twitter Discourse (2021)]