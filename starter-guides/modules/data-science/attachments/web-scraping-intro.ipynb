{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e5fcce-3380-4298-934e-891249d7c604",
   "metadata": {},
   "source": [
    "# Web Scraping Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecfd0c-8b1f-4210-a7a7-f914b99b141a",
   "metadata": {},
   "source": [
    "Webscraping is the process of collecting data (information) from public websites that is then exported into an easier-to-read format. It can be done automatically or manually. Often, we are looking for external data sources that can be relevant to our learnings.\n",
    "\n",
    "### What are websites are made of?\n",
    "We can think of it this way. In order to build a house, we must first understand what materials are used in the construction.  In the same way to gather the relevant data from websites, we must first learn the way that websites are built. \n",
    "\n",
    "Websites are created using HTML (Hypertext Markup Language), along with CSS (Cascading Style Sheets) and JavaScript. We are going to focus on the HTML. HTML, in a simple explanation, is the way that material is formatted/displayed over the internet. It allows creators to create and structure sections, paragraphs, and links with things like elements, tags, and attributes. \n",
    "\n",
    "- Tags: starting and ending parts of an HTML element\n",
    "- They will always begin and end with angle brackets (`<`, `>`)\n",
    "- Whatever is written inside the angle brackets is a tag. \n",
    "- Tags are like keywords with a distinctive meaning. \n",
    "- They also must be opened and closed in order to function. \n",
    "\n",
    "#### Example:\n",
    "    <a> _content_ </a>  \n",
    "#### Elements: the content in between the tag\n",
    "    <a> THIS IS THE ELEMENT </a> \n",
    "#### Attributes: used to definite the characteristics of the HTML element in detail\n",
    "    <a align=\"right\"> _content_ </a>\n",
    "\n",
    "When we are scraping, we need to find the tags that have the relevant information between them. \n",
    "\n",
    "### Tools to webscrape\n",
    "\n",
    "There are several structures used to webscrape, such as `requests`, `lxml`, and `beautifulsoup4`, but we will be focusing today on using `selenium`. This will let us create a script to webscrape multiple pages to create our dataframe. Through `selenium`, the script can interact, scrape, and parse through the browser. \n",
    "\n",
    "In getting started, we must choose a browser and it's web driver.\n",
    "\n",
    "* Firefox: GeckoDriver\n",
    "* Chrome: ChromeDriver\n",
    "* Safari: SafariDriver\n",
    "\n",
    "For this exercise, we will be using Firefox and Geckodriver.\n",
    "\n",
    "If you are running this on Anvil, double check where the Firefox binary is at; you might have to change the line that says firefox_options.binary_location = \"/path/to/firefox\" so that Selenium knows where the Firefox binary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6ec5f6-3847-42e6-a8c3-dabe5d4ab2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfirefox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdesired_capabilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DesiredCapabilities\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import uuid\n",
    "\n",
    "firefox_options = Options()\n",
    "firefox_options.add_argument(\"window-size=1920,1080\")\n",
    "# Headless mode means no GUI\n",
    "firefox_options.add_argument(\"--headless\")\n",
    "firefox_options.add_argument(\"start-maximized\")\n",
    "firefox_options.add_argument(\"disable-infobars\")\n",
    "firefox_options.add_argument(\"--disable-extensions\")\n",
    "firefox_options.add_argument(\"--no-sandbox\")\n",
    "firefox_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "firefox_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "\n",
    "# Set the location of the executable Firefox program on Anvil\n",
    "firefox_options.binary_location = '/usr/bin/firefox'\n",
    "\n",
    "profile = webdriver.FirefoxProfile()\n",
    "\n",
    "profile.set_preference(\"dom.webdriver.enabled\", False)\n",
    "profile.set_preference('useAutomationExtension', False)\n",
    "profile.update_preferences()\n",
    "\n",
    "desired = DesiredCapabilities.FIREFOX\n",
    "\n",
    "# Set the location of the executable geckodriver program on Anvil\n",
    "uu = uuid.uuid4()\n",
    "driver = webdriver.Firefox(options=firefox_options, executable_path='/usr/bin/firefox', firefox_profile=profile, desired_capabilities=desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed500b-28f4-4c08-82f2-62f9184d713a",
   "metadata": {},
   "source": [
    "Next, we will take a look at the website https://www.goodreads.com/list/show/18645.Best_Books_That_Grow_You\n",
    "\n",
    "Highlight and right click the 1st book titled \"The Alchemist\", you should see a drop-down menu, go ahead and click `Inspect`. \n",
    "\n",
    "You will see that at the bottom of the webpage, there is a new box that shows you the HTML code that builds the website. \n",
    "\n",
    "Here is where we will find all the information that we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661caa15-a546-4c2c-aca2-f14a8e6c2346",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.goodreads.com/list/show/18645.Best_Books_That_Grow_You\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m my_elements \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//a[@class=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbookTitle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]/span\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m good_reads\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.get(\"https://www.goodreads.com/list/show/18645.Best_Books_That_Grow_You\")\n",
    "my_elements = driver.find_elements(\"xpath\", \"//a[@class='bookTitle']/span\")\n",
    "good_reads=[]\n",
    "# Created a for loop that allows for us to keep adding more data to the end of the list\n",
    "for element in my_elements:\n",
    "    good_reads.append(element.text)\n",
    "# Prints out the list that we just created\n",
    "print(good_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36de13-7b05-4e8f-98a6-99d4e0878cab",
   "metadata": {},
   "source": [
    "Try to use this code and edit it to find +\n",
    "\n",
    "1. the author's names +\n",
    "*HINT*\n",
    "[source,python]\n",
    "#author \n",
    "my_elements = driver.find_elements(\"xpath\", \"//a[@class='authorName']/span\")\n",
    "\n",
    "2. the average ratings +\n",
    "*HINT*\n",
    "[source,python]\n",
    "#Average Rating \n",
    "my_elements = driver.find_elements(\"xpath\", \"//span[@class='minirating']\")\n",
    "3. the number of ratings +\n",
    "*HINT*\n",
    "[source,python]\n",
    "#Number of Ratings \n",
    "my_elements = driver.find_elements(\"xpath\", \"//span[@class='minirating']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc9e2b-47f5-4c54-8000-0d00b7a5fb05",
   "metadata": {
    "tags": []
   },
   "source": [
    "When you get to the average ratings and the number of ratings you will need to make an adjustment, the code below takes the ratings information that you scraped (as connected strings) and separates them into the information that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f9fb1-5880-45f6-a86d-40dc6bf132bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reads_rating = driver.find_elements(\"xpath\", \"//span[@class='minirating']\")\n",
    "\n",
    "my_elements = driver.find_elements(\"xpath\", \"//a[@class='authorName']/span\")\n",
    "\n",
    "my_avg_rating=[]\n",
    "my_num_rating=[]\n",
    "for string in good_reads_rating:\n",
    "    my_avg_rating.append(string.text.split(\" \")[0])\n",
    "    my_num_rating.append(int(string.text.split(\" \")[-2].replace(\",\",\"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43ff12-b008-4e77-ad05-a0a47ad929b5",
   "metadata": {},
   "source": [
    "Notice that with the first code, we are only scraping the one page. If you print your data, you should see that you have 100 values BUT we want to scrape multiple pages. We do that by creating a `for` loop that edits the URL each time it hits the end of the page. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b0f00-d13d-40a5-8f63-26fb415d37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reads_2=[]\n",
    "good_reads_2_authors=[]\n",
    "for page in range(1,12,1):\n",
    "    page_url = f\"https://www.goodreads.com/list/show/18645.Best_Books_That_Grow_Youpage={page}\"\n",
    "    driver.get(page_url)\n",
    "    my_elements = driver.find_elements(\"xpath\", \"//a[@class='bookTitle']/span\")\n",
    "    my_authors = driver.find_elements(\"xpath\", \"//a[@class='authorName']/span\")\n",
    "    for element in my_elements:\n",
    "        good_reads_2.append(element.text)\n",
    "    for author in my_authors:\n",
    "        good_reads_2_authors.append(author.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1c376-6af5-49e0-ac9b-6484a00061b8",
   "metadata": {},
   "source": [
    "This code above will scrape multiple pages for the book titles, but now take this code and edit it so we can find the author's names, average ratings, and number of ratings. **Remember to split the rating strings just as we did previously**\n",
    "\n",
    "Now that we have all of the data, we want to take it and create a CSV file so that it is easy to look at, read, and analyze. \n",
    "\n",
    "The first step to that is to take all of our data and create a neat dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf5770-41be-4199-9da2-7986e84c56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "dict = {'Title': good_reads_2, 'Authors': good_reads_2_authors, 'Average Rating': my_avg_rating_2, 'Number of Ratings': my_num_rating_2}\n",
    "GoodReads2022 = pd.DataFrame(dict)\n",
    "print(GoodReads2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00ffbb-6feb-41d7-b5bb-3ce99fe47d90",
   "metadata": {},
   "source": [
    "Once we have created the dataframe, we just need to export it into a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487a8e2-7886-4cf3-9799-b62074d469fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GoodReads2022.to_csv('GoodReads2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae7ae1-bcf2-4cf5-b773-b0a427098321",
   "metadata": {},
   "source": [
    "We did it!!! Hooray we took data and cleaned it up so that it becomes usable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edf4e5-e787-4111-b532-e3fee0f04bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c1fa7-f520-4080-999d-6e981befabfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae951cd9-4a67-41bf-93e5-a12360798293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
