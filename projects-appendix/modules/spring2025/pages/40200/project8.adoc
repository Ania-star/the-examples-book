= TDM 40200: Project 08 - Feature Selection (Regularization)
:page-mathjax: true

== Project Objectives

Sentence here

[IMPORTANT]
====
For this project, we will use R instead of Python, as it offers a more streamlined and efficient approach to modeling. 

In this project, rather than focusing on which programming language is used, we will emphasize why certain tools and techniques are applied. For ease of understanding, we will demonstrate the concept using a simple linear model. However, the regularization methods we cover here are based on the same principles used in many machine learning algorithms and neural network models, too.

Please use the `seminar-r` kernel (not the `seminar` kernel), unless otherwise specified. When you use the `seminar-r` kernel with R, you do not need to use the `%%R` cell magic.
====

.Learning Objectives
****
- 
- 
- 
****

== Dataset
We will work with the Lahman Baseball Database, which is available in R through the `Lahman` package. 

== Introduction

One of the fundamental steps in modeling is identifying the relevant variables. While machine learning models can handle large datasets and a high number of features, strong dependencies among variables can reduce the reliability of model parameters. Also, when the number of variables (*p*) exceeds the number of observations (*n*), the difficulties become more significant.

For instance, in linear models, having *p > n* means that the system of equations is underdetermined, leading to infinitely many possible solutions. This can cause instability in parameter estimation and increase the risk of overfitting. To address this issue, regularization techniques such as *Ridge regression (L2), LASSO (L1),* or *Elastic Net* can be used to impose constraints on the model and improve generalization. Additionally, dimensionality reduction techniques like *principal component analysis (PCA)* or *feature selection methods* can help reduce the impact of high-dimensionality.

== Questions

=== Question 1 (2 points)

In `Lahman` data, we used `Batting` table before, which includes batting statistics. More specifically, we focused on  *Homeruns (HR)* and *Runs Batted In (RBI)* varaibles. Remember that the change in RBI can be explained with HR. There are still a lot of possibilities to think about and add more variables into your model. The same table has several other variables and you can see them by typing the following code in R:

[source, R]
----
library(Lahman)

names(Batting)
----

It is also a good idea to check for variable descriptions with `?Batting` in R. 

`People` is another table including player names, date of births and demographichs for baseball players. It provides more details about players and it shares unique 'playerID' which can be used to merge `Batting` and `People` data sets. `People` contains 26 variables, but many of them are expected to have a large number of missing (`NA`) values. For instance, since the data includes records up to 2023, the `death-` columns will have many empty entries, as most players in recent years are alive. Additionally, these variables are not meaningfully related to `RBI`, so they do not need to be considered in the modeling process.

Some variables may provide interesting insights about the data but be less important for modeling. For example, while the age at which players make their debut can be an intriguing variable, their actual age might contribute more significantly to the model. Additionally, the most common birthplaces highlight Major League Baseball’s international reach, but its contribution to the model may be less. For curious readers, below are chart and tables related to debut ages and the most common (first ten) birthplaces. (Reproducing the visual and table is not required for the project.)


image::Debut.png[width=600, height=450, title="Debut Ages of Players"]

==== Most common birth countries
[cols="1,1",options="header"]
|===
| Birth country | n
| USA | 18098
| Dominican Republic | 890
| Venezuela | 472
| Puerto Rico | 281
| Canada | 266
| Cuba | 237
| Mexico | 147
| Japan | 76
| Panama | 70
| Ireland | 50
|===

[NOTE]
====
R code for Debut Age:
[source, R]
----
library(dplyr)
library(ggplot2)

people <- People %>%
  mutate(age_at_debut = as.numeric(substr(debut, 1, 4)) - birthYear)

# Plot distribution of debut ages
ggplot(people, aes(x = age_at_debut)) +
  geom_histogram(binwidth = 1, fill = "blue", alpha = 0.7) +
  labs(title = "Distribution of Player Debut Ages", x = "Age", y = "Count")
----

====

All those variables in 'Batting' and 'People' tables make us hungry and we want to eat them all in our model! Similar to the linear model we run before, assume that you want to explain the change in your target variable (Runs Batted In, RBI) but this time using more variables of interest. For this aim, lets first merge data sets with a unique ID which is `playerID` for this data.

[source, R]
----
library(dplyr)

# Merge all datasets
full_data <- Batting %>%
  left_join(People, by = "playerID") %>%
  na.omit()
----

In `People` data set, we see birth year of each player. This can be an important variable to be added in the model. But keeping in mind that this data includes records from 1871 through 2023, it maybe not be correct to add year variable into your setting. Instead of year, adding age variable can be a better solution to make sure that we count for the age differences in the model.

.Deliverables
====
- 1.1. Check out column names, first six lines and examine summary statistics for Batting and People tables. 
- 1.2. Merge two data sets by playerID
- 1.3. Add age of players into your data and named the column as 'age'.
====

[NOTE]
====
For the ease of reading, all variable are added with their codes and explanations in Appendix at the end of this document. Tables includes all variables for 'Batting' and 'People' data respectively. 
====

=== Question 2 (2 points)

Before starting to modeling, lets see the following heat map with numeric variables planned to use in modeling:

image::HeatMap.png[title="Correlations between numeric variables"]

Data visualization offers essential clues to understanding your data before initiating the modeling process. For instance: At Bats (AB), Hits (H), and Doubles (X2B) are highly correlated because they follow a natural hierarchy in baseball statistics. AB represents a player's batting opportunities, H is a subset of AB, counting successful hits, and X2B is a further subset, representing only doubles. Since more at-bats generally lead to more hits, and more hits increase the likelihood of doubles, these variables are inherently linked, resulting in strong correlations. In most cases, H (Hits) may be the best choice because it captures a player's ability to reach base successfully, encompassing both singles and extra-base hits while avoiding redundancy. 

image::HitsPhoto.png[width=600, height=450, title = "Pittsburgh Pirates’ Oneil Cruz hits a two-run home run against the Milwaukee Brewers during the seventh inning of a baseball game Wednesday, Aug. 3, 2022, in Pittsburgh. (AP Photo/Keith Srakocic)"]

The source of the picture is https://apnews.com/article/mlb-sports-new-york-pittsburgh-yankees-8b38b65da61ce122b0433886abf8643a[apnews] accessed at 03/08/2025. 

The following linear model uses as much variables as possible to explain the changes in target, RBI variable. 

[source, R]
----
# Fit a linear model
lm_model <- lm(RBI ~ yearID + lgID + H + X3B + HR + SB + CS + BB + SO + age + height + weight + bats, data = full_data)

# Display model summary
summary(lm_model)
----

Although a linear model provides P-values as evidence for variable selection, shrinkage or https://en.wikipedia.org/wiki/Regularization_(mathematics)[regularization] (e.g., Lasso, Ridge) methods are used for variable selection because P-values can sometimes be misleading, especially when the sample size is large, or when variables are highly correlated with one another. In such cases, a variable with a high P-value might still be relevant for the model, but its contribution isn't significant enough to pass the threshold. In some cases, especially when dealing with a large number of variables, instead of focusing on interpreting the output, you may want the model to perform both parameter estimation and variable selection simultaneously. Furthermore, many machine learning algorithms, due to their lack of interpretability, do not provide evidence (such as P-values) of which variables contribute to the model.

Variable selection (feature engineering) in statistical models is crucial for improving both the model's performance and *interpretability*. By choosing only the most relevant variables, we can simplify the model, making it easier to understand and interpret, which is particularly important in fields where the relationships between variables are essential. Additionally, variable selection helps prevent *overfitting*, a common issue when too many irrelevant variables are included, which can lead to a model that fits the noise in the data rather than the underlying patterns. A more focused model with fewer predictors tends to generalize better to new data, leading to improved prediction accuracy. By selecting the right variables, we can also reduce computational costs, as fewer predictors mean less memory and processing power are required, which is especially important in large datasets.

Also, If you have $p$ variables in the model, the number of possible models you could try is determined by the subsets of variables that can be included in the model. For each variable, you have two options: include it or exclude it from the model. Therefore, the total number of possible models is: $2^p$. This includes all combinations of variables, ranging from using none (the null model) to using all $p$ variables. However, this number can be very large if $p$ is high, leading to an impractical amount of models to test. This is why techniques like https://en.wikipedia.org/wiki/Regularization_(mathematics)[regularization] (e.g., Lasso, Ridge) are often used to reduce the number of models considered and make the selection process more manageable.

While these models continue to evolve and new ones (such as https://en.wikipedia.org/wiki/Elastic_net_regularization[Elastic net]) are introduced, understanding *Ridge* and *LASSO* provides a strong foundation for grasping more advanced techniques. Now, let's recall the linear model we discussed earlier and review its mathematical representation with a single predictor.

$RBI_{ij} = \beta_0 + \beta_1 HR_{ij} + \epsilon_{ij}$

where:  

- $\beta_0 =$ Common starting point for all players (overall intercept)
- $\beta_1 =$ Average effect of HR across all players (overall slope)
- $\epsilon_{ij} =$ Error term (noise comes from modeling)

We can replace the variable names in the model with their symbolic representations:

$y_{ij} = \beta_0 + \beta_1 x_{ij} \epsilon_{ij}$

Since we are looking for the best estimates for the unknown $\beta$ parameters in this model, we want to minimize $\epsilon_{ij}$ as much as possible. The method that provides the best estimates for these parameters is the least squares approach, which finds $\beta$ values that minimize the residual sum of squares (RSS):

$RSS = \sum_{i = 1}^{n}(y_i - \beta_0 - \sum_{j = 1}^{p}\beta_1 x_{ij})^2$

Even without prior knowledge of this method, if we were to discuss how to best predict $y_i$ based on the defined model, we would naturally aim to make both sides of the equation as close as possible. The variables in the model are known, and the parameters are estimated from the model, but $\epsilon$ represents an uncontrollable term arising from inherent uncertainty. Since we cannot eliminate this term, we aim to minimize it. Instead of minimizing $\epsilon$ directly, we minimize its squared value to prevent positive and negative errors from canceling each other out.

.Deliverables
====
- 2.1. Run a linear model with RBI as dependent (Target) variable and include all the variables you wanted to add as independent variable from both tables. It is Ok to use the variables used for full model in this question.
- 2.2. What is model interpretability? 
- 2.3. Consider a linear model where RBI is the target (dependent) variable, and HR and age are the independent variables. We can fit this model using the `lm` function and obtain the model parameters with the following R command:

[source, R]
----
lmmodel <- lm(RBI ~ HR + age, data = full_data)

coef(lmmodel)
----

Calculate the *RSS* from the results of this model. However, when performing the calculations, avoid using built-in residual functions in R. Instead, calculate it manually by substituting the values into the following formula:

$RSS = \sum_{i = 1}^{n}(y_i - \beta_0 - \sum_{j = 1}^{p}\beta_1 x_{ij})^2$
====

=== Question 3 (2 points)

Shrinkage or regularization methods offer penalties while minimizing RSS - also called least squares *loss* function or *objective* function. These methods add a penalty term to RSS minimized when estimating parameter values. Changing the magnitude of this penalty term helps adjust the parameter values.

Imagine you're packing a suitcase for a trip. You want to bring everything you might need, but if you pack too much, your suitcase becomes heavy and difficult to carry. Shrinkage methods like Ridge and LASSO work similarly in a statistical model. Without any penalty, the model can include as many variables as possible, making it complex and potentially overfitting the data. However, by adding a penalty term (like an airline imposing a weight limit on luggage), the model is forced to prioritize important variables while reducing the impact of less significant ones. Ridge regression acts like a soft weight limit—allowing you to bring all your items but compressing them slightly to make the suitcase more manageable. LASSO, on the other hand, is stricter, forcing you to completely remove unnecessary items to meet the weight limit. This way, shrinkage methods prevent your model from becoming too complex while ensuring it still performs well.

*Ridge regression* adds a penalty term to the least squares objective function. This penalty term (L-2 penalty) is proportional to the squared magnitude of the coefficients, shrinking them towards zero:

$\sum_{i = 1}^{n}(y_i - \beta_0 - \sum_{j = 1}^{p}\beta_1 x_{ij})^2 + \lambda \sum_{j = 1}^{p} \beta_j^2$

where $\lambda$ is a hyperparameter or it is called as *tuning parameter*. Before moving on to the next paragraph, I recommend taking a minute to pause and consider what a hyperparameter is, how it differs from a parameter, and how it can be found in a general model setting.

image::Ridge.png[]

As illustrated in the figure above, we can control the magnitude of the $\beta$ coefficients by adjusting the value of the tuning parameter, $\lambda$. During the minimization process, the Residual Sum of Squares (RSS) is minimized while simultaneously penalizing the size of the coefficients. This leads to the following relationships:

- If $\lambda \rightarrow 0$, the estimated parameter values converge to those obtained from ordinary least squares.  
- If $\lambda \rightarrow \infty$, the ridge regression coefficients shrink toward zero.

[NOTE]
====
Before using shrinkage methods like Ridge or LASSO, it is crucial to scale the predictor variables. These methods apply a penalty to the regression coefficients, and since the penalty depends on the magnitude of the coefficients, variables with larger scales can dominate the shrinkage process.  

For example, if one variable is measured in thousands (e.g., salary in dollars) and another in single digits (e.g., years of experience), the penalty term will affect the variable with larger numerical values, even if both have similar importance in predicting the outcome. This can lead to biased coefficient estimates and misinterpretation of variable importance. To avoid this issue, standardization (subtracting the mean and dividing by the standard deviation) ensures that all variables contribute equally to the shrinkage process. 
====

The programming languages such as R and Python estimates the parameter for different values of $\lambda$. `glmnet` is one of the libraries in R which can be used to run a regularization methid. 'glmnet' function can run several types of regression models with a grid of values for the regularization parameter, `\lambda`. Here is the example code:

[source, R]
----
# Load necessary libraries
library(glmnet)

# Prepare the matrix of predictors (excluding the response variable)
X <- model.matrix(RBI ~ yearID + lgID + H + X3B + HR + SB + CS + BB + SO + age + height + weight + bats, data = full_data)[, -1] 
y <- full_data$RBI 

# Ridge Regression
# Standardize predictors
X_scaled <- scale(X)

# Replace NaNs (from zero variance) with 0
X_scaled[is.na(X_scaled)] <- 0 

# Fit ridge regression model (alpha = 0 for ridge)
ridge_model <- glmnet(X_scaled, y, alpha = 0)
coef(ridge_model)
ridge_model$lambda
----

The following plot shows how coefficient values are changing with the change in Log Lambda in Ridge regression.

image::RidgeLambda.png[]

.Deliverables
====
- 3.1. What is the role of $\lambda$ in penalized RSS?
- 3.2. Run ridge regression with the same variables used in Question 2.1.
- 3.3. How many different values of $\lambda$ is tried in the ridge regression you run in 3.2.
====

=== Question 4 (2 points)

While Ridge Regression effectively shrinks coefficients towards zero, it rarely eliminates them entirely. This means that even with Ridge, you might end up with a model containing many features, some of which may be redundant. To address this, we can use LASSO (Least Absolute Shrinkage and Selection Operator). Lasso, like Ridge, uses a penalty term to regularize the model, but it employs the L1 norm instead of the L2 norm. This crucial difference allows Lasso to perform feature selection by driving some coefficients to exactly zero, effectively removing those features from the model.

$\sum_{i = 1}^{n}(y_i - \beta_0 - \sum_{j = 1}^{p}\beta_1 x_{ij})^2 + \lambda \sum_{j = 1}^{p} |\beta_j|$

Also, you can define a grid for $\lambda$ and run your model for the grid you choose



ACABA, plot(ridge_model, xvar...) ile elde ettigim ridge ploti ile LASSO plotu farkli mi? Karsilastir.

.Deliverables
====
- 
====

=== Question 5 (2 points)


.Deliverables
====
- 
====

=== Appendix

==== Batting Data Variables
[cols="1,1",options="header"]
|===
| Variable Code | Explanation
| playerID | Player ID code (links to People dataset)
| yearID | Year
| stint | Player's stint (order of appearances within a season)
| teamID | Team; a factor
| lgID | League; a factor with levels AA, AL, FL, NL, PL, UA
| G | Games: number of games in which a player played
| AB | At Bats
| R | Runs
| H | Hits: times reached base because of a batted, fair ball without error by the defense
| X2B | Doubles: hits on which the batter reached second base safely
| X3B | Triples: hits on which the batter reached third base safely
| HR | Homeruns
| RBI | Runs Batted In
| SB | Stolen Bases
| CS | Caught Stealing
| BB | Base on Balls
| SO | Strikeouts
| IBB | Intentional walks
| HBP | Hit by pitch
| SH | Sacrifice hits
| SF | Sacrifice flies
| GIDP | Grounded into double plays
|===

==== People Data Variables
[cols="1,1",options="header"]
|===
| Variable Code | Explanation
| playerID | A unique code assigned to each player. Links to other files.
| birthYear | Year player was born
| birthMonth | Month player was born
| birthDay | Day player was born
| birthCountry | Country where player was born
| birthState | State where player was born
| birthCity | City where player was born
| deathYear | Year player died
| deathMonth | Month player died
| deathDay | Day player died
| deathCountry | Country where player died
| deathState | State where player died
| deathCity | City where player died
| nameFirst | Player's first name
| nameLast | Player's last name
| nameGiven | Player's given name (typically first and middle)
| weight | Player's weight in pounds
| height | Player's height in inches
| bats | Player's batting hand (left (L), right (R), or both (B))
| throws | Player's throwing hand (left (L) or right (R))
| debut | Date that player made first major league appearance
| finalGame | Date that player made first major league appearance (blank if still active)
| retroID | ID used by Retrosheet, https://www.retrosheet.org/
| bbrefID | ID used by Baseball Reference website, https://www.baseball-reference.com/
| birthDate | Player's birthdate, in as.Date format
| deathDate | Player's deathdate, in as.Date format
|===


== Submitting your Work

Once you have completed the questions, save your Jupyter notebook. You can then download the notebook and submit it to Gradescope.

.Items to submit
====
- firstname_lastname_project1.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====