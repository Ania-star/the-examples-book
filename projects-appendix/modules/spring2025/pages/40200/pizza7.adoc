= TDM 40200: Project 07 - Cross Validation
:page-mathjax: true

== Project Objectives

We will try to reach to the final product for our model with cross validation. 

.Learning Objectives
****
- Model selection (algorith is known, trying to find the "best" parameters)
- Algorithm selection (parameter is fixed, trying to decide which algorithm fits well for the data)
- Generalize the performance (algortihm and model is known)
****

== Dataset
- We will work with the 'Batting' Table in 'Lahman' Baseball Database, which is available at '/anvil/projects/x-cis220051/data/lahman/data/BattingNEW.csv'

== Introduction

No matter which model or algorithm you use, it is essential to evaluate how well your model captures and generalizes 
the patterns in the data. Simply fitting a model to a dataset is not enough; you need to assess its performance 
to ensure that it will work well on new, unseen data.  

One of the most commonly used techniques for this purpose is called **Cross-Validation**. 
Cross-validation helps in estimating the model’s performance by splitting the dataset into multiple subsets, 
training the model on some of them, and testing it on the remaining unseen ones. 
This process is repeated several times to get a more reliable measure of how well the model generalizes to new data.

We covered the topics related to cross-validation, such as hyperparameter tuning before. Now, we will focus on conceptualizing these concepts.

The fact that modeling tools automatically applies cross-validation can somewhat obscure the reality 
that the method's results may vary when the technique is adapted to different use cases or data types. 
It is crucial to conduct further research and understand the logic behind the methods we use, 
as it helps to gain deeper knowledge.

If you apply a statistical model to your data, you will obtaion your model parameters from the data.
And the model guarantees it is optimized, generalizable, and you find the same parameter for the same data anywhere in the world. However, some models include not only parameters but also hyperparameters. 
Unlike parameters, hyperparameters cannot be directly derived from the data and do not have a closed-form equations. Instead, they require tuning through trial and error. Since there is no universal equation to compute these hyperparameters, techniques like cross-validation are used to determine their optimal values.

For example:

- $\beta$ in regression is a parameter
- K in K-means algorithm is a hyperparameter

[IMPORTANT]
====
In many machine learning resources, the term "parameters" is often used interchangeably with "variables." However, in this context, we use "parameters" in its true meaning—referring to terms in the model that are not explicitly known but can be estimated from the data.
====

== Questions

=== Question 1 (2 points)

Lets try cross validation with https://en.wikipedia.org/wiki/Polynomial_regression[polynomial regression] which is a type of regression analysis that models the relationship between the independent (explanatory, x)  and the dependent (dependent, y) variables using a polynomial function of x. For example, the quadratic form can be defined as following:

$y = \beta_0 + \beta_1x + \beta_2x^2 + \epsilon$

Bu modeldeki polinamin derecesine karar verebilmek icin, iyi yapilandirilmis bir karar verme surecine ihtiyaciniz vardir. And if you are not sure which degree to use for your data, you can always use cross validation for a more structured decision making.

Let us use the 'Lahman' data again, but this time, lets use it in Python with Runs Battled In (RBI) versus (HR) variables again. We will read the data in Python as follows:

[source,python]
----
import pandas as pd
Batting = pd.read_csv("/anvil/projects/x-cis220051/data/lahman/data/BattingNEW.csv", names = ["playerID","yearID","stint","teamID","lgID","G","G_batting","AB","R","H","2B","3B","HR","RBI","SB","CS","BB","SO","IBB","HBP","SH","SF","GIDP"])
pd.set_option('display.max_columns', None)
Batting.head()
----

Assume that you want to run a polynomial regression with a degree of 3. You can run this model in Python with a very simple code since polynomial regression is already defined in 'LinearRegression()' in https://scikit-learn.org/stable/['sklearn']. Assuming X is 

[source,python]
----
poly = PolynomialFeatures(degree=3, include_bias=True)
X_poly = poly.fit_transform(X)

poly_model = LinearRegression()
poly_model.fit(X_poly, y)
----

In here '.fit_transform(X)' takes the original input feature 'X' (HR) and expands it into multiple polynomial terms. For example, if d=3, this transformation produces:

$X_{poly} = [X, X^2, X^3]$

And when 'include_bias=True', the transformed feature matrix includes an additional column of ones, which represents the intercept term.

[NOTE]
====
Here are the example packages can be used to handle this question:

[source,python]
----
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
----
====

.Deliverables
====
- 1.1. Read the Lahman Data in Python including the column names
- 1.2. Run a linear model and show coefficients
- 1.3. Run a polynomial regression with a degree of 2.
- 1.4. What do you think about using polynommial regression model with degree three?
====

=== Question 2 (2 points)

We can always try different degrees for the polynomial regression. however, at the end of the day, We want to decide degree that provides a significant improvement in fit while avoiding overfitting. Remember overfitting happens when a model learns patterns that are too specific to the training data, including noise, instead of capturing the true underlying trend. It performs very well on the training data but fails to generalize to new, unseen data. Think of a student who memorizes answers for an exam instead of understanding the concepts—great for that test, but struggles with new questions.

If we increase the degree of a polynomial, it can lead to overfitting, so it is important to find a balance between complexity and accuracy. There are several ways to test it, one of them is using cross validation by splitting your data into two parts training and test data. The model will learn from the training data and never see the test data until you want to see how model is performaing. Then, test data is used to test model performance

Veriyi bolme islemi de aslinda basli basina cross validation in konusu ancak literaturde ve bir cok uygulamada genellikle train icin 80% ve test icin 20% olarak gormekteyiz. Ama kendi kendinize praktise yapmak isterseniz, veriniz icin farkli bolme oranlarinda performansin nasil calistigina bakabilirsiniz. Bu noktada genel bir kural eklemek de fayda var, Büyük Test Verisi – Kötümser Yanlılık vs. Küçük Test Verisi – Yüksek Varyansa neden olacaktir. 

Splitting proportions for train and test data is another problem can be solved by using cross-validation. In the literature and many applications, we commonly see an 80% training and 20% test split. However, if you want to experiment on your own, you can try different splitting ratios and observe how performance changes. It's also important to keep in mind a general rule: a *large test set* can lead to *pessimistic bias*, while a *small test set* may result in *high variance*. The plot below is an illustration from the Raschka's paper. It is also very good source if you want to learn more about model evaluation.

image::TrainTest.png[]

https://arxiv.org/pdf/1811.12808[Image Source]: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, S. Raschka,
arXiv:1811.1280v2, page.15, accessed Feb 28, 2025.

Lets use the Hold-out method to find the polynomial degree for our data:

image::HoldOut.png[]


.Deliverables
====
2.1.
2.2. 
2.3.and calculate the https://en.wikipedia.org/wiki/Mean_squared_error[Mean Squared Error] value.
====

=== Question 3 (2 points)

.Deliverables
====
- 
====

=== Question 4 (2 points)

.Deliverables
====
- 
====

=== Question 5 (2 points)


.Deliverables
====
- 
====

=== Question 6 (2 points)


.Deliverables
====
- 
====

== Submitting your Work

Once you have completed the questions, save your Jupyter notebook. You can then download the notebook and submit it to Gradescope.

.Items to submit
====
- firstname_lastname_project1.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====