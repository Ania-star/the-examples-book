= TDM 20200: Web Scraping Project 5 -- Spring 2025

**Motivation:** We learn how to handle pages with differences in style.  In particular, we consider pages that have a next page and pages that do not have a next page.

**Context:** We will use lxml for web scraping in this project, with XPath queries.  We will first analyze example pages, and then will build a powerful function to automate scraping pages.

**Scope:** Web Scraping in Python

.Learning Objectives:
****
- We continue to learn how to scrape data more systematically from the web
****

Make sure to read about, and use the template found xref:ROOT:templates.adoc[here], and the important information about project submissions xref:ROOT:submissions.adoc[here].

== Dataset(s)

In this project we will scrape data from the following website:

- https://books.toscrape.com

== Questions

Preface to the Project:

Recall that there are 50 types of pages on the Books To Scrape website.  We already learned that we can get these 50 pages as follows:

[source, python]
----
import requests

from lxml import html

myresponse = requests.get("https://books.toscrape.com")

mytree = html.fromstring(myresponse.content)

myurls = ['https://books.toscrape.com/' + element.attrib['href'] for element in mytree.xpath('//li/ul/li/a')]

mylistofurls = [element for element in myurls]
----

If you explore these 50 links, you will see that some of them are individual pages, for instance, consider:

https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html

https://books.toscrape.com/catalogue/category/books/humor_30/index.html

https://books.toscrape.com/catalogue/category/books/novels_46/index.html

https://books.toscrape.com/catalogue/category/books/parenting_28/index.html

https://books.toscrape.com/catalogue/category/books/politics_48/index.html

and some of the categories have multiple pages, for instance:

https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html

https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html

https://books.toscrape.com/catalogue/category/books/mystery_3/index.html

https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html

https://books.toscrape.com/catalogue/category/books/fiction_10/index.html

Whenever a category has multiple pages, we can safely change the `index.html` to `page-1.html` and then the following pages are `page-2.html` and `page-3.html` and so on....

BUT on the Books To Scrape website, the categories with just one page do not have that option.  So it makes sense to handle these two types of pages separately.



=== Question 1 (2 pts)

Write a function called `myscrapingfunction` that works as follows:

It takes a url called `myurl` as input and returns a list of all of the books from that url's category.

For instance,

`myscrapingfunction('https://books.toscrape.com/catalogue/category/books/travel_2/index.html')` should return a list of 11 url's like this:

[source, python]
----
['https://books.toscrape.com/catalogue/category/books/travel_2/../../../its-only-the-himalayas_981/index.html',
 'https://books.toscrape.com/catalogue/category/books/travel_2/../../../full-moon-over-noahs-ark-an-odyssey-to-mount-ararat-and-beyond_811/index.html',
 'https://books.toscrape.com/catalogue/category/books/travel_2/../../../see-america-a-celebration-of-our-national-parks-treasured-sites_732/index.html',
...
 'https://books.toscrape.com/catalogue/category/books/travel_2/../../../the-road-to-little-dribbling-adventures-of-an-american-in-britain-notes-from-a-small-island-2_277/index.html',
 'https://books.toscrape.com/catalogue/category/books/travel_2/../../../neither-here-nor-there-travels-in-europe_198/index.html',
 'https://books.toscrape.com/catalogue/category/books/travel_2/../../../1000-places-to-see-before-you-die_1/index.html']
----

and, as another example,

`myscrapingfunction('https://books.toscrape.com/catalogue/category/books/childrens_11/index.html')` should return a list of 29 url's like this:

[source, python]
----
['https://books.toscrape.com/catalogue/category/books/childrens_11/../../../birdsong-a-story-in-pictures_975/index.html',
 'https://books.toscrape.com/catalogue/category/books/childrens_11/../../../the-bear-and-the-piano_967/index.html',
 'https://books.toscrape.com/catalogue/category/books/childrens_11/../../../the-secret-of-dreadwillow-carse_944/index.html',
...
 'https://books.toscrape.com/catalogue/category/books/childrens_11/../../../diary-of-a-minecraft-zombie-book-1-a-scare-of-a-dare-an-unofficial-minecraft-book_99/index.html',
 'https://books.toscrape.com/catalogue/category/books/childrens_11/../../../matilda_32/index.html',
 'https://books.toscrape.com/catalogue/category/books/childrens_11/../../../charlie-and-the-chocolate-factory-charlie-bucket-1_13/index.html']
----

Here is pseudocode for how Dr Ward defined his function `myscrapingfunction` with input `myurl`:

`import requests`

`from lxml import html`

Make an empty list called `mytemplist`.

Let `mytree` be defined for `myurl` just like in Project 4 (and earlier).

Create `mystub` from `myurl` by using a string replace of `'index.html'` by `''` (in other words, remove `'index.html'` from `myurl`.

Extend `mytemplist` by the list `[mystub + element.attrib['href'] for element in mytree.xpath('//h3/a')]` (this basically gets the books from page 1 of the category corresponding to `myurl`).

Set `mycounter` to be 1.

While `len([element for element in mytree.xpath('//li[@class="next"]/a')])` is strictly positive (in other words, while there are still "next" pages to handle, do the following:

Increment the value of `mycounter` by `.

Create a variable `mynewurl` from concatenating together `mystub` and `page-` and `mycounter` (converted to a string) and `.html`

Create `mytree` from `mynewurl`

and then extend `mytemplist` by the list `[mystub + element.attrib['href'] for element in mytree.xpath('//h3/a')]` (this basically gets the books from the page numbered `mycounter` of the category corresponding to `myurl`)

Now the `while` loop is over, and the function should return `mytemplist`.

.Deliverables
====
- Demonstrate that `myscrapingfunction` works by running it on each of the 10 example URLs given in the preface to the project.
- Be sure to document your work from Question 1, using some comments and insights about your work.
====

=== Question 2 (2 pts)

If your list in Question 1 is called `mylist`, then you can remove the duplicates and put the list in sorted order as follows:

[source, python]
----
mysortedandcleanedlist = sorted(list(set(mylist)))
----

At this point, `mysortedandcleanedlist` should have 475 URLs for all of the individual parks.

We can ignore 8 of the parks in this project:

'https://www.nps.gov/cbpo/'

'https://www.nps.gov/deva/'

'https://www.nps.gov/grsp/'

'https://www.nps.gov/gwmp/'

'https://www.nps.gov/maac/'

'https://www.nps.gov/roca/'

'https://www.nps.gov/wwim/'

'https://www.nps.gov/yose/'

because these 8 webpages for these 8 parks either do not have their street address listed, or they have one address in the USA and one in Canada.

Remove these 8 parks from your list, so that (now) you have only 467 URLs for parks.

[TIP]
====
In Dr Ward's list of 475 parks, the ones to remove were 82, 121, 202, 208, 281, 373, 468, 471, but this may vary for you!
====


.Deliverables
====
- Make a list of 467 parks, by removing duplicates from your list from Question 1, removing duplicates, and also removing the 8 parks listed above.
- Be sure to document your work from Question 2, using some comments and insights about your work.
====

=== Question 3 (2 pts)

Extract the street address from each of these 467 parks.

[TIP]
====
The location of the street address of each park is in a `div` tag with attribute `itemprop` equal to `"address"` and then inside the `div` tag, there is a `p` tag, and then inside the `p` tag, there is a `span` tag with attribute `itemprop` equal to `"streetAddress"`.
====

Also, extract the city from each of these 467 parks.  The location is similar to the street address, but the `span` tag has attribute `itemprop` equal to `"addressLocality"` in this case.  (Do not worry about the fact that each city has a comma attached; that is OK!)

Additionally, please extract the state from each of these 467 parks.  The location is similar to the street address and to the city, but the `span` tag has attribute `itemprop` equal to `"addressRegion"` in this case.

Finally, please extract the zip code from each of these 467 parks.  The location is similar to the street address and to the city, but the `span` tag has attribute `itemprop` equal to `"postalCode"` in this case.


.Deliverables
====
- Build 4 lists (one for the street addresses, one for the cities, one for the states, and one for the zip codes), each of which has length 467, corresponding to the 467 parks from Question 2.
- Be sure to document your work from Question 3, using some comments and insights about your work.
====


=== Question 4 (2 pts)

Join your 4 lists from Question 3 into a data frame with 4 columns and 467 rows.

Which state has the most national parks?

Which city-and-state pair has the most national parks?

.Deliverables
====
- A data frame with 4 columns and 467 rows, corresponding to the information from Question 3.
- The state which has the most national parks.
- The city-and-state pair which has the most national parks.
- Be sure to document your work from Question 4, using some comments and insights about your work.
====

=== Question 5 (2 pts)

Make a visualization about something that you find interesting from the National Park Service data, either using your work from Project 3 or Project 4; any visualization is OK.

.Deliverables
====
- Make a visualization about something that you find interesting from the National Park Service data, either using your work from Project 3 or Project 4; any visualization is OK.
- Be sure to document your work from Question 5, using some comments and insights about your work.
====





== Submitting your Work

Please make sure that you added comments for each question, which explain your thinking about your method of solving each question.  Please also make sure that your work is your own work, and that any outside sources (people, internet pages, generating AI, etc.) are cited properly in the project template.

Congratulations! Assuming you've completed all the above questions, you are learning to apply your web scraping knowledge effectively!

Prior to submitting your work, you need to put your work xref:ROOT:templates.adoc[into the project template], and re-run all of the code in your Jupyter notebook and make sure that the results of running that code is visible in your template.  Please check the xref:ROOT:submissions.adoc[detailed instructions on how to ensure that your submission is formatted correctly]. To download your completed project, you can right-click on the file in the file explorer and click 'download'.

Once you upload your submission to Gradescope, make sure that everything appears as you would expect to ensure that you don't lose any points. We hope your first project with us went well, and we look forward to continuing to learn with you on future projects!!

.Items to submit
====
- firstname_lastname_project5.ipynb
====

[WARNING]
====
It is necessary to document your work, with comments about each solution.  All of your work needs to be your own work, with citations to any source that you used.  Please make sure that your work is your own work, and that any outside sources (people, internet pages, generating AI, etc.) are cited properly in the project template.

You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not.

**Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====

