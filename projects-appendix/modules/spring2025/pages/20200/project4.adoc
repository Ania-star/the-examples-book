= TDM 20200: Web Scraping Project 4 -- Spring 2025

**Motivation:** We continue to learn how to leverage our skills for systematic web scraping.

**Context:** We will use lxml for web scraping in this project, with XPath queries.  We will wrap our web scraping into functions, which we will use systematically, to gether data.

**Scope:** Web Scraping in Python

.Learning Objectives:
****
- We continue to learn how to scrape data more systematically from the web
****

Make sure to read about, and use the template found xref:ROOT:templates.adoc[here], and the important information about project submissions xref:ROOT:submissions.adoc[here].

== Dataset(s)

In this project we will scrape data from the following website:

- https://www.nps.gov

== Questions

=== Question 1 (2 pts)

We have now scraped information from the main page of the National Park Service, located at:  https://www.nps.gov/

and also from each of the 56 states that are listed on the front page of the NPS website.  For instance, we scraped data from the Indiana page:  https://www.nps.gov/state/in/index.htm

BUT we did not (yet) scrape the actual webpages of each park!  There are 645 park webpages, and we will scrape these individual park pages in this project.

First, to get started, use the main page of the National Park Service website to get the URLs for the 56 state pages, as follows:

[source, python]
----
import requests

from lxml import etree
from lxml import html

mytree = html.fromstring(requests.get("https://www.nps.gov").content)

mystateurls = ['https://www.nps.gov' + element.attrib['href'] for element in mytree.xpath('//a[@class = "dropdown-item dropdown-state"]')]

mystateurls
----

We did something similar to this in Project 3, Question 4, but just to get you started, we are giving you this code directly.  For instance, `mystateurls[16]` is the URL for the parks in Indiana.

Now we extract the webpages for the four parks in Indiana, as follows:

[source, python]
----
mytree = html.fromstring(requests.get(mystateurls[16]).content)
myparksdirectories = [element.attrib['href'] for element in mytree.xpath('//h3/a')]
----

These four Indiana parks directories are:

[source, python]
----
['/gero/', '/indu/', '/lecl/', '/libo/']
----

We can modify the code above, to put the NPS website before each of these, as follows:

[source, python]
----
mytree = html.fromstring(requests.get(mystateurls[16]).content)
myparksdirectories = ['https://www.nps.gov' + element.attrib['href'] for element in mytree.xpath('//h3/a')]
----

which gives us the locations of the 4 parks in Indiana:

[source, python]
----
['https://www.nps.gov/gero/',
 'https://www.nps.gov/indu/',
 'https://www.nps.gov/lecl/',
 'https://www.nps.gov/libo/']
----

Now use similar work, to extract the URLs for all 645 parks in all 56 states.  (There will be some duplicates in your list, and that is OK.  We will remove the duplicates in Question 2.)


.Deliverables
====
- Make a list of the URLs for all 645 parks in all 56 states.
- Be sure to document your work from Question 1, using some comments and insights about your work.
====

=== Question 2 (2 pts)

If your list in Question 1 is called `mylist`, then you can remove the duplicates and put the list in sorted order as follows:

[source, python]
----
mysortedandcleanedlist = sorted(list(set(mylist)))
----

At this point, `mysortedandcleanedlist` should have 475 URLs for all of the individual parks.

We can ignore 6 of the parks in this project:

'https://www.nps.gov/cbpo/'

'https://www.nps.gov/grsp/'

'https://www.nps.gov/gwmp/'

'https://www.nps.gov/roca/'

'https://www.nps.gov/wwim/'

'https://www.nps.gov/yose/'

because these 6 webpages for these 6 parks either do not have their street address listed, or they have one address in the USA and one in Canada.

Remove these 6 parks from your list, so that (now) you have only 469 URLs for parks.

[TIP]
====
In Dr Ward's list of 475 parks, the ones to remove were 82, 202, 208, 373, 468, 471, but this may vary for you!
====


.Deliverables
====
- Make a list of 469 parks, by removing duplicates from your list from Question 1, removing duplicates, and also removing the 6 parks listed above.
- Be sure to document your work from Question 2, using some comments and insights about your work.
====

=== Question 3 (2 pts)

Now wrap your work from Question 2 into a Python function called `mystateparkinformation`, which takes a 2-letter string as input, and returns a data frame that corresponds to the information about the parks from that state.  For instance:

`mystateparkinformation('in')` should return a data frame with 4 rows and 4 columns.

`mystateparkinformation('ca')` should return a data frame with 34 rows and 4 columns.

`mystateparkinformation('tx')` should return a data frame with 18 rows and 4 columns.

Test your function on each of those three states, and then test your function on two more states of your choosing.

[IMPORTANT]
====
Be sure to document your function using docstrings as described, for instance, here:  https://pandas.pydata.org/docs/development/contributing_docstring.html
====

.Deliverables
====
- Make a function that takes a two-letter state abbreviation as input, and returns a data frame with 4 columns and the correct number of rows, in other words, one row per park from that state.
- Please test your function for the inputs `'in'`, `'ca'`, `'tx'`, and for two other states of your choosing.
- Make sure that your function uses docstring notation that describes it well.
- Be sure to document your work from Question 3, using some comments and insights about your work.
====


=== Question 4 (2 pts)

If you run the following code on the homepage of the NPS at:  https://www.nps.gov  you will see that it produces a list of all two-digit letter codes, for the 56 states and territories from the National Park Service website.  Explain (in words) what this one line of code does.  (In particular, make sure that you understand and can explain how the `split` function in this line of code is working.)

[source, python]
----
mystatelist = [element.attrib['href'].split('/')[2] for element in mytree.xpath('//a[@class = "dropdown-item dropdown-state"]')]
----

Now run your function from Question 3 on each of the 56 elements in the list named `mystatelist`.  In this way, you should create a list of 56 data frames, one for each state.  This approach might help:

[source, python]
----
mylistofdataframes = [mystateparkinformation(element) for element in mystatelist]
----

.Deliverables
====
- Explain why the code to create `mystatelist` works.
- Run your function from Question 3 on each of the 56 elements of `mystatelist`, to create a list of 56 data frames, one for each state.
- Be sure to document your work from Question 4, using some comments and insights about your work.
====

=== Question 5 (2 pts)

Build the 56 data frames from Question 4 into one large data frame with 645 rows (or 638 rows?) and 4 columns.  Show the first five rows and the last five rows of this data frame, to convince yourself and the TAs that you did this properly.

It might help to use an approach like this:

[source, python]
----
mylistofdataframes = [mystateparkinformation(element) for element in mystatelist]
mybigDF = pd.concat(mylistofdataframes, axis=0, ignore_index=True)
----

image::bigstateparkdataframe.png[mybigDF, width=800, height=600, loading=lazy, title="mybigDF"]

[IMPORTANT]
====
When Dr Ward first wrote this project, there were 645 rows altogether in the large data frame.  BUT before releasing the project, there were only 638 rows.  Perhaps some changes in the federal government administration led to this change?  The number of rows might continue to change slightly.  The graders will be flexible with you, if your work is slightly different from the work shown in the videos.
====

.Deliverables
====
- Build the 56 data frames from Question 4 into one large data frame with 645 rows (or 638 rows?) and 4 columns.  Show the first five rows and the last five rows of this data frame, to convince yourself and the TAs that you did this properly.
- Be sure to document your work from Question 5, using some comments and insights about your work.
====





== Submitting your Work

Please make sure that you added comments for each question, which explain your thinking about your method of solving each question.  Please also make sure that your work is your own work, and that any outside sources (people, internet pages, generating AI, etc.) are cited properly in the project template.

Congratulations! Assuming you've completed all the above questions, you are learning to apply your web scraping knowledge effectively!

Prior to submitting your work, you need to put your work xref:ROOT:templates.adoc[into the project template], and re-run all of the code in your Jupyter notebook and make sure that the results of running that code is visible in your template.  Please check the xref:ROOT:submissions.adoc[detailed instructions on how to ensure that your submission is formatted correctly]. To download your completed project, you can right-click on the file in the file explorer and click 'download'.

Once you upload your submission to Gradescope, make sure that everything appears as you would expect to ensure that you don't lose any points. We hope your first project with us went well, and we look forward to continuing to learn with you on future projects!!

.Items to submit
====
- firstname_lastname_project4.ipynb
====

[WARNING]
====
It is necessary to document your work, with comments about each solution.  All of your work needs to be your own work, with citations to any source that you used.  Please make sure that your work is your own work, and that any outside sources (people, internet pages, generating AI, etc.) are cited properly in the project template.

You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not.

**Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====

