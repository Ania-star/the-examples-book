= TDM 20200: Web Scraping Project 3 -- Spring 2025

**Motivation:** Now that we know how to use Beautiful Soup and lxml, we will expand and apply our skills, to achieve some systematic web scraping.

**Context:** We will use both Beautiful Soup and lxml, and will expand our knowledge of XPath.

**Scope:** Web Scraping in Python

.Learning Objectives:
****
- We learn how to scrape data more systematically from the web
****

Make sure to read about, and use the template found xref:ROOT:templates.adoc[here], and the important information about project submissions xref:ROOT:submissions.adoc[here].

== Dataset(s)

In this project we will scrape data from the following websites:

- https://datamine.purdue.edu/about/about-welcome/
- https://www.nps.gov
- https://books.toscrape.com
- https://www.gocomics.com/peanuts/

== Questions

=== Question 1 (2 pts)

++++
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_48o1h9zk&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_aheik41m" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="TDM 10100 Project 13 Question 1"></iframe>
++++

In Project 2, Question 5, we scraped the types, names, and locations of all four parks in Indiana from the National Park Service website:  https://www.nps.gov/state/in/index.htm

BUT we did not (yet) scrape the paragraph of information about each park from this webpage.

If we right-click on such a paragraph and use the Inspect button in Firefox, we will see that such a paragraph is contained in a `p` tag with no attributes.  This `p` tag follows a `div` tag with class attribute equal to `"list_left"`.  Unfortunately, there are several other `p` tags that follow that `div` tag, as we already saw from Project 2, Question 5, namely:

The park types have the `div` tag, followed by a `p` with class equal to `"list_left__kicker"`.  Similarly, the park locations have the `div` tag, followed by a `p` with class equal to `"list_left__subtitle"`.  So we want to exclude both of those results.

We can do this by using:

[source, python]
----
mytree.xpath('//div[@class="list_left"]/p[not(@class="list_left__kicker") and not(@class="list_left__subtitle")]')
----

.Deliverables
====
- Use `lxml` with the `xpath` described above, to extract a list with 4 elements about the four respective parks from Indiana.
- Be sure to document your work from Question 1, using some comments and insights about your work.
====

=== Question 2 (2 pts)

Starting with the data frame from Project 2, Question 5 (which has 4 rows and 3 columns), add the results from Project 3, Question 1, as a new 4th column in this data frame, so that you have a data frame of 4 rows, corresponding to the 4 parks from Indiana.

Now solve Project 2, Question 5, and Project 3, Question 1, again but this time use California instead of Indiana.  You should get a data frame with 34 rows, corresponding to the 34 parks from California, and 4 columns (again, corresponding to the type, name, location, and description of each park).

Do this one more time for Texas, creating a data frame with 18 rows and 4 columns.


.Deliverables
====
- Use Python to make a Pandas data frame with 4 rows and 4 columns, namely, 1 row per park from Indiana, with the type of park in the first column, the name of the park in the second column, the location of the park in the third column, and the location of the park in the fourth column.
- Make an analogous data frame for parks from California, with 34 rows and 4 columns.
- Make an analogous data frame for parks from Texas, with 18 rows and 4 columns.
- Be sure to document your work from Question 2, using some comments and insights about your work.
====

=== Question 3 (2 pts)

Now wrap your work from Question 2 into a Python function called `mystateparkinformation`, which takes a 2-letter string as input, and returns a data frame that corresponds to the information about the parks from that state.  For instance:

`mystateparkinformation('in')` should return a data frame with 4 rows and 4 columns.

`mystateparkinformation('ca')` should return a data frame with 4 rows and 34 columns.

`mystateparkinformation('tx')` should return a data frame with 4 rows and 18 columns.

Test your function on each of those three states, and then test your function on two more states of your choosing.

.Deliverables
====
- Make a function that takes a two-letter state abbreviation as input, and returns a data frame with 4 columns and the correct number of rows, in other words, one row per park from that state.
- Please test your function for the inputs `'in'`, `'ca'`, `'tx'`, and for two other states of your choosing.
- Be sure to document your work from Question 3, using some comments and insights about your work.
====


=== Question 4 (2 pts)

++++
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_7yfpotc7&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_aheik41m" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="TDM 10100 Project 13 Question 1"></iframe>
++++

For *academic purposes only* now we extract the Snoopy comic for June 22, 1970 from the URL https://www.gocomics.com/peanuts/1970/06/22 but this time, we use `lxml` instead of Beautiful Soup.

Instead of using:

[source, python]
----
mysoup.select('img[alt = "Peanuts Comic Strip for June 22, 1970 "]')
----

we will use an `xpath` like this:

[source, python]
----
mysoup.xpath('//img[@alt = "Peanuts Comic Strip for June 22, 1970 "]')
----

In this way, using `lxml` instead of Beautiful Soup, we can extract the URL for the comic, for this particular day (June 22, 1970) that Woodstock got named:  https://assets.amuniversal.com/2181aa70f895013014ff001dd8b71c47

Now use `lxml` to find and check the location of the Peanuts comic *for two additional days*, and explain your steps.

[IMPORTANT]
====
We want you to (please) extract the url for the image of the Peanuts comic strip for June 22, 1970, and also for two additional days of your choice.
====

.Deliverables
====
- Verify that this URL contains the comic for the day that Woodstock got named:  https://assets.amuniversal.com/2181aa70f895013014ff001dd8b71c47
- For two additional days of your choice, give the days and the locations of the Peanuts comic image for those two days, *using `lxml` instead of Beautiful Soup*.
- Be sure to document your work from Question 4, using some comments and insights about your work.
====

=== Question 5 (2 pts)

++++
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_1ccqvuer&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_aheik41m" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="TDM 10100 Project 13 Question 1"></iframe>
++++

Consider the website for Indiana parks from the National Park Service:

https://www.nps.gov/state/in/index.htm

There are four parks listed on this page.

First, scrape the types of all four parks (National Historical Park, National Park, National Historic Trail, National Memorial).

Next, scrape the names of all four parks (George Rogers Clark, Indiana Dunes, Lewis & Clark, Lincoln Boyhood).

Next, scrape the names of all four locations (Vincennes, IN; Porter, IN; Sixteen States: IA,ID,IL,IN,KS,KY,MO,MT,NE,ND,OH,OR,PA,SD,WA,WV; Lincoln City, IN).

Finally, make a Pandas data frame with 4 rows and 3 columns, namely, 1 row per park, with the type of park in the left column, the name of the park in the middle column, and the location of the park in the right column.


.Deliverables
====
- Use Python to make a Pandas data frame with 4 rows and 3 columns, namely, 1 row per park, with the type of park in the left column, the name of the park in the middle column, and the location of the park in the right column.
- Be sure to document your work from Question 5, using some comments and insights about your work.
====




== Submitting your Work

Please make sure that you added comments for each question, which explain your thinking about your method of solving each question.  Please also make sure that your work is your own work, and that any outside sources (people, internet pages, generating AI, etc.) are cited properly in the project template.

Congratulations! Assuming you've completed all the above questions, you've just finished your second project for TDM 20200!  You have a very good introduction to web scraping now, using Beautiful Soup (from Project 1) or `lxml` (from Project 2).

Prior to submitting your work, you need to put your work xref:ROOT:templates.adoc[into the project template], and re-run all of the code in your Jupyter notebook and make sure that the results of running that code is visible in your template.  Please check the xref:ROOT:submissions.adoc[detailed instructions on how to ensure that your submission is formatted correctly]. To download your completed project, you can right-click on the file in the file explorer and click 'download'.

Once you upload your submission to Gradescope, make sure that everything appears as you would expect to ensure that you don't lose any points. We hope your first project with us went well, and we look forward to continuing to learn with you on future projects!!

.Items to submit
====
- firstname_lastname_project1.ipynb
====

[WARNING]
====
It is necessary to document your work, with comments about each solution.  All of your work needs to be your own work, with citations to any source that you used.  Please make sure that your work is your own work, and that any outside sources (people, internet pages, generating AI, etc.) are cited properly in the project template.

You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not.

**Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====

