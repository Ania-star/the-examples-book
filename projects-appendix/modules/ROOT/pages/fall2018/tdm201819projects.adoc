= TDM Fall 2018 STAT 19000 Projects

== Project 1

Question 1.

Use the airline data stored in this directory:

`/depot/statclass/data/dataexpo2009`

In the year 2005, find:

a.  the number of flights that occurred, on every day of the year, and

b.  find the day of the year on which the most flights occur.

Solution:

We switch to the directory for the airline data

`cd /depot/statclass/data/dataexpo2009`

a. The number of flights that occurred, on every day of the year, can be obtained by extracting the 1st, 2nd, and 3rd fields, sorting the data, and then summarizing the data using the uniq command with the -c flag

`sort 2005.csv | cut -d, -f1-3 | sort | uniq -c`

The first few lines of the output are:

[source,bash]
----
16477 2005,10,1
19885 2005,10,10
19515 2005,10,11
19701 2005,10,12
19883 2005,10,13
----

and the last few lines of the output are:

[source,bash]
----
20051 2005,9,6
19629 2005,9,7
19968 2005,9,8
19938 2005,9,9
    1 Year,Month,DayofMonth
----

b. The day of the year on which the most flights occur can be found by sorting the results above, in numerical order, using sort -n and then (if desired, although it is optional) we can extract the last line of the output using tail -n1

`sort 2005.csv | cut -d, -f1-3 | sort | uniq -c | sort -n | tail -n1`

and we conclude that the most flights occur on August 5:

`21041 2005,8,5`


Question 2.

Again considering the year 2005, did United or Delta have more flights?

Solution:

We can extract the 9th field, which is the carrier (i.e., the airline company) and then, in the same way as above, we can sort the data, and then we can summarize the data using uniq -c

This yields the number of flights for each carrier. We can either read the number of United or Delta flights with our eyeballs, or we can use the grep command, searching for both the pattern UA and DL to isolate (only) the number of flights for United and Delta, respectively.

`sort 2005.csv | cut -d, -f9 | sort | uniq -c | grep "UA\|DL"`

The output is

[source,bash]
----
658302 DL
485918 UA
----

so Delta has more flights than United in 2005.


Question 3.

Consider the June 2017 taxi cab data, which is located in this folder:

`/depot/statclass/data/taxi2018`

What is the distribution of the number of passengers in the taxi cab rides?  In other words, make a list of the number of rides that have 1 passenger; that have 2 passengers; etc.

Solution:

Now we change directories to consider the taxi cab data

`cd ../taxi2018`

The ".." in the previous command just indicates that we want to go up one level to

`/depot/statclass/data`

and then, from that point, we want to go into the taxi cab directory. If this sounds complicated, then (instead) it is safe to use the longer version:

`cd /depot/statclass/data/taxi2018`

The number of passengers is given in the 4th column, `passenger_count`

We use a method that is similar to the one from the first three questions, we extract the 4th column, sort the data, and then summarizing the data using the uniq command with the -c flag

`sort yellow_tripdata_2017-06.csv | cut -d, -f4 | sort | uniq -c`

and the distribution of the number of passengers is:

[source,bash]
----
      1 
    548 0
6933189 1
1385066 2
 406162 3
 187979 4
 455753 5
 288220 6
     26 7
     30 8
     20 9
      1 passenger_count
----

Notice that we have some extraneous information, i.e., there is one blank line and also one line for the passenger_count (from the header)


== Project 2

Question 1.

Use the airline data stored in this directory:

`/depot/statclass/data/dataexpo2009`

a. What was the average arrival delay (in minutes) for flights in 2005?

b. What was the average departure delay (in minutes) for flights in 2005?

cd. Now revise your solution to 1ab, to account for the delays (of both types) in the full set of data, across all years.


Question 2.

Revise your solutions to 1abcd to only include flights that took place on the weekends.

Question 3.

Consider the June 2017 taxi cab data, which is located in this folder:

`/depot/statclass/data/taxi2018`

What is the average distance of a taxi cab ride in New York City in June 2017?


== Project 3

Use R to revisit these questions.  They can each be accomplished with 1 line of code.

Question 1.

As in Project 1, question 2:  In the year 2005, did United or Delta have more flights?

Question 2.

As in Project 2, question 2a:  Restricting attention to weekends (only), what was the average arrival delay (in minutes) for flights in 2005?

Question 3.

As in Project 1, question 3:  In June 2017, what is the distribution of the number of passengers in the taxi cab rides?

Question 4.

As in Project 2, question 3:   What is the average distance of a taxi cab ride in New York City in June 2017?




== Project 4

Revisit the map code on the STAT 19000 webpage:

http://www.stat.purdue.edu/datamine/19000/

Goal:  Make a map of the State of Indiana, which shows all of Indiana's airports.

Notes:

You will need to install the ggmap package, which takes a few minutes to install.

You can read in the data about the airports from the Data Expo 2009 Supplementary Data:

http://stat-computing.org/dataexpo/2009/supplemental-data.html

It will be necessary to extract (only) the airports with "state" equal to "IN"

It is possible to either dynamically load the longitude and latitude of Indianapolis from Google,

or to manually specify the longitude and latitude (e.g., by looking them up yourself in Google and entering them).

After you plot the State of Indiana with all of the airports shown,

you can print the resulting plot to a pdf file as follows:

dev.print(pdf, "filename.pdf")

Please submit your GitHub code in a ".R" file and also the resulting ".pdf" file.

It is not (yet) necessary to submit your work in RMarkdown.



== Project 5

Question 1.

a.  Compute the average distance for the flights on each airline in 2005.

b.  Sort the result from 1a, and make a dotchart to display the results in sorted order.  (Please display all of the values in the dotchart.)

Hint:  You can use:

`?dotchart`

if you want to read more about how to make a dotchart about the data.


Question 2.

a.  Compute the average total amount of the cost of taxi rides in June 2017, for each pickup location ID.  You can see which variables have the total amount of the cost of the ride, as well as the pickup location ID, if you look at the data dictionary for the yellow taxi cab rides, which you can download here: `http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml`

b.  Sort the result from 2a, and make a dotchart to display the results in sorted order.  (Please ONLY display the results with value bigger than 80.)

Question 3.

Put the two questions above -- including your comments -- into an RMarkdown file.  Submit the .Rmd file itself and either the html or pdf output, when you submit your project in GitHub.



== Project 6

Consider the election donation data:

https://www.fec.gov/data/advanced/?tab=bulk-data

from "Contributions by individuals" for 2017-18. Download this data.


Unzip the file (in the terminal).

Use the cat command to concatenate all of the files in the by_date folder into one large file (in the terminal).

Read the data dictionary:

https://www.fec.gov/campaign-finance-data/contributions-individuals-file-description/


Hint: When working with a file that is not comma separated, you can use the read.delim command in R, and *be sure to specify* the character that separates the various pieces of data on a row.
To do this, you can read the help file for read.delim by typing: ?read.delim
(Look for the "field separator character".)

Also there is no header, so also use header=F


Question 1.

Rank the states according to how many times that their citizens contributed (i.e., total number of donations). Which 5 states made the largest numbers of contributions?

Question 2.

Use awk in the terminal to verify your solution to question 1.

Question 3.

Now (instead) rank the states according to how much money their citizens contributed (i.e., total amount of donations).  Which 5 states contributed the largest amount of money?

(Optional!!) challenge question:  Use awk in the terminal to verify your solution to question 3.
This can be done with 1 line of awk code, but you need to use arrays in awk,
as demonstrated (for instance) on Andrey's solution on this page:

https://unix.stackexchange.com/questions/242946/using-awk-to-sum-the-values-of-a-column-based-on-the-values-of-another-column/242949

Submit your solutions in RMarkdown.
For question 2 (and for the optional challenge question), it is OK to just
put your code into your comments in RMarkdown,
so that the TA's can see how you solved question 2,
but (of course) the awk code does not run in RMarkdown!
You are just showing the awk code to the TA's in this way!


== Project 7

Consider the Lahman baseball database available at:
http://www.seanlahman.com/baseball-archive/statistics/

Download the 2017 comma-delimited version and unzip it.
Inside the "core" folder of the unzipped file, you will find many csv files.

If you want to better understand the contents of the files,
there is a helpful readme file available here:
http://www.seanlahman.com/files/database/readme2017.txt

Question 1.

Use the Batting.csv file (inside the "core" folder) to discover who is a member of the 40-40 club, namely, who has hit 40 home runs and also has (simultaneously) stolen 40 bases in the same season.
Hint: There are multiple ways to solve this question. It is not necessary to use a tapply function. This can be done with one line of code.

Question 2.

Make a plot that depicts the total number of home runs per year (across all players on all teams).  The plot should have the years as the labels for the x-axis, and should have the number of home runs as the labels for the y-axis.
Hints:  Use the tapply function. Save the results of the tapply function in a vector v. If do this, then names(v) will have a list of the years. The plot command has options that include xlab and ylab, so that you can put intelligent labels on the axes, for instance, you can label the x-axis as "years" and the y-axis as "HR".

Question 3.

a.  Try this example: Store the Batting table into a data frame called myBatting. Store the People table into a date frame called myPeople. Merge the two data frames into a new data frame, using the "merge" function: `myDF <- merge(myBatting, myPeople, by="playerID")`

b.  Use the paste command to paste the first and last name columns from myDF into a new vector. Save this new vector as a new column in the data frame myDF.

c.  Return to question 1, and resolve it. Now we can see the person's full name instead of their playerID.



Fun Side Project (to accompany Project 7)

Not required, but fun!

read `Teams.csv` file into a `data.frame` called myDF

break the data.frame into smaller data frames,
according to the `teamID`, using this code:

`by(myDF, myDF$teamID, function(x) {plot(x$W)} )`

For each team, this draws 1 plot of the number of wins per year.  The number of wins will be on the y-axis of the plots.

For an improved version, we can add the years on the x-axis, as follows:

`by(myDF, myDF$teamID, function(x) {plot(x$year, x$W)} )`

Change your working directory in R to a new folder, using the menu option:

`Session -> Set Working Directory -> Choose Directory`

We are going to make 149 new plots!

After changing the directory, try this code, which makes 149 separate pdf files:

`by(myDF, myDF$teamID, function(x) {pdf(as.character(x$teamID[1])); plot(x$year, x$W); dev.off()} )`


== SQL Example 1

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here are the players from the Boston Red Sox in the year 2008

[source,r]
----
myDF <- easyquery("SELECT m.playerID, b.yearID, b.teamID,
                   m.nameFirst, m.nameLast
                   FROM Batting b JOIN Master m
                   ON b.playerID = m.playerID
                   WHERE b.teamID = 'BOS'
                   AND b.yearID = 2008;")
myDF
----

== SQL Example 2

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here are the total number of home runs hit by each player in their entire career

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.playerID, SUM(b.HR)
                   FROM Batting b JOIN Master m
                   ON m.playerID = b.playerID
                   GROUP BY b.playerID;")

myDF
----

Here are the players who hit more than 600 home runs in their careers

`myDF[ myDF$"SUM(b.HR)" >= 600, ]`

== SQL Example 3

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here is basic version for the players who have more than 60 Home Runs during one season.

[source,r]
----
myDF <- easyquery("SELECT b.playerID, b.yearID, b.HR
                  FROM Batting b
                  WHERE b.HR >= 60;")

myDF
----

Here is an improved version, which includes the Batting and the Master table, so that we can have the players' full names.

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                  b.playerID, b.yearID, b.HR
                  FROM Master m JOIN Batting b
                  ON m.playerID = b.playerID
                  WHERE b.HR >= 60;")

myDF
----

== SQL Example 4

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here is basic version for the 40-40 club question. (Same question as last week.)

[source,r]
----
myDF <- easyquery("SELECT b.playerID, b.yearID, b.SB, b.HR
                   FROM Batting b
                   WHERE b.SB >= 40 AND b.HR >= 40;")

myDF
----

Here is an improved version, which includes the Batting and the Master table, so that we can have the players' full names.

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.yearID, b.SB, b.HR
                   FROM Master m JOIN Batting b
                   ON m.playerID = b.playerID
                   WHERE b.SB >= 40 AND b.HR >= 40;")

myDF
----

Here is a further improved version, which includes the Batting, Master, and Teams table, so that we can have the players' full names, and the teams that they played on.

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.yearID, b.SB, b.HR, t.name
                   FROM Master m JOIN Batting b
                   ON m.playerID = b.playerID
                   JOIN Teams t
                   ON b.yearID = t.yearID
                   AND b.teamID = t.teamID
                   WHERE b.SB >= 40 AND b.HR >= 40;")
myDF
----




== Project 8

Question 1.

Modify SQL Example 2 to find the Pitcher who has the most Strikeouts in his career.

Hint:  You need to use a "Pitching p" table instead of a "Batting b" table.

Hint:  The strikeouts are in column "SO" of the Pitching table.

Hint:  This pitcher is named "Nolan Ryan"... but you need to use SQL to figure that out.

I am just trying to give you a way to know when you are correct.

Please momentarily forget that I am giving you the answer at the start!

Question 2.

Which years was Nolan Ryan a pitcher?

For this project, to make your life easier, it is OK to just submit a regular R file, rather than an RMarkdown file.


== Project 9

(Please remember that you have a "ReadMe" file, posted on Piazza last week, which tells you about all of the tables, including the table that tells you where the students went to school.)

1.  Find the first and last names of all players who attended Purdue.
 
2.  Find all of the pitchers who have pitched 300 or more strikeouts during a single season.

In the output, give their first and last name and the year in which this achievement occurred.
(You can just modify Example 3.)

3a. Modify Example 5 to find out which pitchers were able to achieve 300 or more strikeouts AND 20 or more wins during the same season.

3b. Consider the years in which this achievement occurred.  Use R to find the list of distinct years in which this achievement occurred at least once.

Background discussion:

If you look at the example for the 40-40 club (in Example 4), it works because each time that a player achieved 40 (or more) HR's and 40 (or more) SB's during the same season, he was only playing for one team.  A player never got traded to a new team, in any of those years.  Some complications will arise if a player switches teams (i.e., gets traded) during the season.  For this reason, we introduce Example 5.

Here are some notes about Example 5:

If we incorporate the SUM function into a condition, for instance, `WHERE SUM(b.SB) >= 40` the query will not work.  Instead, if the condition has a `SUM` inside it, we change `WHERE` to `HAVING`.  See Example 5 as a perfect example of this.  We can also return the results in a given order, using:  `ORDER BY`  for instance, `ORDER BY by.yearID` if we want to get the results (say) in order by the year.

== SQL Example 5

We only need to install this package 1 time.

`install.packages("RMySQL")`

 No need to run the line above, if you already ran it.

We need to run this library every time we load R.

`library("RMySQL")`

[source,r]
----
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here is basic version for the 30-30 club question.
(Same question as last week.)


[source,r]
----
myDF <- easyquery("SELECT b.playerID, b.yearID, SUM(b.SB), SUM(b.HR)
                   FROM Batting b
                   GROUP BY b.playerID, b.yearID
                   HAVING SUM(b.SB) >= 30 AND SUM(b.HR) >= 30
                   ORDER BY b.yearID;")
myDF
----

Here is an improved version, which includes the Batting and the Master table,
so that we can have the players' full names.


[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.yearID, SUM(b.SB), SUM(b.HR)
                   FROM Master m JOIN Batting b
                   ON m.playerID = b.playerID
                   GROUP BY b.playerID, b.yearID
                   HAVING SUM(b.SB) >= 30 AND SUM(b.HR) >= 30
                   ORDER BY b.yearID;")
myDF
----


== Project 10

Use the results of the National Park Service scraping example to answer the following two questions:

1.	Which states have at least 20 NPS properties?

2.	One zip code has 13 properties in the same zip code!  What are the names of those 13 properties?

If you want to learn XPath (as demonstrated in the case study) to scrape data from a website of your choice, you can make up the grades from 1 or 2 of the previous projects.  If you scrape at least 500 pieces of data from the XML of a page,you can replace the grade from 1 previous project.  If you scrape at least 1000 pieces of data from the XML of a page, you can replace the grade from 2 previous projects.  Your project plan will require written approval from Dr Ward, and it will require you to scrape the data from XML itself (not just download the data).

case study: scraping National Park Service data

[source,r]
----
# This is a short project to download the data about the
# properties in the National Park Service (NPS).
# They are all online through the office NPS webpage:
#   https://www.nps.gov/findapark/index.htm
# (Please note that some parks extend into more than one state.)

# At the end of the project, when we export the data,
# we do not want to use comma-separated values (i.e., a csv file)
# because there are also some commas in our data.
# So we will use tabs as our delimiter at the end of this process.

# We will use the RCurl package to download the NPS files.
# Normally we could just parse the XML (or html) content
# on-the-fly, without downloading the files, but in this case,
# it wasn't working on about 10 of the files, and somehow
# when I downloaded the files, it worked completely.
# I tried this several times, and just going ahead and downloading
# the files seems to be the most consistent solution.
install.packages("RCurl")
library(RCurl)

# We will use the XML package to parse the html (or XML) data
install.packages("XML")
library(XML)

# We will use the xlsx package to export the results at the end,
# into an xlsx file, for viewing in Microsoft Excel, if desired.
install.packages("xlsx")
library(xlsx)

# To see the list of the parks, we can go here:
#    https://www.nps.gov/findapark/index.htm
# in any browser.
# In most browsers, if you navigate to a page and then type:
# Control-U (i.e., the Control Key and the letter U Key at once)
# on a Windows or UNIX machine,
# or if you type Command-U (i.e., the Command Key and the letter U Key at once)
# on an Apple Macintosh machine,
# then you can see the code for the way that the webpage is created.

# This webpage that I mentioned:
#    https://www.nps.gov/findapark/index.htm
# has 1489 lines of code.  Wow.


# From (roughly) lines 206 through 756, we see that the
# data for the parks are wrapped in a "div" (on line 206)
# and then in a "select" (on line 208)
# and then in an "optgroup" and then an "option".
# We want to extract the "value" of each "option".
# (We skip the "label" on line 205 because it ends on line 205 too.)
# So we do the following:

myparks <- xpathSApply(htmlParse(getURL("https://www.nps.gov/findapark/index.htm")), "//*/div/select/optgroup/option", xmlGetAttr, "value")
myparks

# If the line of code (above) doesn't work,
# then perhaps you forgot to actually run the three "library" commands
# near the start of the file.

# We did a lot of things with 1 line of code.
# The "getURL" temporarily downloads all of the code from this webpage.
# We do not save the webpage, but rather, we send it to the htmlParse command.
# Once the page is parsed, we send the parsed results to the xpathSApply command.
# The pattern we want to look for is:
#    "//*/div/select/optgroup/option"
# The star means that anything is OK before this chunk of the pattern,
# but we definitely want our pattern to end with /div/select/optgroup/option
# and then we get the xmlGetAttr attribute called "value"
# which is one of the parks.

# When we check the results, we got 498 results:
length(myparks)

# For the Abraham Lincoln Birthplace, we want to run the following command,
# so that we are prepared to download the webpage.
# After downloading it, we will extract information from the parsed page:
system("mkdir ~/Desktop/myparks/")
download.file("https://www.nps.gov/abli/index.htm", "~/Desktop/myparks/abli.htm")
htmlParse("~/Desktop/myparks/abli.htm")

# but we want to do that for each park.
# So we build the following function:
myparser <- function(x) {
  download.file(paste("https://www.nps.gov/", x, "/index.htm", sep=""), paste("~/Desktop/myparks/", x, ".htm", sep=""))
  htmlParse(paste("~/Desktop/myparks/", x, ".htm", sep=""))
}

# Now, we apply this function to each element of "myparks"
# and we save the results in a variable called "mydocs":
mydocs <- sapply(myparks, myparser)

# The webpage for the Abraham Lincoln Birthplace is now parsed and stored here:
mydocs[[1]]
# The webpage for Zion National Park is now parsed and stored here:
mydocs[[498]]

# Next we look at the source for the Abraham Lincoln Birthplace:
#   https://www.nps.gov/abli/index.htm
# We load that webpage in any browser and then type:
# Control-U if we are on a Windows or UNIX machine, or
# Command-U if we are on a Mac.

# Then we can search in this page (using Control-F on Windows or UNIX,
# or using Command-F on a Mac) for any pattern we want.
# If we search for "itemprop"
# we find the information about the address:

# They are all within a "span" tag, with different "itemprop" attributes:
# The street address has attribute: "streetAddress"
# The city has attribute: "addressLocality"
# The state has attribute: "addressRegion"
# The zip code has attribute: "postalCode"
# The telephone has attribute: "telephone"

# So, for instance, we can find all of these as follows:
xpathSApply(mydocs[[1]], "//*/span[@itemprop='streetAddress']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='addressLocality']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='addressRegion']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='postalCode']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='telephone']", xmlValue)

# Then the title stuff:

xpathSApply(mydocs[[1]], "//*/div[@id='HeroBanner']/div/div/div/a", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@class='Hero-designation']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@class='Hero-location']", xmlValue)

# and, finally, the social media links:

paste(xpathSApply(mydocs[[1]], "//*/div/ul/li[@class='col-xs-6 col-sm-12 col-md-6']/a", xmlGetAttr, "href"),collapse=",")


# Here are the versions for the entire data set:

streets <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='streetAddress']", xmlValue))
cities <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='addressLocality']", xmlValue))
states <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='addressRegion']", xmlValue))
zips <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='postalCode']", xmlValue))
phones <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='telephone']", xmlValue))

mynames <- sapply(mydocs, function(x) xpathSApply(x, "//*/div[@id='HeroBanner']/div/div/div/a", xmlValue))
mytypes <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@class='Hero-designation']", xmlValue))
mylocations <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@class='Hero-location']", xmlValue))

mylinks <- sapply(mydocs, function(x) paste(xpathSApply(x, "//*/div/ul/li[@class='col-xs-6 col-sm-12 col-md-6']/a", xmlGetAttr, "href"),collapse=","))

# with some cleaning up:

streets <- sapply(streets, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
cities <- sapply(cities, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
states <- sapply(states, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
zips <- sapply(zips, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
phones <- sapply(phones, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mynames <- sapply(mynames, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mytypes <- sapply(mytypes, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mylocations <- sapply(mylocations, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mylinks <- sapply(mylinks, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)

myDF <- data.frame(
streets=do.call(rbind,streets),
cities=do.call(rbind,cities),
states=do.call(rbind,states),
zips=do.call(rbind,zips),
phones=do.call(rbind,phones),
mynames=do.call(rbind,mynames),
mytypes=do.call(rbind,mytypes),
mylocations=do.call(rbind,mylocations),
mylinks=do.call(rbind,mylinks)
)
----

Project 11:

The names in the election data are in CAPITAL LETTERS!

When asking about names in the questions, we assume that you are using the names from the election data, available on Scholar.

You might want to practice on a smaller data set:  `/depot/statclass/data/election2018/itsmall.txt`

The full data is available here:  `/depot/statclass/data/election2018/itcont.txt`

We are assuming that you are using unique names from column 8, i.e., that you have already removed duplicates of any names of the donors.

Hint:  Save column 8 (which contains the donor names) into a new variable.  Then extract the unique values from the column using the "unique" command.

Answer these questions using the full data given above.  BUT, for convenience, you might want to *start* by using the smaller data set to practice.

Please note that we can read the data into R using the command:

`myDF <- read.csv("/depot/statclass/data/election2018/itsmall.txt", header=F, sep="|")`

or, for the full data set:

`myDF <- read.csv("/depot/statclass/data/election2018/itcont.txt", header=F, sep="|")`

1.  Find the number of (unique) donor names who have your first name,
    embedded somewhere in the donor's name (not necessarily as the
    first or last name--any location is OK).

2. a. How many donors have a consecutive repeated letter in their name? b. How many donors have a consecutive repeated vowel in their name? c. How many donors have a consecutive repeated consonant in their name?

3.  Just for fun:  Come up with an interesting question about text patterns, and answer it yourself, using regular expressions.  Of course you can compare questions and answers with another member of The Data Mine.  Have fun!

[source,bash]
----

Regular expressions enable us to find patterns in text.
Here are a handful of examples of regular expressions.

The best way to learn them in earnest is to just read some documentation about regular expressions and then try them!

Here is an example:
v <- c("me", "you", "mark", "laura", "kale", "emma", "err", "eat", "queue", "kangaroo", "kangarooooo", "kangarooooooooo")

The elements of v that contain the letter "m":
v[grep("m", v)]

containing the phrase "me":
v[grep("me", v)]

containing the letter "a":
v[grep("a", v)]

containing the letter "e":
v[grep("e", v)]

containing the letter "k":
v[grep("k", v)]

containing the letter "k" at the start of the word:
v[grep("^k", v)]

containing the letter "k" at the end of the word:
v[grep("k$", v)]

containing the letter "a" at the end of the word:
v[grep("a$", v)]

containing the letter "o" at the end of the word:
v[grep("o$", v)]

containing the letter "o" anywhere in the word:
v[grep("o", v)]

containing the letter "o" two times in a row, anywhere in the word:
v[grep("o{2}", v)]

containing the letter "o" three times in a row, anywhere in the word:
v[grep("o{3}", v)]

containing the letter "o" two to five times in a row, anywhere in the word:
v[grep("o{2,5}", v)]

containing the letter "q" followed by "ue":
v[grep("q(ue){1}", v)]

containing the letter "q" followed by "ue" two times:
v[grep("q(ue){2}", v)]

containing the letter "q" followed by "ue" three times:
v[grep("q(ue){3}", v)]

containing the letter "e" followed by "m" or "r":
v[grep("e(m|r)", v)]

again, same idea, but different way, to find words
containing the letter "e" followed by "m" or "r":
v[grep("e[mr]", v)]

containing the letter "e" followed by "ma" or "rr":
v[grep("e(ma|rr)", v)]

containing a repeated letter:
v[grep("([a-z])\\1", v)]
In this example, the \\1 refers to whatever was found in the first match
(which is just given in parentheses for convenience)

Here is a summary of regular expressions:

https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285 

You are welcome to use any source or reference for regular expressions that you like.

We need to use double backslash for back-references, in R.
We gave a demonstration of this, in the last example given above.
In general, in R, when writing a backslash in a regular expression, a double backslash is usually needed.
----


