= TDM Fall 2018 STAT 19000 Projects

== Project 1

Question 1.

Use the airline data stored in this directory:

`/depot/statclass/data/dataexpo2009`

In the year 2005, find:

a.  the number of flights that occurred, on every day of the year, and

b.  find the day of the year on which the most flights occur.

Solution:

We switch to the directory for the airline data

`cd /depot/statclass/data/dataexpo2009`

a. The number of flights that occurred, on every day of the year, can be obtained by extracting the 1st, 2nd, and 3rd fields, sorting the data, and then summarizing the data using the uniq command with the -c flag

`sort 2005.csv | cut -d, -f1-3 | sort | uniq -c`

The first few lines of the output are:

[source,bash]
----
16477 2005,10,1
19885 2005,10,10
19515 2005,10,11
19701 2005,10,12
19883 2005,10,13
----

and the last few lines of the output are:

[source,bash]
----
20051 2005,9,6
19629 2005,9,7
19968 2005,9,8
19938 2005,9,9
    1 Year,Month,DayofMonth
----

b. The day of the year on which the most flights occur can be found by sorting the results above, in numerical order, using sort -n and then (if desired, although it is optional) we can extract the last line of the output using tail -n1

`sort 2005.csv | cut -d, -f1-3 | sort | uniq -c | sort -n | tail -n1`

and we conclude that the most flights occur on August 5:

`21041 2005,8,5`


Question 2.

Again considering the year 2005, did United or Delta have more flights?

Solution:

We can extract the 9th field, which is the carrier (i.e., the airline company) and then, in the same way as above, we can sort the data, and then we can summarize the data using uniq -c

This yields the number of flights for each carrier. We can either read the number of United or Delta flights with our eyeballs, or we can use the grep command, searching for both the pattern UA and DL to isolate (only) the number of flights for United and Delta, respectively.

`sort 2005.csv | cut -d, -f9 | sort | uniq -c | grep "UA\|DL"`

The output is

[source,bash]
----
658302 DL
485918 UA
----

so Delta has more flights than United in 2005.


Question 3.

Consider the June 2017 taxi cab data, which is located in this folder:

`/depot/statclass/data/taxi2018`

What is the distribution of the number of passengers in the taxi cab rides?  In other words, make a list of the number of rides that have 1 passenger; that have 2 passengers; etc.

Solution:

Now we change directories to consider the taxi cab data

`cd ../taxi2018`

The ".." in the previous command just indicates that we want to go up one level to

`/depot/statclass/data`

and then, from that point, we want to go into the taxi cab directory. If this sounds complicated, then (instead) it is safe to use the longer version:

`cd /depot/statclass/data/taxi2018`

The number of passengers is given in the 4th column, `passenger_count`

We use a method that is similar to the one from the first three questions, we extract the 4th column, sort the data, and then summarizing the data using the uniq command with the -c flag

`sort yellow_tripdata_2017-06.csv | cut -d, -f4 | sort | uniq -c`

and the distribution of the number of passengers is:

[source,bash]
----
      1 
    548 0
6933189 1
1385066 2
 406162 3
 187979 4
 455753 5
 288220 6
     26 7
     30 8
     20 9
      1 passenger_count
----

Notice that we have some extraneous information, i.e., there is one blank line and also one line for the passenger_count (from the header)


== Project 2

Question 1.

Use the airline data stored in this directory:

`/depot/statclass/data/dataexpo2009`

a. What was the average arrival delay (in minutes) for flights in 2005?

b. What was the average departure delay (in minutes) for flights in 2005?

cd. Now revise your solution to 1ab, to account for the delays (of both types) in the full set of data, across all years.


Question 2.

Revise your solutions to 1abcd to only include flights that took place on the weekends.

Question 3.

Consider the June 2017 taxi cab data, which is located in this folder:

`/depot/statclass/data/taxi2018`

What is the average distance of a taxi cab ride in New York City in June 2017?


== Project 3

Use R to revisit these questions.  They can each be accomplished with 1 line of code.

Question 1.

As in Project 1, question 2:  In the year 2005, did United or Delta have more flights?

Question 2.

As in Project 2, question 2a:  Restricting attention to weekends (only), what was the average arrival delay (in minutes) for flights in 2005?

Question 3.

As in Project 1, question 3:  In June 2017, what is the distribution of the number of passengers in the taxi cab rides?

Question 4.

As in Project 2, question 3:   What is the average distance of a taxi cab ride in New York City in June 2017?




== Project 4

Revisit the map code on the STAT 19000 webpage:

http://www.stat.purdue.edu/datamine/19000/

Goal:  Make a map of the State of Indiana, which shows all of Indiana's airports.

Notes:

You will need to install the ggmap package, which takes a few minutes to install.

You can read in the data about the airports from the Data Expo 2009 Supplementary Data:

http://stat-computing.org/dataexpo/2009/supplemental-data.html

It will be necessary to extract (only) the airports with "state" equal to "IN"

It is possible to either dynamically load the longitude and latitude of Indianapolis from Google,

or to manually specify the longitude and latitude (e.g., by looking them up yourself in Google and entering them).

After you plot the State of Indiana with all of the airports shown,

you can print the resulting plot to a pdf file as follows:

dev.print(pdf, "filename.pdf")

Please submit your GitHub code in a ".R" file and also the resulting ".pdf" file.

It is not (yet) necessary to submit your work in RMarkdown.



== Project 5

Question 1.

a.  Compute the average distance for the flights on each airline in 2005.

b.  Sort the result from 1a, and make a dotchart to display the results in sorted order.  (Please display all of the values in the dotchart.)

Hint:  You can use:

`?dotchart`

if you want to read more about how to make a dotchart about the data.


Question 2.

a.  Compute the average total amount of the cost of taxi rides in June 2017, for each pickup location ID.  You can see which variables have the total amount of the cost of the ride, as well as the pickup location ID, if you look at the data dictionary for the yellow taxi cab rides, which you can download here: `http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml`

b.  Sort the result from 2a, and make a dotchart to display the results in sorted order.  (Please ONLY display the results with value bigger than 80.)

Question 3.

Put the two questions above -- including your comments -- into an RMarkdown file.  Submit the .Rmd file itself and either the html or pdf output, when you submit your project in GitHub.



== Project 6

Consider the election donation data:

https://www.fec.gov/data/advanced/?tab=bulk-data

from "Contributions by individuals" for 2017-18. Download this data.


Unzip the file (in the terminal).

Use the cat command to concatenate all of the files in the by_date folder into one large file (in the terminal).

Read the data dictionary:

https://www.fec.gov/campaign-finance-data/contributions-individuals-file-description/


Hint: When working with a file that is not comma separated, you can use the read.delim command in R, and *be sure to specify* the character that separates the various pieces of data on a row.
To do this, you can read the help file for read.delim by typing: ?read.delim
(Look for the "field separator character".)

Also there is no header, so also use header=F


Question 1.

Rank the states according to how many times that their citizens contributed (i.e., total number of donations). Which 5 states made the largest numbers of contributions?

Question 2.

Use awk in the terminal to verify your solution to question 1.

Question 3.

Now (instead) rank the states according to how much money their citizens contributed (i.e., total amount of donations).  Which 5 states contributed the largest amount of money?

(Optional!!) challenge question:  Use awk in the terminal to verify your solution to question 3.
This can be done with 1 line of awk code, but you need to use arrays in awk,
as demonstrated (for instance) on Andrey's solution on this page:

https://unix.stackexchange.com/questions/242946/using-awk-to-sum-the-values-of-a-column-based-on-the-values-of-another-column/242949

Submit your solutions in RMarkdown.
For question 2 (and for the optional challenge question), it is OK to just
put your code into your comments in RMarkdown,
so that the TA's can see how you solved question 2,
but (of course) the awk code does not run in RMarkdown!
You are just showing the awk code to the TA's in this way!


== Project 7

Consider the Lahman baseball database available at:
http://www.seanlahman.com/baseball-archive/statistics/

Download the 2017 comma-delimited version and unzip it.
Inside the "core" folder of the unzipped file, you will find many csv files.

If you want to better understand the contents of the files,
there is a helpful readme file available here:
http://www.seanlahman.com/files/database/readme2017.txt

Question 1.

Use the Batting.csv file (inside the "core" folder) to discover who is a member of the 40-40 club, namely, who has hit 40 home runs and also has (simultaneously) stolen 40 bases in the same season.
Hint: There are multiple ways to solve this question. It is not necessary to use a tapply function. This can be done with one line of code.

Question 2.

Make a plot that depicts the total number of home runs per year (across all players on all teams).  The plot should have the years as the labels for the x-axis, and should have the number of home runs as the labels for the y-axis.
Hints:  Use the tapply function. Save the results of the tapply function in a vector v. If do this, then names(v) will have a list of the years. The plot command has options that include xlab and ylab, so that you can put intelligent labels on the axes, for instance, you can label the x-axis as "years" and the y-axis as "HR".

Question 3.

a.  Try this example: Store the Batting table into a data frame called myBatting. Store the People table into a date frame called myPeople. Merge the two data frames into a new data frame, using the "merge" function: `myDF <- merge(myBatting, myPeople, by="playerID")`

b.  Use the paste command to paste the first and last name columns from myDF into a new vector. Save this new vector as a new column in the data frame myDF.

c.  Return to question 1, and resolve it. Now we can see the person's full name instead of their playerID.



Fun Side Project (to accompany Project 7)

Not required, but fun!

read `Teams.csv` file into a `data.frame` called myDF

break the data.frame into smaller data frames,
according to the `teamID`, using this code:

`by(myDF, myDF$teamID, function(x) {plot(x$W)} )`

For each team, this draws 1 plot of the number of wins per year.  The number of wins will be on the y-axis of the plots.

For an improved version, we can add the years on the x-axis, as follows:

`by(myDF, myDF$teamID, function(x) {plot(x$year, x$W)} )`

Change your working directory in R to a new folder, using the menu option:

`Session -> Set Working Directory -> Choose Directory`

We are going to make 149 new plots!

After changing the directory, try this code, which makes 149 separate pdf files:

`by(myDF, myDF$teamID, function(x) {pdf(as.character(x$teamID[1])); plot(x$year, x$W); dev.off()} )`


== SQL Example 1

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here are the players from the Boston Red Sox in the year 2008

[source,r]
----
myDF <- easyquery("SELECT m.playerID, b.yearID, b.teamID,
                   m.nameFirst, m.nameLast
                   FROM Batting b JOIN Master m
                   ON b.playerID = m.playerID
                   WHERE b.teamID = 'BOS'
                   AND b.yearID = 2008;")
myDF
----

== SQL Example 2

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here are the total number of home runs hit by each player in their entire career

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.playerID, SUM(b.HR)
                   FROM Batting b JOIN Master m
                   ON m.playerID = b.playerID
                   GROUP BY b.playerID;")

myDF
----

Here are the players who hit more than 600 home runs in their careers

`myDF[ myDF$"SUM(b.HR)" >= 600, ]`

== SQL Example 3

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here is basic version for the players who have more than 60 Home Runs during one season.

[source,r]
----
myDF <- easyquery("SELECT b.playerID, b.yearID, b.HR
                  FROM Batting b
                  WHERE b.HR >= 60;")

myDF
----

Here is an improved version, which includes the Batting and the Master table, so that we can have the players' full names.

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                  b.playerID, b.yearID, b.HR
                  FROM Master m JOIN Batting b
                  ON m.playerID = b.playerID
                  WHERE b.HR >= 60;")

myDF
----

== SQL Example 4

We only need to install this package 1 time.

`install.packages("RMySQL")`

No need to run the line above, if you already ran it.

We need to run this library every time we load R.

[source,r]
----
library("RMySQL")
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here is basic version for the 40-40 club question. (Same question as last week.)

[source,r]
----
myDF <- easyquery("SELECT b.playerID, b.yearID, b.SB, b.HR
                   FROM Batting b
                   WHERE b.SB >= 40 AND b.HR >= 40;")

myDF
----

Here is an improved version, which includes the Batting and the Master table, so that we can have the players' full names.

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.yearID, b.SB, b.HR
                   FROM Master m JOIN Batting b
                   ON m.playerID = b.playerID
                   WHERE b.SB >= 40 AND b.HR >= 40;")

myDF
----

Here is a further improved version, which includes the Batting, Master, and Teams table, so that we can have the players' full names, and the teams that they played on.

[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.yearID, b.SB, b.HR, t.name
                   FROM Master m JOIN Batting b
                   ON m.playerID = b.playerID
                   JOIN Teams t
                   ON b.yearID = t.yearID
                   AND b.teamID = t.teamID
                   WHERE b.SB >= 40 AND b.HR >= 40;")
myDF
----




== Project 8

Question 1.

Modify SQL Example 2 to find the Pitcher who has the most Strikeouts in his career.

Hint:  You need to use a "Pitching p" table instead of a "Batting b" table.

Hint:  The strikeouts are in column "SO" of the Pitching table.

Hint:  This pitcher is named "Nolan Ryan"... but you need to use SQL to figure that out.

I am just trying to give you a way to know when you are correct.

Please momentarily forget that I am giving you the answer at the start!

Question 2.

Which years was Nolan Ryan a pitcher?

For this project, to make your life easier, it is OK to just submit a regular R file, rather than an RMarkdown file.


== Project 9

(Please remember that you have a "ReadMe" file, posted on Piazza last week, which tells you about all of the tables, including the table that tells you where the students went to school.)

1.  Find the first and last names of all players who attended Purdue.
 
2.  Find all of the pitchers who have pitched 300 or more strikeouts during a single season.

In the output, give their first and last name and the year in which this achievement occurred.
(You can just modify Example 3.)

3a. Modify Example 5 to find out which pitchers were able to achieve 300 or more strikeouts AND 20 or more wins during the same season.

3b. Consider the years in which this achievement occurred.  Use R to find the list of distinct years in which this achievement occurred at least once.

Background discussion:

If you look at the example for the 40-40 club (in Example 4), it works because each time that a player achieved 40 (or more) HR's and 40 (or more) SB's during the same season, he was only playing for one team.  A player never got traded to a new team, in any of those years.  Some complications will arise if a player switches teams (i.e., gets traded) during the season.  For this reason, we introduce Example 5.

Here are some notes about Example 5:

If we incorporate the SUM function into a condition, for instance, `WHERE SUM(b.SB) >= 40` the query will not work.  Instead, if the condition has a `SUM` inside it, we change `WHERE` to `HAVING`.  See Example 5 as a perfect example of this.  We can also return the results in a given order, using:  `ORDER BY`  for instance, `ORDER BY by.yearID` if we want to get the results (say) in order by the year.

== SQL Example 5

We only need to install this package 1 time.

`install.packages("RMySQL")`

 No need to run the line above, if you already ran it.

We need to run this library every time we load R.

`library("RMySQL")`

[source,r]
----
myconnection <- dbConnect(dbDriver("MySQL"),
                          host="mydb.ics.purdue.edu",
                          username="mdw_guest",
                          password="MDW_csp2018",
                          dbname="mdw")

easyquery <- function(x) {
  fetch(dbSendQuery(myconnection, x), n=-1)
}
----

Here is basic version for the 30-30 club question.
(Same question as last week.)


[source,r]
----
myDF <- easyquery("SELECT b.playerID, b.yearID, SUM(b.SB), SUM(b.HR)
                   FROM Batting b
                   GROUP BY b.playerID, b.yearID
                   HAVING SUM(b.SB) >= 30 AND SUM(b.HR) >= 30
                   ORDER BY b.yearID;")
myDF
----

Here is an improved version, which includes the Batting and the Master table,
so that we can have the players' full names.


[source,r]
----
myDF <- easyquery("SELECT m.nameFirst, m.nameLast,
                   b.yearID, SUM(b.SB), SUM(b.HR)
                   FROM Master m JOIN Batting b
                   ON m.playerID = b.playerID
                   GROUP BY b.playerID, b.yearID
                   HAVING SUM(b.SB) >= 30 AND SUM(b.HR) >= 30
                   ORDER BY b.yearID;")
myDF
----


== Project 10

Use the results of the National Park Service scraping example to answer the following two questions:

1.	Which states have at least 20 NPS properties?

2.	One zip code has 13 properties in the same zip code!  What are the names of those 13 properties?

If you want to learn XPath (as demonstrated in the case study) to scrape data from a website of your choice, you can make up the grades from 1 or 2 of the previous projects.  If you scrape at least 500 pieces of data from the XML of a page,you can replace the grade from 1 previous project.  If you scrape at least 1000 pieces of data from the XML of a page, you can replace the grade from 2 previous projects.  Your project plan will require written approval from Dr Ward, and it will require you to scrape the data from XML itself (not just download the data).

case study: scraping National Park Service data

[source,r]
----
# This is a short project to download the data about the
# properties in the National Park Service (NPS).
# They are all online through the office NPS webpage:
#   https://www.nps.gov/findapark/index.htm
# (Please note that some parks extend into more than one state.)

# At the end of the project, when we export the data,
# we do not want to use comma-separated values (i.e., a csv file)
# because there are also some commas in our data.
# So we will use tabs as our delimiter at the end of this process.

# We will use the RCurl package to download the NPS files.
# Normally we could just parse the XML (or html) content
# on-the-fly, without downloading the files, but in this case,
# it wasn't working on about 10 of the files, and somehow
# when I downloaded the files, it worked completely.
# I tried this several times, and just going ahead and downloading
# the files seems to be the most consistent solution.
install.packages("RCurl")
library(RCurl)

# We will use the XML package to parse the html (or XML) data
install.packages("XML")
library(XML)

# We will use the xlsx package to export the results at the end,
# into an xlsx file, for viewing in Microsoft Excel, if desired.
install.packages("xlsx")
library(xlsx)

# To see the list of the parks, we can go here:
#    https://www.nps.gov/findapark/index.htm
# in any browser.
# In most browsers, if you navigate to a page and then type:
# Control-U (i.e., the Control Key and the letter U Key at once)
# on a Windows or UNIX machine,
# or if you type Command-U (i.e., the Command Key and the letter U Key at once)
# on an Apple Macintosh machine,
# then you can see the code for the way that the webpage is created.

# This webpage that I mentioned:
#    https://www.nps.gov/findapark/index.htm
# has 1489 lines of code.  Wow.


# From (roughly) lines 206 through 756, we see that the
# data for the parks are wrapped in a "div" (on line 206)
# and then in a "select" (on line 208)
# and then in an "optgroup" and then an "option".
# We want to extract the "value" of each "option".
# (We skip the "label" on line 205 because it ends on line 205 too.)
# So we do the following:

myparks <- xpathSApply(htmlParse(getURL("https://www.nps.gov/findapark/index.htm")), "//*/div/select/optgroup/option", xmlGetAttr, "value")
myparks

# If the line of code (above) doesn't work,
# then perhaps you forgot to actually run the three "library" commands
# near the start of the file.

# We did a lot of things with 1 line of code.
# The "getURL" temporarily downloads all of the code from this webpage.
# We do not save the webpage, but rather, we send it to the htmlParse command.
# Once the page is parsed, we send the parsed results to the xpathSApply command.
# The pattern we want to look for is:
#    "//*/div/select/optgroup/option"
# The star means that anything is OK before this chunk of the pattern,
# but we definitely want our pattern to end with /div/select/optgroup/option
# and then we get the xmlGetAttr attribute called "value"
# which is one of the parks.

# When we check the results, we got 498 results:
length(myparks)

# For the Abraham Lincoln Birthplace, we want to run the following command,
# so that we are prepared to download the webpage.
# After downloading it, we will extract information from the parsed page:
system("mkdir ~/Desktop/myparks/")
download.file("https://www.nps.gov/abli/index.htm", "~/Desktop/myparks/abli.htm")
htmlParse("~/Desktop/myparks/abli.htm")

# but we want to do that for each park.
# So we build the following function:
myparser <- function(x) {
  download.file(paste("https://www.nps.gov/", x, "/index.htm", sep=""), paste("~/Desktop/myparks/", x, ".htm", sep=""))
  htmlParse(paste("~/Desktop/myparks/", x, ".htm", sep=""))
}

# Now, we apply this function to each element of "myparks"
# and we save the results in a variable called "mydocs":
mydocs <- sapply(myparks, myparser)

# The webpage for the Abraham Lincoln Birthplace is now parsed and stored here:
mydocs[[1]]
# The webpage for Zion National Park is now parsed and stored here:
mydocs[[498]]

# Next we look at the source for the Abraham Lincoln Birthplace:
#   https://www.nps.gov/abli/index.htm
# We load that webpage in any browser and then type:
# Control-U if we are on a Windows or UNIX machine, or
# Command-U if we are on a Mac.

# Then we can search in this page (using Control-F on Windows or UNIX,
# or using Command-F on a Mac) for any pattern we want.
# If we search for "itemprop"
# we find the information about the address:

# They are all within a "span" tag, with different "itemprop" attributes:
# The street address has attribute: "streetAddress"
# The city has attribute: "addressLocality"
# The state has attribute: "addressRegion"
# The zip code has attribute: "postalCode"
# The telephone has attribute: "telephone"

# So, for instance, we can find all of these as follows:
xpathSApply(mydocs[[1]], "//*/span[@itemprop='streetAddress']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='addressLocality']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='addressRegion']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='postalCode']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@itemprop='telephone']", xmlValue)

# Then the title stuff:

xpathSApply(mydocs[[1]], "//*/div[@id='HeroBanner']/div/div/div/a", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@class='Hero-designation']", xmlValue)
xpathSApply(mydocs[[1]], "//*/span[@class='Hero-location']", xmlValue)

# and, finally, the social media links:

paste(xpathSApply(mydocs[[1]], "//*/div/ul/li[@class='col-xs-6 col-sm-12 col-md-6']/a", xmlGetAttr, "href"),collapse=",")


# Here are the versions for the entire data set:

streets <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='streetAddress']", xmlValue))
cities <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='addressLocality']", xmlValue))
states <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='addressRegion']", xmlValue))
zips <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='postalCode']", xmlValue))
phones <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@itemprop='telephone']", xmlValue))

mynames <- sapply(mydocs, function(x) xpathSApply(x, "//*/div[@id='HeroBanner']/div/div/div/a", xmlValue))
mytypes <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@class='Hero-designation']", xmlValue))
mylocations <- sapply(mydocs, function(x) xpathSApply(x, "//*/span[@class='Hero-location']", xmlValue))

mylinks <- sapply(mydocs, function(x) paste(xpathSApply(x, "//*/div/ul/li[@class='col-xs-6 col-sm-12 col-md-6']/a", xmlGetAttr, "href"),collapse=","))

# with some cleaning up:

streets <- sapply(streets, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
cities <- sapply(cities, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
states <- sapply(states, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
zips <- sapply(zips, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
phones <- sapply(phones, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mynames <- sapply(mynames, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mytypes <- sapply(mytypes, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mylocations <- sapply(mylocations, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)
mylinks <- sapply(mylinks, function(x) ifelse(length(x)==0,NA,sub("^\\s+","",sub("\\s+$","",x))), simplify=FALSE)

myDF <- data.frame(
streets=do.call(rbind,streets),
cities=do.call(rbind,cities),
states=do.call(rbind,states),
zips=do.call(rbind,zips),
phones=do.call(rbind,phones),
mynames=do.call(rbind,mynames),
mytypes=do.call(rbind,mytypes),
mylocations=do.call(rbind,mylocations),
mylinks=do.call(rbind,mylinks)
)
----

Project 11:

The names in the election data are in CAPITAL LETTERS!

When asking about names in the questions, we assume that you are using the names from the election data, available on Scholar.

You might want to practice on a smaller data set:  `/depot/statclass/data/election2018/itsmall.txt`

The full data is available here:  `/depot/statclass/data/election2018/itcont.txt`

We are assuming that you are using unique names from column 8, i.e., that you have already removed duplicates of any names of the donors.

Hint:  Save column 8 (which contains the donor names) into a new variable.  Then extract the unique values from the column using the "unique" command.

Answer these questions using the full data given above.  BUT, for convenience, you might want to *start* by using the smaller data set to practice.

Please note that we can read the data into R using the command:

`myDF <- read.csv("/depot/statclass/data/election2018/itsmall.txt", header=F, sep="|")`

or, for the full data set:

`myDF <- read.csv("/depot/statclass/data/election2018/itcont.txt", header=F, sep="|")`

1.  Find the number of (unique) donor names who have your first name,
    embedded somewhere in the donor's name (not necessarily as the
    first or last name--any location is OK).

2. a. How many donors have a consecutive repeated letter in their name? b. How many donors have a consecutive repeated vowel in their name? c. How many donors have a consecutive repeated consonant in their name?

3.  Just for fun:  Come up with an interesting question about text patterns, and answer it yourself, using regular expressions.  Of course you can compare questions and answers with another member of The Data Mine.  Have fun!

[source,bash]
----

Regular expressions enable us to find patterns in text.
Here are a handful of examples of regular expressions.

The best way to learn them in earnest is to just read some documentation about regular expressions and then try them!

Here is an example:
v <- c("me", "you", "mark", "laura", "kale", "emma", "err", "eat", "queue", "kangaroo", "kangarooooo", "kangarooooooooo")

The elements of v that contain the letter "m":
v[grep("m", v)]

containing the phrase "me":
v[grep("me", v)]

containing the letter "a":
v[grep("a", v)]

containing the letter "e":
v[grep("e", v)]

containing the letter "k":
v[grep("k", v)]

containing the letter "k" at the start of the word:
v[grep("^k", v)]

containing the letter "k" at the end of the word:
v[grep("k$", v)]

containing the letter "a" at the end of the word:
v[grep("a$", v)]

containing the letter "o" at the end of the word:
v[grep("o$", v)]

containing the letter "o" anywhere in the word:
v[grep("o", v)]

containing the letter "o" two times in a row, anywhere in the word:
v[grep("o{2}", v)]

containing the letter "o" three times in a row, anywhere in the word:
v[grep("o{3}", v)]

containing the letter "o" two to five times in a row, anywhere in the word:
v[grep("o{2,5}", v)]

containing the letter "q" followed by "ue":
v[grep("q(ue){1}", v)]

containing the letter "q" followed by "ue" two times:
v[grep("q(ue){2}", v)]

containing the letter "q" followed by "ue" three times:
v[grep("q(ue){3}", v)]

containing the letter "e" followed by "m" or "r":
v[grep("e(m|r)", v)]

again, same idea, but different way, to find words
containing the letter "e" followed by "m" or "r":
v[grep("e[mr]", v)]

containing the letter "e" followed by "ma" or "rr":
v[grep("e(ma|rr)", v)]

containing a repeated letter:
v[grep("([a-z])\\1", v)]
In this example, the \\1 refers to whatever was found in the first match
(which is just given in parentheses for convenience)

Here is a summary of regular expressions:

https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285 

You are welcome to use any source or reference for regular expressions that you like.

We need to use double backslash for back-references, in R.
We gave a demonstration of this, in the last example given above.
In general, in R, when writing a backslash in a regular expression, a double backslash is usually needed.
----


== Project 12

There was no project 12

== Project 13

There was no project 13

== Project 14

Remind ourselves how to use bash and awk tools (previously we did this in the terminal).

We will do it in Jupyter Notebooks this semester:  `http://notebook.scholar.rcac.purdue.edu/`

1. a. Start a new Jupyter Notebook with type "bash" (instead of "R").  We are going to put bash code directly inside the Jupyter Notebook.  (In the past, we only wrote bash code directly inside the terminal.)  b.  Look at the first 10 lines of the 2007 flight data, which is found at:  `/depot/statclass/data/dataexpo2009/2007.csv`  All of the flights in those first 10 lines are on the same carrier.  Which carrier is it?  Remember that you can check:  `http://stat-computing.org/dataexpo/2009/the-data.html`  Now we are going to put awk code directly inside the Jupyter Notebook.  (In the past, we only wrote awk code directly inside the terminal.)

2.  Save the information about every flight departing from Indianapolis since January 1, 2000 into a common file, named `MyIndyFlights.csv`

Hint 1:  You only need the files 2000.csv, 2001.csv, ..., 2008.csv  You can work on all of those files at once, using 2*.csv because the "*" is like a wildcard, that matches any pattern.

Hint 2:  You can use awk to do this.  For comparison, ONLY as an example, we can extract all flights
that are on Delta airlines in 1998 as follows:
`cat /depot/statclass/data/dataexpo2009/1998.csv | awk -F, '{ if($9 == "DL") {print $0} }' >MyDeltaFlights.csv`

== Project 14 Solutions


[source,bash]
----
# 1.  The head of the file with the 2007 flights is: 
head /depot/statclass/data/dataexpo2009/2007.csv 

#     We see that the UniqueCarrier is found in column 9. 
#     One way to extract the UniqueCarrier is with the cut command 
#     using a comma as the delimiter and retrieving (cut out) the 9th column: 
cut -d, -f9 /depot/statclass/data/dataexpo2009/2007.csv | head -n11 
#     We only displayed the head, because we only want the first 10 flights. 
#     We specified -n11 because this prints the first 11 lines of the file, 
#     namely, the header itself, and the first 10 flights. 
#     We can check the data dictionary, available at:   http://stat-computing.org/dataexpo/2009/ 
#     The information about the carrier codes is found there, 
#     by clicking on the link for supplemental data sources:   http://stat-computing.org/dataexpo/2009/supplemental-data.html 
and then choosing the carriers file:  http://stat-computing.org/dataexpo/2009/carriers.csv 
#     The carrier code "WN" for each of these first ten flights is Southwest. 

# 2.  We save the information about the Indianapolis flights by using awk. 
#     First we recall how to see the information about all such flights. 
#     Here are the first 10 lines of that data. 
cat /depot/statclass/data/dataexpo2009/2*.csv | head 
#     Then we change the "head" to the "awk" command. 
#     We use comma as the field separator 
#     (this is the same as the role of the delimiter from cut) 
#     We modify the example from the project assignment, 
#     so that we focus on the 17th field (which are the Origin airports) 
#     and we save the resulting data into a file called MyIndyFlights.csv 

cat /depot/statclass/data/dataexpo2009/2*.csv | awk -F, '{ if($17 == "IND") {print $0} }' >MyIndyFlights.csv 
#     Some of you were not working in your home directory when you ran this commmand. 
#     If you want to be sure to save the file into your home directory, 
#     remember that you can explicitly specify your home directory using a tilde, as follows: 
cat /depot/statclass/data/dataexpo2009/2*.csv | awk -F, '{ if($17 == "IND") {print $0} }' >~/MyIndyFlights.csv 
#     It is not required that you check things, 
#     but if you want to check that things worked properly, you can use the wc command 
#     which gives the number of lines, words, and bytes in the resulting file: 
wc MyIndyFlights.csv 
#     or, even more explicitly, 
wc ~/MyIndyFlights.csv 
#     An alternative is to check the head and the tail: 
head MyIndyFlights.csv 
tail MyIndyFlights.csv 
#     or, even more explicitly, 
head ~/MyIndyFlights.csv 
tail ~/MyIndyFlights.csv 
----

== Project 15

Remind ourselves how to use R tools (previously we did this in the terminal).  We will do it in Jupyter Notebooks this semester.

Question 1

a.  Start a new Jupyter Notebook with type "R"

b.  Import the flight data from the file MyIndyFlights.csv in a data frame.  You just created this file in Project 14.  It contains all of the flights that departed from Indianapolis since January 1, 2000.  (There should be 356561 flights altogether, and there is no header.)   Hint:  When you import the data, if you use the read.csv command, there is no header, so be sure to use header=FALSE.

c.  What are the five most popular destinations for travelers who depart Indianapolis since January 1, 2000?  List each of these 5 destinations, and the number of flights to each one.


Question 2

a.  Consider the year 2005 (only).  Tabulate the number of flights per day.

b.  On each of the most popular five days, how many flights are there?

c.  On each of the least popular five days, how many flights are there?

Hint:  You might be surprised to see the wide range of the number of flights per day!

== Project 15 Solutions


[source,R]
----

# 1.  We first import the flight data from the file MyIndyFlights.csv 

myDF <- read.csv("MyIndyFlights.csv", header=F) 

# or, if you prefer to explicitly state that the file 
# is in your home directory, you can add the tilde for your home: 

myDF <- read.csv("~/MyIndyFlights.csv", header=F) 

# We check that there are 356561 flights altogether: 

dim(myDF) 

# The five most popular destinations for travelers 
# who depart Indianapolis since January 1, 2000 are: 
tail(sort(table(myDF[[18]])),n=5) 

# We used the 18th column, which has the Destination airports. 
# We tabulated the results, using the table command, 
# and then we sorted the results. 
# Finally, at the end, we took the tail of the results, 
# using n=5, since we wanted to see the largest 5 values. 

# 2a.  We load the 2005 data: 

myDF <- read.csv("/depot/statclass/data/dataexpo2009/2005.csv") 

# To get the number of flights per day, 
# we can first paste together the Month and Day columns. 
# We check the head, to make sure that this worked: 

head(paste(myDF$Month, myDF$DayofMonth)) 

# It is also possible, for instance, to separate the 
# month and the day by separators, such as a slash: 

head(paste(myDF$Month, myDF$DayofMonth, sep="/")) 

# or a dash: 

head(paste(myDF$Month, myDF$DayofMonth, sep="-")) 

# Now we can tabulate the number of flights per day, 
# using the table command: 

table(paste(myDF$Month, myDF$DayofMonth, sep="/")) 

# 2b. To find the most popular five days, 
#     we can sort the table, and then consider the tail, 
#     using the n=5 option, 
#     since we only want the 5 most popular dates. 

tail(sort(table(paste(myDF$Month, myDF$DayofMonth, sep="/"))),n=5) 

# 2c. We just change tail to head, 
#     to find the 5 least popular dates: 

head(sort(table(paste(myDF$Month, myDF$DayofMonth, sep="/"))),n=5)

----


== Project 16

Project 16 needs to be saved as a `.ipynb` file. This is different from the previous two assignments where the file was uploaded directly from each students Github page. Students need to download it from this link. Thanks!

https://raw.githubusercontent.com/TheDataMine/STAT-19000/master/Assignments/hw16.ipynb

Question 1

Consider the flights from 2005 in the Data Expo 2009 data set.  The actual departure times, as you know, are given in the DepTime column. In this question, we want to categorize the departure times according to the hour of departure.  For instance, any time in the 4 o'clock in the (very early morning) hour should be classified together.  These are the times between 0400 and 0459 (because the times are given in military time). One way to do this is to divide each of the times by 100, and then to take the "floor" of the results, and then make a "table" of the results.  For practice (just to understand things), give this a try with the head of the DepTime, one step at a time, to make sure that you understand what is happening.  Then: a.  Classify all of the 2005 departure times, according to the hour of departure, using this method. b.  During which hour of the day did the most flights depart?

Question 2

a.  Here is another way to solve the question above.  Read the documentation for the "cut" command.  For the "breaks" parameter, use:
seq(0, 2900, by=100)
and be sure to set the parameter "right" to be FALSE.

b.  Check that you get the same result as in question 1, using this method.

c.  Why did we choose to use 2900 instead of (say) 2400 in this method?

== Project 16 Solutions


[source,R]
----
# 1a.  We read the data from the 2005 flights into a data frame 

myDF <- read.csv("/depot/statclass/data/dataexpo2009/2005.csv") 

#      Then we divide each time by 100 and take the floor: 

table(floor(myDF$DepTime/100)) 

#      and we get: 

#      0      1      2      3      4      5      6      7 8      9     10 
#  21747   7092   2027    458   1610 114469 430723 440532 469386 447705 432526 
#     11     12     13     14     15     16     17     18     19 20     21 
# 446432 443252 440903 416661 441021 424299 457678 431613 390398 321680 235810 
#     22     23     24     25     26     27     28 
# 128382  58386   1711    301     56      7      1 

# 1b.  The most flights departed during 8 AM to 9 AM; 

sort(table(floor(myDF$DepTime/100))) 

#     28     27     26     25      3      4     24      2 1      0     23 
#      1      7     56    301    458   1610   1711   2027   7092 21747  58386 
#      5     22     21     20     19     14     16      6     18 10      7 
# 114469 128382 235810 321680 390398 416661 424299 430723 431613 432526 440532 
#     13     15     12     11      9     17      8 
# 440903 441021 443252 446432 447705 457678 469386 

# 2a.  We cut the DepTime column, using the breaks of 0000 through 2900 

table(cut(myDF$DepTime, breaks=seq(0000,2900,by=100), right=FALSE)) 

#      and we get: 

#           [0,100)         [100,200)         [200,300) [300,400) 
#             21747              7092 2027               458 
#         [400,500)         [500,600)         [600,700) [700,800) 
#              1610            114469            430723 440532 
#         [800,900)       [900,1e+03)   [1e+03,1.1e+03) [1.1e+03,1.2e+03) 
#            469386            447705            432526 446432 
# [1.2e+03,1.3e+03) [1.3e+03,1.4e+03) [1.4e+03,1.5e+03) [1.5e+03,1.6e+03) 
#            443252            440903            416661 441021 
# [1.6e+03,1.7e+03) [1.7e+03,1.8e+03) [1.8e+03,1.9e+03) [1.9e+03,2e+03) 
#            424299            457678            431613 390398 
#   [2e+03,2.1e+03) [2.1e+03,2.2e+03) [2.2e+03,2.3e+03) [2.3e+03,2.4e+03) 
#            321680            235810            128382 58386 
# [2.4e+03,2.5e+03) [2.5e+03,2.6e+03) [2.6e+03,2.7e+03) [2.7e+03,2.8e+03) 
#              1711               301 56                 7 
# [2.8e+03,2.9e+03) 
#                 1 

#      or if you want to re-format the output, you can write, for instance: 

table(cut(myDF$DepTime, breaks=seq(0000,2900,by=100), dig.lab=4, right=FALSE)) 

#     [0,100)   [100,200)   [200,300)   [300,400)   [400,500) [500,600) 
#       21747        7092        2027         458        1610 114469 
#   [600,700)   [700,800)   [800,900)  [900,1000) [1000,1100) [1100,1200) 
#      430723      440532      469386      447705      432526 446432 
# [1200,1300) [1300,1400) [1400,1500) [1500,1600) [1600,1700) [1700,1800) 
#      443252      440903      416661      441021      424299 457678 
# [1800,1900) [1900,2000) [2000,2100) [2100,2200) [2200,2300) [2300,2400) 
#      431613      390398      321680      235810      128382 58386 
# [2400,2500) [2500,2600) [2600,2700) [2700,2800) [2800,2900) 
#        1711         301          56           7           1 

#     We just sort the command above, and we see that 
#     the most flights departed during 8 AM to 9 AM 

sort(table(cut(myDF$DepTime, breaks=seq(0000,2900,by=100), dig.lab=4, right=FALSE))) 

# [2800,2900) [2700,2800) [2600,2700) [2500,2600)   [300,400) [400,500) 
#           1           7          56         301         458 1610 
# [2400,2500)   [200,300)   [100,200)     [0,100) [2300,2400) [500,600) 
#        1711        2027        7092       21747       58386 114469 
# [2200,2300) [2100,2200) [2000,2100) [1900,2000) [1400,1500) [1600,1700) 
#      128382      235810      321680      390398      416661 424299 
#   [600,700) [1800,1900) [1000,1100)   [700,800) [1300,1400) [1500,1600) 
#      430723      431613      432526      440532      440903 441021 
# [1200,1300) [1100,1200)  [900,1000) [1700,1800)   [800,900) 
#      443252      446432      447705      457678      469386 

#   2b. We do get the same results as in question 1. 

#   2c.  We choose to use 2900 instead of (say) 2400 in this method 
#        because some flights departed after midnight. 
#        The time stamps are between 0000 and 2400 
#        (this is like military time, between 00:00 and 24:00). 
#        Some flights have delays until after midnight, 
#        and they are recorded in a surprising way, 
#        e.g., 24:30 for 30 minutes past midnight, 
#        or 26:10 for 2 hours and 10 minutes past midnight. 
#        In our data set, it happens that all of the ranges of the times 
#        are between 0000 and 2900.  I just checked the max to find that out. 
#        So that's why we use 2900 as an upper boundary, instead of 2400. 

max(myDF$DepTime, na.rm=T)

----


