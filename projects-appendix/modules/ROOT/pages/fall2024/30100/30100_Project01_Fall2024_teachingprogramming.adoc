= 301 Project 01- Choosing a Model 

== Project Objectives

This project will help us understand different machine learning models and their applications using the Boston Housing dataset:

- Flexibility vs. Interpretability
- Classification vs. Regression
- Prediction vs. Inference
- Supervised vs. Unsupervised Learning
- Non-parameterization with Splines

== Reading and Resources

- https://the-examples-book.com/starter-guides/data-science/data-modeling/choosing-model/[DataMine Examples Book-Choosing model]
- https://www.statlearning.com/[An Introduction to Statistical Learning]

== DataSet
- `/anvil/projects/tdm/data/boston.csv`

[NOTE]
====
The following steps will prepare data before building and training the specific models

- Load dataset
[source,python]
----
import pandas as pd

file_path = '/anvil/projects/tdm/data/boston.csv'  
my_df = pd.read_csv(file_path)

# Display the first few rows of the dataset
my_df.head()
----

- Understand dataset
[source,python]
----
my_df.info()
my_df.describe()
my_df.isnull().sum()
----

- Prepare data for modeling
[source,python]
----
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the dataset into features and target variable
X = my_df.drop('MEDV', axis=1)   
y = my_df['MEDV']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
----
====

== Questions

[NOTE]
====
Flexibility vs. Interpretability

Flexible modeling techniques can handle complex and unstructured datasets effectively, but often operate as "black boxes" with outputs not easily explainable

Interpretable model produce outputs are easily understood and have insight into which features are responsible for the prediction

**Neural networks (Flexibility)**

Neural networks are good example for highly flexible models, they are capable of capturing complex, non-linear relationships in the data.
You do not need to understand how neural networks function but just they are flexible models. Following is a typical step to create the model:

[source,python]
----
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the model
nn_model = MLPRegressor(hidden_layer_sizes=(50, 50, 50), max_iter=1000, random_state=42)

# Train the model
nn_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_nn = nn_model.predict(X_test_scaled)

# Evaluate the model
mse_nn = mean_squared_error(y_test, y_pred_nn)
r2_nn = r2_score(y_test, y_pred_nn)

print(f'Neural Network - Mean Squared Error: {mse_nn}')
print(f'Neural Network - R^2 Score: {r2_nn}')
----

**Decision Trees (Interpretability)**

Decision trees are highly interpretable models. They can be applied to both regression and classification problems. the following is a regression model

[source,python]
----
from sklearn.tree import DecisionTreeRegressor

# Initialize the model
dt_model = DecisionTreeRegressor(max_depth=4, random_state=42)

# Train the model
dt_model.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluate the model
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)

print(f'Decision Tree - Mean Squared Error: {mse_dt}')
print(f'Decision Tree - R^2 Score: {r2_dt}')
----
====
=== Question 1 (2 points)
.. Please describe the main difference between flexible and interpretable models in your own words.

.. Using the provided code examples, compare the two models and describe which is more flexible and interpretable and why

[NOTE]
====
Classification vs. Regression

Classification problems categorize data into distinct groups such as disease status or eye color
Regression problems yield quantitative outputs like income or age

**Logistic Regression (Classification)**

Logistic regression is for binary classification. A binary classification problem is also called Binomial, which the output can only be one of two possible values, usually true or false, 0 or 1 etc.

The following output will indicate the house price, whether it is above or below the median.

[source,python]
----
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Convert target variable into binary classification
y_binary = (y > y.median()).astype(int)

# Split the data
X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_binary, test_size=0.2, random_state=42)

# Initialize the model
log_reg = LogisticRegression(max_iter=1000, random_state=42)

# Train the model
log_reg.fit(X_train_bin, y_train_bin)

# Make predictions
y_pred_log = log_reg.predict(X_test_bin)

# Evaluate the model
accuracy_log = accuracy_score(y_test_bin, y_pred_log)

print(f'Logistic Regression - Accuracy: {accuracy_log}')
----

** Linear Regression (Regression)**

Linear regression is for predicting a continuous target variable, which is numeric data and can be any value within a certain range, like temperature, time etc. 

[source,python]
----
from sklearn.linear_model import LinearRegression

# Initialize the model
lin_reg = LinearRegression()

# Train the model
lin_reg.fit(X_train_scaled, y_train)

# Make predictions
y_pred_lin = lin_reg.predict(X_test_scaled)

# Evaluate the model
mse_lin = mean_squared_error(y_test, y_pred_lin)
r2_lin = r2_score(y_test, y_pred_lin)

print(f'Linear Regression - Mean Squared Error: {mse_lin}')
print(f'Linear Regression - R^2 Score: {r2_lin}')
----

====

===  Question 2 (2 points)
.. Describe the main difference between classification problems and regression problems.
 
.. Using the provided code examples, compare the two models and describe the classification model and regression model in your own words 


[NOTE]
====
Prediction vs. Inference

Prediction models focus on forecasting
Inference models focus on understanding relationships

**Random Forests (Prediction)**

Random forests are powerful for making predictions 

[source,python]
----
from sklearn.ensemble import RandomForestRegressor

# Initialize the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f'Random Forest - Mean Squared Error: {mse_rf}')
print(f'Random Forest - R^2 Score: {r2_rf}')
----

**OLS Regression (Inference)**

Ordinary Least Squares (OLS) regression is good for understanding the relationships between variables. 

[source,python]
----
import statsmodels.api as sm

# Add a constant term for the intercept
X_train_sm = sm.add_constant(X_train)
X_test_sm = sm.add_constant(X_test)

# Initialize and fit the model
ols_model = sm.OLS(y_train, X_train_sm).fit()

# Make predictions
y_pred_ols = ols_model.predict(X_test_sm)

# Evaluate the model
mse_ols = mean_squared_error(y_test, y_pred_ols)
r2_ols = r2_score(y_test, y_pred_ols)

print(f'OLS Regression - Mean Squared Error: {mse_ols}')
print(f'OLS Regression - R^2 Score: {r2_ols}')
----

====
=== Question 3 (2 points)

.. Explain when you would use prediction versus inference in modeling.
 
.. Using the provided code examples, compare the two models and describe the differences in your own words and why

[NOTE]
====
Supervised vs. Unsupervised Learning

Supervised learning allows the model to learn mapping from inputs to outputs, where both input and output are provided
Unsupervised learning only use input data without any labeled output, focus on uncover patterns or structures within the data

**Support Vector Machine (Supervised Learning)**

Support Vector Machines (SVM) are supervised learning models  

[source,python]
----
from sklearn.svm import SVR

# Initialize the model
svm_model = SVR()

# Train the model
svm_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test_scaled)

# Evaluate the model
mse_svm = mean_squared_error(y_test, y_pred_svm)
r2_svm = r2_score(y_test, y_pred_svm)

print(f'Support Vector Machine - Mean Squared Error: {mse_svm}')
print(f'Support Vector Machine - R^2 Score: {r2_svm}')
----

**K-means Clustering (Unsupervised Learning)**

K-means clustering is an unsupervised learning.

[source,python]
----
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Initialize the model
kmeans = KMeans(n_clusters=3, random_state=42)

# Fit the model
kmeans.fit(X_train_scaled)

# Get cluster labels
clusters = kmeans.labels_

# Plotting the clusters (using first two features for simplicity)
plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=clusters, cmap='viridis')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-means Clustering')
plt.show()
----
====
=== Question 4 (2 points)

.. Explain the difference between supervised and unsupervised learning.

.. Using the provided code examples, compare the two models and describe the differences in your own words and why

[NOTE]
====
Parameterization vs. Non-Parameterization

Parameterization involves assigning parameters(coefficients) to develop a function approximation, as in methods like Ordinary Least Squares (OLS)

Non-Parameterization relies on data itself to determine the function shape instead of predefined parameters

Splines are non-parameterized approach

The following example uses the `dmatrix` function to create spline features, which fit a regression model then

[source,python]
----
from patsy import dmatrix
import statsmodels.api as sm
import numpy as np
import matplotlib.pyplot as plt

# Generate spline basis with 4 degrees of freedom for the feature RM (average number of rooms per dwelling)
# This is just an example; you can apply it to any other feature or multiple features.
spline = dmatrix("bs(X_train[:, 5], df=4, include_intercept=False)",
                 {"X_train": X_train}, return_type='dataframe')

# Fit the model
model = sm.OLS(y_train, spline).fit()

# Make predictions
spline_test = dmatrix("bs(X_test[:, 5], df=4, include_intercept=False)",
                      {"X_test": X_test}, return_type='dataframe')
y_pred = model.predict(spline_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
----
Visualize the spline regression to understand the relationship between the feature and the target variable.

[source,python]
----
# Create a scatter plot of the original data
plt.scatter(X_train[:, 5], y_train, facecolor='None', edgecolor='k', alpha=0.5)

# Create a plot of the predicted spline
x_plot = np.linspace(X_train[:, 5].min(), X_train[:, 5].max(), 100)
X_plot = dmatrix("bs(x_plot, df=4, include_intercept=False)", {"x_plot": x_plot}, return_type='dataframe')
y_plot = model.predict(X_plot)
plt.plot(x_plot, y_plot, color='r')

plt.xlabel('Average number of rooms per dwelling (RM)')
plt.ylabel('Median value of owner-occupied homes in $1000 (MEDV)')
plt.title('Spline Regression')
plt.show()
----

====

=== Question 5 (2 points)

.. Explain what splines are and their purpose in regression modeling. 

.. Using the provided code example, describe the output relationship how the target variable changes based on features  

Project 01 Assignment Checklist
====
* Jupyter Lab notebook with your code, comments, and output for the assignment
    ** `firstname-lastname-project01.ipynb` 

* Submit files through Gradescope
====

[WARNING]
====
_Please_ make sure to double-check that your submission is complete and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted was what you _actually_ submitted.

In addition, please review our xref:projects:current-projects:submissions.adoc[submission guidelines] before submitting your project.
====
