= 301 Project 02- K-nearest neighbor Model introduction 

== Project Objectives

This project we will understand how to use the Iris dataset for training and evaluating a K-Nearest Neighbors (KNN) classifier.
the steps to create a K-nearest neighbor model using the iris dataset:

- Load and Explore the Data
- Preprocess the Data
- Train the KNN Classifier
- Evaluate the Model

== Reading and Resources

- https://the-examples-book.com/starter-guides/data-science/data-modeling/knn/[DataMine Examples Book-KNN ]
- https://www.statlearning.com/[An Introduction to Statistical Learning]

== DataSet
- `/anvil/scratch/x-nzhou1/iris.csv`
 
[NOTE]
====
Overview of the Iris Dataset

- The Iris dataset consists of 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. There are three classes of iris flowers: Setosa, Versicolor, and Virginica.
====
== Questions

[NOTE]
====
The first step is to Load and Explore the Data

[source,python]
----
import pandas as pd
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
data['target'] = iris.target

# Display the first few rows of the dataset
print(data.head())
----
====
=== Question 1 (2 points)
.. Based on the initial exploration, what are the mean, median, and standard deviation of the sepal length for each iris class?

[NOTE]
====
Then you will need to Pre-process the Data; Split the dataset into training and testing sets and standardize the features.

[source,python]
----
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
----
====

=== Question 2 (2 points)
.. How does the distribution of each feature change after standardization? Provide a comparison of the distributions before and after standardization.

[NOTE]
====
Next, you will need to `Initialize and Train the KNN Classifier`

[source, python]
----
from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)

# Train the classifier
knn.fit(X_train, y_train)
----
====
=== Question 3 (2 points)

.. What impact does changing the number of neighbors (k) have on the model's performance? Test k values of 1, 5, and 10, and compare their accuracies

[NOTE]
====
Next step will be `Evaluate the Model`; Predict the labels for the test set and calculate the accuracy.

[source,python]
----
from sklearn.metrics import accuracy_score, classification_report

# Predict on the test set
y_pred = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Test set accuracy: {accuracy * 100:.2f}%')

# Print classification report
print(classification_report(y_test, y_pred, target_names=iris.target_names))
----
====
== Question 4 (2 points)

.. Analyze the classification report. Which class has the highest precision and which class has the lowest recall? What might this indicate about the model's performance on different classes?

[NOTE]
====
Full Project Code

[source,python]
----
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the Iris dataset
iris = load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
data['target'] = iris.target

# Display the first few rows of the dataset
print(data.head())

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize the KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)

# Train the classifier
knn.fit(X_train, y_train)

# Predict on the test set
y_pred = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Test set accuracy: {accuracy * 100:.2f}%')

# Print classification report
print(classification_report(y_test, y_pred, target_names=iris.target_names))
---- 
====

[NOTE]
====
Using cross-validation can help to ensure that the model's performance is consistent and not dependent on a specific train-test split.

[source,python]
----
from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), iris.data, iris.target, cv=5)

# Display the cross-validation scores
print(f'Cross-validation scores: {cv_scores}')
print(f'Average cross-validation score: {cv_scores.mean():.2f}')
----
====

=== Question 5(2 points):

.. What are the cross-validation scores for the model with k=3, and what does the average cross-validation score indicate about the model's performance?

[Tip]
====
[source,python]
----
from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), iris.data, iris.target, cv=5)

# Display the cross-validation scores
print(f'Cross-validation scores: {cv_scores}')
print(f'Average cross-validation score: {cv_scores.mean():.2f}')
----

 
Project 01 Assignment Checklist
====
* Jupyter Lab notebook with your code, comments, and output for the assignment
    ** `firstname-lastname-project01.ipynb` 
* Python file with code and comments for the assignment
    ** `firstname-lastname-project01.py`
* Submit files through Gradescope
====

[WARNING]
====
_Please_ make sure to double-check that your submission is complete and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted was what you _actually_ submitted.

In addition, please review our xref:projects:current-projects:submissions.adoc[submission guidelines] before submitting your project.
====
```