= 301 Project 02- K-nearest neighbor Model introduction 

== Project Objectives

This project we will understand how to train and evaluate a K-Nearest Neighbors (KNN) classifier using the `Iris` dataset.
the steps to create a K-nearest neighbor model using the iris dataset:

- Load and Explore the Data
- Preprocess the Data
- Train the KNN Classifier
- Evaluate the Model

== Reading and Resources

- https://www.statlearning.com/[An Introduction to Statistical Learning]

== DataSet
- `/anvil/scratch/x-nzhou1/iris.csv`
 
[NOTE]
====
Overview of the Iris Dataset

- The Iris dataset consists of 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. There are three classes of iris flowers: Setosa, Versicolor, and Virginica.
====
== Questions

[NOTE]
====
The first step is to Load and Explore the Data

[source,python]
----
import pandas as pd
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
feature_names = iris.feature_names
data['target'] = iris.target

# Display the first few rows of the dataset
print(data.head())
----
====
=== Question 1 (2 points)
.. Based on the initial exploration, what are the mean, median, and standard deviation of the sepal length for each iris class (Setosa, Versicolor, and Virginica are represented by 0,1 and 2 in the target column correspondingly)?

[TIP]
====
- The following statement can be used to map the iris class description with numbers (0, 1, 2) with a new column 'class' created
[source, python]
----
iris_class_names = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}
data['class'] = data['target'].map(iris_class_names)
----
- Then `groupby()` will be useful to group the data 
====

[NOTE]
====
Then you will need to Pre-process the Data; Split the dataset into training and testing sets 

[source,python]
----
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

----

and then standardize the data, to ensure that each feature contributes equally to the distance calculations and other operations in machine learning algorithms, improving the model's performance.

[source,python]
----
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
----

You may use the following function to plot the distribution of features for both original training data and standardized training data to understand the differences

[source,python]
----
import matplotlib.pyplot as plt
feature_names = iris.feature_names
# Function to plot the distribution of features using matplotlib
def plot_distribution_matplotlib(X, title):
    plt.figure(figsize=(12, 8))
    for i in range(X.shape[1]):
        plt.subplot(2, 2, i + 1)
        plt.hist(X[:, i], bins=20, alpha=0.75, edgecolor='black')
        plt.title(f'{title} - {feature_names[i]}')
        plt.xlabel(feature_names[i])
        plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()
----
Run the function for "X_train" before and after standardizing the Features
[source,python]
plot_distribution_matplotlib(X_train, 'Original Training Data')
[source,python]
plot_distribution_matplotlib(X_train, 'Standardized Training Data')
====

=== Question 2 (2 points)
.. How does the distribution of each feature change after standardization? Provide a comparison of the distributions before and after standardization by plotting the distribution of the training data

[NOTE]
====
- Next, you will need to `Initialize and Train the KNN Classifier`

- The parameter `k` in K-Nearest Neighbors (KNN) determines the number of nearest neighbors to consider when making predictions. Changing `k` impacts the model's performance:

    - k = 1: The model is highly flexible (low bias) but may be sensitive to noise in the data (high variance).
    - k = 5: The model is less sensitive to noise compared to k = 1, offering a balance between bias and variance.
    - k = 10: The model is even less sensitive to noise and has higher bias but lower variance.
- However, for the small clean datasets like iris.csv, since the dataset is pretty well-separated and easy to classify, you may get all perfect accuracy for different K values
[source, python]
----
from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)

# Train the classifier
knn.fit(X_train, y_train)
----
====
=== Question 3 (2 points)

.. What impact does changing the number of neighbors (k) have on the model's performance? Test k values of 1, 5, and 10, and compare their accuracies

[TIP]
====
You will need to import library for calculating the accuracy 

[source,python]
----
from sklearn.metrics import accuracy_score
# make prediction
y_pred = knn.predict(X_test)
# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy with k={k}: {accuracy:.4f}')
print(f"\nEvaluation for k={k}:")
print(classification_report(y_test, y_pred))
----      
====
 

[NOTE]
====
- Even when the accuracy are all perfect (1.0000) for different k values in a KNN model for iris dataset, there are valuable insights to be gained by examining the decision boundaries. Decision boundaries can reveal differences in how the model makes classifications and how it might behave with new, unseen data.

- A decision boundary is a surface that separates different classes in the feature space. For K-Nearest Neighbors (KNN), the shape and complexity of the decision boundary depend on the value of `k`.

- Low `k` Values like k = 1: The decision boundary is highly flexible and can closely follow the training data points, resulting in a complex and detailed boundary. This can lead to overfitting, where the model captures noise in addition to the underlying patterns.
- High `k` Values like k = 10 : The decision boundary is smoother and more generalized. This can lead to underfitting, where the model oversimplifies the data patterns.
- You may use the following code to visualize decision boundaries for different k values using the first two features of the Iris dataset:

[source,python]
----
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# Load the Iris dataset
iris = load_iris()
data = iris.data
target = iris.target

# Use only the first two features for visualization
X = data[:, :2]
y = target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def plot_decision_boundaries(X, y, k_values):
    h = .02  # step size in the mesh
    cmap_light = plt.cm.Paired  # for mesh
    cmap_bold = plt.cm.jet  # for points
    
    for k in k_values:
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X, y)
        
        # Plot the decision boundary. For that, we will assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max].
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        
        Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        plt.figure()
        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
        
        # Plot also the training points
        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)
        plt.title(f"Decision boundary with k={k}")
        plt.xlim(xx.min(), xx.max())
        plt.ylim(yy.min(), yy.max())
        plt.show()

# Define the k values to test
k_values = [1, 5, 10]

# Plot the decision boundaries for different k values
plot_decision_boundaries(X_train, y_train, k_values)
----
====
=== Question 4 ( 2 points)

.. How does the complexity of the decision boundary change with different K values?
 
=== Question 5 (2 points)

.. How do different k values affect the classification of new, unseen data points near the decision boundaries?

[TIP]
====
 
- Visualize the decision boundaries for different k values.
- Observe changes in classification near the boundaries.
- Consider the model's sensitivity to noise with different k values.
- Evaluate the stability of classifications as k  varies.
- Analyze the model's robustness and generalization ability.
====

 
Project 02 Assignment Checklist
====
* Jupyter Lab notebook with your code, comments, and output for the assignment
    ** `firstname-lastname-project02.ipynb` 
* Python file with code and comments for the assignment
    ** `firstname-lastname-project02.py`
* Submit files through Gradescope
====

[WARNING]
====
_Please_ make sure to double-check that your submission is complete and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted was what you _actually_ submitted.

In addition, please review our xref:projects:current-projects:submissions.adoc[submission guidelines] before submitting your project.
====
 