= 301 Project 2 - Intro to ML - Basic Concepts

== Project Objectives

In this project, we will learn how to select an appropriate machine learning model. Understanding specifics of how the models work may help in this process, but other aspects can be investigated for this. 

.Learning Objectives
****
- Learn the difference between classification and regression
- Learn the difference between supervised and unsupervised learning
- Learn how our dataset influences our model selection
****

== Supplemental Reading and Resources

- https://the-examples-book.com/starter-guides/data-science/data-modeling/choosing-model/[DataMine Examples Book - Choosing a Model]

== Datasets

- `/anvil/projects/tdm/data/Iris.csv`
- `/anvil/projects/tdm/data/boston.csv`

== Questions

=== Question 1 (2 points)

In this project, we will use the Iris dataset and the boston dataset as samples to learn about the various aspects that go into choosing a machine learning model. Let's review last project by loading the Iris and boston datasets, then printing the first 5 rows of each dataset.

.Deliverables
====
- Output of running code to print the first 5 rows of both datasets.
====

=== Question 2 (2 points)

One of the most distinguishing features of machine learning is the difference between classification and regression.

Classification is the process of predicting a discrete class label. For example, predicting whether an email is spam or not spam, whether a patient has a disease or not, or if an animal is a dog or a cat.

Regression is the process of predicting a continuous quantity. For example, predicting the price of a house, the temperature tomorrow, or the weight of a person.

[NOTE]
====
Some columns may be misleading. Just because a column is a number does not mean it is a regression problem. One-hot encoding is a technique used to convert categorical variables into numerical variables (we will cover this deeper in future projects). Therefore, it is important to try and understand what a column represents, as just seeing a number does not necessarily mean it corresponds to a continuous quantity.
====

Let's look at the `Species` column of the Iris dataset, and the `MEDV` column of the boston dataset. Based on these columns, classify the type of machine learning problem that we would be solving with each dataset.

.Deliverables
====
- Would we likely be solving a classification or regression problem with the `Species` column of the Iris dataset? Why?
- Would we likely be solving a classification or regression problem with the `MEDV` column of the boston dataset? Why?
====

=== Question 3 (2 points)

Another important distinction in machine learning is the difference between supervised and unsupervised learning.

Supervised learning is the process of training a model on a labeled dataset. The model learns to map some input data to an output label based on examples in the training data.
[IMPORTANT]
====
The projects in 30100 will focus on supervised learning. From our dataset, there will be a single column we want to predict, and the rest will be used to train the model. The column we want to predict is called the label/target, while the remaining columns are called features.
====

Unsupervised learning is the process of training a model on an unlabeled dataset. The model learns patterns in the data without any guidance. This is often used in clustering problems. For example, a store wants to group items based on how often they are purchased together.

[NOTE]
====
In unsupervised learning, there is no output to predict. The model is simply trying to find patterns in the dataset. Examples include clustering and identifying associations.
====

.Deliverables
====
- Should we use supervised or unsupervised learning if we want to predict the `Species` of some data using the Iris dataset? Why?
====

=== Question 4 (2 points)

Another important tradeoff in machine learning is the flexibility of the model versus the interpretability of the model.

Imagine a simple function `f(x) = 2x`. This function is very easy to interpret, it doubles x. However, it is not very flexible, as doubling the input is all it can do. A function like `f(x) = 2x^2 + 3x + 4` is considered more flexible, as it models a more complex relationships, however it is less interpretable.

We can also see this complexity increase as we increase the number of variables. `f(x)` will typically be more interpretable than `f(x,y)`, which will typically be more interpretable than `f(x,y,z)`. When we get to a large number of variables, eg. `f(a,b,c,...,x,y,z)`, it can become difficult to understand the impact of each variables on the output.

Machine learning models are the same way. Many factors, including the type of model and the number of features can impact the interpretability of the model. 

[NOTE]
====
`Black box` is a term often used to describe models that are too complex for humans to easily interpret. Large neural networks can be considered black boxes. Other models, such as linear regression, are much easier to interpret. These easy to interpret models are often called `white box` models.
====

Please print the number of columns in the Iris dataset and the boston dataset.

.Deliverables
====
- How many columns are in the Iris dataset?
- How many columns are in the boston dataset?
- Based purely on the number of features, would you expect a machine learning model trained on the Iris dataset to be more or less interpretable than a model trained on the boston dataset? Why?
====

=== Question 5 (2 points)

Parameterization is the idea of approximating a function with a model. If we have some complex function `f`, and we have examples of `f(x)` for many different `x`, we can use these examples to train a model to approximate `g`. We use `parameters` or coefficients to adjust the model to best fit the data. Usually, the model will use the data to `discover` the parameters. However, in some cases, we may need to manually adjust or `tune` the parameters to get the best fit.

A non-parametrized model does not necessarily mean that the model does not have parameters. However, it means that we don't know how many of these parameters exist or how they are used before training. The model will figure out what parameters it needs while training on the dataset. This can be visualized with splines, which are a type of curve that can be used to approximate a function. 

If we have 5 points (x, y) and want to find a function to fit these points, through parameterization we would have a single function with multiple parameters that need to be adjusted to give us the best fit. With splines, however, we could create a piecewise function, where each piece is a linear function between two points. This function has no parameters, and is created by the model solely based on the data.

[NOTE]
====
If we already have a good understanding of the data, eg. we know it to be some third order polynomial, it is likely best to choose a parametrized model. However, if we don't have an understanding of the data, or if it is very complex, a non-parametrized model that learns the function from the data may be a better fit.
====

.Deliverables
====
- 
====

== Submitting your Work

.Items to submit
====
- firstname_lastname_project2.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====