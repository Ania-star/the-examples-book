= 401 Project 10 - Regression: Perceptrons

== Project Objectives

In this project, we will be learning about perceptrons and how they can be used for regression. We will be using the Boston Housing dataset as it has many different potential features and target variables.

.Learning Objectives
****
- Understand the basic concepts behind a perceptron
- Implement activation functions and their derivatives
- Implement a perceptron class for regression
****

== Supplemental Reading and Resources

== Dataset

- `/anvil/projects/tdm/data/boston_housing/boston.csv`

== Questions

=== Question 1 (2 points)

A perceptron is a simple model that can be used for regression. These perceptrons can be combined together to create neural networks. In this project, we will be creating a perceptron from scratch.

To start, let's load in the Bostong Housing dataset with the below code:
[source,python]
----
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
df = pd.read_csv('/anvil/projects/tdm/data/boston_housing/boston.csv')

X = df.drop(columns=['MEDV'])
y = df[['MEDV']]

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=7)

y_train = y_train.to_numpy()
y_test = y_test.to_numpy()
----

Now, we can begin discussing what a perceptron is. A perceptron is a simple model that takes in a set of inputs and produces an output. The perceptron is defined by a set of weights and a bias term (similar to our linear regression model having coefficients and an y intercept term). The perceptron then takes the dot product of the input features and the weights and adds the bias term.

Then, the perceptron will apply some activation function before outputting the final value. This activation function is some non-linear function that allows the perceptron to learn complex data, instead of behaving as a linear model.

There are many different activation functions, some of the most common are listed below:

[cols="2,2,2",options="header"]
|===
|Activation Function | Formula | Derivative
|ReLU | max(0, x) | 1 if x > 0, 0 otherwise
|Sigmoid | 1 / (1 + exp(-x)) | sigmoid(x) * (1 - sigmoid(x))
|Tanh | (exp(x) - exp(-x)) / (exp(x) + exp(-x)) | 1 - tanh(x)^2
|===

For this project, we will be creating a perceptron class that can be used for regression. There are many different parameters that can be set when creating a perceptron, such as the learning rate, number of epochs, and activation function.

For this question, please implement functions for the ReLU, Sigmoid, and Tanh activation functions. Additionally, implement the derivative of each of these functions. These functions should be able to take in a numpy array and return the transformed array.

[source,python]
----
import numpy as np
def relu(x):
    pass
def relu_d(x):
    pass
def sigmoid(x):
    pass
def sigmoid_d(x):
    pass
def tanh(x):
    pass
def tanh_d(x):
    pass

----

To test your functions, you can use the below code:
[source,python]
----
x = np.array([-1, 0, 1])
print(relu(x)) # should return [0, 0, 1]
print(relu_d(x)) # should return [0, 0, 1]
print(sigmoid(x)) # should return [0.26894142, 0.5, 0.73105858]
print(sigmoid_d(x)) # should return [0.19661193, 0.25, 0.19661193]
print(tanh(x)) # should return [-0.76159416, 0, 0.76159416]
print(tanh_d(x)) # should return [0.41997434, 1, 0.41997434]
----

.Deliverables
====
- Completed activation and derivative functions
- Test the functions with the provided code
====

=== Question 2 (2 points)

Now that we have our activation functions, let's start working on our Perceptron class. This class will create a perceptron that can be used for regression problems. Below is a skeleton of our Perceptron class:

[source,python]
----
class Perceptron:
    def __init__(self, learning_rate=0.01, n_epochs=1000, activation='relu'):
        # this will initialize the perceptron with the given parameters
        pass
    
    def activate(self, x):
        # this will apply the activation function to the input
        pass
    
    def activate_derivative(self, x):
        # this will apply the derivative of the activation function to the input
        pass

    def compute_linear(self, X):
        # this will calculate the linear combination of the input and weights
        pass
    
    def error(self, y_pred, y_true):
        # this will calculate the error between the predicted and true values
        pass

    def backward_gradient(self, error, linear):
        # this will update the weights and bias of the perceptron
        pass

    def predict(self, X):
        # this will predict the output of the perceptron given the input
        pass

    def train(self, X, y):
        # this will train the perceptron on the given input and target values
        pass

    def test(self, X, y):
        # this will test the perceptron on the given input and target values
        pass
----

Now, it may seem daunting to implement all of these functions. However, most of these functions are as simple as one mathematical operation.

*For this question, please implement the `__init__`, `activate`, and `activate_derivative` functions.*
The `__init__` function should initialize the perceptron with the given parameters, as well as setting weights and bias terms to None. 
 
The `activate` function should apply the activation function to the input, and the `activate_derivative` function should apply the derivative of the activation function to the input. It is important that these functions use the proper function based on the value of `self.activation`. Additionally, if the activation function is not set to one of the three functions we implemented earlier, the default should be the ReLU function.

To test your functions, you can use the below code:
[source,python]
----
test_x = np.array([-2, 0, 1.5])
p = Perceptron(learning_rate=0.01, n_epochs=1000, activation='relu')
print(p.activate(test_x)) # should return [0, 0, 1.5]
print(p.activate_derivative(test_x)) # should return [0, 0, 1]
p.activation = 'sigmoid'
print(p.activate(test_x)) # should return [0.11920292, 0.5, 0.81757448]
print(p.activate_derivative(test_x)) # should return [0.10499359, 0.25, 0.14914645]
p.activation = 'tanh'
print(p.activate(test_x)) # should return [-0.96402758, 0, 0.90514825]
print(p.activate_derivative(test_x)) # should return [0.07065082, 1, 0.18070664]
p.activation = 'invalid'
print(p.activate(test_x)) # should return [0, 0, 1.5]
print(p.activate_derivative(test_x)) # should return [0, 0, 1]
----
.Deliverables
====
- Implement the `__init__`, `activate`, and `activate_derivative` functions
- Test the functions with the provided code
====

=== Question 3 (2 points)

Now, let's move onto the harder topics. The basic concept behind how this perceptron works is that it will take in an input, calculate the predicted value, find the error between the predicted and true value, and then update the weights and bias based on this error and it's learning rate. This process is then repeated for the set number of epochs.

In this sense, there are to main portions of the perceptron that need to be implemented: the forward and backward passes. The forward pass is the process of calculating the predicted value, and the backward pass is the process of updating the weights and bais based on the calculated error.

*For this question, we will implement the `compute_linear`, `predict`, `error`, and `backward_gradient` functions.*

The `compute_linear` function should calculate the linear combination of the input, weights, and bias, by computing the dot product of the input and weights and adding the bias term. 

The `predict` function should compute the linear combination of the input and then apply the activation function to the result.

The `error` function should calculate the error between the predicted (y_pred) and true (y_true) values, ie true - predicted. 

The `backward_gradient` should calculate the gradient of the error, which is simply the negative of the error multiplied by the activation derivative of the linear combination.

To test your functions, you can use the below code:
[source,python]
----
p = Perceptron(learning_rate=0.01, n_epochs=1000, activation='sigmoid')
p.weights = np.array([1, 2, 3])
p.bias = 4

test_X = np.array([1,2,3])
test_y = np.array([20])

linear = p.compute_linear(test_X)
print(linear) # should return 18
error = p.error(linear, test_y)
print(error) # should return 2
gradient = p.backward_gradient(error, linear)
print(gradient) # should return -36
pred = p.predict(test_X) # should return 0.9999999847700205
print(pred)
----

.Deliverables
====
- Implement the `compute_linear`, `predict`, `error`, and `backward_gradient` functions
- Test the functions with the provided code
====

=== Question 4 (2 points)

Now that we have implemented all of our helper functions, we can implement our `train` function. 

First, this function will set our weights to a numpy array of random numbers from 0 to 1 with the same dimensions as our features. It will also set our bias to zero.

Then, this function will train the perceptron on the given training data. For each datapoint in the training data, we will get the linear combination of the input and the predicted value through our activation function. Then, we will compute the error and get the backward gradient. Then, we will calculate the gradient for our weights (simply the input times the gradient) and the gradient for our bias (simply the gradient). Finally, we will update the weights and bias by multiplying the gradients by the learning rate, and subtracting them from the current weights and bias. This process will be repeated for the set number of epochs.

In order to test your function, we will create a perceptron and train it on the Boston Housing dataset. We will then print the weights and bias of the perceptron.

[source,python]
----
np.random.seed(3)
p = Perceptron(learning_rate=0.01, n_epochs=1000, activation='relu')
p.train(X_train, y_train)
print(p.weights)
print(p.bias)
----

If you implemented the functions correctly, you should see the following output:
```
[ 0.33042986 -0.6159478   0.28543612 -0.04436198 -1.05827801 -0.26465103
 -0.08906756 -0.00575332 -0.96360834 -0.64396254 -0.05772422  0.8626006
  0.34971014]
[-4.96372221]
```
.Deliverables
====
- Implement the `train` function
- Test the function with the provided code
====

=== Question 5 (2 points)

Finally, let's implement the `test` function. This function will test the perceptron on the given test data. This function will return our summary statistics from the previous project (mean squared error, root mean squared error, mean absolute error, and r squared) in a dictionary.


To test your function, you can use the below code:
[source,python]
----
p.test(X_test, y_test)
----

.Deliverables
====
- Implement the `test` function
- Test the function with the provided code
====

=== Question 6 (2 points)

.Deliverables
====
- 
====

== Submitting your Work

.Items to submit
====
- firstname_lastname_project10.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====