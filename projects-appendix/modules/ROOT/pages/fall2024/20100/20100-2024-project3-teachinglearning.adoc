= TDM 20100: Project 3 -- Pipelines

**Motivation:** In the previous project, at each stage in our analysis, we saved the output to a file.  A more efficient method is to take the output from one command and use it as the input to the next command.  This is called a pipeline of bash commands.

**Context:** Once we learn how to write bash commands in a pipeline, we can (more easily) use several bash commands in tandem.

**Scope:** Pipelines in Bash

.Learning Objectives:
****
- Learn about the concept of bash pipelines, to use several bash commands in sequence.
****

Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about project submissions xref:submissions.adoc[here].

== Dataset(s)

This project will use the following dataset(s):

- `/anvil/projects/tdm/data/flights/subset/` (airplane data)
- `/anvil/projects/tdm/data/election` (election data)
- `/anvil/projects/tdm/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv` (grocery store data)

== Questions

=== Question 1 (2 pts)

In the previous project, on this collection of files:

`/anvil/projects/tdm/data/flights/subset/[12]*.csv`

we ran the following commands in bash:

[source, bash]
----
`cat` (to print the files; we did this in Project 2, Question 1)
`cut -d, -f17,18` (to extract the 17th and 18th fields, for the Origin and Destination columns)
`sort` (to get all of the same flight paths next to each other in the file)
`uniq -c` (to discover how many times that each flight path occurs)
`sort -n` (to numerically sort the number of times that the flight paths occur)
`tail` (to get the 10 most popular flight paths from the years 1987 to 2008 and the number of times that airplanes flew on each of these flight paths)
----

Now we can do all of this together, in one long line:

[source, bash]
----
cat /anvil/projects/tdm/data/flights/subset/[12]*.csv | cut -d, -f17,18 | sort | uniq -c | sort -n | tail
----

(To simplify things, we are not removing the head of each file.)

[IMPORTANT]
====
Please use 3 or 4 cores when working on this question.
====

.Deliverables
====
- Show the 10 most popular flight paths from the years 1987 to 2008 and the number of times that airplanes flew on each of these flight paths.
====

=== Question 2 (2 pts)

In the previous project, from this file:

`/anvil/projects/tdm/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv`

we discovered how many times that each of the `STORE_R` values appear in the file, using the following commands in bash:

[source, bash]
----
`cut -d, -f7` (to extract the `STORE_R` values from this file)
`sort` (to get all of the same `STORE_R` values next to each other in the file)
`uniq -c` (to discover how many times that each `STORE_R` value occurs)
`sort -n` (to numerically sort the number of times that the `STORE_R` values occur)
----

Now we can do all of this together, in one long line:

[source, bash]
----
cut -d, -f7 /anvil/projects/tdm/data/8451/The_Complete_Journey_2_Master/5000_transactions.csv | sort | uniq -c | sort -n
----

.Deliverables
====
- List the number of times that each of the `STORE_R` values appear in the file.
====

=== Question 3 (2 pts)

Using a pipeline to discover the 10 states in which the largest number of donations have been made (and the number of donations from each of these states), using the data stored in:

`/anvil/projects/tdm/data/election/itcont*.txt`

[HINT]
====
The data can be extracted from the 10th field of the files.  The symbol "|" is the delimiter.  So the cut command should look like:  `cut -d'|' -f10`
====


.Deliverables
====
- The 10 states in which the largest number of donations have been made (and the number of donations from each of these states)
====

=== Question 4 (2 pts)

Modify your solution to Question 3 so that you extract both the city and the state (simultaneously) for each donation.  In this way, you can discover the 10 city-and-state pairs in which the largest number of donations have been made (and the number of donations from each of these city-and-state pairs).

- The 10 city-and-state pairs in which the largest number of donations have been made (and the number of donations from each of these city-and-state pairs).

=== Question 5 (2 pts)

For this last question, we're going to combine our `cut` and `grep` skills, along with providing a small tease at a new tool we'll spend the next two projects learning about and using in depth: `piping`. For now, you don't have to fully understand piping and can just imagine it at its simplest: taking the output from the first command and giving it as an input to the second command.

Let's look at the below example:

[source, python]
----
%%bash
cut -d "," -f3 /anvil/projects/tdm/data/youtube/USvideos.csv | grep -Ec "[Aa]pology"
----

Now let's deconstruct each of the above actions. First, we cut our line on `,` and select the third field, which we know from the previous question is the title of the Youtube video for each line. Next, we use `grep` to search for the pattern `[Aa]pology` case-insensitive, and return the count of lines with that pattern in the title field. As a result, we can see that there are 16 videos containing "Apology" in the title in our `USvideos.csv` data. That's less apology videos than I thought there would be!

Your task for this question is rather basic, as we're asking you to combine both of the commands that you learned about in this project using a new tool we haven't yet discussed in-depth (piping). Modify the above example to search for how many times a channel named "HowToBasic" appears in the `channel_title` field of `/anvil/projects/tdm/data/youtube/USvideos.csv`. (Hint: Your code's answer to this should be 16.)

As an added test, do the same, but this time for the `channel_title` "The Tonight Show Starring Jimmy Fallon".

.Deliverables
====
- A `cut` and `grep` to count the number of times "HowToBasic" appears in `/anvil/projects/tdm/data/youtube/USvideos.csv` as a `channel_title`
- - A `cut` and `grep` to count the number of times "The Tonight Show Starring Jimmy Fallon" appears in `/anvil/projects/tdm/data/youtube/USvideos.csv` as a `channel_title`
====

== Submitting your Work

Congratulations, with regular expressions in your toolset, you can now show your mom and dad a string like `\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b` and explain to them how what looks like complete nonsense is actually how we can search for emails (a famous and notoriously difficult problem solved with regex)! As we move forward in this semester's curriculum, continue to think about how regular expressions and pattern-matching incorporate into data science generally, and feel free to refer back to previous projects from TDM 101-102 and ask questions about how languages like Python and R might be utilizing regex behind-the-scenes for some of their built-in functions!

.Items to submit
====
- firstname-lastname-project3.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====