= TDM 20100: Project 5 -- Pipelines, continued

**Motivation:** In this project, we'll continue to cement the concept of pipelines into your toolset. We will gradually work up to more complex examples of piping and pipelines, culminating in complicated pipelines that provide loads of valuable data processing at once, along with some that begin to perform some data analysis!

**Context:** This project will incorporate commands learned in all the previous ones, so reviewing projects 1-4 may help with completion of this project.

**Scope:** Pipes, pipelines, data processing, Bash, GNU, CLI

.Learning Objectives:
****
- Build basic pipelines that reformat and analyze data
- Build complex pipelines that process data into appropriate formats for analysis
- Perform basic data analysis using pipelines in `bash`
****

Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about project submissions xref:submissions.adoc[here].

== Dataset(s)

This project will use the following dataset(s):

- `/anvil/projects/tdm/data/youtube/USvideos.csv`
- `/anvil/projects/tdm/data/flights/*`

== Questions

=== Question 1 (2 pts)

In the last project, we learned about the `|` pipe operator and began building our first pipelines to process data. In this project we will briefly review our work from the previous project before building some more complex pipelines.

First, let's get familiar with one of the two datasets we'll be working with in this project. Using a bash command, print the first three rows of `/anvil/projects/tdm/data/youtube/USvideos.csv`.

Then, piping the first 3 lines of the file into a `cut`, isolate just the `publish_time` field from the first 3 lines of the file. If done correctly, the output from your `cut` pipeline should look like this:

[source, bash]
----
publish_time
2017-11-13T17:13:01.000Z
2017-11-13T07:30:00.000Z
----

.Deliverables
====
- Printed first 3 lines of the `USvideos.csv` file
- Isolated `publish_time` field from the first 3 lines of the file
====

=== Question 2 (2 pts)

Purely from looking at the output of your code from Question 1, you likely have a good idea of what the `publish_time` field means. However, there are two oddities that may be throwing you off: `T` and `Z`. In order to store and retrieve data efficiently, _conventions_ are established that put forward rules about the format for storing data of different types. This datetime data is stored using the _ISO 8601_ standard, where the `T` is meant to mark the end of date data and the beginning of time data and the ending character (`Z`, in our case) denotes the time zone that the time data is being provided in. The `Z` in our case means that our provided time zone is UTC.

[NOTE]
====
The `Z` that represents UTC comes from "Zulu" time being the military name for UTC.
====

For this question, copy/paste your pipeline from the previous question. Then, using another `cut`, isolate just the dates for the first 10 lines of the file (including the column header line). If done correctly, your code's output should look like this:

[source, bash]
----
publish_time
2017-11-13
2017-11-13
23
2017-11-13
2017-11-12
2017-11-13
2017-11-12
2017-11-12
2017-11-13
----

[IMPORTANT]
====
You should take notice of the seemingly random `23` in our code's output. If we take a look at the line that this `23` comes from, we can see that the `title` of the video is `Racist Superman | Rudy Mancuso, King Bach & Lele Pons` **which contains a comma**! As we can see, performing a `cut` using a comma as our delimiter sometimes doesn't get the field we want or expect it to! In languages like Python and R, this is often handled behind-the-scenes, but on the CLI we will have to think about these issues and handle them more intentionally. In the next few questions, we'll do just that.
====

.Deliverables
====
- The first 10 lines of the `publish_time` field from `USvideos.csv`
====

=== Question 3 (2 pts)

Okay, so we've identified an issue with our current approach of isolating the dates from our current pipeline. Should we just steamroll ahead and remove all the errors in post? _Definitely not_!! Instead, let's refer back to a tool we learned about in project 3: `grep`!

For the purposes of this question, you can assume that the string you are looking for in a given line is of the format `dddd-dd-ddTdd:dd:dd.dddZ` where `d` can be any digit between 0-9. Using `grep` and a regular expression of your own design, isolate the `publish_time` field from the first 10 lines of the file.

As a short reminder, https://regex101.com[regular expression building website] can be extremely helpful when designing and creating a regular expression, and we encourage you to use them throughout your work.

[NOTE]
====
The `-o` flag to `grep` will print only the parts of a line that match a given pattern. Additionally, we recommend you tackle this problem using the Perl flavor of regex `-P`, but any working pattern will be accepted for full credit.
====

If done correctly, your output should look like:

[source, bash]
----
2017-11-13T17:13:01.000Z
2017-11-13T07:30:00.000Z
2017-11-12T19:05:24.000Z
2017-11-13T11:00:04.000Z
2017-11-12T18:01:41.000Z
2017-11-13T19:07:23.000Z
2017-11-12T05:37:17.000Z
2017-11-12T21:50:37.000Z
2017-11-13T14:00:23.000Z
----

If we use `wc -l` to count the total lines in the file (40950) and compare that to the total number of lines from running our `grep` on the file as a whole, we should see a total of 40949 matches. The one line with no match is the header line! Using your `grep` on the entire file, pipe the output into a `wc` to count the number of lines ouput. If you get an answer other than 40949, something is amiss.

.Deliverables
====
- A `grep` regular expression to retrieve the dates
- A `wc` counting the number of lines that matched our pattern
====

=== Question 4 (2 pts)

We've now established a wicked-fast and tested way to extract every single `publish_time` field from our file. Let's now begin to analyze our isolated data!

By building on the `grep` pipeline you created in the previous question, isolate just the dates from the `grep` output (Hint: `cut` with a delimiter of "T"). Then, use `cut` again to isolate just the months (the second field in the date). Finally, use `sort` and then `uniq` to count the number of occurrences of each month in our data. Which month saw the most videos published? Which month had the least videos published? Write your answers in a markdown cell.

[NOTE]
====
https://stackoverflow.com/questions/6044539/generating-frequency-table-from-file[This stackoverflow post] will help you figure out how `sort` can be used with `uniq` to be sure that you are getting a concise frequency table for each month in the data.
====

.Deliverables
====
- Which months had the most and least videos published, and the code used to find this answer
====

=== Question 5 (2 pts)

This last question serves as a build onto the complexity of the previous one, and should largely be copy-paste of your already-existing pipeline. Instead of the month that had the most videos published, we're interested in the specific day of the year that had the most videos published. This means that now, we need to take both the month and day of the publish date into account when finding our answer (Hint: `cut` can select multiple fields!).

Building on your pipeline from the previous question, figure out what day of the year had the most videos published, and how many videos were published on that day. Put your final answers in a markdown cell.

[NOTE]
====
https://stackoverflow.com/questions/13690461/using-cut-command-to-remove-multiple-columns[This stackoverflow post] may help you understand how `cut` can be used to remove multiple fields from a string at the same time. As you can see, `cut` is an extremely powerful tool!
====

.Deliverables
====
- The day and number of videos published on the day of the year with the most videos published in our data.
====

== Submitting your Work

Before you submit this project, take a look back through your work. You started out this week's work with a simple `cut`, and by the end had built up a pipeline that created a frequency table using nothing but Bash commands that is able to almost instantly run through a file that is almost 50 thousand lines long! That is impressive, and it clearly demonstrates one of the main strengths of pipelines: they allow you to systematically break down a problem and solve it in steps, as opposed to approaching it all at once. 

Up until this point, I've held your hand through most of the steps to build the pipelines for the past two projects. In this upcoming project, you'll be presented with problems and asked to build pipelines on your own to find answers, with less guidance from the project instructions. If you are struggling, think about how we approached problems in these projects. Try to think of each problem as a series of steps, and solve them one by one until you have built up to the solution you are looking for.

As always, your helpful team of TAs will be here to assist you through difficulties you may have. We hope you have a great rest of your week.

.Items to submit
====
- firstname-lastname-project5.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====