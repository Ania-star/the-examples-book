= TDM 20100: Project 4 -- Intro to Piping

**Motivation:** In the past 3 projects, you've built up a truly impressive set of CLI skills that have drastically increased your ability to work with files in the terminal. However, even with all these different tools and commands at your disposal, each step you take to process some data at this point is quite fragmented. If you wanted to `cut` and then use `grep`, your choice up until now (excepting the last question of project 3) was to simply save the results of your `cut` to a file and then use `grep` on that file. So far, that hasn't caused any major issues. If you had a file with million, billions, or even trillions of lines of data, though, you would have a serious problem. This project will begin our investigation into an elegant and efficient method of solving this fragmentation problem: piping.

**Context:** This project will incorporate commands learned in all the previous ones, so reviewing projects 1-3 may help with completion of this project.

**Scope:** Pipes, pipelines, data processing, Bash, GNU, CLI

.Learning Objectives:
****
- Understand the basic concept of piping and pipelines
- Use the pipe operator to connect two basic commands
- Build your first data processing pipeline
****

Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about project submissions xref:submissions.adoc[here].

== Dataset(s)

This project will use the following dataset(s):

- `/anvil/projects/tdm/data/youtube/USvideos.csv`
- `/anvil/projects/tdm/data/flights/*`

== Questions

=== Question 1 (2 pts)

In TDM 101 and 102, you learned about Python and R, two of the most popular programming languages when it comes to data science. As you go through this project, you will likely see many parallels to things we did in those classes using R functions or `pandas` and wondering why we're bothering to do some of the same things using a different language, bash. Bash gives us easy access to highly optimized tools dedicated to a specific purpose.  Some of those specialized tools can run orders of magnitude faster while simultaneously being easier to write than similar code written in R or python.  Bash is far more limited in the scope of what it can do than Python and R, however.  Python and R provide us a high degree of flexibly when processing complex data like integers, datetimes, and more. Bash is often used to prepare your data for more complex processing with Python and R.  It is important that your portfolio of skills as a data scientist is wide and varied so that you will be able to choose the correct tool for a given problem.

To start this project, let's look at piping in its most basic sense. Run the following 2 code snippets, and observe their outputs:

[source, python]
----
%%bash
cd ~/../../anvil/projects/tdm/data/youtube
ls 
----

[source, python]
----
%%bash
cd ~/../../anvil/projects/tdm/data/youtube
ls | head -n3
----

Now let's examine in detail what's going on. In the first code snippet, we navigate to our `youtube` directory and use `ls` to list the files in that directory. In the second code snippet, we do the same `ls` as before, but then we use the pipe operator `|` to send the _output_ of `ls` to become the _input_ for `head`, which results in us only getting the first three outputs from `ls`!

For this question, we just want you to get used to using `|` by doing something almost identical to the above code snippet. Using `ls`, `head`, and `|`, output just the first 7 files/directories in `/anvil/projects/tdm/data/flights`.

.Deliverables
====
- An `ls` piped to a `head` to display the first 7 files/directories in `/anvil/projects/tdm/data/flights`
====

=== Question 2 (2 pts)

For these next questions, we'll be having you apply skills you learned in previous projects, then introducing piping to establish a more efficient way to solve the same problems.

To start, take a look at the first 5 rows of data for this project, which can be found at `/anvil/projects/tdm/data/youtube/USvideos.csv` (Hint: `head`), and store those 5 rows to a file in your `$SCRATCH` directory called `vid_head.csv`.

Next, use `cut` on `vid_head.csv` to get just the title of each channel (the 4th field in the comma-separated data), and store those titles to a file in your `$SCRATCH` directory called `channels.csv`.

Finally, use `cat` to print the contents of `channels.csv`.

[NOTE]
====
Remember from previous projects, the `>` redirect operator can be used to put the output of a command into a given file.
====

.Deliverables
====
- Command to store the first three lines of `/anvil/projects/tdm/data/youtube/USvideos.csv` to `$SCRATCH/vid_head.csv`
- Command to store just the titles of the videos from `$SCRATCH/vid_head.csv` to `$SCRATCH/channels.csv`
- Command to store the first three lines of `/anvil/projects/tdm/data/youtube/USvideos.csv` to `$SCRATCH/channels.csv`
====

=== Question 3 (2 pts)

Let's concisely summarize what we just did:

- We got the `head` of our data, and stored it to a file
- We cut the data from that file, and stored it to a new file
- We outputted the results of the file

While this works, it is by no means pretty nor efficient. Let's try to do the same thing, but this time using piping and without storing to any intermediary files along the way. Feel free to attempt this on your own now, or continue reading for some more in-depth instructions.

First, we want to use `head` to get the first 5 rows of data for our `USvideos.csv` data. Next, we want to use `|` to pipe the output of `head` to the `cut` command we wrote in the last question. Remember, because we're piping the output of `head` to `cut`, we shouldn't include the name of an input file to our `cut` command, as the output from `head` will be used instead. If you did everything correctly, they should match the output you got in the last problem but this time without making a whole bunch of temporary files and breaking everything into unnecessarily discrete steps!

[IMPORTANT]
====
At this point, you may want to `rm $SCRATCH/vid_head.csv` and `rm $SCRATCH/channels.csv`. While it won't immediately break anything if you don't, it is always good practice to keep your directories tidy and only containing necessary data.
====

.Deliverables
====
- The list of commands from Question 2, collapsed into one line using piping without storing to any files.
====

=== Question 4 (2 pts)

Briefly, we should discuss the idea of _pipelines_ that has been underlying our actions thus far. A pipeline (in this context) is just another name for a series of commands, where the output of each command is piped to the input of another. This concept is one that is mirrored throughout all of data science, computer science, engineering, and just about any STEM-related field. In non-STEM related fields, pipelines like manufacturing and product transport are prevalent. The takeaway from all of this is that the logical concept of a pipeline is extremely powerful, and it is an effective idea to try and translate problems that you encounter in data science into a pipeline of smaller steps towards your final solution.

[NOTE]
====
For a videogame that heavily emphasizes the importance of planned and efficient pipelines, the author of this project recommends https://factorio.com/[Factorio] as a personal favorite.
====

With our semantic understanding now established, let's return to our pipeline from the last problem. Copy your completed pipeline from question 3, and add a call to `wc` so that it tells us the number of lines in our output. You should note that it should match the number of lines that `head` is outputting, at the moment.

Modify this pipeline such that you are getting the titles of each channel for every line in `USvideos.csv`, then using `uniq` to get rid of any duplicates, and finally using `wc` to count the number of unique channels in our data. 

[IMPORTANT]
====
You may have noticed some strange quotations around certain titles in our data. While it is good to note that these are there, they will not affect your answer to this question and you don't have to deal with them right now.
====

Then run your pipeline again in another code cell, this time without using `uniq`. How many duplicates were there in our data? Is this surprising?

[NOTE]
====
For more information about how to use `uniq`, we would recommend you view https://explainshell.com/explain/1/uniq[its man page]. As `wc` was covered in Project 2, your work for that project may be helpful when trying to figure out this question.
====

.Deliverable
====
- A pipeline that counts the number of unique channel titles in `/anvil/projects/tdm/data/youtube/USvideos.csv`
- A pipeline that counts the number of total channel titles in `/anvil/projects/tdm/data/youtube/USvideos.csv`
- The number of duplicate channel titles calculated based on the results of your two pipelines
====

=== Question 5 (2 pts)

Using a slight variation on the pipeline you built in the last question, count how many tail numbers (the 11th field when you cut on ",") there are in `/anvil/projects/tdm/data/flights/2023.csv`, and compare it to how many unique ones there are. Notice that both answers are over 2 million. If we tried to do something like this in Python or R, simply loading the data normally takes about 10 seconds. Using bash, we can do the whole thing in less than 5!

Finally, calculate how many duplicate tail numbers there are in the data, using the outputs of your two individual pipelines.

.Deliverables
====
- The number of tail numbers in `2023.csv`
- The number of unique tail numbers in `2023.csv`
- The number of duplicate tail numbers in `2023.csv`
====

== Submitting your Work

With this question complete, you've successfully completed The Data Mine's introduction to piping and pipelines! While this project was syntactically quite simple, the concepts at play are hugely important and complex. As we continue on the next few projects building some more complex pipelines, continue to think about how we're breaking down large problems into groups of smaller ones, making the processing of the data both easier to perform and more readable.

.Items to submit
====
- firstname-lastname-project4.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====