= STAT-LLC Fall 2014 STAT 29000 Projects

== Project 1

Question 1.

During which pair of years did the level of Lake Huron rise the most?
The data to use is from the built-in `LakeHuron` data set.
(E.g., during 1875 to 1876, Lake Huron rose 1.48 feet.)  It might help to use the `diff` command.

Question 2.

a. What is the average duration of an eruption in the `geyser` dataset in the `MASS` library?

b. What were the 10 longest durations?

c. How many durations were 3 minutes or longer?
(You do not need to install the `MASS` library; it is installed already.  You do, however, need to load the `MASS` library.)

Question 3.

a.  Which car(s) in the `mtcars` data set had the highest gas mileage?

b.  Which car(s) had the highest horsepower?

c.  Which car(s) had the shortest (i.e., fastest) 1/4 mile time?

d.  How many cars had manual transmission?

e.  How many cars had manual transmission and also six cylinders?

Question 4.

a.   Which states are (strictly) larger in population than Indiana but (strictly) smaller in population than Pennsylvania, according to the data in the `state` data set?
Hint: You can get the state populations using `state.x77[,"Population"]`.

b.   Which states are (strictly) larger in land area than Indiana but (strictly) smaller in land area than Pennsylvania, according to the data in the `state` data set, as listed in `state.x77[,"Area"]`?

Question 5.

If `Z` is a standard normal random variable, we know that `Z` has average 0 and variance 1.  Use `R` to simulate:

a. the value of the average of `|Z|`, and

b. the value of the variance of `|Z|`.

Here, `|Z|` is just the absolute value of `Z`.

Question 6.

Write a function called `countas` that takes a sequence of words and returns the number of words that have 1 or more `a`s. For instance, `countas(  c("ate", "hello", "duolingo", "pat", "aa")  )` should return the value 3.  Hint:  It might help to use the `grep` function.

Question 7.

a.  Write a function called:  `firstthree` that returns the location of the first occurrence of 3 in a vector.  For instance, `firstthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 2.

b.  Write a function called:  `thirdthree` that returns the location of the third occurrence of 3 in a vector.  Ffor instance, `thirdthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 9.

Question 8.

Write a function called:  `topfive` that returns the most common five values in a vector, along with the counts for each of the 5 values.

Question 9.

a. Euler's number is 2.718281828459...  Euler's number is defined as `1 + 1/1 + 1/(1*2) + 1/(1*2*3) + 1/(1*2*3*4) + 1/(1*2*3*4*5) + ...` Find a good way to calculate this in `R`, with few keystrokes. If you subtract 2.718281828459 from your estimate, you should get something very small, e.g., roughly `4.5 * 10^{-14}.`

b.  Find a good way to approximate the value of Pi, using only the fact that `Pi^2 / 6 = 1/1^2 + 1/2^2 + 1/3^2 + 1/4^2 + 1/5^2 + 1/6^2 + 1/7^2 + ....`

Question 10.

a. The triangular numbers are: `1, 3, 6, 10, 15, 21, 28, 36, 45, 55, ...` See http://oeis.org/A000217 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

b. The tetrahedral numbers are: `1, 4, 10, 20, 35, 56, 84, 120, 165, 220, ...` See: http://oeis.org/A000292 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

== Project 2

Question 1.

Consider the Columbia River Estuary dataset discussed in the `week 2 notes`

a.  Download the data set (we no longer need to do this).

b.  Import the `saturn03.240.A.CT_2012_06_PD0.csv` data set into `R`, using the `read.csv` function.

c.  Use the `strptime` function to convert the first column of the data into numerical times that `R` can easily handle.

Question 2.

a.  What is the most common time (in seconds) between consecutive measurements, in the data set?  How often is the data sampled with this exact difference in time, between consecutive measurements?

b.  What is the mean time between consecutive measurements?  Why is this significantly different from the most common time, found in part `2a` above?

Question 3.

a.  Suppose that we treat "15 seconds" as a threshold in consecutive time measurements, i.e., if the machine goes more than 15 seconds without taking a measurement, we consider that the machine is temporarily broken/clogged/stuck/etc.  With this level of threshold, how many times did this particular machine (at this particular location) get stuck during June 2012?

b.  How long is longest duration when the machine was broken?  When did this occur? Specifically: when did it break, and when did it start working properly again?

c.  Find the ten longest durations for when the machine was broken; just give each such measurement in seconds.

Question 4.

a. Does the device which measures the electrical conductivity ever give a false reading?  If so, when?  Give the specific times (e.g., the day(s), hours, minutes, seconds), when this occurs in June, for each such occurrence.

b. Are any of these times in `4a` the same as the one (unique) time when the temperature device gave a false reading?  (We saw, in the notes, that the temperature device had one false reading.)

c.  Does the device which measures the salinity ever give a false reading?  What evidence to you have to support this claim?

Question 5.

a.  Repeat the questions from `2a`/`2b`/`3a`/`3b`/`3c`, but now use the data set from the same point on the Columbia River Estuary but at the depth of `8.2m` (the data from the questions above was measured at `2.4m` below the surface).  The data set from `8.2m` below the surface is available at `saturn03.820.A.CT_2012_06_PD0.csv`.

b.  Does the longest time in which the machine was broken in `3b` (at depth `2.4m`) correspond roughly to the same longest time in which the machine was broken in this current data set, at depth `8.2m`?  For this longest time interval, what are the times (at depth `8.2m`), when the machine did break, and when did it start working properly again?

c.  Make a plot of the temperature data at depth `8.2m`.  There is exactly one false reading in which the temperature is too high, and exactly one false reading in which the temperature is too low.  Be sure to remove these points before plotting.

Question 6.

a.  We also have data from depth `13m` below the surface in the file `saturn03.1300.R.CT_2012_06_PD0.csv`.  Import this data into `R`.

b.  Is the water temperature generally highest, on average, at depth `2.4m`, `8.2m`, or `13m` below the surface?  Does your answer make intuitive sense?

Question 7.

a.  What is the average salinity of the water at depth `2.4m`?  At depth `8.2m`?  At depth `13m`?  What about the variance of the salinity at all 3 depths?  Be sure to remove any outliers, when appropriate.

b.  At depth `13m`, make a plot of time versus salinity.

c.  As we saw in `7b`, much more data is available during the first two weeks of June, as opposed to the second two weeks of June.  Make a revised plot, showing only the time versus salinity from the start of the day on June 6, through the end of the day on June 12 (i.e., for a full 7-day period).  How many cycles of the salinity do you think you see on this plot?  Is there a natural reason for this number of cycles?

Question 8.

At depth `2.4m`, what fraction of the temperature data points are between 10 and 12?  Between 12 and 14?  Between 14 and 16?  Between 16 and 18?  Use the `tapply` function to answer all four of these questions with one line of code.

Question 9.

At depth `2.4m`, what is the average temperature between the start of the day on June 1 and the end of the day on June 7?

What is the average temperature between the start of the day on June 8 and the end of the day on June 14?

What is the average temperature between the start of the day on June 15 and the end of the day on June 21?

What is the average temperature between the start of the day on June 22 and the end of the day on June 28?

Use the `tapply` function to answer all four of these questions with one line of code.

[Note: The original problem statement had an off-by-one typographical error on some of the dates.]

Question 10.

At depth `13m`, how many data points have salinity greater than 12 and temperature greater than 14?

How many data points have salinity greater than 12 and temperature at most 14?

How many data points have salinity at most 12 and temperature greater than 14?

How many data points have salinity at most 12 and temperature at most 14?

Use the `tapply` function to answer all four of these questions with one line of code.

Hint:  You will need to embed a `list` into your `tapply`, as we did in the notes file `CO2examplecontinued.R` (the second CO2 example).

== Project 3

This project is all about the `Airline on-time performance`, from the American Statistical Association's http://stat-computing.org/dataexpo/2009/[2009 Data Expo]

There is also some `supplemental-data.html` provided by the ASA.

You can see href="http://stat-computing.org/dataexpo/2009/the-data.html[the data on the ASA site] too.  In particular, there is a listing of all of the parameters, which might be helpful for you to print.

I already downloaded it for you, to make things a little easier for you.  Since the data itself is so large, I saved it into a common data directory:
`/data/public/dataexpo2009/`

Notes:  If you want to read ALL of the data into `R` at once, you can do it, but it takes quite awhile (it might take more than 15 minutes to initially load the data).

You can import just a year or two of the data at a time, to start working with the data.  You are not expected to import all of the data while you are solving the questions.  You can wait until you have solved the questions, and then come back and try to get the answers with all of the data.  So, for instance, you might want to start with just a few specific years only:

`bigDF <- rbind( read.csv("/data/public/dataexpo2009/2006.csv"), read.csv("/data/public/dataexpo2009/2007.csv"), read.csv("/data/public/dataexpo2009/2008.csv") )`

and once you are sure that everything works, before you get ready to submit your data, you can load all of the years.

There are over 3.5 billion pieces of data in the files altogether, if you load all of the years from 1987 through 2008.

Just loading the data itself (if you choose all of the years) might take roughly 15 or 20 minutes to accomplish.  It would be done with some code like this:  (WARNING! This will take quite a long time to load, if you load all years at once.)

`bigDF <- rbind(
read.csv("/data/public/dataexpo2009/1987.csv"),
read.csv("/data/public/dataexpo2009/1988.csv"),
read.csv("/data/public/dataexpo2009/1989.csv"),
read.csv("/data/public/dataexpo2009/1990.csv"),
read.csv("/data/public/dataexpo2009/1991.csv"),
read.csv("/data/public/dataexpo2009/1992.csv"),
read.csv("/data/public/dataexpo2009/1993.csv"),
read.csv("/data/public/dataexpo2009/1994.csv"),
read.csv("/data/public/dataexpo2009/1995.csv"),
read.csv("/data/public/dataexpo2009/1996.csv"),
read.csv("/data/public/dataexpo2009/1997.csv"),
read.csv("/data/public/dataexpo2009/1998.csv"),
read.csv("/data/public/dataexpo2009/1999.csv"),
read.csv("/data/public/dataexpo2009/2000.csv"),
read.csv("/data/public/dataexpo2009/2001.csv"),
read.csv("/data/public/dataexpo2009/2002.csv"),
read.csv("/data/public/dataexpo2009/2003.csv"),
read.csv("/data/public/dataexpo2009/2004.csv"),
read.csv("/data/public/dataexpo2009/2005.csv"),
read.csv("/data/public/dataexpo2009/2006.csv"),
read.csv("/data/public/dataexpo2009/2007.csv"),
read.csv("/data/public/dataexpo2009/2008.csv"))`

Therefore, it is probably better (instead) to test your code on (say) three years of data, e.g., 2006-2008, before working on the full data set.

Question 1.

a. Consider the departure times (`DepTime`).  What fraction of the data are missing, i.e., are stored as `NA` values?

b. Within the departure times that are recorded (i.e., that are not `NA` values), the times are stored in `hhmm` format.  So there should be at most `24*60 = 1440` such possible times.  Are there other `DepTime` values?  Are they correct or perhaps erroneous?  How many such `DepTime` values (overall) seem to be erroneous?

Question 2.

a.  Which departure times are the best, for minimizing the arrival delay (`ArrDelay`)?  More specifically, if our goal is to minimize the arrival delay, which of these 4 time categories is best time of day for our departure?  Between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Or between 6 PM and 12 midnight?

b.  Which of the 4 time categories for the departure will have the highest variance for arrival delay?

c.  Now please solve `2a` and `2b` again, splitting the data not only by the best time of day but also by the airline too.  That way, we can know what time of day and which airline we might prefer to use.

Question 3.

a.  Which 10 airports have the most departures?

b.  Which 10 airports have the most arrivals?

c.  If we reconsider `3a` and `3b`, by splitting the data year by year, are the answers to `3a` and `3b` relatively consistent from year to year?

d.  Which are the most 10 popular pairs of departure/arrival city pairs?  (For instance, `IND-to-ORD` might be one such popular pair.)

Question 4.

a.  Which 5 airports are most likely to be on time for arrivals (on average)?

b.  Which 5 airports are most likely to be on time for departures (on average)?

c.  Which 5 airports are most likely to be delayed for arrivals (on average)?

d.  Which 5 airports are most likely to be delayed for departures (on average)?

Question 5.

a.  Which is the best day of the week to fly, if you want to minimize delayed arrivals?

b.  Which portion of the flights depart on which days?

c.  What percent of flights depart between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Between 6 PM and 12 midnight?

d.  Can you study 5b and 5c simultaneously, e.g., can you give an analysis by day of the week and time of day (in tandem), so that we know precisely which days of the week and which portions of the days are busiest for departures, i.e., so that we have a finer breakdown of the departure data?

Question 6.

a.  Which 5 carriers are the most likely to be delayed?

b.  Which 5 carriers are the most likely to be on time?

Question 7.

a.  Give a month-by-month breakdown of the percentage of cancelled flights.

b.  What are the worst 3 months of the year for cancelled flights?  I.e., during which 3 months are the most flights cancelled?  (Since 1987 is an incomplete year, please avoid the data from 1987 for `7a` and `7b`, because we do not want to unfairly balance the months.)

Question 8.

Make a plot that shows how the number of flights departing `ORD` has changed, year by year.  Then add similar data to the same plot, for the number of flights departing `IND`, year by year.

Question 9.

Read the documentation for the `dotchart` function.  Make a `dotchart` as follows:  The x-axis should be the percentage of the time that flights are delayed more than 30 minutes.  On the y-axis, the main groupings should be according to month, and within each month, please show O'Hare and Indianapolis as cities of departure for flights.  The data to be displayed are the `DepDelay` data for 2007 only.  So the overall plot will show, month-by-month, a comparison of the `DepDelay` data for O'Hare and Indianapolis.

Question 10.

Make another `dotchart`, similar to the one in question `9`, where the main groupings on the y-axis are O'Hare and Indianapolis, and within each city, display all 12 months.  Again, the data to be displayed are the `DepDelay` data for 2007.  The x-axis should again be the percentage of the time that flights are delayed more than 30 minutes.  So the overall plot will show, for each of the two cities, a month-by-month comparison of the `DepDelay` data.  If you are able, you can organize the months according to their percentage of time delayed more than 30 minutes, rather than according to alphabetic order.

== Project 4

This project is about visualizing data.  It will give you some time to write ababout data visualization and to take a little break from coding.

Question 1.

Check out the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM).  Find 4 (or more) separate plots on `Many Eyes` (please give links to each of these plots) that violate the concepts of effective data visualization that are discussed in the handouts from class (e.g., in Cleveland's book and Robbins's book, and in the paper "How to display data badly").  Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved.  Imagine, for instance, that you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data.  (Your discussion of these 4 plots should be at least one single-spaced page in (say) 12 point Times font, for example... but more than 1 page is certainly allowed.)  Each student should write about at least 1 plot.

Question 2.

Revisit the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM).  Find 4 (or more) separate plots (again, with links to the plots) on `Many Eyes` that do an overall good job of effective data visualization.  Justify the reasons why you think that the plots are effective.  (Again, please write at least one page total, justifying the reasons that you think the plot is effective.)  Each student should write about at least 1 plot.

Question 3.

Check out the website http://www.informationisbeautiful.net[Information Is Beautiful].  Find 4 (or more) separate plots on `Information Is Beautiful` (please give links to each of these plots) that violate the concepts of effective data visualization.  Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved.  Imagine you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data.  Your constructive criticism should be at least 1 page altogether.

Question 4.

The http://www.gapminder.org/world[Wealth and Health of Nations] is a fun depiction of data.  On the other hand, as with many depictions of data, it violates some of the techniques of effective data display.  Please write an explanation of which techniques of effective data display are violated.  If you imagine you are writing a constructive criticism to the authors of this animation, please make suggestions for how the depiction of data (for the health and wealth, over the years displayed) could have been done more effectively.  Please make sure your explanation is at least 1 page long.

Question 5.

Describe (at least!) 4 very significant ways that the poster winner "Congestion in the sky" http://stat-computing.org/dataexpo/2009/posters/ from the `Data Expo 2009` poster competition results could be significantly improved, using the concepts of effective data visualization.  Write a constructive criticism (of at least 1 page) that gives suggestions for improvement on each aspect that you criticize.

Question 6.

For the other posters (do not use the winner, "Congestion in the sky", since it was discussed already in question `5`), find a total of at least 4 significant ways that some of the other posters can be improved.  You can analyze several different posters, that is OK.  Your constructive critique should be at least 1 page.

Question 7.

Which of the posters in the `Data Expo 2009` do you think should be the winner?  Why?  (It is OK if you choose the poster that actually won, or any of the other posters.)  Thoroughly justify your answer, using the techniques of effective data visualization, to justify your answer, with an explanation that is at least 1 page long.

Questions 8, 9, 10.

Imagine that you are going to enter the `Data Expo 2009`.  Rather than having to organize your information into a poster, prepare 3 pages of analysis, exploring some aspects of the airline data set that are interesting to you, and which you think might be of broad interest to potential readers too.  Your discussion and plots should be at least 3 pages long.

