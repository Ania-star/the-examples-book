= STAT-LLC Fall 2015 STAT 29000 Projects

== Project 1

Question 1.

a. An `.Rda` file format is used to store data for use with R. Use the `load` function to load `review.Rda` into the R environment. Use the `ls` function to discover the data frame's name. What is the data frame's name?

b. When dealing with an unfamiliar dataset, it is typically best to get a bird's eye view of the data. Use the summary function to find the names of the columns.

The data frame contains 8 components (the votes component has 3 columns):

[source,r]
----
review_frame$votes$funny
review_frame$votes$useful
review_frame$votes$cool
review_frame$user_id
review_frame$review_id
review_frame$stars
review_frame$date
review_frame$text
review_frame$type
review_frame$business_id
}
----

Alternatively, you can use the names function to learn this information:

`names(review_frame)`

You can see that votes has some embedded columns this way:

`names(review_frame$votes)`

You can check the type of each vector as follows:

[source,r]
----
class(review_frame$votes$funny)
class(review_frame$votes$useful)
class(review_frame$votes$cool)
class(review_frame$user_id)
class(review_frame$review_id)
class(review_frame$stars)
class(review_frame$date)
class(review_frame$text)
class(review_frame$type)
class(review_frame$business_id)
}
----

Also you can see that review_frame is a data.frame

`class(review_frame)`

and that the votes are an embedded data.frame with the review_frame:

`class(review_frame$votes)`

c. How long are each of the columns?

d. What is the average number of stars given to a review?

e. What is the user id of the individual who wrote the review with the most “useful” votes?

f. Assuming the funniest reviews got the most funny votes, print the text of the funniest review.

g. What is the distribution of the number of stars?

Question 2.

a. Create a new factor called totalvotes, which sums the numbers of funny, useful, and cool votes.

b. How many of the reviews received at least 160 votes?

c. Print the user_id’s of the people who wrote the ten reviews that were voted on the most.

Question 3.

a. Now use the “load” function to load business.Rda into the R environment. Once again, use the “ls” function to discover the data frames’s name. What is the data frame’s name?

b. Use the names command on the data frame to find out what variables are stored in the data frame.

c. How many unique states are a part of this data set? Hint: The factor “state” is useful here.

d. The state closest to Purdue that is also in YELP’s dataset is Illinois. How many businesses in Illinois are in the dataset?

e. How many Illinois businesses have strictly more than 50 reviews?

Question 4.

a. How many businesses are listed in Illinois?

b. How many businesses are listed in Arizona?

c. The review dataset and the business dataset have a single factor in common–business_id. The business dataset has the state in which the business resides, and the review dataset doesn’t. Let’s say that a state is more popular if it has the most votes per business (regardless of whether the votes are high or low). Which state’s businesses are more popular by this measure (i.e., by most votes per business), Arizona or Illinois? (You will need to use both data sets for this.)

Sketch of one kind of method for solution: Essentially we want to first identify which business_id’s are in Illinois, and then use the %in% command to identify which of the businesses in the review_frame correspond to Illinois, and then tally their number of reviews, and finally divide by the answer in 4a. Then we want to repeat this process for Arizona.

Question 5.

a. What does the function tolower do?

b. How many of the review texts contain the word happy? (case-insensitive) Hint: it will be helpful to read about the grepl command.

c. How many of the review texts contain the word good? (case-insensitive)

d. How many of the review texts contain both of these two words? (case-insensitive) Hint: You can use one ampersand for the logical “and”.


== Project 2

Question 1.

a. A `.csv` file format stands for `comma separated value`, and is a very popular format to store data. The file `review.csv` is extracted from `review.Rda`. Even though the file contains the same data, it is twice the size!

Import `review.csv` into the variable called `review` using the `read.csv` function. Using the function `proc.time()`, find out how long it takes R to load this csv file. Simply run

`startingtime <- proc.time()

before the command runs and then

`stoppingtime <- proc.time()

after the `read.csv` command runs, and then take the difference of the two times, to find out how long it took to load the data.

Notice that read.csv has the parameter `head=TRUE` by default, which is good, since the csv file has the variable names stored on line 1, as a header, that lets R know the intended names of the variables found on all of the rest of the lines of the file.

b. Now load the equivalent `review.Rda` file into R using the `load` function. As above, use `proc.time()` to time how long this takes.

c. Which format is faster to read into R? Rda or csv?

d. Make sure that the data frames from the Rda and csv files of the `review.Rda` versus `review.csv` are the same dimensions.

Question 2.

a. Use the strptime function to convert the `date` factor from a factor to a `POSIXt` data type. This will allow you to add and subtract dates easily.

b. Find the time (in the format `%Y-%m-%d`) of the first review (chronologically). Find the time of the last review (chronologically). Now take a difference. This allows us to see the length of the time period in which reviews were collected.

Question 3.

a. Use strsplit (with `-` as the split parameter) to break the strings in the dates of the reviews into their component years, months, and dates. Then use unlist to combine the results into a vector that has all of the years, months, and dates.

b. From the vector above, extract the years of each review. (Hint: You can use the seq command, with by=3, as an index to your vector; this will allow you to extract every third element of your vector.) Check to make sure that the number of years in the vector that you created is the same as the number of reviews in the data set.

Question 4.

a. Use tapply to find the average number of stars per year. Hint: Use the vector of years that you created in question 3b above.

b. Similarly to when you identified the total votes in Project 1, create a new column in the review_frame data frame that contains the mean of all three votes. Hint: If you use the mapply function, it is necessary to take a sum first, and then divide by 3. It is not necessary to use the mapply. It is probably just easier to take a mean directly.

c. Use tapply with the business data set to see how many reviews have been made of open businesses and how many have been made of closed businesses. (Use the review_count column to get the number of reviews of each business.)

Question 5.

a. Use tapply to get the number of businesses per state.

b. Use tapply to get the average number of reviews within each state.

c. How many businesses in each state have karaoke?

d. With regard to alcohol service, how many businesses are listed as having a full_bar? beer_and_wine? none? For how many businesses is it unknown whether alcohol is served? Use the table command to answer all four of these questions at once. The table command has a parameter that allows the NA elements to show up (check the documentation for the table command).

Question 6.

a. Create a function that takes a factor with categorical variables and spits out a labeled pie chart (it is up to you whether or not to include the NA values). Call the function: givemepie. You can use the function “pie” within your function.

b. Use your function on the alcohol factor.

Question 7.

a. What fraction of businesses have latitude between 32 and 40? 40 and 48? 48 and 57? Use only 1 line of code (Hint: Use the tapply function with specified values of cuts and breaks.)

b. What fraction of businesses have longitude between -120 and -80? -80 and -40? -40 and 0? 0 and 40? Use only 1 line of code.

c. In one line of code show what percent of businesses lies within the intersection of each of (a) and (b)’s breaks. You should end up with a 3x4 matrix of percentages.


== Project 3

This project is all about the `Airline on-time performance`, from the American Statistical Association's http://stat-computing.org/dataexpo/2009/[2009 Data Expo]. There is also some href="http://stat-computing.org/dataexpo/2009/supplemental-data.html[supplemental data] provided by the ASA as well.

You can see href="http://stat-computing.org/dataexpo/2009/the-data.html[the data on the ASA site]. In particular, there is a listing of all of the parameters, which might be helpful for you to print.

I already downloaded it for you, to make things a little easier for you. Since the data itself is so large, I saved it into a common data directory:

`/data/public/dataexpo2009/`

Notes: If you want to read ALL of the data into R at once, you can do it, but it takes quite awhile (it might take more than 15 minutes to initially load the data).

You can import just a year or two of the data at a time, to start working with the data. You are not expected to import all of the data while you are solving the questions. You can wait until you have solved the questions, and then come back and try to get the answers with all of the data. So, for instance, you might want to start with just a few specific years only:

`bigDF <- rbind( read.csv(“/data/public/dataexpo2009/2006.csv”), read.csv(“/data/public/dataexpo2009/2007.csv”), read.csv(“/data/public/dataexpo2009/2008.csv”) )`

and once you are sure that everything works, before you get ready to submit your data, you can load all of the years, by typing:

bigDF <- rbind( read.csv(“/data/public/dataexpo2009/allyears.csv”) )`

There are over 3.5 billion pieces of data in the files altogether, if you load all of the years from 1987 through 2008.

Question 1.

a. What percentage of data is missing (`NA`) from `DepTime`? How about from `ArrTime`?

b. Focus on `DepTime`, `CRSDepTime`, `ArrTime`, and `CRSArrTime`. These are times in the `hhmm` format. Use the `strptime` function to convert the time `1359` to `POSIXlt` using strptime. What is the resulting output?

c. Now use the `strptime` function to convert the time `1360` to `POSIXlt` using `strptime`. What happens? Why?

d. Consider times that cannot exist (as in 1c) as erroneous data (it makes no sense!). Are there any erroneous times in `DepTime`, `CRSDepTime`, `ArrTime`, and `CRSArrTime`? If so, how many such times, in each category?

Question 2.

a. Everyone hates late departure times. Of the late departures (DepDelay), what percentage of flights depart 0-5 minutes late? 5-10 minutes late? 10-15? 15-20? 20-25? Etc.?

b. Make boxplots that show, for each of the 7 days of the week, the degree to which departure times are delayed.

`If you want to only plot a random selection of the points, that is OK too. The reason is that it will probably take your R session forever to render the plot with all of the millions of dots for the millions of flights. If you choose to only plot a random selection of plots, please do not just plot the points at the start of the vector, since that would just correspond to the 1987 data. Instead, for instance, take every 1000th point. I.e., if the points that you wanted to plot are stored in vector v, then instead of plotting all of v, you could plot

`v[seq(1,length(v),by=1000)]`

This will save you a lot of time when you render your plot in R, and it will still give you a very good picture of what is going on, i.e., it will still give you a good understanding of the behavior of your data. In this case, you would need to be sure to take every 1000th point of your data, and also every 1000th day too, so that your data and the days of the week are in agreement.]

Question 3.

a. Give a chart with 12 columns (corresponding to the months) and 22 rows (corresponding to the years), which computes how many flights have DepDelay &gt; 0 in each of the months and years.

b. Restrict attention to only the flights with delays. You can find whether a flight is delayed by checking whether the DepDelay is positive. What are the 5 carriers who are most responsible for these delays?

Question 4.

a. The airports.csv file contains data on each of the airports. Load airports.csv into a data frame called airports.

b. Add a factor to the airports data frame called “freq”, which gives the total number of flights both into and out of the respective airport.

c. Identify the 5 most popular departure-to-arrival paths in the USA.

d. Find the very most popular departure-to-arrival path in each year.

Question 5.

a. The file plane-data.csv contains data on the planes. Load plane-data.csv into a data frame called planes.

b. Rank the 10 manufacturers, according to the total number of miles flown. It will be necessary to use the TailNum information from the plane-data file (which has tailnum and manufacturer) and from the large dataexpo data (which has TailNum and Distance).

c. Consider all of the planes that flew over 10000 miles in 2008. How many such planes are there? How old is the oldest such plane?

d. There are 5 airplane types in the plane-data (“Co-Owner”, “Corporation”, “Foreign Corporation”, “Individual”, “Partnership”, and also one unknown “”). Show the total breakdown of miles, according to these types of plane.

Question 6.

a. Use the airports.csv file to determine how many airports are listed for each state.

b. Using the iata codes from the airports.csv file, and restricting attention to the airports from Indiana, which 5 airports in Indiana had the most arriving flights?

c. Using the iata codes from the airports.csv file, and restricting attention to the airports from the Midwest (which we will call “IL”, “IN”, “MI”, “OH”, “WI”), identify the 5 most popular departure-to-arrival paths within the Midwest (i.e., which both depart and also arrive in the Midwest).

Question 7.

Use mapply to print sentences for corresponding to question 4c, e.g., the sentences might say something like:

"The number 1 departure-to-arrival path in the USA is `ORD` to `IND` with 000000 flights altogether."

(but of course use the actual values for the origin, destination, and number of flights, and do this for all 5 results in 4c, by using the `mapply` function with the `paste` command.)

Question 8.

a. One way that we might try to predict the hub airport for each of the airlines is to find the airport where that airline departs most often, i.e., the airport that is most often used as the origin for that airline. Print a table that shows, for each airline, this top airport origin.

b. Solve question 8a again, using the destination airports instead of origin airports this time.

c. Now consider each airport, and find which airline departs from that airport most often.

d. Solve question 8c again, this time finding which airline arrives to that airport most often.

Question 9.

a. If we classify flights by their distance (e.g., 0 to 500 miles; 500 to 1000 miles; 1000 to 1500 miles; etc.), which classification of flights have the longest delays, on average? This will give us some information about whether shorter or longer flights have a longer average delay.

b. If we classify flights by their departure time (e.g., before 6 AM; 6 AM to 12 noon; 12 noon to 6 PM; 6 PM to 12 midnight), which classification of flights have the longest delays, on average? This will give us some information about whether it is preferable to depart earlier or later in the day.

Question 10.

a. Write a function that takes two airports as inputs and finds the number of flights from the first airport to the second airport (you can call it numflightsfunc).

b. Try your function from 10a on a pair of airpots, e.g., flights from IND to ORD.

c. Write a “most popular destination function” (you can call it mostpopfunc) that takes a group of airports as the input and finds which of them is the most popular destination, i.e., which airport has the most arrivals.

d. Try your function from 10c on 3 popular airports, e.g., JFK, ORD, and LAX, to see which of these 3 airports is the most popular destination.



== Project 4

Question 1.

Check out the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM). Find 3 (or more) separate plots on Many Eyes (please give links to each of these plots) that violate the concepts of effective data visualization that are discussed in the handouts from class (e.g., in Cleveland’s book and Robbins’s book, and in the paper “How to display data badly”). Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved. Imagine, for instance, that you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data. (Your discussion of these 3 plots should be about 1/3 of a page per plot, i.e., about 1 page altogether; more than 1 page altogether is certainly allowed.) Each student should write about at least 1 plot.

Question 2.

Revisit the website http://www.ibm.com/manyeyes[Many Eyes]. Find 3 (or more) separate plots (again, with links to the plots) on Many Eyes that do an overall good job of effective data visualization. Justify the reasons why you think that the plots are effective. (Again, please write at least 1/3 of a page for each plot, i.e., one page total, justifying the reasons that you think each plot is effective.) Each student should write about at least 1 plot.

Question 3.

Check out the website http://www.informationisbeautiful.net[Information Is Beautiful]. Find 3 (or more) separate plots on Information Is Beautiful (please give links to each of these plots) that violate the concepts of effective data visualization. Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved. Imagine you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data. Your constructive criticism should be at least 1/3 of a page per plot, i.e., at least 1 page altogether.

Question 4.

The http://www.gapminder.org/world[Wealth and Health of Nations] is a fun depiction of data. On the other hand, as with many depictions of data, it violates some of the techniques of effective data display. Please write an explanation of which techniques of effective data display are violated. If you imagine you are writing a constructive criticism to the authors of this animation, please make suggestions for how the depiction of data (for the health and wealth, over the years displayed) could have been done more effectively. Please make sure your explanation is at least 1 page long.

Question 5.

Describe (at least!) 3 very significant ways that the poster winner `Congestion in the sky` (http://stat-computing.org/dataexpo/2009/posters/[from the Data Expo 2009 poster competition results]) could be significantly improved, using the concepts of effective data visualization. Write a constructive criticism (of at least 1 page) that gives suggestions for improvement on each aspect that you criticize.

Question 6.

For the other posters (do not use the winner, `Congestion in the sky`, since it was discussed already in question 5), find a total of at least 3 significant ways that some of the other posters can be improved. You can analyze several different posters, that is OK. Your constructive critique should be at least 1 page.

Question 7.

Which of the posters in the Data Expo 2009 do you think should be the winner? Why? (It is OK if you choose the poster that actually won, or any of the other posters.) Thoroughly justify your answer, using the techniques of effective data visualization, to justify your answer, with an explanation that is at least 1 page long.

Questions 8, 9, 10

Imagine that you are going to enter the Data Expo 2009. Rather than having to organize your information into a poster, prepare 3 pages of analysis, exploring some aspects of the airline data set that are interesting to you, and which you think might be of broad interest to potential readers too. Your discussion and plots should be at least 3 pages long.


== Project 5

Question 1.

a. Load the airplane data from 2008. Make a new data frame that contains only the 15th, 16th, and 19th columns, i.e., the ArrDelay, DepDelay, and the Distance, and that only contains every 1000th row of the original data frame, i.e., it contains the 1st row, 1001st row, 2001st row, etc. (You can either index the columns by the numbers 15, 16, 19, or by the names of the columns; it is worthwhile to make sure that you know how to do this both ways.)

b. Read the help documentation for the “pairs” function (which generates scatterplot matrices) and take a look at the examples at the end of the “pairs” documentation.

c. Use the pairs function to build a scatterplot of the data frame that you built in 1a.

d. Which two of the three variables (ArrDelay, DepDelay, and Distance) do you think are most correlated? Why?

Question 2

a. Using Google and the help utility in RStudio, install the package called `ggmap`

b. Using Google and the help utility in RStudio, load ggmap into the R environment.

c. Create a map containing all of Europe.

d. Create a map containing the United States (excluding Hawaii and Alaska).

e. Map the points of each business from the business_frame (in the business.Rda from the Yelp Dataset Challenge) on the USA map.

f. Map only the Illinois businesses from business_frame on the USA map.

g. Repeat 2e, but this time make the points for each business be equal to the size of the square root of the number of review counts for that business.

Question 3

a. Use ggmap to plot out the locations of the airports in the United States.

b. Add 5 lines to the USA map. Each line corresponds to one of the 5 most popular departure-to-arrival paths in the USA, as studied in Question 4 on Problem Set 3.

Question 4

a. The england_outcome data contains a lot of cool information about the outcomes of the crimes in the city of London. It shows the outcome of the crime, and the longitude and latitude. Whenever there is longitude and latitude, you should know that you can easily use ggmap to plot. For this question, however, please create a colored bar graph of the counts of the 20 different outcomes of the crimes denoted by the factor “V1”. What is the most common outcome (in non numeric form. i.e. 2 is “court case unable to proceed”).

b. Do the same thing (using ggplot) to similarly plot the crime_data crime types. What is the most common crime?

c. Stack the different types of crimes (like in b), and then put them side by side based on “Month”. Make an observation as time goes on.

d. Do the same for (a) like you did for (b) in (c). Make an observation.

e. As time goes on, what appears to change more, the outcomes of the crimes or the crimes?

Question 5

a. Use ggmap to get a map of London. Show the map.

b. Plot the crimes as points on the map you made in (a). Use zoom = 12.

c. Add color to (b).

d. Repeat (c) but limit to “Violent Crimes” and “Violent and Sexual offenses”.

Question 6

a. Plot a density map of the United States (zoom = 4) of airports.

b. Plot a density map of the United States with a color gradient where low is green and high is red.

c. On top of the map in (b), add points to the map that represent the airports. Size those points based on the “total” factor. The “total” factor is simply the frequency of inbound and outbound flights.

Question 7

Generate the first 20 Lucas numbers and store them in a vector. You can either use recursion or an explicit formula. If you are able to do both, which way is faster? How much faster?

https://en.wikipedia.org/wiki/Lucas_number

Question 8

a. Create a data frame called random_vars where:

the first column contains 10000 Bernoulli random variables, each with p=1/3.

the second column contains 10000 Binomial random variables, each with n=5 and p=1/3.

the third column contains 10000 Geometric random variables, each with expected value 3.

the fourth column contains 10000 Negative Binomial random variables, each of which is a sum of 5 Geometric random variables, and each of those Geometric random variables has expected value 3.

the fifth column contains 10000 Poisson random variables, each with expected value 3.

the sixth column contains 10000 Hypergeometric random variables, each with parameters N=20, M=5, and n=3 (using the notation from STAT/MA 41600).

the seventh column contains 10000 continuous Uniform random variables, each with min 5 and max 10.

the eighth column contains 10000 discrete Uniform random variables, each with min 5 and max 10.

the ninth column contains 10000 Exponential random variables, each with expected value 3.

the tenth column contains 10000 Gamma random variables, each with lambda = 3 and r = 5 (using the notation from STAT/MA 41600).

the eleventh column contains 10000 Beta random variables, each with alpha = 3 and beta = 8 (using the notation from STAT/MA 41600).

the twelveth vector contains 10000 Normal random variables with mean = 3 and variance = 5

b. Find the mean and variance of each column. (Do this efficiently, i.e., do not write 12 separate lines of code.)


== Project 6

Question 1.

a.  In the DataFest 2015 file for the visitors to Edmunds.com, found in:

`/data/public/datafest2015/visitor.csv`

identify the zip codes that had 800 or more visitors.
It is OK to ignore the blank and undefined zip codes in your answer.

b.  There should be 4 such zip codes in part a. For each of these 4 zip codes, identify the city corresponding to that zip code. (Hint: The cities are listed in the field called `dma_name`.)

c.  If we focus only on the cities directly (ignoring the zip codes, and only using the `dma_name` field), what are the 20 most popular cities?

Question 2

If we study the `first_device_model` and `last_device_model` fields, we can discover the first and last devices that the visitors used during their visit to the site.  (Sometimes visitors change platforms while they are visiting, e.g., if they continue a search that they began on an earlier device.)

a.  Categorize the types of `first_device_model` used, according to how many times each device was used.  Sort your resulting list according to the number of each kind of device.

b.  Same question, for `last_device_model` used.

c.  Now consider only the people who switched devices.  Ignoring blank entries, and ignoring "other" entries, what was the most common device switch?

Question 3

Now consider the `shopping.csv` file.

a.  What are the ten most popular makes of cars that people shopped for?

b.  If we consider both the make and the model of the car, what are the ten most popular make-and-model pairs of cars?

c.  What are all the models that Toyota sells?

d.  For this question (only), consider instead the leads.csv file, and identify the top 10 makes of cars for which there is a lead.  Please note that the cars do not have a uniform capitalization, so it is necessary for you to standardize the capitalization before you make your tally.

Question 4

a.  If we classify the click dates in the shopping file, how many shopping entries were made per year?

b.  Now consider only the year and the month but not the day.  How many shopping entries were made per each year-and-month pair?

c.  During which year-and-month pair were the most shopping entries made?

Question 5

a.  Back to the visitor file, if you look at the first_referring_url, what are the top 10 URL's that people used when first getting a reference to the site?  (It is necessary to only use the first part of an address, e.g., to only use "www.google.com" for example, and to trim the rest of the URL off.)

b.  When people are actually making the purchase, which model year do they tend to buy?  Use the `transactions.csv` to rank the years according to how many cars were bought in that model year.

c.  Same question, but this time, limit yourself to the 381 cars bought in Indiana (abbreviation IN)

d.  For which colors were there 800 or more cars sold with that color?

e.  How many car colors have the word "blue" in the title?

Question 6

Consider the file yow.lines, which is distributed with `emacs 21.4`. It can be downloaded from the llc server or you can access it directly from `/proj/www/2015/29000/projects/yow.lines` if you prefer.

a. Consider the number of fields on each line of the file.  What is the maximum number of fields on a line?

b. Print the lines that have at least 15 fields.

c. Do any lines contain the word pizza?  Print all such lines, regardless of how the word pizza is capitalized.

Question 7

Consider the file `/usr/share/dict/words`

a.  How many words contain 2 or more consecutive vowels?

b.  How many words contain the pattern `n't` or the pattern `'ve` ?
(Hint:  Use '\'' for the single quote in your pattern.)

[source,bash]
----
'\''
----

c.  Print all of the words that contain 5 or more consecutive vowels.

d.  Print the following words from the file `/usr/share/dict/words`
The 1st word, the 10001st word, the 20001st word, the 30001st word, etc.  I.e., print every 10000th word, starting with the first word.  There should be 48 words in the resulting list.

Question 8

a.  Print the names of all of the files and directories in the `/etc` directory that were modified in the current month (i.e., in October 2015).

b.  Make a list of all the file names in the three directories:

[source,bash]
----
    /usr/local/bin
    /bin
    /usr/bin 
----

Then sort the list, remove any duplicate file names, and store the results in a file called `myprograms.txt`.


== Project 7

Question 1.

a.  In R, use the `system` function with the parameter `intern=TRUE` to solve question 1a from project 3.  Inside the system function, you can use any method from bash that you like.  The goal is to be able to solve this question relatively quickly, without having to import the complete file `allyears.csv` into R.

b.  In R, use the `pipe` function, wrapped inside the `read.csv` function, to solve question 1a in a different way, without using the `system` function.

c.  Use `system.time` to see which of these two methods is faster.  By the way, both methods should be MUCH faster than importing the entire `allyears.csv` file, as we naively did back in project 3.

Question 2.

See what is the quickest method that you can use to solve question 4c from project 3, using your knowledge of bash and/or awk tools, as well as the system or pipe functions in R.

Question 3.

Solve questions 8a and 8c from project 3 again, using your knowledge of bash and/or awk tools, as well as the system or pipe functions in R.

Question 4.

Use `awk` (and the `system` or `pipe` function in R) to solve question 1 from project 5 again.  How much faster is your solution, using these tools, as compared to the method you used from project 5?

Question 5.

a.  Use `awk` to find the lengths of the lines in the `yow.lines` file, and then use R to make a plot of the distribution of the lengths.

b.  Is it faster to (a) use awk to find the lengths of the lines, and then import these lengths in R (instead of the whole lines themselves), or (b) is it faster use R to import all of the lines and find the lengths within R?

c.  Find the distribution of the words in the `/usr/share/dict/words` file, according to the starting character.  The letters should be treated as case insensitive.

d.  Use R to plot the distribution from part c.  Plot the letters in decreasing order, according to how many words start with those letters.

Question 6.

Working with the DataFest 2015 `visitor.csv` file, use question 1c from project 6 to make a dotchart in R of the twenty cities with the most entries, showing the number of entries per city.  Please put the data in the dotchart into numerical order, according to the number of entries for the city.

Question 7.

For parts a, b, c, use `bash` or `awk` tools.

a.  The file `babynames.txt` has 134 years of data, with all of the baby names from 1880 to 2013.  Extract a list of all of the names (regardless of gender).

b.  Remove the duplicates from the list in part a.

c.  Count the number of (unique) names that remain, according to the length of the name.

d.  Finally, import the resulting distribution of lengths to R, and make a plot of the distribution of the number of names, according to the length of the name.

e.  Redo parts 7a through 7d using only R functions, without resorting to bash or awk.

f.  Which method was faster?  The method that blended bash/awk/R tools, or the method that used only tools from R?

Question 8.

Make a list (in increasing order) of all of the integers from 1 to 1000000 whose prime factors are only 2's and/or 3's.  Hint: It might help to think cleverly and use an inner product, but you can do this in any way that you like.  Time your solution.  What is the fastest way that you can solve the problem?  Compare with your peers to see what kinds of solutions that they found, and how fast the solution worked.  [Hint: there are 142 such numbers, starting with 1, 2, 3, 4, 6, 8, 9, 12, 16, 18, 24, ..., and ending with 995328.]


== Project 8

Question 1.

a.  How many pitchers have ever hit more than 20 home runs in one season?

b.  How many pitchers have ever hit a home run in their lifetime?

Question 2.

a.  Which team has committed the most errors altogether?

b.  Which team has hit the most home runs altogether?

c.  Re-do questions 2a and 2b, limiting our attention to 2010 - present.

Question 3.

a.  Which batter has been hit by the most pitches overall?  How many times?

b.  Which pitcher has hit the most batters overall?  How many batters did he hit?

c.  What are the 10 overall wildest pitchers, according to the number of wild pitches they have thrown altogether?

Question 4.

a.  What is the most number of wins that a team (or teams) ever had in one year?  Which team(s) and year(s)?

b.  During such year(s) and team(s), who was the team's leader in home runs, during that year?

Question 5.

Sum the number of home runs hit by players who studied in each university while they were in college. (E.g., for Purdue, consider how many home runs that all of the Purdue alums have ever hit, altogether.) Which universities are the ten best, in terms of yielding the players with the most home runs altogether?

Question 6.

a.  Which manager's teams have had the most home runs overall?

b.  Which manager's teams have had the most stolen bases overall?

Question 7.

a.  How many players have played at least one game in each of all three outfield positions?

b.  Considering all positions (not just the outfield), which player has played in the greatest number of different positions altogether at least one time?

Question 8.

What percent of batters are lefties?  Switchhitters?

Question 9.

a. Which team has the highest (i.e., worst) average number of errors per year?

b. Which team has the highest (i.e., best) average number of home runs per year?

Question 10.

How many bases have been stolen in each year?  Plot the data.

Question 11.

Make a dotchart of the number of home runs hit by Derek Jeter each year.

Question 12.

Make a plot with years on the x axis, home runs on the y axis, and one data point for each player on the Yankees. Jitter the data so that overlaps among the players can be seen.

Question 13.

Discuss the extent to which a player's number of hits is correlated with his number of home runs.
 
Question 14.

Plot how many players were born in each state.

These kinds of questions can be a lot of fun.  Maybe some of you have some more questions/trivia to suggest?  All suggestions are welcome, and I could even post more questions here, if you have additional suggestions.  It is hard to know how quickly you will move through this material.


== Project 9

Project 9 is about scraping data from the web in XML format,
and then parsing the data using XML tools.

The goal is to scrape the Hot 100 chart from Billboard.
This chart is posted every Saturday.  So the first chart is here:

`http://www.billboard.com/charts/hot-100/1958-08-09`

and the most current chart is here:

`http://www.billboard.com/charts/hot-100/2015-11-14`

So there are 2989 such charts during the history of the Hot 100.

You can do questions 1 and 2 at the same time, if your group decides to split up its efforts.  I.e., questions 1 and 2 can be solved independently from questions 3 and 4.

Question 1.

Make a list in R of all of the Saturdays from `1958-08-09` to `2015-11-14`.
Dr. Ward did this in about 6 lines of code, using commands in R such as:
`rep`, `sprintf`, `sapply`, `unlist`, and `paste`.
You might have other solutions.  Please try to resist the urge to use a for loop.  Notice that functions like sapply can be used instead, e.g., `sapply(1:12, function(x) ...... )` can be used, if you put the thing that you want to do for each month, where the ...... goes.

Question 2.

Scrape the data for all of the charts into files on your computer.

Question 3.

Parse the XML data from one of the Saturdays.  More specifically, parse the title and artist for each of the 100 songs.  Hint:  You might want to try some early years and some later years, to make sure that your code works consistently.  For instance, the title of a song does not appear as an html link.  In contrast, the artist usually appears as an html link, but not always.  (There is more variability in the earlier years of the chart about that.)

Question 4.

Once you have your code working for 3, wrap the code into a function that can be called with just one input, namely, the Saturday you want to parse.  For instance, you might call:  `myfunction( "1958-08-09" )`  to parse the data from the `1958-08-09` file.

Question 5.

Now use the results from question 2 (where you scraped all of the files from the web) along with the results from question 4 (where you wrote a parser), with the goal of parsing all 2989 charts.  Hint:  You might want to just try this for a few of the charts at a time, until you have this working well.




Now answer some interesting questions about the data in the charts, for instance:

Question 6.

a.  What song(s) stayed in the Hot 100 for the most weeks overall?

b.  What song(s) stayed at number 1 in the Hot 100 for the most weeks overall?

c.  What song(s) stayed in the Top 10 for the most weeks overall?

Question 7.

a.  What artist(s) had the most songs in the Hot 100?

b.  What artist(s) had the most number 1 songs in the Hot 100?

c.  What artist(s) spent the most weeks in the Hot 100?

Question 8.

What song(s) have been at number 1 in the Hot 100, with 2 or more covers by different artists?

Question 9.

What artist(s) have had a number 1 song for the longest number of consecutive years?

Question 10.

What artist(s) had the most number 1 singles during a calendar year?  How many singles in the same calendar year was that?

As always, you are welcome to suggest some questions/answers of your own too, if you find some interesting trends.



