= TDM 30100: Project 11 -- 2023

**Motivation:** In general, scraping data from websites has always been a popular topic in The Data Mine. In addition, it was one of the requested topics. We will continue use "books.toscrape.com" to practice scraping skills

**Context:** This is a second project focusing on web scraping combined with BeautifulSoup library

**Scope:** Python, web scraping, selenium, BeautifulSoup

.Learning Objectives
****
- Use selenium and xpath expressions to efficiently scrape targeted data.
- Use beautifulSoup to scrape data from web pages
****

Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about projects submissions xref:submissions.adoc[here].


At previous project, you have understood how to get the 'Music' category link in the webpage of "books.toscrape.com", and use `selenium` to scrape books' information. The follow is the sample codes for the solution for the question 1

[source,python]
----
import time
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
 
firefox_options = Options()
firefox_options.add_argument("--window-size=810,1080")
# Headless mode means no GUI
firefox_options.add_argument("--headless")
firefox_options.add_argument("--disable-extensions")
firefox_options.add_argument("--no-sandbox")
firefox_options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Firefox(options=firefox_options)

driver.get("https://books.toscrape.com")
e_t = driver.find_element("xpath",'//article[@class="product_pod"]/h3/a')
e_p = driver.find_element("xpath",'//p[@class="price_color"]')
fst_b_t = e_t.text
fst_b_p =e_p.text

# find book titled " how music works"
book_link = driver.find_element(By.LINK_TEXT, "How Music Works")
book_link.click()
time.sleep(5)

#scrape and print book information : product description, upc and availability
product_desc=driver.find_element(By.CSS_SELECTOR,'meta[name="description"]').get_attribute('content')
product_desc
table = driver.find_element(By.XPATH, "//table[@class='table table-striped']")
upc= table.find_element(By.XPATH, ".//th[text()='UPC']/following-sibling::td[1]")
upc_value = upc.text
upc_value

availability = table.find_element(By.XPATH, ".//th[text()='Availability']/following-sibling::td[1]")
availability_value = availability.text
availability_value
driver.quit()
----
[NOTE]
In this project we will include BeautifulSoup in our webscraping journey. BeautifulSoup is a python library, you can use it to extract data from HTML, XML files. You may find more BeautifulSoup information here  https://www.crummy.com/software/BeautifulSoup/bs4/doc/ [BeautifulSoup]
 
== Questions

=== Question 1 (2 pts)
.. Now Please create a function called "get_category" to extract all categories' names, it does not need argument, and returns a list of categories

[TIP]
====
* Use BeautifulSoup for this question
[source,python]
from bs4 import BeautifulSoup
====
[TIP]
====
* You can parse the page with BeautifulSoup
[source,python]
bs = BeautifulSoup(driver.page_source,'html.parser')
====
[TIP]
====
* Look at the page source, you may use following statement to locate the sidebar, extract text from category
[source,python]
categories = [c.text.strip() for c in bs.select('.nav-list li a')]
====

=== Question 2 ( 2 pts)

.. Please create another function called "get_all_books" to get all books from a given category， use "Music_14" for this question, argument will be a category name, returns will be a list of books objects with book titles, book price and book availability

[TIP]
====
* Using find_all to find all the "<article>" tags
articles=bs.find_all("article",class_="product_pod")
====
[TIP]
===
* You may create an object to hold book information, like:
[source,python]
book = {
    "title":title,
    "price":price,
    "availability":availability
}
===
[TIP]
====
* You may use a loop to go through the books, like
[source,python] 
for article in articles:
    title = article.h3.a.attrs['title']
    price = article.find('p',class_='price_color').text
    availability = article.find('p',class_='instock availability').text
# create a book object with the extract information
    ....
====
[TIP]
====
* You may need a list to hold all book objects, add all books to it, like
[source,python]
all_books=[]
...
all_books.append(book)
====
[NOTE]
====
* You may use other ways to solve the question, like use function "map" etc. It is not request to use those kinds of advanced ways for this project
====

=== Question 3 (2 pts)

Let's look for books in category "fantasy_19". Instead of just a single page like for music books, they have more than 1 page.
.. Please update the function "get_all_books" from question 2 to be able to get all books even if there have more than 1 page

 
=== Question 4 (2 pts)

.. Please visualize the books' price of music category, have price ranges as below 20, above 30
[TIP]
====
* You may need to change the price to float, like
[source, python]
prices = [float(book['price'].replace('£','')) for book in books]

* books is the book list from the function "get_all_books", like 
[source, python]
books = get_all_books("Music_14")
====
[TIP]
====
* You may use sum to group prices, like
[source,python]
price_less_20 = sum(1 for price in prices if price<20)
price_20_30 = sum(1 for price in prices if 30<=price<50)
====
[TIP]
===
* You may use a bar chart, like
price_counts = [price_less_20, price_20_30,price_above_30]
labels = ["1","2","3"]
plt.bar(labels,price_counts,color=['purple','orange','green'])
# More plt settings and display statements
===
 

Project 11 Assignment Checklist
====
* Jupyter Lab notebook with your code, comments and output for the assignment
    ** `firstname-lastname-project11.ipynb` 
* Submit files through Gradescope
====

[WARNING]
====
_Please_ make sure to double check that your submission is complete, and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted, was what you _actually_ submitted.

In addition, please review our xref:projects:current-projects:submissions.adoc[submission guidelines] before submitting your project.
====