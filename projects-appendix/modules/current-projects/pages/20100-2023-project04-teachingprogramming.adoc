= TDM 20100: Project 4 -- 2023

**Motivation:** Becoming comfortable chaining commands and getting used to navigating files in a terminal is important for every data scientist to do. By learning the basics of a few useful tools, you will have the ability to quickly understand and manipulate files in a way which is just not possible using tools like Microsoft Office, Google Sheets, etc. While it is always fair to whip together a script using your favorite language, you may find that these UNIX tools are a better fit for your needs.

**Context:** We've been using UNIX tools in a terminal to solve a variety of problems. In this project we will continue to solve problems by combining a variety of tools using a form of redirection called 'piping'. 

**Scope:** grep, regular expression basics, UNIX utilities, redirection, piping

.Learning Objectives
****
- Use `cut` to section off and slice up data from the command line.
- Use `|` piping to string UNIX commands together.
- Use `sort` and it's options to sort data in different ways.
- Use `head` to isolate n lines of output.
- Use `wc` to summarize the number of lines in a file or in output.
- Use `uniq` to filter out non-unique lines.
- Use `grep` to search files effectively.
****

Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about projects submissions xref:submissions.adoc[here].

== Dataset(s)

The following questions will use the following dataset(s):

- `/anvil/projects/tdm/data/stackoverflow/unprocessed/*`
- `/anvil/projects/tdm/data/stackoverflow/processed/*`
- `/anvil/projects/tdm/data/iowa_liquor_sales/iowa_liquor_sales_cleaner.txt`

== Questions

[WARNING]
====
For this project, please submit a `.sh` text file with all of you `bash` code written inside of it. This should be submitted _in addition to_ your notebook (the `.ipynb` file). Failing to submit the accompanying `.sh` file may result and points being removed from your final submission. Thanks!
====


=== Question 1 (1 pt)

[NOTE]
====
The following statement will check how many columns in the first line of a csv file  
[source,bash]
head -n 1 /anvil/projects/tdm/data/stackoverflow/unprocessed/2011.csv | tr ',' '\n' | wc -l

* In a csv file, there are numOfComma+1 columns where numOfComma is the number of commas (in theory). 
* `head -n 1` takes the first line of the file 
* `tr ',' '\n'` replaces all commas with the newline character `\n` 
* `wc -l ` to count the resulting number of lines.  
====
[upperalpha]

.. Please use commands `head`, 'tr` and `wc` to find out how many times does the character 'A' appear in the first 10 lines of the file /anvil/projects/tdm/data/stackoverflow/unprocessed/2011.csv? 


=== Question 2 (1 pt)
[NOTE]
====
As you can see, csv files are not always so straightforward to parse. For this particular set of questions, we want to focus on using some other UNIX tools that are more useful on semi-clean datasets, e.g. `awk` 

The following statement outputs column number of the first line of the csv file  
[source, bash]
head -n 1 /anvil/projects/tdm/data/stackoverflow/processed/2011.csv | awk -F";" '{print NF}'  

* `awk`can be used for simple data manipulation tasks that involve pattern matching, field extraction, arithmetic, and string operations 

    ** -F";": Set the field separator to ;.
    ** {print NF}: Print the number of fields in each line.

====
[upperalpha]

.. Use 'awk' to find out how many columns in the fifth row  of the file `/anvil/projects/tdm/data/iowa_liquor_sales/iowa_liquor_sales_cleaner.txt`

=== Question 3 (2 pts)

[NOTE]
====
With appropriate commands, the following statement use finds the 5 largest orders by number from the 'Bottles Sold' column of the file 
[source, bash]
cat /anvil/projects/tdm/data/iowa_liquor_sales/iowa_liquor_sales_cleaner.txt | cut -d ';' -f21 | sort -nr | head -n 5

* `cat` is used to display the entire content of the file
* `cut` is an UNIX command used to remove or "cut out" certain sections of each line from a file or the output of a command.
** -d ';' specifies that the delimiter (or separator) between fields is the semicolon (;).
** -f21 tells cut to only retrieve the 21st field/column ('Bottles Sold' column) based on the semicolon delimiter. So, after this command, you'll get only the 'Bottles Sold' values from the 21st column of the file iowa_liquor_sales_cleaner.txt.
* `sort` arranges lines of text alphabetically or numerically.
** -n means "numeric sort", so the values are treated as numbers and not as strings.
** -r means "reverse", so the output will be in descending order
* `head` is used to display only first 5 lines 

====
[upperalpha]
.. Use `cat`, `cut`, `sort`, and `head` to find out which 5 items have the highest 'state bottle retail' price from the file /anvil/projects/tdm/data/iowa_liquor_sales/iowa_liquor_sales_cleaner.txt

[TIP]
====
* column 16 is for 'item description' and column 20 is for 'state bottle retail' price
====

=== Question 4 (2 pts)

[NOTE]
====
Here is another example. We can `pipeline` `cat`, `cut`,`sort` and `uniq` to display how many times each unique bottle volume appears in the file
[source,bash]
cat iowa_liquor_sales_cleaner.txt |cut -d ';' -f18 | sort -n |uniq -c

* column 18 (-f18) is for 'Bottle Volume (ml)'
* `uniq` removes consecutive duplicate lines from its input and with the `-c` option, prefixes with number of occurrences
====
[upperalpha]

.. Please find out how many times does each unique vendor appear in the file, exclude the vendor with empty value

[TIP]
====
* column 14 is for vendor in the file
* use `grep` to filter out vendor with empty value
====


=== Question 5 (2 pts)

[NOTE]
====
https://en.wikipedia.org/wiki/Benford%27s_law[Benford's law] states that the leading digit in real-life sets of numerical data, the leading digit is likely to follow a distinct distribution (see the plot in the https://en.wikipedia.org/wiki/Benford%27s_law[provided link]). 

By this logic, let's use `bash` tools to get a good idea of the count or percentage of the sales (in dollars) by starting digit in the file iowa_liquor_sales_cleaner.txt, analyze if the dollar amount in the orders roughly match this law. 
[source,bash]
cat iowa_liquor_sales_cleaner.txt | cut -d';' -f22 | cut -c 1 | grep '^[1-9]$' | sort | uniq -c

* 'Sale (Dollars)' is the 22nd column in the file
* cut -c 1  extracts the first character of each line of the input
* grep '^[0-9]$' filters only lines that are numeric from 1-9
* uniq -c removes consecutive duplicate lines from its input and with the `-c` option, prefixes with number of occurrences

Let's analyze the output result:

Output of the statement  
9200529 1
4506653 2
2670272 3
1891364 4
1365944 5
2533875 6
1857115 7
1273206 8
1743259 9

Hence, the observed percentages:

1: ≈ 38.18%
2: ≈ 18.70%
3: ≈ 11.08%
4: ≈ 7.85%
5: ≈5.67%
6: ≈10.51%
7: ≈7.70%
8: ≈5.28%
9: ≈7.23%

Benford's Law expected percentages for the leading digits are approximately:

1: 30.1%
2: 17.6%
3: 12.5%
4: 9.7%
5: 7.9%
6: 6.7%
7: 5.8%
8: 5.1%
9: 4.6%

Comparing the observed percentages to Benford's Law:

The digit 1 appears notably more frequently than expected.
The digit 6 shows a significant deviation, appearing much more frequently than anticipated.
Digits 2, 7, 8, and 9 are also above their expected percentages but to a lesser extent than 1 and 6.
Digits 3, 4, and 5 appear less frequently than expected based on Benford's Law.
====
[upperalpha]
.. Please do the same analysis on 'State Bottle Retail' price, column 20 in the file
* Instead of using `cut -c 1` and `grep '^[0-9]$'`, use only one grep command to filter out the first digit of the input
* Analyze the results and write 1-2 sentences explaining what you think.
 
 
Project 03 Assignment Checklist
====
* Jupyter Notebook for the assignment
    ** `firstname-lastname-project04.ipynb`.
* A `.sh` text file with all of you `bash` code written inside of it
     ** codes used to solve quesiton 1 to 5
* Submit files through gradescope
====
[WARNING]
====
_Please_ make sure to double check that your submission is complete, and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted, was what you _actually_ submitted.
                                                                                                                             
In addition, please review our xref:submissions.adoc[submission guidelines] before submitting your project.
====