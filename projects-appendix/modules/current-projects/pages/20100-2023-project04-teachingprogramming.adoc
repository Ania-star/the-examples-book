= TDM 20100: Project 4 -- 2023

**Motivation:** Becoming comfortable chaining commands and getting used to navigating files in a terminal is important for every data scientist to do. By learning the basics of a few useful tools, you will have the ability to quickly understand and manipulate files in a way which is just not possible using tools like Microsoft Office, Google Sheets, etc. While it is always fair to whip together a script using your favorite language, you may find that these UNIX tools are a better fit for your needs.

**Context:** We've been using UNIX tools in a terminal to solve a variety of problems. In this project we will continue to solve problems by combining a variety of tools using a form of redirection called 'piping'. 

**Scope:** grep, regular expression basics, UNIX utilities, redirection, piping

.Learning Objectives
****
- Use `cut` to section off and slice up data from the command line.
- Use `|` piping to string UNIX commands together.
- Use `sort` and it's options to sort data in different ways.
- Use `head` to isolate n lines of output.
- Use `wc` to summarize the number of lines in a file or in output.
- Use `uniq` to filter out non-unique lines.
- Use `grep` to search files effectively.
****

Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about projects submissions xref:submissions.adoc[here].

== Dataset(s)

The following questions will use the following dataset(s):

- `/anvil/projects/tdm/data/stackoverflow/unprocessed/*`
- `/anvil/projects/tdm/data/stackoverflow/processed/*`
- `/anvil/projects/tdm/data/iowa_liquor_sales/iowa_liquor_sales_cleaner.txt`

== Questions

[WARNING]
====
For this project, please submit a `.sh` text file with all of you `bash` code written inside of it. This should be submitted _in addition to_ your notebook (the `.ipynb` file). Failing to submit the accompanying `.sh` file may result and points being removed from your final submission. Thanks!
====


=== Question 1 (1 pt)

[NOTE]
====
Let's look at the following statement
[source,bash]
head -n 1 /anvil/projects/tdm/data/stackoverflow/unprocessed/2011.csv | tr ',' '\n' | wc -l

The output of this statement will be how many columns in the first line of the csv file "2011.csv"

In a csv file, there are numOfComma+1 columns where numOfComma is the number of commas (in theory). The statement takes the first line of `unprocessed/2011.csv` with `head` command, replaces all commas with the newline character `\n` with command `tr`, then use command `wc` to count the resulting number of lines.  
====
[upperalpha]

.. Let's look at the same file of 2011.csv, Please use commands `head`, 'tr` and `wc` to find out how many times does the character 'A' appear in the first 10 lines of the file? 


=== Question 2 (1 pt)
[NOTE]
====
As you can see, csv files are not always so straightforward to parse. For this particular set of questions, we want to focus on using some other UNIX tools that are more useful on semi-clean datasets, e.g., awk 

Let's look at the first few lines of the data in 'processed/2011.csv'
[source, bash]
head -n 1 /anvil/projects/tdm/data/stackoverflow/processed/2011.csv | awk -F";" '{print NF}'  

The output of this statement will be the column number of the first line of the file 2011.csv

The statement use `awk` to

    -F";": Set the field separator to ;.
    {print NF}: Print the number of fields in each line.

====
[upperalpha]

.. Use UNIX tool 'awk' to find out how many columns in the fifth row of the file - 'iowa_liquor_sales_cleaner.txt'

=== Question 3 (2 pts)

[NOTE]
====
Let's continuing to look at the file '/anvil/projects/tdm/data/iowa_liquor_sales/iowa_liquor_sales_cleaner.txt'. We can use the following statement to find the 5 largest orders by number from the 'Bottles Sold' column of the file
[source, bash]
cat iowa_liquor_sales_cleaner.txt | cut -d ';' -f21 | sort -nr | head -n 5

* `cat` is used to display the entire content of the file
* `cut` is an UNIX command used to remove or "cut out" certain sections of each line from a file or the output of a command.
** -d ';' specifies that the delimiter (or separator) between fields is the semicolon (;).
** -f21 tells cut to only retrieve the 21st field/column ('Bottles Sold' column) based on the semicolon delimiter. So, after this command, you'll get only the 'Bottles Sold' values from the 21st column of the file iowa_liquor_sales_cleaner.txt.
* `sort` arranges lines of text alphabetically or numerically.
** -n means "numeric sort", so the values are treated as numbers and not as strings.
** -r means "reverse", so the output will be in descending order
* `head` is used to display only first 5 lines 

so the output of the statement will be the 5 largest orders by number
====
[upperalpha]
.. From the file iowa_liquor_sales_cleaner.txt, which 5 items have the highest 'state bottle retail' price? 

[TIP]
====
* `cat`, `cut`, `sort`, and `head` will be useful.
* column 16 is for 'item description' and column 20 is for 'state bottle retail' price

====

=== Question 4 (2 pts)

[NOTE]
====
Let's continue looking at the file iowa_liquor_sales_cleaner.txt. We can `pipeline` `cat`, `cut`,`sort` and `uniq` to display how many times each unique bottle volume appears in the file
[source,bash]
cat iowa_liquor_sales_cleaner.txt |cut -d ';' -f18 | sort -n |uniq -c

* column 18 (-f18) is for 'Bottle Volume (ml)'
* `uniq` removes consecutive duplicate lines from its input and with the `-c` option, prefixes with number of occurrences
====
[upperalpha]

.. Please find out How many times does each unique vendor appear in the file, exclue the vendor with emplty value?

[TIP]
====
* column 14 is for vendor in the file
* use `grep` to filter out empty lines
====


=== Question 5 (2 pts)

[NOTE]
====
https://en.wikipedia.org/wiki/Benford%27s_law[Benford's law] states that the leading digit in real-life sets of numerical data, the leading digit is likely to follow a distinct distribution (see the plot in the https://en.wikipedia.org/wiki/Benford%27s_law[provided link]). 

By this logic, let's use `bash` tools to get a good idea of the count or percentage of the sales (in dollars) by starting digit in the file iowa_liquor_sales_cleaner.txt, to see if the dollar amount in the orders roughly match this law. 'Sale (Dollars)' is the 22nd column. 
[source,bash]
cat iowa_liquor_sales_cleaner.txt | cut -d';' -f22 | cut -c 1 | grep '^[1-9]$' | sort | uniq -c

* cut -c 1  extracts the first character of each line of the input
* grep '^[0-9]$' filters only lines that are numeric from 1-9
* uniq -c removes consecutive duplicate lines from its input and with the `-c` option, prefixes with number of occurrences

Let's check on the result:

Output of the statement for the file comes out as
9200529 1
4506653 2
2670272 3
1891364 4
1365944 5
2533875 6
1857115 7
1273206 8
1743259 9

So, the observed percentages:

1: ≈ 38.18%
2: ≈ 18.70%
3: ≈ 11.08%
4: ≈ 7.85%
5: ≈5.67%
6: ≈10.51%
7: ≈7.70%
8: ≈5.28%
9: ≈7.23%

Benford's Law expected percentages for the leading digits are approximately:

1: 30.1%
2: 17.6%
3: 12.5%
4: 9.7%
5: 7.9%
6: 6.7%
7: 5.8%
8: 5.1%
9: 4.6%

Comparing the observed percentages to Benford's Law:

The digit 1 appears notably more frequently than expected.
The digit 6 shows a significant deviation, appearing much more frequently than anticipated.
Digits 2, 7, 8, and 9 are also above their expected percentages but to a lesser extent than 1 and 6.
Digits 3, 4, and 5 appear less frequently than expected based on Benford's Law.
====
[upperalpha]
.. Please do the same checking on State Bottle Retail, column 20 in the file
* Instead of using `cut -c 1` and `grep '^[0-9]$'`, use only one grep command to filter out the first digit of the input
* Analysis the results and write 1-2 sentences explaining what you think.
 
 
Project 03 Assignment Checklist
====
* Jupyter Notebook for the assignment
    ** `firstname-lastname-project04.ipynb`.
* A `.sh` text file with all of you `bash` code written inside of it
     ** codes used to solve quesiton 1 to 5
* Submit files through gradescope
====
[WARNING]
====
_Please_ make sure to double check that your submission is complete, and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted, was what you _actually_ submitted.
                                                                                                                             
In addition, please review our xref:submissions.adoc[submission guidelines] before submitting your project.
====