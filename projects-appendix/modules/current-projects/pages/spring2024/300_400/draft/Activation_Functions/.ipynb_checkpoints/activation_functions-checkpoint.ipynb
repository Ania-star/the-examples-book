{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d593891-3c0e-4506-92e2-31e089454c3c",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d9384-0435-4690-80be-7bc2c2c3cdca",
   "metadata": {},
   "source": [
    "## Our Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d60941-1b64-4786-b920-b1634e0ca0c6",
   "metadata": {},
   "source": [
    "* https://keras.io/api/layers/activations/\n",
    "* https://www.statlearning.com/ (Chapter 10)\n",
    "* https://en.wikipedia.org/wiki/Activation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f49a96-8595-4898-bd8e-ad631b224e82",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5cdc5-c8a5-4875-acf5-20de2897d0a7",
   "metadata": {},
   "source": [
    "Recall the equation given for a neural network layer (see Basics of Neural Networks for a reminder of its interpretation): \n",
    "\n",
    "$f(X) =\\beta_0+\\sum_{k=1}^{K}\\beta_kh_k(X)$\n",
    "<br>$ f(X)=\\beta_0+\\sum_{k=1}^{K}\\beta_kg(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_j)$\n",
    "\n",
    "Recall that the activation functions are the portions\n",
    "\n",
    "$A_k = h_k(X) = g(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_{j})=g(z)$\n",
    "\n",
    "The commonly used shorthand is $g(z)$.\n",
    "\n",
    "Activation functions are critical to neural networks. They serve 2 primary purposes:\n",
    "\n",
    "1. They create nonlinearity (where otherwise you might have a simple linear relationship)\n",
    "2. They ensure the model can capture complex nonlinearities and interactions between the variables\n",
    "\n",
    "In short, activation functions are what ensure neural networks produce nonlinear output. There are other properties that activation functions can bring, but they differ depending on which activation function gets used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cf3d2-cd06-458e-a05e-9cb8832b8f7d",
   "metadata": {},
   "source": [
    "## Activation Functions provided by `keras` (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8da9b5-fc08-4d31-aa3b-7635ebcaa9a6",
   "metadata": {},
   "source": [
    "There are a few major \"families\" of activation functions that have the desirable properties for activation functions, including:\n",
    "\n",
    "* *-LU: Linear Unit (such as ReLU)\n",
    "* Sigmoid\n",
    "* Hyperbolic (tanh)\n",
    "* Softmax\n",
    "\n",
    "There are certainly others as activation functions are still heavily under research, however these are the most common varieties you will see in use today.\n",
    "\n",
    "`keras` carries all of these, with multiple different varieties. Below we list some of the most commonly used activation functions and their use case:\n",
    "\n",
    "* ReLU (Rectified Linear Unit): Most commonly used. Good general purpose activation function. Can be computed and stored more efficiently than sigmoid (see https://dl.acm.org/doi/pdf/10.1145/3065386, under section 4.1).\n",
    "* Sigmoid: Used to be the most commonly used, before ReLU. Is also used in logistic regression to convert a linear function into probabilities between 0 and 1.\n",
    "* Softmax: Also used for (multinomial) logistic regression. Used when there is categorical output and when you want a probability of the chance of each given category. A classic example is using a neural network to classify images of animals: softmax could be used to create an output of probabilities that sum to 1, where a probability is outputted for the chance that the given test image is a specific animal (label).\n",
    "\n",
    "In general, it's not a bad idea to stick to the Linear Unit family (ReLU, GELU, etc) of activation functions (ReLU in particular) unless you are doing categorical output, in which case a softmax family function (softmax, softplus, etc) might come in handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddf2ac-be18-47cd-81f1-ee18e3318c51",
   "metadata": {},
   "source": [
    "## Implementing Activation Functions in `keras` (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d5bc7-eefe-48bb-96e8-5da756797f89",
   "metadata": {},
   "source": [
    "Adding activation functions in `keras` is a breeze. Below is a simple implementation of a single layer neural network **that will have ReLU in the hidden layer and softmax for the output layer**. You will notice that the final output layer has 10 neurons; this means that our softmax activation function will have 10 total categories to make probabilities for (which all sum to 1).\n",
    "\n",
    "**For the cell below, type in the `keras` syntax for both of the activation functions**. Verify it's correct with https://keras.io/api/layers/activations/ if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed50764a-7bf2-42fa-881e-37de35f54ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 15:28:52.700833: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 15:28:53.086533: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 15:28:53.086569: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 15:28:53.089238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 15:28:53.295797: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 15:28:53.298007: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 15:28:54.963605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a6fd54-3dff-4c6a-8ec8-2861ec3fcf3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3510875304.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    hidden_layer_1_activation = ??\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_1_activation = ??\n",
    "output_layer_activation = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e82c9e-b4c8-421d-91f5-0ace8f27eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the strings on these for points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130bc0b1-108a-4889-a298-a760d415e79c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_layer_activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m41\u001b[39m, activation\u001b[38;5;241m=\u001b[39mhidden_layer_1_activation)) \u001b[38;5;66;03m#hidden layer 1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m20\u001b[39m, activation\u001b[38;5;241m=\u001b[39mhidden_layer_2_activation)) \u001b[38;5;66;03m#hidden layer 2\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[43moutput_layer_activation\u001b[49m)) \u001b[38;5;66;03m#output layer\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_layer_activation' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(82,))) #input layer\n",
    "model.add(Dense(41, activation=hidden_layer_1_activation)) #hidden layer 1\n",
    "model.add(Dense(10, activation=output_layer_activation)) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f2045-62bf-41a2-bec2-1633cf145864",
   "metadata": {},
   "source": [
    "## ReLU: Deep Dive (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d3c13-168f-4121-801f-4d644224b681",
   "metadata": {},
   "source": [
    "The ReLU activation function is the most commonly used because of its efficient computational properties (relative to the previous standard, sigmoid). \n",
    "\n",
    "The piecewise function is defined as\n",
    "\n",
    "$ g(z) = \\left\\{\\begin{array}{ll}0 & \\text{if} \\ z<0 \\\\z & \\text{otherwise} \\\\\\end{array} \\right. $\n",
    "\n",
    "It's a rather simple function that works incredibly well. One thing to point out is that it thresholds at 0; its important to recall that there is still the $w_{k0}$ which will shift the whole equation. Recall the single layer neural network equation for the hidden layer:\n",
    "\n",
    "$ f(X)=\\beta_0+\\sum_{k=1}^{K}\\beta_kg(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_j)$\n",
    "\n",
    "We pointed out in the introduction that \n",
    "\n",
    "$g(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_{j})=g(z)$\n",
    "\n",
    "So $z$ here is the result of the equation $w_{k0}+\\sum_{j=1}^{p}w_{kj}X_{j}$. Recall that $w_k$ is the weight at the layer $k$, and $p$ is the total number of predictors/inputs. $w_{k0}$ is the bias intercept at the layer; and the summation we see is the summation of all the weights \n",
    "\n",
    "There are many other varieties of the Linear Unit family of functions, such as GELU (\"Gaussian Error Linear Units\") which is used for BERT, ChatGPT and the like (source: https://arxiv.org/pdf/1606.08415.pdf). ReLU is the \"tried and true\" gold standard and the other Linear Unit functions often attempt to improve ReLU.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Implement ReLU with options\n",
    "\n",
    "https://keras.io/api/layers/activations/#relu-function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39917ce7-f22f-4216-8c49-d240e7545c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbbf90-7730-44c9-8f71-97101e70d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_model = Sequential()\n",
    "relu_model.add(Input(shape=(129,))) #input layer\n",
    "relu_model.add(Dense(13, activation=hidden_layer_1_activation)) #hidden layer 1\n",
    "relu_model.add(Dense(1, activation='sigmoid')) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687d74c-be61-4a67-8450-4fbd2bdad072",
   "metadata": {},
   "source": [
    "## Softmax: Deep Dive (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d409d-0f7d-4742-a4f4-840455b29230",
   "metadata": {},
   "source": [
    "Explore the ins and outs of softmax, when it gets used, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a629fc5-37ec-4cdb-aedb-4494e8922174",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8190aedc-9d3e-4058-8763-8d542cc01cbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f72d5082-5df7-4c93-a0f5-89c327b6afd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "184a11a7-74e9-4d89-b8a7-5ba0d97643a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a856f531-115a-41d9-a6e0-c57be6cc43ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a6cfd32-276f-438a-89c4-ef7a6c5c0cb2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary_python_env",
   "language": "python",
   "name": "primary_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
