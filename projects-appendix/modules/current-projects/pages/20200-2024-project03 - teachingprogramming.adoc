= TDM 20200: Project 3 -- 2024

**Motivation:** Web scraping takes practice, and it is important to work through a variety of common tasks in order to know how to handle those tasks when you next run into them. In this project, we will use a variety of scraping tools in order to scrape data from https://zillow.com.

**Context:** In the previous project, we got our first taste at actually scraping data from a website,   In this project, we will introduce some tasks that will require you to use a tool that let's you interact with a browser, selenium. 

**Scope:** python, requests web scraping, selenium

.Learning Objectives
****
- Use the requests package to scrape a web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping. 
****

Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about projects submissions xref:submissions.adoc[here].

== Questions

=== Question 1

.. use selenium to click the button to to to course page for 101-102
.. gut list 

=== Question 2


Okay, let's go forward with the assumption that we will always see the "For sale", "For rent", and "Skip this question" page. We need our code to handle this situation and click the "Skip this question" button so we can get our search results!

Write Python code that uses `selenium` to find the "Skip this question" button and click it. Confirm your code works by printing the current URL of the page _after_ the button has been clicked. Is the URL what you expected? 

[TIP]
====
Don't forget, it may be best to put a `time.sleep(5)` after the `click()` method call -- _before_ printing the current URL.
====

Uh oh! If you did this correctly, it is likely that the URL is not quite right -- something like: `https://www.zillow.com/homes/_rb/`, or maybe a captcha page. By default, websites employ a variety of anti-scraping techniques. On the bright side, we _did_ notice (when doing this search manually) that the URL _should_ look like: `https://www.zillow.com/homes/32607_rb/` -- we can just insert our zip code directly in the URL and that should work without any fuss, _plus_ we save some page loads and clicks. Great! Alternatively, you could also narrow down the search to homes "For Sale" by using `https://www.zillow.com/homes/for_sale/32607_rb/`.

[NOTE]
====
If you are paying close attention -- you will find that this is an inconsistency between using a browser manually and using `selenium`. `selenium` isn't saving the same data (cookies and local storage) as your browser is, and therefore doesn't "remember" the zip code you are search for after that intermediate "For sale", "For rent", and "Skip this question" step. Luckily, modifying the URL works better anyways.
====

Test out (using `selenium`) that simply inserting the zip code in the URL works as intended. Finding the `title` element and printing the contents should verify quickly that it works as intended. Bake this functionality into a function called `print_title` that takes a search term, `search_term`, and returns the contents of the `title` element.

[source,python]
----
element = driver.find_element("xpath", "//title")
print(element.get_attribute("outerHTML"))
----

[source,python]
----
# make sure this works
print_title("32607")
----

.Items to submit
====
- Code used to solve this problem.
- Output from running the code.
====

=== Question 3

 

Okay great! Take your time to open a browser to `https://www.zillow.com/homes/for_sale/32607_rb/` and use the Inspector to figure out how the web page is structured. For now, let's not worry about any of the filters. The main useful content is within the cards shown on the page. Price, number of beds, number of baths, square feet, address, etc., is all listed within each of the cards. 

What non `li` element contains the cards in their entirety? Use `selenium` and XPath expressions to extract those elements from the web page. Print the value of the `id` attributes for all of the cards. How many cards was there? (this _could_ vary depending on when the data was scraped -- that is ok).

[TIP]
====
You can use the `id` attribute in combination with the `starts-with` XPath function to find these elements, because each `id` starts with the same 4-5 letter prefix.

Some examples of how to use `starts-with`:

----
//div[starts-with(@id, 'card_')] # all divs with an id attribute that starts with 'card_'
//div[starts-with(text(), 'okay_')] # all divs with content that starts with 'okay_'
----
====

.Items to submit
====
- Code used to solve this problem.
- Output from running the code.
====

=== Question 4



How many cards were there? For me, there were just about 8. Let's verify things. Open a browser to `https://www.zillow.com/homes/for_sale/32607_rb/` and scroll down to the bottom of the page. How many cards are there?

For me, there were _significantly_ more than 8. There were 40. Something is going on here. What is going on is lazy loading. What this means is the web page is only loading the first 8 cards, and then loading the rest of the cards as you scroll down the page. This is a common technique to reduce the initial load time of a web page. This is also the perfect scenario for us to really utilize the power of `selenium`. If we just use a package like `requests`, we are unable to scroll down the page and load the rest of the cards. 

Check out the function below called `load_all_cards` that accepts the `driver` as an argument, and scrolls down the page until all of the cards have been loaded. Examine the function and explain (in a markdown cell) what it is doing. In addition, use the function in combination with your code from the previous question to print the `id` attribute for all of the cards. How many cards were there this time?

 

[WARNING]
====
_Please_ make sure to double check that your submission is complete, and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted, was what you _actually_ submitted.

In addition, please review our xref:projects:current-projects:submissions.adoc[submission guidelines] before submitting your project.
====